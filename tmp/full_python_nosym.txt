04/19/2022 11:24:11 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name python --data_dir ../../data/ --model_dir ../../tmp --model_name full_python_nosym --train_src train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_tgt dev/javadoc.original --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 400 --max_tgt_len 30 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 32 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 0 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True --split_decoder False ]
04/19/2022 11:24:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
04/19/2022 11:24:11 AM: [ Load and process data files ]
04/19/2022 11:24:13 AM: [ Num train examples = 55538 ]
04/19/2022 11:24:13 AM: [ Dataset weights = {1: 1.0} ]
04/19/2022 11:24:14 AM: [ Num dev examples = 18505 ]
04/19/2022 11:24:14 AM: [ ---------------------------------------------------------------------------------------------------- ]
04/19/2022 11:24:14 AM: [ Training model from scratch... ]
04/19/2022 11:24:14 AM: [ ---------------------------------------------------------------------------------------------------- ]
04/19/2022 11:24:14 AM: [ Build word dictionary ]
04/19/2022 11:24:15 AM: [ Num words in source = 50000 and target = 30000 ]
04/19/2022 11:24:16 AM: [ Trainable #parameters [encoder-decoder] 44.2M [total] 86M ]
04/19/2022 11:24:16 AM: [ Breakdown of the trainable paramters
+------------------------------------------------------------------------------+--------------+----------+
| Layer Name                                                                   | Output Shape |  Param # |
+------------------------------------------------------------------------------+--------------+----------+
| embedder.src_word_embeddings.make_embedding.emb_luts.0.weight                | [50000, 512] | 25600000 |
| embedder.tgt_word_embeddings.make_embedding.emb_luts.0.weight                | [30000, 512] | 15360000 |
| embedder.tgt_pos_embeddings.weight                                           |    [32, 512] |    16384 |
| encoder.transformer.layer.0.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.0.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.0.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.0.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.0.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.0.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.0.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.0.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.0.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.0.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.0.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.1.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.1.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.1.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.1.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.1.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.1.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.1.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.1.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.1.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.1.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.1.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.2.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.2.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.2.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.2.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.2.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.2.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.2.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.2.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.2.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.2.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.2.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.3.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.3.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.3.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.3.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.3.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.3.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.3.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.3.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.3.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.3.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.3.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.4.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.4.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.4.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.4.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.4.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.4.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.4.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.4.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.4.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.4.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.4.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.5.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.5.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.5.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.5.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.5.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.5.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.5.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.5.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.5.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.5.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.5.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.0.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.0.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.0.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.0.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.0.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.0.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.0.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.1.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.1.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.1.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.1.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.1.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.1.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.1.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.2.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.2.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.2.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.2.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.2.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.2.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.2.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.3.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.3.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.3.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.3.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.3.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.3.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.3.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.4.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.4.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.4.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.4.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.4.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.4.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.4.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.5.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.5.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.5.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.5.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.5.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.5.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.5.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.layer_norm.bias                     |        [512] |      512 |
| generator.bias                                                               |      [30000] |    30000 |
| copy_attn.linear_in.weight                                                   |   [512, 512] |   262144 |
| copy_attn.linear_out.weight                                                  |  [512, 1024] |   524288 |
| copy_generator.linear_copy.weight                                            |     [1, 512] |      512 |
| copy_generator.linear_copy.bias                                              |          [1] |        1 |
+------------------------------------------------------------------------------+--------------+----------+ ]
04/19/2022 11:24:18 AM: [ ---------------------------------------------------------------------------------------------------- ]
04/19/2022 11:24:18 AM: [ Make data loaders ]
04/19/2022 11:24:18 AM: [ ---------------------------------------------------------------------------------------------------- ]
04/19/2022 11:24:18 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 32,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "python"
    ],
    "dataset_weights": {
        "1": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/python/dev/code.original_subtoken"
    ],
    "dev_src_tag": null,
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/python/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/full_python_nosym.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 400,
    "max_tgt_len": 30,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/full_python_nosym.mdl",
    "model_name": "full_python_nosym",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 55538,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/full_python_nosym.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/python/train/code.original_subtoken"
    ],
    "train_src_tag": null,
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/python/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 0,
    "weight_decay": 0
} ]
04/19/2022 11:24:18 AM: [ ---------------------------------------------------------------------------------------------------- ]
04/19/2022 11:24:18 AM: [ Starting training... ]
04/19/2022 11:27:34 AM: [ train: Epoch 1 | perplexity = 22026.47 | ml_loss = 335.46 | Time for epoch = 195.97 (s) ]
04/19/2022 11:30:31 AM: [ dev valid official: Epoch = 1 | bleu = 12.27 | rouge_l = 21.51 | Precision = 22.10 | Recall = 24.90 | F1 = 21.66 | examples = 18505 | valid time = 175.30 (s) ]
04/19/2022 11:30:31 AM: [ Best valid: bleu = 12.27 (epoch 1, 1736 updates) ]
04/19/2022 11:33:52 AM: [ train: Epoch 2 | perplexity = 20493.52 | ml_loss = 145.58 | Time for epoch = 200.19 (s) ]
04/19/2022 11:36:50 AM: [ dev valid official: Epoch = 2 | bleu = 15.56 | rouge_l = 29.09 | Precision = 31.94 | Recall = 31.18 | F1 = 29.90 | examples = 18505 | valid time = 176.81 (s) ]
04/19/2022 11:36:50 AM: [ Best valid: bleu = 15.56 (epoch 2, 3472 updates) ]
04/19/2022 11:40:11 AM: [ train: Epoch 3 | perplexity = 4870.78 | ml_loss = 85.55 | Time for epoch = 200.32 (s) ]
04/19/2022 11:43:06 AM: [ dev valid official: Epoch = 3 | bleu = 16.09 | rouge_l = 30.46 | Precision = 37.67 | Recall = 31.33 | F1 = 31.88 | examples = 18505 | valid time = 172.94 (s) ]
04/19/2022 11:43:06 AM: [ Best valid: bleu = 16.09 (epoch 3, 5208 updates) ]
04/19/2022 11:46:23 AM: [ train: Epoch 4 | perplexity = 286.79 | ml_loss = 58.39 | Time for epoch = 196.27 (s) ]
04/19/2022 11:49:19 AM: [ dev valid official: Epoch = 4 | bleu = 16.63 | rouge_l = 32.36 | Precision = 36.78 | Recall = 34.60 | F1 = 33.54 | examples = 18505 | valid time = 174.97 (s) ]
04/19/2022 11:49:19 AM: [ Best valid: bleu = 16.63 (epoch 4, 6944 updates) ]
04/19/2022 11:52:33 AM: [ train: Epoch 5 | perplexity = 102.04 | ml_loss = 48.72 | Time for epoch = 192.62 (s) ]
04/19/2022 11:55:31 AM: [ dev valid official: Epoch = 5 | bleu = 17.95 | rouge_l = 34.05 | Precision = 45.00 | Recall = 33.49 | F1 = 36.00 | examples = 18505 | valid time = 176.55 (s) ]
04/19/2022 11:55:31 AM: [ Best valid: bleu = 17.95 (epoch 5, 8680 updates) ]
04/19/2022 11:58:58 AM: [ train: Epoch 6 | perplexity = 66.76 | ml_loss = 44.80 | Time for epoch = 206.39 (s) ]
04/19/2022 12:02:04 PM: [ dev valid official: Epoch = 6 | bleu = 18.78 | rouge_l = 35.76 | Precision = 46.08 | Recall = 35.61 | F1 = 37.81 | examples = 18505 | valid time = 184.15 (s) ]
04/19/2022 12:02:04 PM: [ Best valid: bleu = 18.78 (epoch 6, 10416 updates) ]
04/19/2022 12:05:31 PM: [ train: Epoch 7 | perplexity = 54.98 | ml_loss = 42.65 | Time for epoch = 206.39 (s) ]
04/19/2022 12:08:37 PM: [ dev valid official: Epoch = 7 | bleu = 18.12 | rouge_l = 34.97 | Precision = 39.50 | Recall = 37.92 | F1 = 36.60 | examples = 18505 | valid time = 184.56 (s) ]
04/19/2022 12:12:00 PM: [ train: Epoch 8 | perplexity = 47.73 | ml_loss = 40.97 | Time for epoch = 203.02 (s) ]
04/19/2022 12:14:59 PM: [ dev valid official: Epoch = 8 | bleu = 19.22 | rouge_l = 36.30 | Precision = 45.01 | Recall = 36.98 | F1 = 38.20 | examples = 18505 | valid time = 177.67 (s) ]
04/19/2022 12:14:59 PM: [ Best valid: bleu = 19.22 (epoch 8, 13888 updates) ]
04/19/2022 12:18:23 PM: [ train: Epoch 9 | perplexity = 40.68 | ml_loss = 39.49 | Time for epoch = 203.33 (s) ]
04/19/2022 12:21:27 PM: [ dev valid official: Epoch = 9 | bleu = 19.64 | rouge_l = 36.58 | Precision = 45.67 | Recall = 37.14 | F1 = 38.59 | examples = 18505 | valid time = 182.28 (s) ]
04/19/2022 12:21:27 PM: [ Best valid: bleu = 19.64 (epoch 9, 15624 updates) ]
04/19/2022 12:24:59 PM: [ train: Epoch 10 | perplexity = 35.48 | ml_loss = 38.09 | Time for epoch = 211.76 (s) ]
04/19/2022 12:28:02 PM: [ dev valid official: Epoch = 10 | bleu = 20.04 | rouge_l = 37.31 | Precision = 44.63 | Recall = 38.66 | F1 = 39.25 | examples = 18505 | valid time = 180.87 (s) ]
04/19/2022 12:28:02 PM: [ Best valid: bleu = 20.04 (epoch 10, 17360 updates) ]
04/19/2022 12:31:31 PM: [ train: Epoch 11 | perplexity = 31.14 | ml_loss = 36.84 | Time for epoch = 207.76 (s) ]
04/19/2022 12:34:37 PM: [ dev valid official: Epoch = 11 | bleu = 19.53 | rouge_l = 37.17 | Precision = 41.12 | Recall = 41.01 | F1 = 38.91 | examples = 18505 | valid time = 184.87 (s) ]
04/19/2022 12:38:05 PM: [ train: Epoch 12 | perplexity = 28.20 | ml_loss = 35.61 | Time for epoch = 207.43 (s) ]
04/19/2022 12:41:08 PM: [ dev valid official: Epoch = 12 | bleu = 20.68 | rouge_l = 38.26 | Precision = 45.28 | Recall = 39.94 | F1 = 40.20 | examples = 18505 | valid time = 181.67 (s) ]
04/19/2022 12:41:08 PM: [ Best valid: bleu = 20.68 (epoch 12, 20832 updates) ]
04/19/2022 12:44:37 PM: [ train: Epoch 13 | perplexity = 25.13 | ml_loss = 34.46 | Time for epoch = 207.90 (s) ]
04/19/2022 12:47:42 PM: [ dev valid official: Epoch = 13 | bleu = 20.40 | rouge_l = 37.85 | Precision = 41.93 | Recall = 41.39 | F1 = 39.53 | examples = 18505 | valid time = 183.45 (s) ]
04/19/2022 12:51:07 PM: [ train: Epoch 14 | perplexity = 23.24 | ml_loss = 33.35 | Time for epoch = 205.34 (s) ]
04/19/2022 12:54:16 PM: [ dev valid official: Epoch = 14 | bleu = 21.58 | rouge_l = 39.18 | Precision = 47.59 | Recall = 39.91 | F1 = 41.19 | examples = 18505 | valid time = 186.80 (s) ]
04/19/2022 12:54:16 PM: [ Best valid: bleu = 21.58 (epoch 14, 24304 updates) ]
04/19/2022 12:57:41 PM: [ train: Epoch 15 | perplexity = 21.00 | ml_loss = 32.27 | Time for epoch = 205.05 (s) ]
04/19/2022 01:00:41 PM: [ dev valid official: Epoch = 15 | bleu = 21.54 | rouge_l = 39.21 | Precision = 45.22 | Recall = 41.66 | F1 = 41.16 | examples = 18505 | valid time = 177.94 (s) ]
04/19/2022 01:04:14 PM: [ train: Epoch 16 | perplexity = 18.38 | ml_loss = 31.22 | Time for epoch = 213.13 (s) ]
04/19/2022 01:07:15 PM: [ dev valid official: Epoch = 16 | bleu = 21.79 | rouge_l = 39.39 | Precision = 46.41 | Recall = 41.23 | F1 = 41.29 | examples = 18505 | valid time = 178.40 (s) ]
04/19/2022 01:07:15 PM: [ Best valid: bleu = 21.79 (epoch 16, 27776 updates) ]
04/19/2022 01:10:38 PM: [ train: Epoch 17 | perplexity = 16.76 | ml_loss = 30.17 | Time for epoch = 202.52 (s) ]
04/19/2022 01:13:59 PM: [ dev valid official: Epoch = 17 | bleu = 22.50 | rouge_l = 40.03 | Precision = 47.97 | Recall = 40.99 | F1 = 42.04 | examples = 18505 | valid time = 199.56 (s) ]
04/19/2022 01:13:59 PM: [ Best valid: bleu = 22.50 (epoch 17, 29512 updates) ]
04/19/2022 01:17:20 PM: [ train: Epoch 18 | perplexity = 15.89 | ml_loss = 29.22 | Time for epoch = 199.77 (s) ]
04/19/2022 01:20:25 PM: [ dev valid official: Epoch = 18 | bleu = 22.99 | rouge_l = 40.42 | Precision = 48.60 | Recall = 41.21 | F1 = 42.35 | examples = 18505 | valid time = 183.39 (s) ]
04/19/2022 01:20:25 PM: [ Best valid: bleu = 22.99 (epoch 18, 31248 updates) ]
04/19/2022 01:23:47 PM: [ train: Epoch 19 | perplexity = 14.25 | ml_loss = 28.27 | Time for epoch = 202.26 (s) ]
04/19/2022 01:26:46 PM: [ dev valid official: Epoch = 19 | bleu = 23.09 | rouge_l = 40.71 | Precision = 48.84 | Recall = 41.59 | F1 = 42.72 | examples = 18505 | valid time = 176.49 (s) ]
04/19/2022 01:26:46 PM: [ Best valid: bleu = 23.09 (epoch 19, 32984 updates) ]
04/19/2022 01:30:05 PM: [ train: Epoch 20 | perplexity = 13.27 | ml_loss = 27.34 | Time for epoch = 198.31 (s) ]
04/19/2022 01:33:01 PM: [ dev valid official: Epoch = 20 | bleu = 23.73 | rouge_l = 41.14 | Precision = 50.54 | Recall = 41.34 | F1 = 43.24 | examples = 18505 | valid time = 175.02 (s) ]
04/19/2022 01:33:01 PM: [ Best valid: bleu = 23.73 (epoch 20, 34720 updates) ]
04/19/2022 01:36:31 PM: [ train: Epoch 21 | perplexity = 11.70 | ml_loss = 26.49 | Time for epoch = 209.60 (s) ]
04/19/2022 01:39:30 PM: [ dev valid official: Epoch = 21 | bleu = 23.27 | rouge_l = 40.87 | Precision = 46.39 | Recall = 43.35 | F1 = 42.61 | examples = 18505 | valid time = 176.61 (s) ]
04/19/2022 01:42:53 PM: [ train: Epoch 22 | perplexity = 10.80 | ml_loss = 25.62 | Time for epoch = 203.05 (s) ]
04/19/2022 01:45:55 PM: [ dev valid official: Epoch = 22 | bleu = 23.28 | rouge_l = 40.81 | Precision = 45.05 | Recall = 44.25 | F1 = 42.55 | examples = 18505 | valid time = 180.83 (s) ]
04/19/2022 01:49:22 PM: [ train: Epoch 23 | perplexity = 10.41 | ml_loss = 24.75 | Time for epoch = 206.26 (s) ]
04/19/2022 01:52:21 PM: [ dev valid official: Epoch = 23 | bleu = 24.40 | rouge_l = 41.65 | Precision = 48.40 | Recall = 43.26 | F1 = 43.57 | examples = 18505 | valid time = 177.98 (s) ]
04/19/2022 01:52:21 PM: [ Best valid: bleu = 24.40 (epoch 23, 39928 updates) ]
04/19/2022 01:55:56 PM: [ train: Epoch 24 | perplexity = 9.16 | ml_loss = 23.99 | Time for epoch = 214.45 (s) ]
04/19/2022 01:59:05 PM: [ dev valid official: Epoch = 24 | bleu = 24.18 | rouge_l = 41.60 | Precision = 46.31 | Recall = 44.76 | F1 = 43.37 | examples = 18505 | valid time = 185.99 (s) ]
04/19/2022 02:02:32 PM: [ train: Epoch 25 | perplexity = 8.18 | ml_loss = 23.16 | Time for epoch = 207.83 (s) ]
04/19/2022 02:05:43 PM: [ dev valid official: Epoch = 25 | bleu = 24.30 | rouge_l = 41.61 | Precision = 46.30 | Recall = 44.66 | F1 = 43.38 | examples = 18505 | valid time = 189.01 (s) ]
04/19/2022 02:09:05 PM: [ train: Epoch 26 | perplexity = 8.08 | ml_loss = 22.36 | Time for epoch = 202.45 (s) ]
04/19/2022 02:12:07 PM: [ dev valid official: Epoch = 26 | bleu = 24.36 | rouge_l = 41.41 | Precision = 45.65 | Recall = 44.81 | F1 = 43.14 | examples = 18505 | valid time = 180.45 (s) ]
04/19/2022 02:15:27 PM: [ train: Epoch 27 | perplexity = 7.64 | ml_loss = 21.65 | Time for epoch = 199.24 (s) ]
04/19/2022 02:18:29 PM: [ dev valid official: Epoch = 27 | bleu = 24.84 | rouge_l = 42.06 | Precision = 47.08 | Recall = 44.81 | F1 = 43.84 | examples = 18505 | valid time = 180.85 (s) ]
04/19/2022 02:18:29 PM: [ Best valid: bleu = 24.84 (epoch 27, 46872 updates) ]
04/19/2022 02:21:53 PM: [ train: Epoch 28 | perplexity = 6.91 | ml_loss = 20.98 | Time for epoch = 202.69 (s) ]
04/19/2022 02:24:54 PM: [ dev valid official: Epoch = 28 | bleu = 25.27 | rouge_l = 42.24 | Precision = 48.45 | Recall = 44.05 | F1 = 44.07 | examples = 18505 | valid time = 179.42 (s) ]
04/19/2022 02:24:54 PM: [ Best valid: bleu = 25.27 (epoch 28, 48608 updates) ]
04/19/2022 02:28:19 PM: [ train: Epoch 29 | perplexity = 6.38 | ml_loss = 20.26 | Time for epoch = 204.33 (s) ]
04/19/2022 02:31:22 PM: [ dev valid official: Epoch = 29 | bleu = 25.55 | rouge_l = 42.48 | Precision = 48.07 | Recall = 44.80 | F1 = 44.32 | examples = 18505 | valid time = 181.23 (s) ]
04/19/2022 02:31:22 PM: [ Best valid: bleu = 25.55 (epoch 29, 50344 updates) ]
04/19/2022 02:34:47 PM: [ train: Epoch 30 | perplexity = 6.10 | ml_loss = 19.63 | Time for epoch = 204.37 (s) ]
04/19/2022 02:37:49 PM: [ dev valid official: Epoch = 30 | bleu = 25.47 | rouge_l = 42.38 | Precision = 48.02 | Recall = 44.38 | F1 = 44.10 | examples = 18505 | valid time = 179.92 (s) ]
04/19/2022 02:41:10 PM: [ train: Epoch 31 | perplexity = 5.79 | ml_loss = 18.96 | Time for epoch = 201.26 (s) ]
04/19/2022 02:44:16 PM: [ dev valid official: Epoch = 31 | bleu = 25.80 | rouge_l = 42.49 | Precision = 48.80 | Recall = 44.28 | F1 = 44.32 | examples = 18505 | valid time = 183.26 (s) ]
04/19/2022 02:44:16 PM: [ Best valid: bleu = 25.80 (epoch 31, 53816 updates) ]
04/19/2022 02:47:39 PM: [ train: Epoch 32 | perplexity = 5.62 | ml_loss = 18.39 | Time for epoch = 202.84 (s) ]
04/19/2022 02:50:45 PM: [ dev valid official: Epoch = 32 | bleu = 26.31 | rouge_l = 42.95 | Precision = 49.26 | Recall = 44.57 | F1 = 44.78 | examples = 18505 | valid time = 183.69 (s) ]
04/19/2022 02:50:45 PM: [ Best valid: bleu = 26.31 (epoch 32, 55552 updates) ]
04/19/2022 02:54:11 PM: [ train: Epoch 33 | perplexity = 5.12 | ml_loss = 17.80 | Time for epoch = 205.62 (s) ]
04/19/2022 02:57:24 PM: [ dev valid official: Epoch = 33 | bleu = 26.37 | rouge_l = 42.91 | Precision = 49.40 | Recall = 44.42 | F1 = 44.74 | examples = 18505 | valid time = 190.79 (s) ]
04/19/2022 02:57:24 PM: [ Best valid: bleu = 26.37 (epoch 33, 57288 updates) ]
04/19/2022 03:00:57 PM: [ train: Epoch 34 | perplexity = 4.69 | ml_loss = 17.22 | Time for epoch = 212.57 (s) ]
04/19/2022 03:04:06 PM: [ dev valid official: Epoch = 34 | bleu = 26.29 | rouge_l = 42.90 | Precision = 48.29 | Recall = 45.16 | F1 = 44.69 | examples = 18505 | valid time = 187.45 (s) ]
04/19/2022 03:07:41 PM: [ train: Epoch 35 | perplexity = 4.46 | ml_loss = 16.68 | Time for epoch = 215.13 (s) ]
04/19/2022 03:10:51 PM: [ dev valid official: Epoch = 35 | bleu = 26.70 | rouge_l = 43.22 | Precision = 50.44 | Recall = 44.29 | F1 = 45.11 | examples = 18505 | valid time = 187.88 (s) ]
04/19/2022 03:10:51 PM: [ Best valid: bleu = 26.70 (epoch 35, 60760 updates) ]
04/19/2022 03:14:21 PM: [ train: Epoch 36 | perplexity = 4.41 | ml_loss = 16.14 | Time for epoch = 209.25 (s) ]
04/19/2022 03:17:29 PM: [ dev valid official: Epoch = 36 | bleu = 26.64 | rouge_l = 43.10 | Precision = 47.78 | Recall = 46.00 | F1 = 44.86 | examples = 18505 | valid time = 186.30 (s) ]
04/19/2022 03:21:04 PM: [ train: Epoch 37 | perplexity = 4.11 | ml_loss = 15.64 | Time for epoch = 214.39 (s) ]
04/19/2022 03:24:19 PM: [ dev valid official: Epoch = 37 | bleu = 26.77 | rouge_l = 43.35 | Precision = 49.25 | Recall = 45.29 | F1 = 45.16 | examples = 18505 | valid time = 194.00 (s) ]
04/19/2022 03:24:19 PM: [ Best valid: bleu = 26.77 (epoch 37, 64232 updates) ]
04/19/2022 03:27:51 PM: [ train: Epoch 38 | perplexity = 4.07 | ml_loss = 15.18 | Time for epoch = 210.73 (s) ]
04/19/2022 03:31:06 PM: [ dev valid official: Epoch = 38 | bleu = 27.24 | rouge_l = 43.47 | Precision = 48.82 | Recall = 45.73 | F1 = 45.22 | examples = 18505 | valid time = 193.98 (s) ]
04/19/2022 03:31:06 PM: [ Best valid: bleu = 27.24 (epoch 38, 65968 updates) ]
04/19/2022 03:34:41 PM: [ train: Epoch 39 | perplexity = 3.84 | ml_loss = 14.72 | Time for epoch = 214.17 (s) ]
04/19/2022 03:37:52 PM: [ dev valid official: Epoch = 39 | bleu = 27.17 | rouge_l = 43.63 | Precision = 48.78 | Recall = 46.09 | F1 = 45.44 | examples = 18505 | valid time = 189.10 (s) ]
04/19/2022 03:41:27 PM: [ train: Epoch 40 | perplexity = 3.51 | ml_loss = 14.29 | Time for epoch = 214.59 (s) ]
04/19/2022 03:44:42 PM: [ dev valid official: Epoch = 40 | bleu = 27.58 | rouge_l = 43.71 | Precision = 49.84 | Recall = 45.43 | F1 = 45.55 | examples = 18505 | valid time = 193.60 (s) ]
04/19/2022 03:44:42 PM: [ Best valid: bleu = 27.58 (epoch 40, 69440 updates) ]
04/19/2022 03:48:16 PM: [ train: Epoch 41 | perplexity = 3.48 | ml_loss = 13.83 | Time for epoch = 213.08 (s) ]
04/19/2022 03:51:31 PM: [ dev valid official: Epoch = 41 | bleu = 27.36 | rouge_l = 43.67 | Precision = 48.36 | Recall = 46.63 | F1 = 45.43 | examples = 18505 | valid time = 193.66 (s) ]
04/19/2022 03:55:13 PM: [ train: Epoch 42 | perplexity = 3.25 | ml_loss = 13.42 | Time for epoch = 221.29 (s) ]
04/19/2022 03:58:33 PM: [ dev valid official: Epoch = 42 | bleu = 27.98 | rouge_l = 44.25 | Precision = 50.85 | Recall = 45.68 | F1 = 46.10 | examples = 18505 | valid time = 198.28 (s) ]
04/19/2022 03:58:33 PM: [ Best valid: bleu = 27.98 (epoch 42, 72912 updates) ]
04/19/2022 04:02:13 PM: [ train: Epoch 43 | perplexity = 3.13 | ml_loss = 12.99 | Time for epoch = 220.11 (s) ]
04/19/2022 04:05:19 PM: [ dev valid official: Epoch = 43 | bleu = 27.44 | rouge_l = 43.80 | Precision = 47.96 | Recall = 47.07 | F1 = 45.46 | examples = 18505 | valid time = 184.31 (s) ]
04/19/2022 04:08:46 PM: [ train: Epoch 44 | perplexity = 3.08 | ml_loss = 12.63 | Time for epoch = 206.98 (s) ]
04/19/2022 04:11:52 PM: [ dev valid official: Epoch = 44 | bleu = 28.05 | rouge_l = 43.89 | Precision = 50.08 | Recall = 45.55 | F1 = 45.68 | examples = 18505 | valid time = 184.56 (s) ]
04/19/2022 04:11:52 PM: [ Best valid: bleu = 28.05 (epoch 44, 76384 updates) ]
04/19/2022 04:15:16 PM: [ train: Epoch 45 | perplexity = 3.07 | ml_loss = 12.29 | Time for epoch = 203.39 (s) ]
04/19/2022 04:18:20 PM: [ dev valid official: Epoch = 45 | bleu = 28.11 | rouge_l = 44.12 | Precision = 50.65 | Recall = 45.53 | F1 = 45.93 | examples = 18505 | valid time = 181.90 (s) ]
04/19/2022 04:18:20 PM: [ Best valid: bleu = 28.11 (epoch 45, 78120 updates) ]
04/19/2022 04:21:47 PM: [ train: Epoch 46 | perplexity = 2.86 | ml_loss = 11.97 | Time for epoch = 206.75 (s) ]
04/19/2022 04:24:56 PM: [ dev valid official: Epoch = 46 | bleu = 27.78 | rouge_l = 43.63 | Precision = 47.92 | Recall = 46.72 | F1 = 45.30 | examples = 18505 | valid time = 186.43 (s) ]
04/19/2022 04:28:24 PM: [ train: Epoch 47 | perplexity = 2.76 | ml_loss = 11.59 | Time for epoch = 208.42 (s) ]
04/19/2022 04:31:33 PM: [ dev valid official: Epoch = 47 | bleu = 28.43 | rouge_l = 44.20 | Precision = 49.94 | Recall = 45.90 | F1 = 45.94 | examples = 18505 | valid time = 186.96 (s) ]
04/19/2022 04:31:33 PM: [ Best valid: bleu = 28.43 (epoch 47, 81592 updates) ]
04/19/2022 04:34:58 PM: [ train: Epoch 48 | perplexity = 2.77 | ml_loss = 11.29 | Time for epoch = 204.83 (s) ]
04/19/2022 04:38:09 PM: [ dev valid official: Epoch = 48 | bleu = 28.52 | rouge_l = 44.34 | Precision = 49.39 | Recall = 46.70 | F1 = 46.10 | examples = 18505 | valid time = 189.11 (s) ]
04/19/2022 04:38:09 PM: [ Best valid: bleu = 28.52 (epoch 48, 83328 updates) ]
04/19/2022 04:41:38 PM: [ train: Epoch 49 | perplexity = 2.67 | ml_loss = 11.00 | Time for epoch = 208.71 (s) ]
04/19/2022 04:44:46 PM: [ dev valid official: Epoch = 49 | bleu = 28.75 | rouge_l = 44.49 | Precision = 50.71 | Recall = 46.07 | F1 = 46.31 | examples = 18505 | valid time = 185.88 (s) ]
04/19/2022 04:44:46 PM: [ Best valid: bleu = 28.75 (epoch 49, 85064 updates) ]
04/19/2022 04:48:15 PM: [ train: Epoch 50 | perplexity = 2.51 | ml_loss = 10.72 | Time for epoch = 208.12 (s) ]
04/19/2022 04:51:19 PM: [ dev valid official: Epoch = 50 | bleu = 28.73 | rouge_l = 44.32 | Precision = 49.57 | Recall = 46.65 | F1 = 46.07 | examples = 18505 | valid time = 182.77 (s) ]
04/19/2022 04:54:44 PM: [ train: Epoch 51 | perplexity = 2.51 | ml_loss = 10.41 | Time for epoch = 204.67 (s) ]
04/19/2022 04:57:50 PM: [ dev valid official: Epoch = 51 | bleu = 28.81 | rouge_l = 44.59 | Precision = 50.61 | Recall = 46.37 | F1 = 46.40 | examples = 18505 | valid time = 184.48 (s) ]
04/19/2022 04:57:50 PM: [ Best valid: bleu = 28.81 (epoch 51, 88536 updates) ]
04/19/2022 05:01:13 PM: [ train: Epoch 52 | perplexity = 2.40 | ml_loss = 10.13 | Time for epoch = 202.73 (s) ]
04/19/2022 05:04:15 PM: [ dev valid official: Epoch = 52 | bleu = 28.65 | rouge_l = 44.42 | Precision = 48.61 | Recall = 47.31 | F1 = 46.09 | examples = 18505 | valid time = 180.32 (s) ]
04/19/2022 05:07:33 PM: [ train: Epoch 53 | perplexity = 2.40 | ml_loss = 9.85 | Time for epoch = 197.82 (s) ]
04/19/2022 05:10:35 PM: [ dev valid official: Epoch = 53 | bleu = 29.17 | rouge_l = 44.71 | Precision = 50.20 | Recall = 46.63 | F1 = 46.35 | examples = 18505 | valid time = 180.81 (s) ]
04/19/2022 05:10:35 PM: [ Best valid: bleu = 29.17 (epoch 53, 92008 updates) ]
04/19/2022 05:13:58 PM: [ train: Epoch 54 | perplexity = 2.27 | ml_loss = 9.63 | Time for epoch = 202.63 (s) ]
04/19/2022 05:17:02 PM: [ dev valid official: Epoch = 54 | bleu = 29.11 | rouge_l = 44.68 | Precision = 49.38 | Recall = 47.27 | F1 = 46.33 | examples = 18505 | valid time = 182.01 (s) ]
04/19/2022 05:20:19 PM: [ train: Epoch 55 | perplexity = 2.29 | ml_loss = 9.35 | Time for epoch = 197.28 (s) ]
04/19/2022 05:23:22 PM: [ dev valid official: Epoch = 55 | bleu = 29.15 | rouge_l = 44.81 | Precision = 49.21 | Recall = 47.59 | F1 = 46.46 | examples = 18505 | valid time = 180.82 (s) ]
04/19/2022 05:26:42 PM: [ train: Epoch 56 | perplexity = 2.22 | ml_loss = 9.14 | Time for epoch = 200.32 (s) ]
04/19/2022 05:29:43 PM: [ dev valid official: Epoch = 56 | bleu = 29.25 | rouge_l = 44.76 | Precision = 49.86 | Recall = 47.00 | F1 = 46.43 | examples = 18505 | valid time = 178.56 (s) ]
04/19/2022 05:29:43 PM: [ Best valid: bleu = 29.25 (epoch 56, 97216 updates) ]
04/19/2022 05:33:00 PM: [ train: Epoch 57 | perplexity = 2.18 | ml_loss = 8.93 | Time for epoch = 197.14 (s) ]
04/19/2022 05:36:02 PM: [ dev valid official: Epoch = 57 | bleu = 29.38 | rouge_l = 44.87 | Precision = 49.34 | Recall = 47.50 | F1 = 46.52 | examples = 18505 | valid time = 179.31 (s) ]
04/19/2022 05:36:02 PM: [ Best valid: bleu = 29.38 (epoch 57, 98952 updates) ]
04/19/2022 05:39:25 PM: [ train: Epoch 58 | perplexity = 2.08 | ml_loss = 8.69 | Time for epoch = 202.97 (s) ]
04/19/2022 05:42:29 PM: [ dev valid official: Epoch = 58 | bleu = 29.54 | rouge_l = 45.14 | Precision = 50.23 | Recall = 47.40 | F1 = 46.84 | examples = 18505 | valid time = 181.77 (s) ]
04/19/2022 05:42:29 PM: [ Best valid: bleu = 29.54 (epoch 58, 100688 updates) ]
04/19/2022 05:45:53 PM: [ train: Epoch 59 | perplexity = 2.04 | ml_loss = 8.48 | Time for epoch = 203.85 (s) ]
04/19/2022 05:48:55 PM: [ dev valid official: Epoch = 59 | bleu = 29.30 | rouge_l = 44.89 | Precision = 48.85 | Recall = 47.90 | F1 = 46.46 | examples = 18505 | valid time = 180.53 (s) ]
04/19/2022 05:52:18 PM: [ train: Epoch 60 | perplexity = 2.03 | ml_loss = 8.27 | Time for epoch = 202.52 (s) ]
04/19/2022 05:55:21 PM: [ dev valid official: Epoch = 60 | bleu = 29.56 | rouge_l = 45.12 | Precision = 50.02 | Recall = 47.55 | F1 = 46.83 | examples = 18505 | valid time = 181.57 (s) ]
04/19/2022 05:55:21 PM: [ Best valid: bleu = 29.56 (epoch 60, 104160 updates) ]
04/19/2022 05:58:47 PM: [ train: Epoch 61 | perplexity = 1.96 | ml_loss = 8.03 | Time for epoch = 205.04 (s) ]
04/19/2022 06:01:52 PM: [ dev valid official: Epoch = 61 | bleu = 29.47 | rouge_l = 44.89 | Precision = 48.53 | Recall = 48.19 | F1 = 46.48 | examples = 18505 | valid time = 182.70 (s) ]
04/19/2022 06:05:12 PM: [ train: Epoch 62 | perplexity = 1.95 | ml_loss = 7.86 | Time for epoch = 200.49 (s) ]
04/19/2022 06:08:13 PM: [ dev valid official: Epoch = 62 | bleu = 29.90 | rouge_l = 45.33 | Precision = 50.36 | Recall = 47.61 | F1 = 47.10 | examples = 18505 | valid time = 179.03 (s) ]
04/19/2022 06:08:13 PM: [ Best valid: bleu = 29.90 (epoch 62, 107632 updates) ]
04/19/2022 06:11:35 PM: [ train: Epoch 63 | perplexity = 1.91 | ml_loss = 7.64 | Time for epoch = 201.27 (s) ]
04/19/2022 06:14:36 PM: [ dev valid official: Epoch = 63 | bleu = 30.04 | rouge_l = 45.11 | Precision = 50.41 | Recall = 47.15 | F1 = 46.79 | examples = 18505 | valid time = 179.61 (s) ]
04/19/2022 06:14:36 PM: [ Best valid: bleu = 30.04 (epoch 63, 109368 updates) ]
04/19/2022 06:18:02 PM: [ train: Epoch 64 | perplexity = 1.88 | ml_loss = 7.49 | Time for epoch = 205.18 (s) ]
04/19/2022 06:21:03 PM: [ dev valid official: Epoch = 64 | bleu = 30.00 | rouge_l = 45.32 | Precision = 49.87 | Recall = 48.03 | F1 = 46.99 | examples = 18505 | valid time = 178.96 (s) ]
04/19/2022 06:24:28 PM: [ train: Epoch 65 | perplexity = 1.85 | ml_loss = 7.31 | Time for epoch = 205.22 (s) ]
04/19/2022 06:27:31 PM: [ dev valid official: Epoch = 65 | bleu = 30.09 | rouge_l = 45.41 | Precision = 49.89 | Recall = 48.11 | F1 = 47.04 | examples = 18505 | valid time = 181.04 (s) ]
04/19/2022 06:27:31 PM: [ Best valid: bleu = 30.09 (epoch 65, 112840 updates) ]
04/19/2022 06:30:54 PM: [ train: Epoch 66 | perplexity = 1.83 | ml_loss = 7.15 | Time for epoch = 202.65 (s) ]
04/19/2022 06:33:56 PM: [ dev valid official: Epoch = 66 | bleu = 30.15 | rouge_l = 45.24 | Precision = 49.91 | Recall = 47.64 | F1 = 46.86 | examples = 18505 | valid time = 180.38 (s) ]
04/19/2022 06:33:56 PM: [ Best valid: bleu = 30.15 (epoch 66, 114576 updates) ]
04/19/2022 06:37:21 PM: [ train: Epoch 67 | perplexity = 1.79 | ml_loss = 6.98 | Time for epoch = 204.52 (s) ]
04/19/2022 06:40:23 PM: [ dev valid official: Epoch = 67 | bleu = 30.01 | rouge_l = 45.28 | Precision = 49.11 | Recall = 48.32 | F1 = 46.82 | examples = 18505 | valid time = 180.37 (s) ]
04/19/2022 06:43:41 PM: [ train: Epoch 68 | perplexity = 1.79 | ml_loss = 6.84 | Time for epoch = 197.94 (s) ]
04/19/2022 06:46:42 PM: [ dev valid official: Epoch = 68 | bleu = 30.44 | rouge_l = 45.58 | Precision = 50.67 | Recall = 47.77 | F1 = 47.24 | examples = 18505 | valid time = 179.46 (s) ]
04/19/2022 06:46:42 PM: [ Best valid: bleu = 30.44 (epoch 68, 118048 updates) ]
04/19/2022 06:50:01 PM: [ train: Epoch 69 | perplexity = 1.78 | ml_loss = 6.70 | Time for epoch = 197.89 (s) ]
04/19/2022 06:53:02 PM: [ dev valid official: Epoch = 69 | bleu = 30.34 | rouge_l = 45.56 | Precision = 50.08 | Recall = 48.05 | F1 = 47.19 | examples = 18505 | valid time = 179.53 (s) ]
04/19/2022 06:56:20 PM: [ train: Epoch 70 | perplexity = 1.75 | ml_loss = 6.55 | Time for epoch = 197.23 (s) ]
04/19/2022 06:59:20 PM: [ dev valid official: Epoch = 70 | bleu = 30.35 | rouge_l = 45.52 | Precision = 49.80 | Recall = 48.35 | F1 = 47.11 | examples = 18505 | valid time = 178.66 (s) ]
04/19/2022 07:02:43 PM: [ train: Epoch 71 | perplexity = 1.70 | ml_loss = 6.43 | Time for epoch = 203.55 (s) ]
04/19/2022 07:05:46 PM: [ dev valid official: Epoch = 71 | bleu = 30.46 | rouge_l = 45.54 | Precision = 49.93 | Recall = 48.09 | F1 = 47.12 | examples = 18505 | valid time = 180.53 (s) ]
04/19/2022 07:05:46 PM: [ Best valid: bleu = 30.46 (epoch 71, 123256 updates) ]
04/19/2022 07:09:12 PM: [ train: Epoch 72 | perplexity = 1.68 | ml_loss = 6.27 | Time for epoch = 205.68 (s) ]
04/19/2022 07:12:15 PM: [ dev valid official: Epoch = 72 | bleu = 30.35 | rouge_l = 45.63 | Precision = 49.74 | Recall = 48.52 | F1 = 47.23 | examples = 18505 | valid time = 180.88 (s) ]
04/19/2022 07:15:38 PM: [ train: Epoch 73 | perplexity = 1.66 | ml_loss = 6.16 | Time for epoch = 203.77 (s) ]
04/19/2022 07:18:40 PM: [ dev valid official: Epoch = 73 | bleu = 30.72 | rouge_l = 45.84 | Precision = 50.59 | Recall = 48.13 | F1 = 47.43 | examples = 18505 | valid time = 179.37 (s) ]
04/19/2022 07:18:40 PM: [ Best valid: bleu = 30.72 (epoch 73, 126728 updates) ]
04/19/2022 07:22:05 PM: [ train: Epoch 74 | perplexity = 1.64 | ml_loss = 5.98 | Time for epoch = 204.17 (s) ]
04/19/2022 07:25:08 PM: [ dev valid official: Epoch = 74 | bleu = 30.67 | rouge_l = 45.96 | Precision = 50.58 | Recall = 48.38 | F1 = 47.58 | examples = 18505 | valid time = 181.16 (s) ]
04/19/2022 07:28:29 PM: [ train: Epoch 75 | perplexity = 1.64 | ml_loss = 5.86 | Time for epoch = 201.32 (s) ]
04/19/2022 07:31:42 PM: [ dev valid official: Epoch = 75 | bleu = 30.89 | rouge_l = 45.95 | Precision = 50.55 | Recall = 48.43 | F1 = 47.59 | examples = 18505 | valid time = 191.04 (s) ]
04/19/2022 07:31:42 PM: [ Best valid: bleu = 30.89 (epoch 75, 130200 updates) ]
04/19/2022 07:35:21 PM: [ train: Epoch 76 | perplexity = 1.61 | ml_loss = 5.76 | Time for epoch = 218.82 (s) ]
04/19/2022 07:38:38 PM: [ dev valid official: Epoch = 76 | bleu = 30.49 | rouge_l = 45.40 | Precision = 49.59 | Recall = 48.21 | F1 = 46.94 | examples = 18505 | valid time = 195.50 (s) ]
04/19/2022 07:42:17 PM: [ train: Epoch 77 | perplexity = 1.59 | ml_loss = 5.62 | Time for epoch = 219.02 (s) ]
04/19/2022 07:45:30 PM: [ dev valid official: Epoch = 77 | bleu = 30.64 | rouge_l = 45.89 | Precision = 50.15 | Recall = 48.70 | F1 = 47.49 | examples = 18505 | valid time = 191.39 (s) ]
04/19/2022 07:49:05 PM: [ train: Epoch 78 | perplexity = 1.58 | ml_loss = 5.50 | Time for epoch = 214.62 (s) ]
04/19/2022 07:52:18 PM: [ dev valid official: Epoch = 78 | bleu = 30.85 | rouge_l = 45.87 | Precision = 50.33 | Recall = 48.33 | F1 = 47.47 | examples = 18505 | valid time = 191.73 (s) ]
04/19/2022 07:55:51 PM: [ train: Epoch 79 | perplexity = 1.56 | ml_loss = 5.40 | Time for epoch = 212.88 (s) ]
04/19/2022 07:59:06 PM: [ dev valid official: Epoch = 79 | bleu = 30.79 | rouge_l = 45.90 | Precision = 50.01 | Recall = 48.76 | F1 = 47.52 | examples = 18505 | valid time = 193.21 (s) ]
04/19/2022 08:02:48 PM: [ train: Epoch 80 | perplexity = 1.55 | ml_loss = 5.30 | Time for epoch = 222.21 (s) ]
04/19/2022 08:06:09 PM: [ dev valid official: Epoch = 80 | bleu = 30.94 | rouge_l = 45.69 | Precision = 50.40 | Recall = 48.06 | F1 = 47.33 | examples = 18505 | valid time = 199.77 (s) ]
04/19/2022 08:06:09 PM: [ Best valid: bleu = 30.94 (epoch 80, 138880 updates) ]
04/19/2022 08:09:43 PM: [ train: Epoch 81 | perplexity = 1.54 | ml_loss = 5.23 | Time for epoch = 212.65 (s) ]
04/19/2022 08:13:07 PM: [ dev valid official: Epoch = 81 | bleu = 30.92 | rouge_l = 46.06 | Precision = 50.05 | Recall = 49.10 | F1 = 47.67 | examples = 18505 | valid time = 202.63 (s) ]
04/19/2022 08:16:43 PM: [ train: Epoch 82 | perplexity = 1.53 | ml_loss = 5.09 | Time for epoch = 215.66 (s) ]
04/19/2022 08:20:05 PM: [ dev valid official: Epoch = 82 | bleu = 31.08 | rouge_l = 46.02 | Precision = 50.39 | Recall = 48.49 | F1 = 47.59 | examples = 18505 | valid time = 199.64 (s) ]
04/19/2022 08:20:05 PM: [ Best valid: bleu = 31.08 (epoch 82, 142352 updates) ]
04/19/2022 08:23:45 PM: [ train: Epoch 83 | perplexity = 1.51 | ml_loss = 5.01 | Time for epoch = 219.88 (s) ]
04/19/2022 08:27:04 PM: [ dev valid official: Epoch = 83 | bleu = 30.90 | rouge_l = 45.89 | Precision = 49.46 | Recall = 49.02 | F1 = 47.43 | examples = 18505 | valid time = 197.29 (s) ]
04/19/2022 08:30:30 PM: [ train: Epoch 84 | perplexity = 1.51 | ml_loss = 4.90 | Time for epoch = 205.83 (s) ]
04/19/2022 08:33:43 PM: [ dev valid official: Epoch = 84 | bleu = 31.42 | rouge_l = 46.13 | Precision = 51.32 | Recall = 48.10 | F1 = 47.78 | examples = 18505 | valid time = 192.00 (s) ]
04/19/2022 08:33:43 PM: [ Best valid: bleu = 31.42 (epoch 84, 145824 updates) ]
04/19/2022 08:37:12 PM: [ train: Epoch 85 | perplexity = 1.49 | ml_loss = 4.83 | Time for epoch = 207.69 (s) ]
04/19/2022 08:40:21 PM: [ dev valid official: Epoch = 85 | bleu = 31.48 | rouge_l = 46.32 | Precision = 51.57 | Recall = 48.16 | F1 = 47.91 | examples = 18505 | valid time = 187.22 (s) ]
04/19/2022 08:40:21 PM: [ Best valid: bleu = 31.48 (epoch 85, 147560 updates) ]

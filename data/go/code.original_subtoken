func ( re Raw Extension ) Marshal JSON ( ) ( [ ] byte , error ) { if re . Raw == nil { // TODO: this is to support legacy behavior of JSON Printer and YAML Printer, which // expect to call json.Marshal on arbitrary versioned objects (even those not in // the scheme). pkg/kubectl/resource#As Versioned // TODO: Check whether Content } 
func Is Context Not if _ , ok := err . ( * err Context Not Found ) ; ok || err == Err No } 
func Is Empty Config ( err error ) bool { switch t := err . ( type ) { case err Configuration Invalid : return len ( t ) == 1 && t [ 0 ] == Err Empty return err == Err Empty } 
func ( e err Configuration Invalid ) Error ( ) string { return fmt . Sprintf ( " " , utilerrors . New } 
func Is Configuration Invalid ( err error ) bool { switch err . ( type ) { case * err Context Not Found , err Configuration return Is Context Not } 
func Validate ( config clientcmdapi . Config ) error { validation if clientcmdapi . Is Config Empty ( & config ) { return new Err Configuration Invalid ( [ ] error { Err Empty if len ( config . Current Context ) != 0 { if _ , exists := config . Contexts [ config . Current Context ] ; ! exists { validation Errors = append ( validation Errors , & err Context Not Found { config . Current for context Name , context := range config . Contexts { validation Errors = append ( validation Errors , validate Context ( context for auth Info Name , auth Info := range config . Auth Infos { validation Errors = append ( validation Errors , validate Auth Info ( auth Info Name , * auth for cluster Name , cluster Info := range config . Clusters { validation Errors = append ( validation Errors , validate Cluster Info ( cluster Name , * cluster return new Err Configuration Invalid ( validation } 
func Confirm Usable ( config clientcmdapi . Config , passed Context Name string ) error { validation if clientcmdapi . Is Config Empty ( & config ) { return new Err Configuration Invalid ( [ ] error { Err Empty var context if len ( passed Context Name ) != 0 { context Name = passed Context } else { context Name = config . Current if len ( context Name ) == 0 { return Err No context , exists := config . Contexts [ context if ! exists { validation Errors = append ( validation Errors , & err Context Not Found { context if exists { validation Errors = append ( validation Errors , validate Context ( context validation Errors = append ( validation Errors , validate Auth Info ( context . Auth Info , * config . Auth Infos [ context . Auth validation Errors = append ( validation Errors , validate Cluster return new Err Configuration Invalid ( validation } 
func validate Cluster Info ( cluster Name string , cluster Info clientcmdapi . Cluster ) [ ] error { validation empty Cluster := clientcmdapi . New if reflect . Deep Equal ( * empty Cluster , cluster Info ) { return [ ] error { Err Empty if len ( cluster Info . Server ) == 0 { if len ( cluster Name ) == 0 { validation Errors = append ( validation } else { validation Errors = append ( validation Errors , fmt . Errorf ( " " , cluster // Make sure CA data and CA file aren't both specified if len ( cluster Info . Certificate Authority ) != 0 && len ( cluster Info . Certificate Authority Data ) != 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , cluster if len ( cluster Info . Certificate Authority ) != 0 { client Cert CA , err := os . Open ( cluster Info . Certificate defer client Cert if err != nil { validation Errors = append ( validation Errors , fmt . Errorf ( " " , cluster Info . Certificate Authority , cluster return validation } 
func validate Auth Info ( auth Info Name string , auth Info clientcmdapi . Auth Info ) [ ] error { validation using Auth if len ( auth if len ( auth Info . Username ) != 0 || len ( auth if len ( auth Info . Client Certificate ) != 0 || len ( auth Info . Client Certificate Data ) != 0 { // Make sure cert data and file aren't both specified if len ( auth Info . Client Certificate ) != 0 && len ( auth Info . Client Certificate Data ) != 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info // Make sure key data and file aren't both specified if len ( auth Info . Client Key ) != 0 && len ( auth Info . Client Key Data ) != 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info // Make sure a key is specified if len ( auth Info . Client Key ) == 0 && len ( auth Info . Client Key Data ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info if len ( auth Info . Client Certificate ) != 0 { client Cert File , err := os . Open ( auth Info . Client defer client Cert if err != nil { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info . Client Certificate , auth Info if len ( auth Info . Client Key ) != 0 { client Key File , err := os . Open ( auth Info . Client defer client Key if err != nil { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info . Client Key , auth Info if auth Info . Exec != nil { if auth Info . Auth Provider != nil { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info if len ( auth Info . Exec . Command ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info if len ( auth Info . Exec . API Version ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info for _ , v := range auth Info . Exec . Env { if len ( v . Name ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info } else if len ( v . Value ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , v . Name , auth Info // auth Path also provides information for the client to identify the server, so allow multiple auth methods in that case if ( len ( methods ) > 1 ) && ( ! using Auth Path ) { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info // Impersonate Groups or Impersonate User Extra should be requested with a user if ( len ( auth Info . Impersonate Groups ) > 0 || len ( auth Info . Impersonate User Extra ) > 0 ) && ( len ( auth Info . Impersonate ) == 0 ) { validation Errors = append ( validation Errors , fmt . Errorf ( " " , auth Info return validation } 
func validate Context ( context Name string , context clientcmdapi . Context , config clientcmdapi . Config ) [ ] error { validation if len ( context Name ) == 0 { validation Errors = append ( validation if len ( context . Auth Info ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , context } else if _ , exists := config . Auth Infos [ context . Auth Info ] ; ! exists { validation Errors = append ( validation Errors , fmt . Errorf ( " " , context . Auth Info , context if len ( context . Cluster ) == 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , context } else if _ , exists := config . Clusters [ context . Cluster ] ; ! exists { validation Errors = append ( validation Errors , fmt . Errorf ( " " , context . Cluster , context if len ( context . Namespace ) != 0 { if len ( validation . Is DNS1123Label ( context . Namespace ) ) != 0 { validation Errors = append ( validation Errors , fmt . Errorf ( " " , context . Namespace , context return validation } 
func ( f Empty Controller Lister ) List ( labels . Selector ) ( [ ] * v1 . Replication } 
func ( f Empty Controller Lister ) Get Pod Controllers ( pod * v1 . Pod ) ( controllers [ ] * v1 . Replication } 
func ( f Empty Replica Set Lister ) Get Pod Replica Sets ( pod * v1 . Pod ) ( rss [ ] * apps . Replica } 
func ( f Empty Stateful Set Lister ) Get Pod Stateful Sets ( pod * v1 . Pod ) ( sss [ ] * apps . Stateful } 
func New Delete Command Flags ( usage string ) * Delete grace all ignore Not label field return & Delete Flags { // Not using helpers.go since it provides function to add '-k' for File Name Options, but not File Name Flags File Name Flags : & genericclioptions . File Name Flags { Usage : usage , Filenames : & filenames , Kustomize : & kustomize , Recursive : & recursive } , Label Selector : & label Selector , Field Selector : & field Selector , Cascade : & cascade , Grace Period : & grace Period , All : & all , All Namespaces : & all Namespaces , Force : & force , Ignore Not Found : & ignore Not } 
func New Delete Flags ( usage string ) * Delete grace return & Delete Flags { File Name Flags : & genericclioptions . File Name Flags { Usage : usage , Filenames : & filenames , Kustomize : & kustomize , Recursive : & recursive } , Cascade : & cascade , Grace Period : & grace } 
func ( g * Cloud ) ensure External Load Balancer ( cluster Name string , cluster ID string , api Service * v1 . Service , existing Fwd Rule * compute . Forwarding Rule , nodes [ ] * v1 . Node ) ( * v1 . Load Balancer host Names := node supports Nodes Health Check := supports Nodes Health hosts , err := g . get Instances By Names ( host load Balancer Name := g . Get Load Balancer Name ( context . TODO ( ) , cluster Name , api requested IP := api Service . Spec . Load Balancer ports := api port for _ , p := range api Service . Spec . Ports { port Str = append ( port service Name := types . Namespaced Name { Namespace : api Service . Namespace , Name : api lb Ref Str := fmt . Sprintf ( " " , load Balancer Name , service klog . V ( 2 ) . Infof ( " " , lb Ref Str , g . region , requested IP , port Str , host Names , api // Check the current and the desired network tiers. If they do not match, // tear down the existing resources with the wrong tier. net Tier , err := g . get Service Network Tier ( api if err != nil { klog . Errorf ( " " , lb Ref klog . V ( 4 ) . Infof ( " " , lb Ref Str , net if g . Alpha Feature Gate . Enabled ( Alpha Feature Network Tiers ) { g . delete Wrong Network Tiered Resources ( load Balancer Name , lb Ref Str , net // Check if the forwarding rule exists, and if so, what its IP is. fwd Rule Exists , fwd Rule Needs Update , fwd Rule IP , err := g . forwarding Rule Needs Update ( load Balancer Name , g . region , requested if ! fwd Rule Exists { klog . V ( 2 ) . Infof ( " " , lb Ref Str , load Balancer // Make sure we know which IP address will be used and have properly reserved // it as static before moving forward with the rest of our operations. // // We use static IP addresses when updating a load balancer to ensure that we // can replace the load balancer's other components without changing the // address its service is reachable on. We do it this way rather than always // keeping the static IP around even though this is more complicated because // it makes it less likely that we'll run into quota issues. Only 7 static // IP addresses are allowed per region by default. // // We could let an IP be allocated for us when the forwarding rule is created, // but we need the IP to set up the firewall rule, and we want to keep the // forwarding rule creation as the last thing that needs to be done in this // function in order to maintain the invariant that "if the forwarding rule // exists, the LB has been fully created". ip Address To // Through this process we try to keep track of whether it is safe to // release the IP that was allocated. If the user specifically asked for // an IP, we assume they are managing it themselves. Otherwise, we will // release the IP in case of early-terminating failure or upon successful // creating of the LB. // TODO(#36535): boil this logic down into a set of component functions // and key the flag values off of errors returned. is User Owned is Safe To Release defer func ( ) { if is User Owned if is Safe To Release IP { if err := g . Delete Region Address ( load Balancer Name , g . region ) ; err != nil && ! is Not Found ( err ) { klog . Errorf ( " " , lb Ref Str , ip Address To } else if is Not Found ( err ) { klog . V ( 2 ) . Infof ( " " , lb Ref Str , ip Address To } else { klog . Infof ( " " , lb Ref Str , ip Address To } else { klog . Warningf ( " " , lb Ref Str , ip Address To if requested IP != " " { // If user requests a specific IP address, verify first. No mutation to // the GCE resources will be performed in the verification process. is User Owned IP , err = verify User Requested IP ( g , g . region , requested IP , fwd Rule IP , lb Ref Str , net ip Address To Use = requested if ! is User Owned IP { // If we are not using the user-owned IP, either promote the // emphemeral IP used by the fwd rule, or create a new static IP. ip Addr , existed , err := ensure Static IP ( g , load Balancer Name , service Name . String ( ) , g . region , fwd Rule IP , net if err != nil { return nil , fmt . Errorf ( " " , lb Ref klog . Infof ( " " , lb Ref Str , ip Addr , net // If the IP was not owned by the user, but it already existed, it // could indicate that the previous update cycle failed. We can use // this IP and try to run through the process again, but we should // not release the IP unless it is explicitly flagged as OK. is Safe To Release ip Address To Use = ip // Deal with the firewall next. The reason we do this here rather than last // is because the forwarding rule is used as the indicator that the load // balancer is fully created - it's what get Load Balancer checks for. // Check if user specified the allow source range source Ranges , err := servicehelpers . Get Load Balancer Source Ranges ( api firewall Exists , firewall Needs Update , err := g . firewall Needs Update ( load Balancer Name , service Name . String ( ) , g . region , ip Address To Use , ports , source if firewall Needs Update { desc := make Firewall Description ( service Name . String ( ) , ip Address To // Unlike forwarding rules and target pools, firewalls can be updated // without needing to be deleted and recreated. if firewall Exists { klog . Infof ( " " , lb Ref if err := g . update Firewall ( api Service , Make Firewall Name ( load Balancer Name ) , g . region , desc , source klog . Infof ( " " , lb Ref } else { klog . Infof ( " " , lb Ref if err := g . create Firewall ( api Service , Make Firewall Name ( load Balancer Name ) , g . region , desc , source klog . Infof ( " " , lb Ref tp Exists , tp Needs Recreation , err := g . target Pool Needs Recreation ( load Balancer Name , g . region , api Service . Spec . Session if ! tp Exists { klog . Infof ( " " , lb Ref // Check which health check needs to create and which health check needs to delete. // Health check management is coupled with target pool operation to prevent leaking. var hc To Create , hc To Delete * compute . Http Health hc Local Traffic Existing , err := g . Get HTTP Health Check ( load Balancer if err != nil && ! is HTTP Error Code ( err , http . Status Not Found ) { return nil , fmt . Errorf ( " " , lb Ref if path , health Check Node Port := servicehelpers . Get Service Health Check Path Port ( api Service ) ; path != " " { klog . V ( 4 ) . Infof ( " " , lb Ref Str , health Check Node if hc Local Traffic Existing == nil { // This logic exists to detect a transition for non-Only Local to Only Local service // turn on the tp Needs Recreation flag to delete/recreate fwdrule/tpool updating the // target pool to use local traffic health check. klog . V ( 2 ) . Infof ( " " , lb Ref if supports Nodes Health Check { hc To Delete = make HTTP Health Check ( Make Nodes Health Check Name ( cluster ID ) , Get Nodes Health Check Path ( ) , Get Nodes Health Check tp Needs hc To Create = make HTTP Health Check ( load Balancer Name , path , health Check Node } else { klog . V ( 4 ) . Infof ( " " , lb Ref if hc Local Traffic Existing != nil { // This logic exists to detect a transition from Only Local to non-Only Local service // and turn on the tp Needs Recreation flag to delete/recreate fwdrule/tpool updating the // target pool to use nodes health check. klog . V ( 2 ) . Infof ( " " , lb Ref hc To Delete = hc Local Traffic tp Needs if supports Nodes Health Check { hc To Create = make HTTP Health Check ( Make Nodes Health Check Name ( cluster ID ) , Get Nodes Health Check Path ( ) , Get Nodes Health Check // Now we get to some slightly more interesting logic. // First, neither target pools nor forwarding rules can be updated in place - // they have to be deleted and recreated. // Second, forwarding rules are layered on top of target pools in that you // can't delete a target pool that's currently in use by a forwarding rule. // Thus, we have to tear down the forwarding rule if either it or the target // pool needs to be updated. if fwd Rule Exists && ( fwd Rule Needs Update || tp Needs Recreation ) { // Begin critical section. If we have to delete the forwarding rule, // and something should fail before we recreate it, don't release the // IP. That way we can come back to it later. is Safe To Release if err := g . Delete Region Forwarding Rule ( load Balancer Name , g . region ) ; err != nil && ! is Not Found ( err ) { return nil , fmt . Errorf ( " " , lb Ref klog . Infof ( " " , lb Ref if err := g . ensure Target Pool And Health Check ( tp Exists , tp Needs Recreation , api Service , load Balancer Name , cluster ID , ip Address To Use , hosts , hc To Create , hc To if tp Needs Recreation || fwd Rule Needs Update { klog . Infof ( " " , lb Ref Str , ip Address To Use , net if err := create Forwarding Rule ( g , load Balancer Name , service Name . String ( ) , g . region , ip Address To Use , g . target Pool URL ( load Balancer Name ) , ports , net Tier ) ; err != nil { return nil , fmt . Errorf ( " " , lb Ref // End critical section. It is safe to release the static IP (which // just demotes it to ephemeral) now that it is attached. In the case // of a user-requested IP, the "is user-owned" flag will be set, // preventing it from actually being released. is Safe To Release klog . Infof ( " " , lb Ref Str , ip Address To status := & v1 . Load Balancer status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : ip Address To } 
func ( g * Cloud ) update External Load Balancer ( cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node ) error { hosts , err := g . get Instances By Names ( node load Balancer Name := g . Get Load Balancer Name ( context . TODO ( ) , cluster return g . update Target Pool ( load Balancer } 
func ( g * Cloud ) ensure External Load Balancer Deleted ( cluster Name , cluster ID string , service * v1 . Service ) error { load Balancer Name := g . Get Load Balancer Name ( context . TODO ( ) , cluster service Name := types . Namespaced lb Ref Str := fmt . Sprintf ( " " , load Balancer Name , service var hc if path , _ := servicehelpers . Get Service Health Check Path Port ( service ) ; path != " " { hc To Delete , err := g . Get HTTP Health Check ( load Balancer if err != nil && ! is HTTP Error Code ( err , http . Status Not Found ) { klog . Infof ( " " , lb Ref // If we got 'Status Not Found' LB was already deleted and it's safe to ignore. if err == nil { hc Names = append ( hc Names , hc To } else { // Ensure Load Balancer Deleted() could be triggered by changing service from // Load Balancer type to others. In this case we have no idea whether it was // using local traffic health check or nodes health check. Attempt to delete // both to prevent leaking. hc Names = append ( hc Names , load Balancer hc Names = append ( hc Names , Make Nodes Health Check Name ( cluster errs := utilerrors . Aggregate Goroutines ( func ( ) error { klog . Infof ( " " , lb Ref fw Name := Make Firewall Name ( load Balancer err := ignore Not Found ( g . Delete Firewall ( fw if is Forbidden ( err ) && g . On XPN ( ) { klog . V ( 4 ) . Infof ( " " , lb Ref Str , fw g . raise Firewall Change Needed Event ( service , Firewall To G Cloud Delete Cmd ( fw Name , g . Network Project } , // Even though we don't hold on to static I Ps for load balancers, it's // possible that Ensure Load Balancer left one around in a failed // creation/update attempt, so make sure we clean it up here just in case. func ( ) error { klog . Infof ( " " , lb Ref return ignore Not Found ( g . Delete Region Address ( load Balancer } , func ( ) error { klog . Infof ( " " , lb Ref // The forwarding rule must be deleted before either the target pool can, // unfortunately, so we have to do these two serially. if err := ignore Not Found ( g . Delete Region Forwarding Rule ( load Balancer klog . Infof ( " " , lb Ref if err := g . Delete External Target Pool And Checks ( service , load Balancer Name , g . region , cluster ID , hc } 
func ( g * Cloud ) Delete External Target Pool And Checks ( service * v1 . Service , name , region , cluster ID string , hc Names ... string ) error { service Name := types . Namespaced lb Ref Str := fmt . Sprintf ( " " , name , service if err := g . Delete Target Pool ( name , region ) ; err != nil && is HTTP Error Code ( err , http . Status Not Found ) { klog . Infof ( " " , lb Ref } else if err != nil { klog . Warningf ( " " , lb Ref // Deletion of health checks is allowed only after the Target Pool reference is deleted for _ , hc Name := range hc Names { if err := func ( ) error { // Check whether it is nodes health check, which has different name from the load-balancer. is Nodes Health Check := hc if is Nodes Health Check { // Lock to prevent deleting necessary nodes health check before it gets attached // to target pool. g . shared Resource defer g . shared Resource klog . Infof ( " " , lb Ref Str , hc if err := g . Delete HTTP Health Check ( hc Name ) ; err != nil { // Delete nodes health checks will fail if any other target pool is using it. if is In Used By Error ( err ) { klog . V ( 4 ) . Infof ( " " , lb Ref Str , hc } else if ! is HTTP Error Code ( err , http . Status Not Found ) { klog . Warningf ( " " , lb Ref Str , hc // Status Not Found could happen when: // - This is the first attempt but we pass in a healthcheck that is already deleted // to prevent leaking. // - This is the first attempt but user manually deleted the heathcheck. // - This is a retry and in previous round we failed to delete the healthcheck firewall // after deleted the healthcheck. // We continue to delete the healthcheck firewall to prevent leaking. klog . V ( 4 ) . Infof ( " " , lb Ref Str , hc // If health check is deleted without error, it means no load-balancer is using it. // So we should delete the health check firewall as well. fw Name := Make Health Check Firewall Name ( cluster ID , hc Name , is Nodes Health klog . Infof ( " " , lb Ref Str , fw if err := ignore Not Found ( g . Delete Firewall ( fw Name ) ) ; err != nil { if is Forbidden ( err ) && g . On XPN ( ) { klog . V ( 4 ) . Infof ( " " , lb Ref Str , fw g . raise Firewall Change Needed Event ( service , Firewall To G Cloud Delete Cmd ( fw Name , g . Network Project } 
func verify User Requested IP ( s Cloud Address Service , region , requested IP , fwd Rule IP , lb Ref string , desired Net Tier cloud . Network Tier ) ( is User Owned IP bool , err error ) { if requested // If a specific IP address has been requested, we have to respect the // user's request and use that IP. If the forwarding rule was already using // a different IP, it will be harmlessly abandoned because it was only an // ephemeral IP (or it was a different static IP owned by the user, in which // case we shouldn't delete it anyway). existing Address , err := s . Get Region Address By IP ( region , requested if err != nil && ! is Not Found ( err ) { klog . Errorf ( " " , requested IP , lb if err == nil { // The requested IP is a static IP, owned and managed by the user. // Check if the network tier of the static IP matches the desired // network tier. net Tier Str , err := s . get Network Tier From Address ( existing if err != nil { return false , fmt . Errorf ( " " , requested net Tier := cloud . Network Tier GCE Value To Type ( net Tier if net Tier != desired Net Tier { klog . Errorf ( " " , requested IP , existing Address . Name , lb Ref , net Tier , desired Net return false , fmt . Errorf ( " " , requested IP , net Tier , desired Net klog . V ( 4 ) . Infof ( " " , requested IP , existing Address . Name , net Tier , lb if requested IP == fwd Rule IP { // The requested IP is not a static IP, but is currently assigned // to this forwarding rule, so we can just use it. klog . V ( 4 ) . Infof ( " " , requested IP , lb // The requested IP is not static and it is not assigned to the // current forwarding rule. It might be attached to a different // rule or it might not be part of this project at all. Either // way, we can't use it. klog . Errorf ( " " , requested IP , lb return false , fmt . Errorf ( " " , requested } 
func merge HTTP Health Checks ( hc , new HC * compute . Http Health Check ) * compute . Http Health Check { if hc . Check Interval Sec > new HC . Check Interval Sec { new HC . Check Interval Sec = hc . Check Interval if hc . Timeout Sec > new HC . Timeout Sec { new HC . Timeout Sec = hc . Timeout if hc . Unhealthy Threshold > new HC . Unhealthy Threshold { new HC . Unhealthy Threshold = hc . Unhealthy if hc . Healthy Threshold > new HC . Healthy Threshold { new HC . Healthy Threshold = hc . Healthy return new } 
func need To Update HTTP Health Checks ( hc , new HC * compute . Http Health Check ) bool { changed := hc . Port != new HC . Port || hc . Request Path != new HC . Request Path || hc . Description != new changed = changed || hc . Check Interval Sec < new HC . Check Interval Sec || hc . Timeout Sec < new HC . Timeout changed = changed || hc . Unhealthy Threshold < new HC . Unhealthy Threshold || hc . Healthy Threshold < new HC . Healthy } 
func ( g * Cloud ) forwarding Rule Needs Update ( name , region string , load Balancer IP string , ports [ ] v1 . Service Port ) ( exists bool , needs Update bool , ip Address string , err error ) { fwd , err := g . Get Region Forwarding if err != nil { if is HTTP Error Code ( err , http . Status Not // If the user asks for a specific static ip through the Service spec, // check that we're actually using it. // TODO: we report loadbalancer IP through status, so we want to verify if // that matches the forwarding rule as well. if load Balancer IP != " " && load Balancer IP != fwd . IP Address { klog . Infof ( " " , fwd . Name , fwd . IP Address , load Balancer return true , true , fwd . IP port Range , err := load Balancer Port if port Range != fwd . Port Range { klog . Infof ( " " , fwd . Name , fwd . Port Range , port return true , true , fwd . IP // The service controller verified all the protocols match on the ports, just check the first one if string ( ports [ 0 ] . Protocol ) != fwd . IP Protocol { klog . Infof ( " " , fwd . Name , fwd . IP return true , true , fwd . IP return true , false , fwd . IP } 
func ( g * Cloud ) target Pool Needs Recreation ( name , region string , affinity Type v1 . Service Affinity ) ( exists bool , needs Recreation bool , err error ) { tp , err := g . Get Target if err != nil { if is HTTP Error Code ( err , http . Status Not // TODO: If the user modifies their Service's session affinity, it *should* // reflect in the associated target pool. However, currently not setting the // session affinity on a target pool defaults it to the empty string while // not setting in on a Service defaults it to None. There is a lack of // documentation around the default setting for the target pool, so if we // find it's the undocumented empty string, don't blindly recreate the // target pool (which results in downtime). Fix this when we have formally // defined the defaults on either side. if tp . Session Affinity != " " && translate Affinity Type ( affinity Type ) != tp . Session Affinity { klog . Infof ( " " , name , tp . Session Affinity , affinity } 
func delete FWD Rule With Wrong Tier ( s Cloud Forwarding Rule Service , region , name , log Prefix string , desired Net Tier cloud . Network Tier ) error { tier Str , err := s . get Network Tier From Forwarding if is Not existing Tier := cloud . Network Tier GCE Value To Type ( tier if existing Tier == desired Net klog . V ( 2 ) . Infof ( " " , log Prefix , existing Tier , desired Net err = s . Delete Region Forwarding return ignore Not } 
func delete Address With Wrong Tier ( s Cloud Address Service , region , name , log Prefix string , desired Net Tier cloud . Network Tier ) error { // We only check the IP address matching the reserved name that the // controller assigned to the LB. We make the assumption that an address of // such name is owned by the controller and is safe to release. Whether an // IP is owned by the user is not clearly defined in the current code, and // this assumption may not match some of the existing logic in the code. // However, this is okay since network tiering is still Alpha and will be // properly gated. // TODO(#51665): Re-evaluate the "ownership" of the IP address to ensure // we don't release IP unintentionally. tier Str , err := s . get Network Tier From if is Not existing Tier := cloud . Network Tier GCE Value To Type ( tier if existing Tier == desired Net klog . V ( 2 ) . Infof ( " " , log Prefix , existing Tier , desired Net err = s . Delete Region return ignore Not } 
func disk Set Up ( manager disk Manager , b rbd Mounter , vol Path string , mounter mount . Interface , fs Group * int64 ) error { global PD Path := manager . Make Global PD not Mnt , err := mounter . Is Likely Not Mount Point ( global PD if err != nil && ! os . Is Not Exist ( err ) { klog . Errorf ( " " , global PD if not Mnt { return fmt . Errorf ( " " , global PD not Mnt , err = mounter . Is Likely Not Mount Point ( vol if err != nil && ! os . Is Not Exist ( err ) { klog . Errorf ( " " , vol if ! not if err := os . Mkdir All ( vol Path , 0750 ) ; err != nil { klog . Errorf ( " " , vol if ( & b ) . Get Attributes ( ) . Read mount Options := util . Join Mount Options ( b . mount err = mounter . Mount ( global PD Path , vol Path , " " , mount if err != nil { klog . Errorf ( " " , global PD klog . V ( 3 ) . Infof ( " " , global PD Path , vol Path , mount if ! b . Read Only { volume . Set Volume Ownership ( & b , fs } 
func disk Tear Down ( manager disk Manager , c rbd Unmounter , vol Path string , mounter mount . Interface ) error { not Mnt , err := mounter . Is Likely Not Mount Point ( vol if err != nil && ! os . Is Not Exist ( err ) { klog . Errorf ( " " , vol if not Mnt { klog . V ( 3 ) . Infof ( " " , vol return os . Remove ( vol // Unmount the bind-mount inside this pod. if err := mounter . Unmount ( vol Path ) ; err != nil { klog . Errorf ( " " , vol not Mnt , mnt Err := mounter . Is Likely Not Mount Point ( vol if mnt Err != nil && ! os . Is Not Exist ( mnt Err ) { klog . Errorf ( " " , mnt return mnt if not Mnt { if err := os . Remove ( vol Path ) ; err != nil { klog . V ( 2 ) . Info ( " " , vol } 
} 
func Is Webhook Configuration Resource ( attr admission . Attributes ) bool { gvk := attr . Get } 
func ( in * Mutating Webhook Configuration ) Deep Copy ( ) * Mutating Webhook out := new ( Mutating Webhook in . Deep Copy } 
func ( in * Mutating Webhook Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Mutating Webhook Configuration List ) Deep Copy Into ( out * Mutating Webhook Configuration out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Mutating Webhook for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Mutating Webhook Configuration List ) Deep Copy ( ) * Mutating Webhook Configuration out := new ( Mutating Webhook Configuration in . Deep Copy } 
func ( in * Mutating Webhook Configuration List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Rule ) Deep Copy if in . API Groups != nil { in , out := & in . API Groups , & out . API if in . API Versions != nil { in , out := & in . API Versions , & out . API * out = new ( Scope } 
func ( in * Rule ) Deep in . Deep Copy } 
func ( in * Rule With Operations ) Deep Copy Into ( out * Rule With * out = make ( [ ] Operation in . Rule . Deep Copy } 
func ( in * Rule With Operations ) Deep Copy ( ) * Rule With out := new ( Rule With in . Deep Copy } 
func ( in * Service Reference ) Deep Copy Into ( out * Service } 
func ( in * Service Reference ) Deep Copy ( ) * Service out := new ( Service in . Deep Copy } 
func ( in * Validating Webhook Configuration ) Deep Copy Into ( out * Validating Webhook out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Validating Webhook Configuration ) Deep Copy ( ) * Validating Webhook out := new ( Validating Webhook in . Deep Copy } 
func ( in * Validating Webhook Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Validating Webhook Configuration List ) Deep Copy Into ( out * Validating Webhook Configuration out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Validating Webhook for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Validating Webhook Configuration List ) Deep Copy ( ) * Validating Webhook Configuration out := new ( Validating Webhook Configuration in . Deep Copy } 
func ( in * Validating Webhook Configuration List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Webhook ) Deep Copy in . Client Config . Deep Copy Into ( & out . Client * out = make ( [ ] Rule With for i := range * in { ( * in ) [ i ] . Deep Copy if in . Failure Policy != nil { in , out := & in . Failure Policy , & out . Failure * out = new ( Failure Policy if in . Namespace Selector != nil { in , out := & in . Namespace Selector , & out . Namespace * out = new ( v1 . Label ( * in ) . Deep Copy if in . Side Effects != nil { in , out := & in . Side Effects , & out . Side * out = new ( Side Effect if in . Timeout Seconds != nil { in , out := & in . Timeout Seconds , & out . Timeout if in . Admission Review Versions != nil { in , out := & in . Admission Review Versions , & out . Admission Review } 
func ( in * Webhook ) Deep in . Deep Copy } 
func ( in * Webhook Client Config ) Deep Copy Into ( out * Webhook Client * out = new ( Service ( * in ) . Deep Copy if in . CA Bundle != nil { in , out := & in . CA Bundle , & out . CA } 
func ( in * Webhook Client Config ) Deep Copy ( ) * Webhook Client out := new ( Webhook Client in . Deep Copy } 
func Get Hostname ( hostname Override string ) ( string , error ) { host Name := hostname if len ( host Name ) == 0 { node host Name = node // Trim whitespaces first to avoid getting an empty hostname // For linux, the hostname is read from file /proc/sys/kernel/hostname directly host Name = strings . Trim Space ( host if len ( host return strings . To Lower ( host } 
func Get Preferred Node Address ( node * v1 . Node , preferred Address Types [ ] v1 . Node Address Type ) ( string , error ) { for _ , address Type := range preferred Address Types { for _ , address := range node . Status . Addresses { if address . Type == address return " " , & No Match } 
func Get Node Host address Map := make ( map [ v1 . Node Address Type ] [ ] v1 . Node for i := range addresses { address Map [ addresses [ i ] . Type ] = append ( address if addresses , ok := address Map [ v1 . Node Internal IP ] ; ok { return net . Parse if addresses , ok := address Map [ v1 . Node External IP ] ; ok { return net . Parse } 
func Get Node IP ( client clientset . Interface , hostname string ) net . IP { var node node , err := client . Core V1 ( ) . Nodes ( ) . Get ( hostname , metav1 . Get node IP , err = Get Node Host return node } 
func Get Zone region , _ := labels [ v1 . Label Zone failure Domain , _ := labels [ v1 . Label Zone Failure if region == " " && failure // We include the null character just in case region or failure Domain has a colon // (We do assume there's no null characters in a region or failure Domain) // As a nice side-benefit, the null character is not printed by fmt.Print or glog return region + " \x00 " + failure } 
func Set Node Condition ( c clientset . Interface , node types . Node Name , condition v1 . Node Condition ) error { generate Patch := func ( condition v1 . Node Condition ) ( [ ] byte , error ) { raw , err := json . Marshal ( & [ ] v1 . Node condition . Last Heartbeat Time = metav1 . New patch , err := generate _ , err = c . Core V1 ( ) . Nodes ( ) . Patch } 
func Patch Node CIDR ( c clientset . Interface , node types . Node patch Bytes := [ ] byte ( fmt . Sprintf ( `{"spec":{"pod if _ , err := c . Core V1 ( ) . Nodes ( ) . Patch ( string ( node ) , types . Strategic Merge Patch Type , patch } 
func Patch Node Status ( c v1core . Core V1Interface , node Name types . Node Name , old Node * v1 . Node , new Node * v1 . Node ) ( * v1 . Node , [ ] byte , error ) { patch Bytes , err := prepare Patch Bytesfor Node Status ( node Name , old Node , new updated Node , err := c . Nodes ( ) . Patch ( string ( node Name ) , types . Strategic Merge Patch Type , patch if err != nil { return nil , nil , fmt . Errorf ( " " , patch Bytes , node return updated Node , patch } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( core . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1 . Scheme Group } 
func Create Init Static Pod Manifest Files ( manifest Dir string , cfg * kubeadmapi . Init return Create Static Pod Files ( manifest Dir , & cfg . Cluster Configuration , & cfg . Local API Endpoint , kubeadmconstants . Kube API Server , kubeadmconstants . Kube Controller Manager , kubeadmconstants . Kube } 
func Get Static Pod Specs ( cfg * kubeadmapi . Cluster Configuration , endpoint * kubeadmapi . API Endpoint , k8s Version * version . Version ) map [ string ] v1 . Pod { // Get the required hostpath mounts mounts := get Host Path Volumes For The Control // Prepare static pod specs static Pod Specs := map [ string ] v1 . Pod { kubeadmconstants . Kube API Server : staticpodutil . Component Pod ( v1 . Container { Name : kubeadmconstants . Kube API Server , Image : images . Get Kubernetes Image ( kubeadmconstants . Kube API Server , cfg ) , Image Pull Policy : v1 . Pull If Not Present , Command : get API Server Command ( cfg , endpoint ) , Volume Mounts : staticpodutil . Volume Mount Map To Slice ( mounts . Get Volume Mounts ( kubeadmconstants . Kube API Server ) ) , Liveness Probe : liveness Probe ( staticpodutil . Get API Server Probe Address ( endpoint ) , int ( endpoint . Bind Port ) , v1 . URI Scheme HTTPS ) , Resources : staticpodutil . Component Resources ( " " ) , Env : get Proxy Env Vars ( ) , } , mounts . Get Volumes ( kubeadmconstants . Kube API Server ) ) , kubeadmconstants . Kube Controller Manager : staticpodutil . Component Pod ( v1 . Container { Name : kubeadmconstants . Kube Controller Manager , Image : images . Get Kubernetes Image ( kubeadmconstants . Kube Controller Manager , cfg ) , Image Pull Policy : v1 . Pull If Not Present , Command : get Controller Manager Command ( cfg , k8s Version ) , Volume Mounts : staticpodutil . Volume Mount Map To Slice ( mounts . Get Volume Mounts ( kubeadmconstants . Kube Controller Manager ) ) , Liveness Probe : liveness Probe ( staticpodutil . Get Controller Manager Probe Address ( cfg ) , ports . Insecure Kube Controller Manager Port , v1 . URI Scheme HTTP ) , Resources : staticpodutil . Component Resources ( " " ) , Env : get Proxy Env Vars ( ) , } , mounts . Get Volumes ( kubeadmconstants . Kube Controller Manager ) ) , kubeadmconstants . Kube Scheduler : staticpodutil . Component Pod ( v1 . Container { Name : kubeadmconstants . Kube Scheduler , Image : images . Get Kubernetes Image ( kubeadmconstants . Kube Scheduler , cfg ) , Image Pull Policy : v1 . Pull If Not Present , Command : get Scheduler Command ( cfg ) , Volume Mounts : staticpodutil . Volume Mount Map To Slice ( mounts . Get Volume Mounts ( kubeadmconstants . Kube Scheduler ) ) , Liveness Probe : liveness Probe ( staticpodutil . Get Scheduler Probe Address ( cfg ) , ports . Insecure Scheduler Port , v1 . URI Scheme HTTP ) , Resources : staticpodutil . Component Resources ( " " ) , Env : get Proxy Env Vars ( ) , } , mounts . Get Volumes ( kubeadmconstants . Kube return static Pod } 
func Create Static Pod Files ( manifest Dir string , cfg * kubeadmapi . Cluster Configuration , endpoint * kubeadmapi . API Endpoint , component Names ... string ) error { // TODO: Move the "pkg/util/version".Version object into the internal API instead of always parsing the string k8s Version , err := version . Parse Semantic ( cfg . Kubernetes // gets the Static Pod Specs, actualized for the current Cluster specs := Get Static Pod Specs ( cfg , endpoint , k8s // creates required static pod specs for _ , component Name := range component Names { // retrieves the Static Pod Spec for given component spec , exists := specs [ component if ! exists { return errors . Errorf ( " " , component // writes the Static Pod Spec to disk if err := staticpodutil . Write Static Pod To Disk ( component Name , manifest Dir , spec ) ; err != nil { return errors . Wrapf ( err , " " , component klog . V ( 1 ) . Infof ( " \n " , component Name , kubeadmconstants . Get Static Pod Filepath ( component Name , manifest } 
func get API Server Command ( cfg * kubeadmapi . Cluster Configuration , local API Endpoint * kubeadmapi . API Endpoint ) [ ] string { default Arguments := map [ string ] string { " " : local API Endpoint . Advertise Address , " " : " " , " " : " " , " " : cfg . Networking . Service Subnet , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Service Account Public Key Name ) , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . CA Cert Name ) , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . API Server Cert Name ) , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . API Server Key Name ) , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . API Server Kubelet Client Cert Name ) , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . API Server Kubelet Client Key Name ) , " " : " " , " " : fmt . Sprintf ( " " , local API Endpoint . Bind Port ) , " " : " " , " " : " " , // add options to configure the front proxy. Without the generated client cert, this will never be useable // so add it unconditionally with recommended values " " : " " , " " : " " , " " : " " , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Front Proxy CA Cert Name ) , " " : " " , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Front Proxy Client Cert Name ) , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Front Proxy Client Key // If the user set endpoints for an external etcd cluster if cfg . Etcd . External != nil { default // Use any user supplied etcd certificates if cfg . Etcd . External . CA File != " " { default Arguments [ " " ] = cfg . Etcd . External . CA if cfg . Etcd . External . Cert File != " " && cfg . Etcd . External . Key File != " " { default Arguments [ " " ] = cfg . Etcd . External . Cert default Arguments [ " " ] = cfg . Etcd . External . Key } else { // Default to etcd static pod on localhost default Arguments [ " " ] = fmt . Sprintf ( " " , kubeadmconstants . Etcd Listen Client default Arguments [ " " ] = filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Etcd CA Cert default Arguments [ " " ] = filepath . Join ( cfg . Certificates Dir , kubeadmconstants . API Server Etcd Client Cert default Arguments [ " " ] = filepath . Join ( cfg . Certificates Dir , kubeadmconstants . API Server Etcd Client Key // Apply user configurations for local etcd if cfg . Etcd . Local != nil { if value , ok := cfg . Etcd . Local . Extra Args [ " " ] ; ok { default if cfg . API Server . Extra Args == nil { cfg . API Server . Extra cfg . API Server . Extra Args [ " " ] = get Authz Modes ( cfg . API Server . Extra command = append ( command , kubeadmutil . Build Argument List From Map ( default Arguments , cfg . API Server . Extra } 
func get Authz Modes ( authz Mode Extra Args string ) string { modes := [ ] string { authzmodes . Mode Node , authzmodes . Mode if strings . Contains ( authz Mode Extra Args , authzmodes . Mode ABAC ) { modes = append ( modes , authzmodes . Mode if strings . Contains ( authz Mode Extra Args , authzmodes . Mode Webhook ) { modes = append ( modes , authzmodes . Mode } 
func calc Node Cidr Size ( pod Subnet string ) string { mask if ip , pod Cidr , err := net . Parse CIDR ( pod Subnet ) ; err == nil { if ip . To4 ( ) == nil { var node Cidr pod Net Size , total Bits := pod switch { case pod Net Size == 112 : // Special case, allows 256 nodes, 256 pods/node node Cidr case pod Net Size < 112 : // Use multiple of 8 for node CIDR, with 512 to 64K nodes node Cidr Size = total Bits - ( ( total Bits - pod Net default : // Not enough bits, will fail later, when validate node Cidr Size = pod Net mask Size = strconv . Itoa ( node Cidr return mask } 
func get Controller Manager Command ( cfg * kubeadmapi . Cluster Configuration , k8s Version * version . Version ) [ ] string { kubeconfig File := filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Controller Manager Kube Config File ca File := filepath . Join ( cfg . Certificates Dir , kubeadmconstants . CA Cert default Arguments := map [ string ] string { " " : " " , " " : " " , " " : kubeconfig File , " " : kubeconfig File , " " : kubeconfig File , " " : ca File , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Front Proxy CA Cert Name ) , " " : ca File , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Service Account Private Key Name ) , " " : ca File , " " : filepath . Join ( cfg . Certificates Dir , kubeadmconstants . CA Key // If using external CA, pass empty string to controller manager instead of ca.key/ca.crt path, // so that the csrsigning controller fails to start if res , _ := certphase . Using External CA ( cfg ) ; res { default default // Let the controller-manager allocate Node CID Rs for the Pod network. // Each node will get a subspace of the address CIDR provided with --pod-network-cidr. if cfg . Networking . Pod Subnet != " " { mask Size := calc Node Cidr Size ( cfg . Networking . Pod default default Arguments [ " " ] = cfg . Networking . Pod default Arguments [ " " ] = mask command = append ( command , kubeadmutil . Build Argument List From Map ( default Arguments , cfg . Controller Manager . Extra } 
func get Scheduler Command ( cfg * kubeadmapi . Cluster Configuration ) [ ] string { default Arguments := map [ string ] string { " " : " " , " " : " " , " " : filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Scheduler Kube Config File command = append ( command , kubeadmutil . Build Argument List From Map ( default Arguments , cfg . Scheduler . Extra } 
func get Proxy Env Vars ( ) [ ] v1 . Env Var { envs := [ ] v1 . Env if strings . Has Suffix ( strings . To Lower ( name ) , " " ) && value != " " { env Var := v1 . Env envs = append ( envs , env } 
func ( in * Node Lifecycle Controller Configuration ) Deep Copy Into ( out * Node Lifecycle Controller out . Node Startup Grace Period = in . Node Startup Grace out . Node Monitor Grace Period = in . Node Monitor Grace out . Pod Eviction Timeout = in . Pod Eviction } 
func ( in * Node Lifecycle Controller Configuration ) Deep Copy ( ) * Node Lifecycle Controller out := new ( Node Lifecycle Controller in . Deep Copy } 
func Repack Subsets ( subsets [ ] api . Endpoint Subset ) [ ] api . Endpoint Subset { // First map each unique port definition to the sets of hosts that // offer it. all Addrs := map [ address Key ] * api . Endpoint port To Addr Ready Map := map [ api . Endpoint Port ] address for i := range subsets { if len ( subsets [ i ] . Ports ) == 0 { // Don't discard endpoints with no ports defined, use a sentinel. map Addresses By Port ( & subsets [ i ] , api . Endpoint Port { Port : - 1 } , all Addrs , port To Addr Ready } else { for _ , port := range subsets [ i ] . Ports { map Addresses By Port ( & subsets [ i ] , port , all Addrs , port To Addr Ready // Next, map the sets of hosts to the sets of ports they offer. // Go does not allow maps or slices as keys to maps, so we have // to synthesize an artificial key and do a sort of 2-part // associative entity. type key key To Addr Ready Map := map [ key String ] address addr Ready Map Key To Ports := map [ key String ] [ ] api . Endpoint for port , addrs := range port To Addr Ready Map { key := key String ( hash key To Addr Ready if port . Port > 0 { // avoid sentinels addr Ready Map Key To Ports [ key ] = append ( addr Ready Map Key To } else { if _ , found := addr Ready Map Key To Ports [ key ] ; ! found { // Force it to be present in the map addr Ready Map Key To // Next, build the N-to-M association the API wants. final := [ ] api . Endpoint for key , ports := range addr Ready Map Key To Ports { var ready Addrs , not Ready Addrs [ ] api . Endpoint for addr , ready := range key To Addr Ready Map [ key ] { if ready { ready Addrs = append ( ready } else { not Ready Addrs = append ( not Ready final = append ( final , api . Endpoint Subset { Addresses : ready Addrs , Not Ready Addresses : not Ready // Finally, sort it. return Sort } 
func map Addresses By Port ( subset * api . Endpoint Subset , port api . Endpoint Port , all Addrs map [ address Key ] * api . Endpoint Address , port To Addr Ready Map map [ api . Endpoint Port ] address Set ) { for k := range subset . Addresses { map Address By Port ( & subset . Addresses [ k ] , port , true , all Addrs , port To Addr Ready for k := range subset . Not Ready Addresses { map Address By Port ( & subset . Not Ready Addresses [ k ] , port , false , all Addrs , port To Addr Ready } 
func New Pbm Client ( ctx context . Context , client * vim25 . Client ) ( * Pbm Client , error ) { pbm Client , err := pbm . New return & Pbm Client { pbm } 
func ( pbm Client * Pbm Client ) Is Datastore Compatible ( ctx context . Context , storage Policy ID string , datastore * Datastore ) ( bool , string , error ) { fault placement Hub := pbmtypes . Pbm Placement Hub { Hub Type : datastore . Reference ( ) . Type , Hub hubs := [ ] pbmtypes . Pbm Placement Hub { placement req := [ ] pbmtypes . Base Pbm Placement Requirement { & pbmtypes . Pbm Placement Capability Profile Requirement { Profile Id : pbmtypes . Pbm Profile Id { Unique Id : storage Policy compatibility Result , err := pbm Client . Check if compatibility Result != nil && len ( compatibility Result ) > 0 { compatible Hubs := compatibility Result . Compatible if compatible Hubs != nil && len ( compatible ds Name , err := datastore . Object if compatibility Result [ 0 ] . Error [ 0 ] . Localized Message == " " { fault Message = " " + ds } else { fault Message = " " + ds Name + " " + compatibility Result [ 0 ] . Error [ 0 ] . Localized return false , fault } 
func ( pbm Client * Pbm Client ) Get Compatible Datastores ( ctx context . Context , dc * Datacenter , storage Policy ID string , datastores [ ] * Datastore Info ) ( [ ] * Datastore Info , string , error ) { var ( ds Mor Name Map = get Ds Mor Name localized Messages For Not Compatible compatibility Result , err := pbm Client . Get Placement Compatibility Result ( ctx , storage Policy if err != nil { klog . Errorf ( " " , datastores , storage Policy compatible Hubs := compatibility Result . Compatible var compatible Datastore List [ ] * Datastore for _ , hub := range compatible Hubs { compatible Datastore List = append ( compatible Datastore List , get Datastore From Placement for _ , res := range compatibility Result { for _ , err := range res . Error { ds Name := ds Mor Name Map [ res . Hub . Hub localized if err . Localized Message != " " { localized Message = " " + ds Name + " " + err . Localized } else { localized Message = " " + ds localized Messages For Not Compatible Datastores += localized // Return an error if there are no compatible datastores. if len ( compatible Hubs ) < 1 { klog . Errorf ( " " , storage Policy return nil , localized Messages For Not Compatible return compatible Datastore List , localized Messages For Not Compatible } 
func ( pbm Client * Pbm Client ) Get Placement Compatibility Result ( ctx context . Context , storage Policy ID string , datastore [ ] * Datastore Info ) ( pbm . Placement Compatibility Result , error ) { var hubs [ ] pbmtypes . Pbm Placement for _ , ds := range datastore { hubs = append ( hubs , pbmtypes . Pbm Placement Hub { Hub Type : ds . Reference ( ) . Type , Hub req := [ ] pbmtypes . Base Pbm Placement Requirement { & pbmtypes . Pbm Placement Capability Profile Requirement { Profile Id : pbmtypes . Pbm Profile Id { Unique Id : storage Policy res , err := pbm Client . Check } 
func get Datastore From Placement Hub ( datastore [ ] * Datastore Info , pbm Placement Hub pbmtypes . Pbm Placement Hub ) * Datastore Info { for _ , ds := range datastore { if ds . Reference ( ) . Type == pbm Placement Hub . Hub Type && ds . Reference ( ) . Value == pbm Placement Hub . Hub } 
func get Ds Mor Name Map ( ctx context . Context , datastores [ ] * Datastore Info ) map [ string ] string { ds Mor Name for _ , ds := range datastores { ds Object Name , err := ds . Object if err == nil { ds Mor Name Map [ ds . Reference ( ) . Value ] = ds Object return ds Mor Name } 
func milli CPU To Shares ( milli CPU int64 , hyperv bool ) int64 { var min Shares int64 = min Shares if hyperv { min Shares = min Shares Hyper if milli CPU == 0 { // Return here to really match kernel default for zero milli CPU. return min // Conceptually (milli CPU / milli CPU To CPU) * shares Per CPU, but factored to improve rounding. total CPU := sysinfo . Num shares := ( milli CPU * ( max Shares - min Shares ) ) / int64 ( total CPU ) / milli CPU To if shares < min Shares { return min if shares > max Shares { return max } 
func ( a * cpu Accumulator ) is Socket Free ( socket ID int ) bool { return a . details . CP Us In Socket ( socket ID ) . Size ( ) == a . topo . CP Us Per } 
func ( a * cpu Accumulator ) is Core Free ( core ID int ) bool { return a . details . CP Us In Core ( core ID ) . Size ( ) == a . topo . CP Us Per } 
func ( a * cpu Accumulator ) free Sockets ( ) [ ] int { return a . details . Sockets ( ) . Filter ( a . is Socket Free ) . To } 
func ( a * cpu Accumulator ) free Cores ( ) [ ] int { socket I Ds := a . details . Sockets ( ) . To sort . Slice ( socket I Ds , func ( i , j int ) bool { i Cores := a . details . Cores In Socket ( socket I Ds [ i ] ) . Filter ( a . is Core j Cores := a . details . Cores In Socket ( socket I Ds [ j ] ) . Filter ( a . is Core return i Cores . Size ( ) < j Cores . Size ( ) || socket I Ds [ i ] < socket I core I for _ , s := range socket I Ds { core I Ds = append ( core I Ds , a . details . Cores In Socket ( s ) . Filter ( a . is Core Free ) . To return core I } 
func ( a * cpu Accumulator ) free CP cores := a . details . Cores ( ) . To sort . Slice ( cores , func ( i , j int ) bool { i j i CP Us := a . topo . CPU Details . CP Us In Core ( i Core ) . To j CP Us := a . topo . CPU Details . CP Us In Core ( j Core ) . To i Socket := a . topo . CPU Details [ i CP Us [ 0 ] ] . Socket j Socket := a . topo . CPU Details [ j CP Us [ 0 ] ] . Socket // Compute the number of CP Us in the result reside on the same socket // as each core. i Socket Colo Score := a . topo . CPU Details . CP Us In Socket ( i j Socket Colo Score := a . topo . CPU Details . CP Us In Socket ( j // Compute the number of available CP Us available on the same socket // as each core. i Socket Free Score := a . details . CP Us In Socket ( i j Socket Free Score := a . details . CP Us In Socket ( j // Compute the number of available CP Us on each core. i Core Free Score := a . details . CP Us In Core ( i j Core Free Score := a . details . CP Us In Core ( j return i Socket Colo Score > j Socket Colo Score || i Socket Free Score < j Socket Free Score || i Core Free Score < j Core Free Score || i Socket < j Socket || i Core < j // For each core, append sorted CPU I Ds to result. for _ , core := range cores { result = append ( result , a . details . CP Us In Core ( core ) . To } 
func New Watcher ( sock Dir string , deprecated Sock Dir string ) * Watcher { return & Watcher { path : sock Dir , deprecated Path : deprecated Sock Dir , fs : & utilfs . Default Fs { } , handlers : make ( map [ string ] Plugin Handler ) , plugins : make ( map [ string ] path Info ) , plugins } 
w . stop fs Watcher , err := fsnotify . New w . fs Watcher = fs // Traverse plugin dir and add filesystem watchers before starting the plugin processing goroutine. if err := w . traverse Plugin // Traverse deprecated plugin dir, if specified. if len ( w . deprecated Path ) != 0 { if err := w . traverse Plugin Dir ( w . deprecated return fmt . Errorf ( " " , w . deprecated go func ( fs for { select { case event := <- fs Watcher . Events : //TODO: Handle errors by taking corrective measures if event . Op & fsnotify . Create == fsnotify . Create { err := w . handle Create } else if event . Op & fsnotify . Remove == fsnotify . Remove { err := w . handle Delete case err := <- fs case <- w . stop } ( fs } 
func ( w * Watcher ) Stop ( ) error { close ( w . stop w . fs } 
func ( w * Watcher ) traverse Plugin Dir ( dir string ) error { return w . fs . Walk ( dir , func ( path string , info os . File switch mode := info . Mode ( ) ; { case mode . Is Dir ( ) : if w . contains Blacklisted Dir ( path ) { return filepath . Skip if err := w . fs case mode & os . Mode //TODO: Handle errors by taking corrective measures if err := w . handle Create } 
func ( w * Watcher ) handle Create if w . contains Blacklisted if strings . Has if ! fi . Is Dir ( ) { if fi . Mode ( ) & os . Mode return w . handle Plugin return w . traverse Plugin } 
func dial ( unix Socket Path string , timeout time . Duration ) ( registerapi . Registration Client , * grpc . Client Conn , error ) { ctx , cancel := context . With c , err := grpc . Dial Context ( ctx , unix Socket Path , grpc . With Insecure ( ) , grpc . With Block ( ) , grpc . With Dialer ( func ( addr string , timeout time . Duration ) ( net . Conn , error ) { return net . Dial if err != nil { return nil , nil , fmt . Errorf ( " " , unix Socket return registerapi . New Registration } 
func ( w * Watcher ) contains Blacklisted Dir ( path string ) bool { return strings . Has Prefix ( path , w . deprecated Path + " " ) || path == w . deprecated } 
func New Defaults ( ) * args . Generator Args { generic Args := args . Default ( ) . Without Default Flag generic Args . Output File Base return generic } 
func Validate ( generic Args * args . Generator Args ) error { if len ( generic Args . Output File Base } 
func Is Kubelet Label ( key string ) bool { if kubelet namespace := get Label for allowed Namespace := range kubelet Label Namespaces { if namespace == allowed Namespace || strings . Has Suffix ( namespace , " " + allowed } 
func new Horizontal Pod Autoscalers ( c * Autoscaling V2beta1Client , namespace string ) * horizontal Pod Autoscalers { return & horizontal Pod Autoscalers { client : c . REST } 
func ( c * horizontal Pod Autoscalers ) Get ( name string , options v1 . Get Options ) ( result * v2beta1 . Horizontal Pod Autoscaler , err error ) { result = & v2beta1 . Horizontal Pod err = c . client . Get ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( name ) . Versioned Params ( & options , scheme . Parameter } 
func ( c * horizontal Pod Autoscalers ) Create ( horizontal Pod Autoscaler * v2beta1 . Horizontal Pod Autoscaler ) ( result * v2beta1 . Horizontal Pod Autoscaler , err error ) { result = & v2beta1 . Horizontal Pod err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( horizontal Pod } 
func ( c * horizontal Pod Autoscalers ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v2beta1 . Horizontal Pod Autoscaler , err error ) { result = & v2beta1 . Horizontal Pod err = c . client . Patch ( pt ) . Namespace ( c . ns ) . Resource ( " " ) . Sub } 
func ( nm * Node Manager ) Get Node Info ( node Name k8stypes . Node Name ) ( Node Info , error ) { get Node Info := func ( node Name k8stypes . Node Name ) * Node Info { nm . node Info Lock . R node Info := nm . node Info Map [ convert To String ( node nm . node Info Lock . R return node node Info := get Node Info ( node if node Info == nil { // Rediscover node if no Node Info found. klog . V ( 4 ) . Infof ( " " , convert To String ( node err = nm . Rediscover Node ( node if err != nil { klog . Errorf ( " " , err , convert To String ( node return Node node Info = get Node Info ( node } else { // Renew the found Node Info to avoid stale v Sphere connection. klog . V ( 4 ) . Infof ( " " , node Info , convert To String ( node node Info , err = nm . renew Node Info ( node if err != nil { klog . Errorf ( " " , err , convert To String ( node return Node nm . add Node Info ( convert To String ( node Name ) , node return * node } 
func ( nm * Node Manager ) Get Node Details ( ) ( [ ] Node Details , error ) { nm . registered Nodes defer nm . registered Nodes var node Details [ ] Node for node Name , node Obj := range nm . registered Nodes { node Info , err := nm . Get Node Info With Node Object ( node klog . V ( 4 ) . Infof ( " " , node Info , node node Details = append ( node Details , Node Details { node Name , node Info . vm , node Info . vm UUID , node return node } 
func ( nm * Node Manager ) renew Node Info ( node Info * Node Info , reconnect bool ) ( * Node Info , error ) { ctx , cancel := context . With vsphere Instance := nm . vsphere Instance Map [ node Info . vc if vsphere Instance == nil { err := fmt . Errorf ( " " , node Info . vc Server , node if reconnect { err := nm . vc Connect ( ctx , vsphere vm := node Info . vm . Renew VM ( vsphere return & Node Info { vm : & vm , data Center : vm . Datacenter , vc Server : node Info . vc Server , vm UUID : node Info . vm UUID , zone : node } 
func ( nm * Node Manager ) vc Connect ( ctx context . Context , vsphere Instance * V Sphere Instance ) error { err := vsphere credential Manager := nm . Credential if ! vclib . Is Invalid Credentials Error ( err ) || credential klog . V ( 4 ) . Infof ( " " , vsphere // Get latest credentials from Secret Credential Manager credentials , err := credential Manager . Get Credential ( vsphere vsphere Instance . conn . Update return vsphere } 
func ( nm * Node Manager ) Get Node Info With Node Object ( node * v1 . Node ) ( Node Info , error ) { node get Node Info := func ( node Name string ) * Node Info { nm . node Info Lock . R node Info := nm . node Info Map [ node nm . node Info Lock . R return node node Info := get Node Info ( node if node Info == nil { // Rediscover node if no Node Info found. klog . V ( 4 ) . Infof ( " " , node err = nm . Discover if err != nil { klog . Errorf ( " " , err , node return Node node Info = get Node Info ( node } else { // Renew the found Node Info to avoid stale v Sphere connection. klog . V ( 4 ) . Infof ( " " , node Info , node node Info , err = nm . renew Node Info ( node if err != nil { klog . Errorf ( " " , err , node return Node nm . add Node Info ( node Name , node return * node } 
func ( f * Config Flags ) To Raw Kube Config Loader ( ) clientcmd . Client Config { if f . use Persistent Config { return f . to Raw Kube Persistent Config return f . to Raw Kube Config } 
func ( f * Config Flags ) to Raw Kube Persistent Config Loader ( ) clientcmd . Client if f . client Config == nil { f . client Config = f . to Raw Kube Config return f . client } 
func ( f * Config Flags ) To Discovery Client ( ) ( discovery . Cached Discovery Interface , error ) { config , err := f . To REST // retrieve a user-provided value for the "cache-dir" // defaulting to ~/.kube/http-cache if no user-value is given. http Cache Dir := default Cache if f . Cache Dir != nil { http Cache Dir = * f . Cache discovery Cache Dir := compute Discover Cache Dir ( filepath . Join ( homedir . Home return diskcached . New Cached Discovery Client For Config ( config , discovery Cache Dir , http Cache } 
func ( f * Config Flags ) To REST Mapper ( ) ( meta . REST Mapper , error ) { discovery Client , err := f . To Discovery mapper := restmapper . New Deferred Discovery REST Mapper ( discovery expander := restmapper . New Shortcut Expander ( mapper , discovery } 
func ( f * Config Flags ) Add Flags ( flags * pflag . Flag Set ) { if f . Kube Config != nil { flags . String Var ( f . Kube Config , " " , * f . Kube if f . Cache Dir != nil { flags . String Var ( f . Cache Dir , flag HTTP Cache Dir , * f . Cache // add config options if f . Cert File != nil { flags . String Var ( f . Cert File , flag Cert File , * f . Cert if f . Key File != nil { flags . String Var ( f . Key File , flag Key File , * f . Key if f . Bearer Token != nil { flags . String Var ( f . Bearer Token , flag Bearer Token , * f . Bearer if f . Impersonate != nil { flags . String Var ( f . Impersonate , flag if f . Impersonate Group != nil { flags . String Array Var ( f . Impersonate Group , flag Impersonate Group , * f . Impersonate if f . Username != nil { flags . String Var ( f . Username , flag if f . Password != nil { flags . String Var ( f . Password , flag if f . Cluster Name != nil { flags . String Var ( f . Cluster Name , flag Cluster Name , * f . Cluster if f . Auth Info Name != nil { flags . String Var ( f . Auth Info Name , flag Auth Info Name , * f . Auth Info if f . Namespace != nil { flags . String Var P ( f . Namespace , flag if f . Context != nil { flags . String Var ( f . Context , flag if f . API Server != nil { flags . String Var P ( f . API Server , flag API Server , " " , * f . API if f . Insecure != nil { flags . Bool Var ( f . Insecure , flag if f . CA File != nil { flags . String Var ( f . CA File , flag CA File , * f . CA if f . Timeout != nil { flags . String Var ( f . Timeout , flag } 
func ( f * Config Flags ) With Deprecated Password Flag ( ) * Config } 
func New Config Flags ( use Persistent Config bool ) * Config Flags { impersonate return & Config Flags { Insecure : & insecure , Timeout : stringptr ( " " ) , Kube Config : stringptr ( " " ) , Cache Dir : stringptr ( default Cache Dir ) , Cluster Name : stringptr ( " " ) , Auth Info Name : stringptr ( " " ) , Context : stringptr ( " " ) , Namespace : stringptr ( " " ) , API Server : stringptr ( " " ) , Cert File : stringptr ( " " ) , Key File : stringptr ( " " ) , CA File : stringptr ( " " ) , Bearer Token : stringptr ( " " ) , Impersonate : stringptr ( " " ) , Impersonate Group : & impersonate Group , use Persistent Config : use Persistent } 
func compute Discover Cache Dir ( parent Dir , host string ) string { // strip the optional scheme from host if its there: schemeless // now do a simple collapse of non-AZ09 characters. Collisions are possible but unlikely. Even if we do collide the problem is short lived safe Host := overly Cautious Illegal File Characters . Replace All String ( schemeless return filepath . Join ( parent Dir , safe } 
return & runner { exec : exec , ipvs } 
func ( runner * runner ) Add Virtual Server ( vs * Virtual Server ) error { svc , err := to IPVS return runner . ipvs Handle . New } 
func ( runner * runner ) Update Virtual Server ( vs * Virtual Server ) error { svc , err := to IPVS return runner . ipvs Handle . Update } 
func ( runner * runner ) Delete Virtual Server ( vs * Virtual Server ) error { svc , err := to IPVS return runner . ipvs Handle . Del } 
func ( runner * runner ) Get Virtual Server ( vs * Virtual Server ) ( * Virtual Server , error ) { svc , err := to IPVS ipvs Svc , err := runner . ipvs Handle . Get v Serv , err := to Virtual Server ( ipvs return v } 
func ( runner * runner ) Get Virtual Servers ( ) ( [ ] * Virtual ipvs Svcs , err := runner . ipvs Handle . Get vss := make ( [ ] * Virtual for _ , ipvs Svc := range ipvs Svcs { vs , err := to Virtual Server ( ipvs } 
return runner . ipvs } 
func ( runner * runner ) Add Real Server ( vs * Virtual Server , rs * Real Server ) error { svc , err := to IPVS dst , err := to IPVS return runner . ipvs Handle . New } 
func ( runner * runner ) Get Real Servers ( vs * Virtual Server ) ( [ ] * Real Server , error ) { svc , err := to IPVS dsts , err := runner . ipvs Handle . Get rss := make ( [ ] * Real for _ , dst := range dsts { dst , err := to Real } 
func to Virtual Server ( svc * libipvs . Service ) ( * Virtual vs := & Virtual Server { Address : svc . Address , Port : svc . Port , Scheduler : svc . Sched Name , Protocol : protocol To // Test Flags >= 0x2, valid Flags ranges [0x2, 0x3] if svc . Flags & Flag Hashed == 0 { return nil , fmt . Errorf ( " " , Flag // Sub Flags to 0x2 // 011 -> 001, 010 -> 000 vs . Flags = Service Flags ( svc . Flags &^ uint32 ( Flag if vs . Address == nil { if svc . Address Family == syscall . AF_INET { vs . Address = net . I } else { vs . Address = net . I } 
func to Real Server ( dst * libipvs . Destination ) ( * Real return & Real Server { Address : dst . Address , Port : dst . Port , Weight : dst . Weight , Active Conn : dst . Active Connections , Inactive Conn : dst . Inactive } 
func to IPVS Service ( vs * Virtual ipvs Svc := & libipvs . Service { Address : vs . Address , Protocol : string To Protocol ( vs . Protocol ) , Port : vs . Port , Sched if ip4 := vs . Address . To4 ( ) ; ip4 != nil { ipvs Svc . Address ipvs } else { ipvs Svc . Address ipvs return ipvs } 
func to IPVS Destination ( rs * Real } 
func string To Protocol ( protocol string ) uint16 { switch strings . To } 
func protocol To } 
func Validate Storage Class ( storage Class * storage . Storage Class ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & storage Class . Object Meta , false , apivalidation . Validate Class Name , field . New all Errs = append ( all Errs , validate Provisioner ( storage Class . Provisioner , field . New all Errs = append ( all Errs , validate Parameters ( storage Class . Parameters , field . New all Errs = append ( all Errs , validate Reclaim Policy ( storage Class . Reclaim Policy , field . New all Errs = append ( all Errs , validate Volume Binding Mode ( storage Class . Volume Binding Mode , field . New all Errs = append ( all Errs , validate Allowed Topologies ( storage Class . Allowed Topologies , field . New return all } 
func Validate Storage Class Update ( storage Class , old Storage Class * storage . Storage Class ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & storage Class . Object Meta , & old Storage Class . Object Meta , field . New if ! reflect . Deep Equal ( old Storage Class . Parameters , storage Class . Parameters ) { all Errs = append ( all Errs , field . Forbidden ( field . New if storage Class . Provisioner != old Storage Class . Provisioner { all Errs = append ( all Errs , field . Forbidden ( field . New if * storage Class . Reclaim Policy != * old Storage Class . Reclaim Policy { all Errs = append ( all Errs , field . Forbidden ( field . New all Errs = append ( all Errs , apivalidation . Validate Immutable Field ( storage Class . Volume Binding Mode , old Storage Class . Volume Binding Mode , field . New return all } 
func validate Provisioner ( provisioner string , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( provisioner ) == 0 { all Errs = append ( all Errs , field . Required ( fld if len ( provisioner ) > 0 { for _ , msg := range validation . Is Qualified Name ( strings . To Lower ( provisioner ) ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func validate Parameters ( params map [ string ] string , fld Path * field . Path ) field . Error List { var total all Errs := field . Error if len ( params ) > max Provisioner Parameter Len { all Errs = append ( all Errs , field . Too Long ( fld Path , " " , max Provisioner Parameter return all for k , v := range params { if len ( k ) < 1 { all Errs = append ( all Errs , field . Invalid ( fld total if total Size > max Provisioner Parameter Size { all Errs = append ( all Errs , field . Too Long ( fld Path , " " , max Provisioner Parameter return all } 
func validate Reclaim Policy ( reclaim Policy * api . Persistent Volume Reclaim Policy , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( string ( * reclaim Policy ) ) > 0 { if ! supported Reclaim Policy . Has ( string ( * reclaim Policy ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path , reclaim Policy , supported Reclaim return all } 
func Validate Volume Attachment ( volume Attachment * storage . Volume Attachment ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & volume Attachment . Object Meta , false , apivalidation . Validate Class Name , field . New all Errs = append ( all Errs , validate Volume Attachment Spec ( & volume Attachment . Spec , field . New all Errs = append ( all Errs , validate Volume Attachment Status ( & volume Attachment . Status , field . New return all } 
func Validate Volume Attachment V1 ( volume Attachment * storage . Volume Attachment ) field . Error List { all Errs := apivalidation . Validate CSI Driver Name ( volume Attachment . Spec . Attacher , field . New if volume Attachment . Spec . Source . Persistent Volume Name != nil { pv Name := * volume Attachment . Spec . Source . Persistent Volume for _ , msg := range apivalidation . Validate Persistent Volume Name ( pv Name , false ) { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , pv return all } 
func validate Volume Attachment Spec ( spec * storage . Volume Attachment Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , validate Attacher ( spec . Attacher , fld all Errs = append ( all Errs , validate Volume Attachment Source ( & spec . Source , fld all Errs = append ( all Errs , validate Node Name ( spec . Node Name , fld return all } 
func validate Attacher ( attacher string , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( attacher ) == 0 { all Errs = append ( all Errs , field . Required ( fld return all } 
func validate Volume Attachment Source ( source * storage . Volume Attachment Source , fld Path * field . Path ) field . Error List { all Errs := field . Error if source . Persistent Volume Name == nil || len ( * source . Persistent Volume Name ) == 0 { all Errs = append ( all Errs , field . Required ( fld return all } 
func validate Node Name ( node Name string , fld Path * field . Path ) field . Error List { all Errs := field . Error for _ , msg := range apivalidation . Validate Node Name ( node Name , false /* prefix */ ) { all Errs = append ( all Errs , field . Invalid ( fld Path , node return all } 
func validate Volume Attachment Status ( status * storage . Volume Attachment Status , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , validate Attachment Metadata ( status . Attachment Metadata , fld all Errs = append ( all Errs , validate Volume Error ( status . Attach Error , fld all Errs = append ( all Errs , validate Volume Error ( status . Detach Error , fld return all } 
func Validate Volume Attachment Update ( new , old * storage . Volume Attachment ) field . Error List { all Errs := Validate Volume // Spec is read-only // If this ever relaxes in the future, make sure to increment the Generation number in Prepare For Update if ! apiequality . Semantic . Deep Equal ( old . Spec , new . Spec ) { all Errs = append ( all Errs , field . Invalid ( field . New return all } 
func validate Volume Binding Mode ( mode * storage . Volume Binding Mode , fld Path * field . Path ) field . Error List { all Errs := field . Error if mode == nil { all Errs = append ( all Errs , field . Required ( fld } else if ! supported Volume Binding Modes . Has ( string ( * mode ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path , mode , supported Volume Binding return all } 
func validate Allowed Topologies ( topologies [ ] api . Topology Selector Term , fld Path * field . Path ) field . Error List { all Errs := field . Error if topologies == nil || len ( topologies ) == 0 { return all raw for i , term := range topologies { idx Path := fld expr Map , term Errs := apivalidation . Validate Topology Selector Term ( term , fld all Errs = append ( all Errs , term // TODO (verult) consider improving runtime for _ , t := range raw Topologies { if helper . Semantic . Deep Equal ( expr Map , t ) { all Errs = append ( all Errs , field . Duplicate ( idx raw Topologies = append ( raw Topologies , expr return all } 
func Validate CSI Node ( csi Node * storage . CSI Node ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & csi Node . Object Meta , false , apivalidation . Validate Node Name , field . New all Errs = append ( all Errs , validate CSI Node Spec ( & csi Node . Spec , field . New return all } 
func Validate CSI Node Update ( new , old * storage . CSI Node ) field . Error List { all Errs := Validate CSI // Validate modifying fields inside an existing CSI Node Driver entry is not allowed for _ , old Driver := range old . Spec . Drivers { for _ , new Driver := range new . Spec . Drivers { if old Driver . Name == new Driver . Name { if ! apiequality . Semantic . Deep Equal ( old Driver , new Driver ) { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , new return all } 
func validate CSI Node Spec ( spec * storage . CSI Node Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , validate CSI Node Drivers ( spec . Drivers , fld return all } 
func validate CSI Node Drivers ( drivers [ ] storage . CSI Node Driver , fld Path * field . Path ) field . Error List { all Errs := field . Error driver Names In for i , driver := range drivers { idx Path := fld all Errs = append ( all Errs , validate CSI Node Driver ( driver , driver Names In Specs , idx return all } 
func validate CSI Node Driver Node ID ( node ID string , fld Path * field . Path ) field . Error List { all Errs := field . Error // node ID is always required if len ( node ID ) == 0 { all Errs = append ( all Errs , field . Required ( fld Path , node if len ( node ID ) > csi Node ID Max Length { all Errs = append ( all Errs , field . Invalid ( fld Path , node ID , fmt . Sprintf ( " " , csi Node ID Max return all } 
func validate CSI Node Driver ( driver storage . CSI Node Driver , driver Names In Specs sets . String , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate CSI Driver Name ( driver . Name , fld all Errs = append ( all Errs , validate CSI Node Driver Node ID ( driver . Node ID , fld // check for duplicate entries for the same driver in specs if driver Names In Specs . Has ( driver . Name ) { all Errs = append ( all Errs , field . Duplicate ( fld driver Names In topo for _ , key := range driver . Topology Keys { if len ( key ) == 0 { all Errs = append ( all Errs , field . Required ( fld if topo Keys . Has ( key ) { all Errs = append ( all Errs , field . Duplicate ( fld topo for _ , msg := range validation . Is Qualified Name ( key ) { all Errs = append ( all Errs , field . Invalid ( fld Path , driver . Topology return all } 
func Validate CSI Driver ( csi Driver * storage . CSI Driver ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate CSI Driver Name ( csi Driver . Name , field . New all Errs = append ( all Errs , validate CSI Driver Spec ( & csi Driver . Spec , field . New return all } 
func Validate CSI Driver Update ( new , old * storage . CSI Driver ) field . Error List { all Errs := Validate CSI // Spec is read-only // If this ever relaxes in the future, make sure to increment the Generation number in Prepare For Update if ! apiequality . Semantic . Deep Equal ( old . Spec , new . Spec ) { all Errs = append ( all Errs , field . Invalid ( field . New return all } 
func validate CSI Driver Spec ( spec * storage . CSI Driver Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , validate Attach Required ( spec . Attach Required , fld all Errs = append ( all Errs , validate Pod Info On Mount ( spec . Pod Info On Mount , fld return all } 
func validate Attach Required ( attach Required * bool , fld Path * field . Path ) field . Error List { all Errs := field . Error if attach Required == nil { all Errs = append ( all Errs , field . Required ( fld return all } 
func validate Pod Info On Mount ( pod Info On Mount * bool , fld Path * field . Path ) field . Error List { all Errs := field . Error if pod Info On Mount == nil { all Errs = append ( all Errs , field . Required ( fld return all } 
func Get From Kubelet Config Map ( client clientset . Interface , version * version . Version ) ( runtime . Object , error ) { // Read the Config Map from the cluster based on what version the kubelet is config Map Name := kubeadmconstants . Get Kubelet Config Map kubelet Cfg , err := client . Core V1 ( ) . Config Maps ( metav1 . Namespace System ) . Get ( config Map Name , metav1 . Get kubelet Config Data , ok := kubelet Cfg . Data [ kubeadmconstants . Kubelet Base Configuration Config Map if ! ok { return nil , errors . Errorf ( " " , config Map Name , kubeadmconstants . Kubelet Base Configuration Config Map // Decodes the kubelet Config Data into the internal component config obj := & kubeletconfig . Kubelet err = unmarshal Object ( obj , [ ] byte ( kubelet Config } 
func Get From Kube Proxy Config Map ( client clientset . Interface , version * version . Version ) ( runtime . Object , error ) { // Read the Config Map from the cluster kubeproxy Cfg , err := client . Core V1 ( ) . Config Maps ( metav1 . Namespace System ) . Get ( kubeadmconstants . Kube Proxy Config Map , metav1 . Get kubeproxy Config Data , ok := kubeproxy Cfg . Data [ kubeadmconstants . Kube Proxy Config Map if ! ok { return nil , errors . Errorf ( " " , kubeadmconstants . Kube Proxy Config Map , kubeadmconstants . Kube Proxy Config Map // Decodes the Config map dat into the internal component config obj := & kubeproxyconfig . Kube Proxy err = unmarshal Object ( obj , [ ] byte ( kubeproxy Config } 
func History Viewer For ( kind schema . Group Kind , c kubernetes . Interface ) ( History Viewer , error ) { elem := kapps . Group Kind visitor := & History // Determine which History } 
func ( h * Deployment History Viewer ) View History ( namespace , name string , revision int64 ) ( string , error ) { versioned Apps Client := h . c . Apps deployment , err := versioned Apps Client . Deployments ( namespace ) . Get ( name , metav1 . Get _ , all Old R Ss , new RS , err := deploymentutil . Get All Replica Sets ( deployment , versioned Apps all R Ss := all Old R if new RS != nil { all R Ss = append ( all R Ss , new history Info := make ( map [ int64 ] * corev1 . Pod Template for _ , rs := range all R history change Cause := get Change if history Info [ v ] . Annotations == nil { history if len ( change Cause ) > 0 { history Info [ v ] . Annotations [ Change Cause Annotation ] = change if len ( history if revision > 0 { // Print details of a specific revision template , ok := history return print // Sort the revision To Change Cause map by revision revisions := make ( [ ] int64 , 0 , len ( history for r := range history sliceutil . Sort return tabbed for _ , r := range revisions { // Find the change-cause of revision r change Cause := history Info [ r ] . Annotations [ Change Cause if len ( change Cause ) == 0 { change fmt . Fprintf ( out , " \t \n " , r , change } 
func ( h * Daemon Set History Viewer ) View History ( namespace , name string , revision int64 ) ( string , error ) { ds , history , err := daemon Set History ( h . c . Apps history Info := make ( map [ int64 ] * appsv1 . Controller for _ , history := range history { // TODO: for now we assume revisions don't overlap, we may need to handle it history if len ( history // Print details of a specific revision if revision > 0 { history , ok := history ds Of History , err := apply Daemon Set return print Template ( & ds Of // Print an overview of all Revisions // Sort the revision To Change Cause map by revision revisions := make ( [ ] int64 , 0 , len ( history for r := range history sliceutil . Sort return tabbed for _ , r := range revisions { // Find the change-cause of revision r change Cause := history Info [ r ] . Annotations [ Change Cause if len ( change Cause ) == 0 { change fmt . Fprintf ( out , " \t \n " , r , change } 
func ( h * Stateful Set History Viewer ) View History ( namespace , name string , revision int64 ) ( string , error ) { _ , history , err := stateful Set History ( h . c . Apps sliceutil . Sort return tabbed } 
func controlled History V1 ( apps clientappsv1 . Apps V1Interface , namespace string , selector labels . Selector , accessor metav1 . Object ) ( [ ] * appsv1 . Controller Revision , error ) { var result [ ] * appsv1 . Controller history List , err := apps . Controller Revisions ( namespace ) . List ( metav1 . List Options { Label for i := range history List . Items { history := history // Only add history that belongs to the API object if metav1 . Is Controlled } 
func daemon Set History ( apps clientappsv1 . Apps V1Interface , namespace , name string ) ( * appsv1 . Daemon Set , [ ] * appsv1 . Controller Revision , error ) { ds , err := apps . Daemon Sets ( namespace ) . Get ( name , metav1 . Get selector , err := metav1 . Label Selector As history , err := controlled } 
func stateful Set History ( apps clientappsv1 . Apps V1Interface , namespace , name string ) ( * appsv1 . Stateful Set , [ ] * appsv1 . Controller Revision , error ) { sts , err := apps . Stateful Sets ( namespace ) . Get ( name , metav1 . Get selector , err := metav1 . Label Selector As history , err := controlled History } 
func apply Daemon Set History ( ds * appsv1 . Daemon Set , history * appsv1 . Controller Revision ) ( * appsv1 . Daemon Set , error ) { clone := ds . Deep clone patched , err := strategicpatch . Strategic Merge Patch ( clone } 
func get Change return accessor . Get Annotations ( ) [ Change Cause } 
func ( c * Authorization V1beta1Client ) REST return c . rest } 
func new Cadvisor Stats Provider ( cadvisor cadvisor . Interface , resource Analyzer stats . Resource Analyzer , image Service kubecontainer . Image Service , status Provider status . Pod Status Provider , ) container Stats Provider { return & cadvisor Stats Provider { cadvisor : cadvisor , resource Analyzer : resource Analyzer , image Service : image Service , status Provider : status } 
func ( p * cadvisor Stats Provider ) List Pod Stats ( ) ( [ ] statsapi . Pod Stats , error ) { // Gets node root filesystem information and image filesystem stats, which // will be used to populate the available and capacity bytes/inodes in // container stats. root Fs Info , err := p . cadvisor . Root Fs image Fs Info , err := p . cadvisor . Images Fs infos , err := get Cadvisor Container // remove Terminated Container Info will also remove pod level cgroups, so save the infos into all Infos first all infos = remove Terminated Container // Map each container to a pod and update the Pod Stats with container data. pod To Stats := map [ statsapi . Pod Reference ] * statsapi . Pod for key , cinfo := range infos { // On systemd using devicemapper each mount into the container has an // associated cgroup. We ignore them to ensure we do not get duplicate // entries in our summary. For details on .mount units: // http://man7.org/linux/man-pages/man5/systemd.mount.5.html if strings . Has // Build the Pod key if this container is managed by a Pod if ! is Pod Managed ref := build Pod // Lookup the Pod Stats for the pod using the Pod Ref. If none exists, // initialize a new entry. pod Stats , found := pod To if ! found { pod Stats = & statsapi . Pod Stats { Pod pod To Stats [ ref ] = pod // Update the Pod Stats entry with the stats from the container by // adding it to pod Stats.Containers. container Name := kubetypes . Get Container if container Name == leaky . Pod Infra Container Name { // Special case for infrastructure container which is hidden from // the user and has network stats. pod Stats . Network = cadvisor Info To Network } else { pod Stats . Containers = append ( pod Stats . Containers , * cadvisor Info To Container Stats ( container Name , & cinfo , & root Fs Info , & image Fs // Add each Pod Stats to the result. result := make ( [ ] statsapi . Pod Stats , 0 , len ( pod To for _ , pod Stats := range pod To Stats { // Lookup the volume stats for each pod. pod UID := types . UID ( pod Stats . Pod var ephemeral Stats [ ] statsapi . Volume if vstats , found := p . resource Analyzer . Get Pod Volume Stats ( pod UID ) ; found { ephemeral Stats = make ( [ ] statsapi . Volume Stats , len ( vstats . Ephemeral copy ( ephemeral Stats , vstats . Ephemeral pod Stats . Volume Stats = append ( vstats . Ephemeral Volumes , vstats . Persistent pod Stats . Ephemeral Storage = calc Ephemeral Storage ( pod Stats . Containers , ephemeral Stats , & root Fs // Lookup the pod-level cgroup's CPU and memory stats pod Info := get Cadvisor Pod Info From Pod UID ( pod UID , all if pod Info != nil { cpu , memory := cadvisor Info To CP Uand Memory Stats ( pod pod pod status , found := p . status Provider . Get Pod Status ( pod if found && status . Start Time != nil && ! status . Start Time . Is Zero ( ) { pod Stats . Start Time = * status . Start // only append stats if we were able to get the start time of the pod result = append ( result , * pod } 
func ( p * cadvisor Stats Provider ) List Pod CPU And Memory Stats ( ) ( [ ] statsapi . Pod Stats , error ) { infos , err := get Cadvisor Container // remove Terminated Container Info will also remove pod level cgroups, so save the infos into all Infos first all infos = remove Terminated Container // Map each container to a pod and update the Pod Stats with container data. pod To Stats := map [ statsapi . Pod Reference ] * statsapi . Pod for key , cinfo := range infos { // On systemd using devicemapper each mount into the container has an // associated cgroup. We ignore them to ensure we do not get duplicate // entries in our summary. For details on .mount units: // http://man7.org/linux/man-pages/man5/systemd.mount.5.html if strings . Has // Build the Pod key if this container is managed by a Pod if ! is Pod Managed ref := build Pod // Lookup the Pod Stats for the pod using the Pod Ref. If none exists, // initialize a new entry. pod Stats , found := pod To if ! found { pod Stats = & statsapi . Pod Stats { Pod pod To Stats [ ref ] = pod // Update the Pod Stats entry with the stats from the container by // adding it to pod Stats.Containers. container Name := kubetypes . Get Container if container Name == leaky . Pod Infra Container Name { // Special case for infrastructure container which is hidden from // the user and has network stats. pod Stats . Start Time = metav1 . New Time ( cinfo . Spec . Creation } else { pod Stats . Containers = append ( pod Stats . Containers , * cadvisor Info To Container CPU And Memory Stats ( container // Add each Pod Stats to the result. result := make ( [ ] statsapi . Pod Stats , 0 , len ( pod To for _ , pod Stats := range pod To Stats { pod UID := types . UID ( pod Stats . Pod // Lookup the pod-level cgroup's CPU and memory stats pod Info := get Cadvisor Pod Info From Pod UID ( pod UID , all if pod Info != nil { cpu , memory := cadvisor Info To CP Uand Memory Stats ( pod pod pod result = append ( result , * pod } 
func ( p * cadvisor Stats Provider ) Image Fs Stats ( ) ( * statsapi . Fs Stats , error ) { image Fs Info , err := p . cadvisor . Images Fs image Stats , err := p . image Service . Image if err != nil || image var image Fs Inodes if image Fs Info . Inodes != nil && image Fs Info . Inodes Free != nil { image Fs IU := * image Fs Info . Inodes - * image Fs Info . Inodes image Fs Inodes Used = & image Fs return & statsapi . Fs Stats { Time : metav1 . New Time ( image Fs Info . Timestamp ) , Available Bytes : & image Fs Info . Available , Capacity Bytes : & image Fs Info . Capacity , Used Bytes : & image Stats . Total Storage Bytes , Inodes Free : image Fs Info . Inodes Free , Inodes : image Fs Info . Inodes , Inodes Used : image Fs Inodes } 
func ( p * cadvisor Stats Provider ) Image Fs Device ( ) ( string , error ) { image Fs Info , err := p . cadvisor . Images Fs return image Fs } 
func build Pod Ref ( container Labels map [ string ] string ) statsapi . Pod Reference { pod Name := kubetypes . Get Pod Name ( container pod Namespace := kubetypes . Get Pod Namespace ( container pod UID := kubetypes . Get Pod UID ( container return statsapi . Pod Reference { Name : pod Name , Namespace : pod Namespace , UID : pod } 
func is Pod Managed Container ( cinfo * cadvisorapiv2 . Container Info ) bool { pod Name := kubetypes . Get Pod pod Namespace := kubetypes . Get Pod managed := pod Name != " " && pod if ! managed && pod Name != pod Namespace { klog . Warningf ( " " , pod Name , pod } 
func get Cadvisor Pod Info From Pod UID ( pod UID types . UID , infos map [ string ] cadvisorapiv2 . Container Info ) * cadvisorapiv2 . Container Info { for key , info := range infos { if cm . Is Systemd Style Name ( key ) { // Convert to internal cgroup name and take the last component only. internal Cgroup Name := cm . Parse Systemd To Cgroup key = internal Cgroup Name [ len ( internal Cgroup if cm . Get Pod Cgroup Name Suffix ( pod } 
func remove Terminated Container Info ( container Info map [ string ] cadvisorapiv2 . Container Info ) map [ string ] cadvisorapiv2 . Container Info { cinfo Map := make ( map [ container ID ] [ ] container Info With for key , cinfo := range container Info { if ! is Pod Managed cinfo ID := container ID { pod Ref : build Pod Ref ( cinfo . Spec . Labels ) , container Name : kubetypes . Get Container cinfo Map [ cinfo ID ] = append ( cinfo Map [ cinfo ID ] , container Info With result := make ( map [ string ] cadvisorapiv2 . Container for _ , refs := range cinfo sort . Sort ( By Creation for ; i < len ( refs ) ; i ++ { if has Memory And CPU Inst } 
func has Memory And CPU Inst Usage ( info * cadvisorapiv2 . Container Info ) bool { if ! info . Spec . Has Cpu || ! info . Spec . Has cstat , found := latest Container if cstat . Cpu return cstat . Cpu } 
func new Timedcache ( ttl time . Duration , getter get Func ) ( * timed return & timed Cache { getter : getter , store : cache . New TTL Store ( cache Key } 
func ( t * timed Cache ) get Internal ( key string ) ( * cache Entry , error ) { entry , exists , err := t . store . Get By if exists { return entry . ( * cache entry , exists , err = t . store . Get By if exists { return entry . ( * cache // Still not found, add new entry with nil data. // Note the data will be filled later by getter. new Entry := & cache t . store . Add ( new return new } 
func ( t * timed Cache ) Get ( key string ) ( interface { } , error ) { entry , err := t . get } 
func ( t * timed Cache ) Delete ( key string ) error { return t . store . Delete ( & cache } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Proxy Configuration ) ( nil ) , ( * config . Kube Proxy Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Proxy Configuration_To_config_Kube Proxy Configuration ( a . ( * v1alpha1 . Kube Proxy Configuration ) , b . ( * config . Kube Proxy if err := s . Add Generated Conversion Func ( ( * config . Kube Proxy Configuration ) ( nil ) , ( * v1alpha1 . Kube Proxy Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Proxy Configuration_To_v1alpha1_Kube Proxy Configuration ( a . ( * config . Kube Proxy Configuration ) , b . ( * v1alpha1 . Kube Proxy if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Proxy Conntrack Configuration ) ( nil ) , ( * config . Kube Proxy Conntrack Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Proxy Conntrack Configuration_To_config_Kube Proxy Conntrack Configuration ( a . ( * v1alpha1 . Kube Proxy Conntrack Configuration ) , b . ( * config . Kube Proxy Conntrack if err := s . Add Generated Conversion Func ( ( * config . Kube Proxy Conntrack Configuration ) ( nil ) , ( * v1alpha1 . Kube Proxy Conntrack Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Proxy Conntrack Configuration_To_v1alpha1_Kube Proxy Conntrack Configuration ( a . ( * config . Kube Proxy Conntrack Configuration ) , b . ( * v1alpha1 . Kube Proxy Conntrack if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Proxy IP Tables Configuration ) ( nil ) , ( * config . Kube Proxy IP Tables Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Proxy IP Tables Configuration_To_config_Kube Proxy IP Tables Configuration ( a . ( * v1alpha1 . Kube Proxy IP Tables Configuration ) , b . ( * config . Kube Proxy IP Tables if err := s . Add Generated Conversion Func ( ( * config . Kube Proxy IP Tables Configuration ) ( nil ) , ( * v1alpha1 . Kube Proxy IP Tables Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Proxy IP Tables Configuration_To_v1alpha1_Kube Proxy IP Tables Configuration ( a . ( * config . Kube Proxy IP Tables Configuration ) , b . ( * v1alpha1 . Kube Proxy IP Tables if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Proxy IPVS Configuration ) ( nil ) , ( * config . Kube Proxy IPVS Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Proxy IPVS Configuration_To_config_Kube Proxy IPVS Configuration ( a . ( * v1alpha1 . Kube Proxy IPVS Configuration ) , b . ( * config . Kube Proxy IPVS if err := s . Add Generated Conversion Func ( ( * config . Kube Proxy IPVS Configuration ) ( nil ) , ( * v1alpha1 . Kube Proxy IPVS Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Proxy IPVS Configuration_To_v1alpha1_Kube Proxy IPVS Configuration ( a . ( * config . Kube Proxy IPVS Configuration ) , b . ( * v1alpha1 . Kube Proxy IPVS if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Proxy Winkernel Configuration ) ( nil ) , ( * config . Kube Proxy Winkernel Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Proxy Winkernel Configuration_To_config_Kube Proxy Winkernel Configuration ( a . ( * v1alpha1 . Kube Proxy Winkernel Configuration ) , b . ( * config . Kube Proxy Winkernel if err := s . Add Generated Conversion Func ( ( * config . Kube Proxy Winkernel Configuration ) ( nil ) , ( * v1alpha1 . Kube Proxy Winkernel Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Proxy Winkernel Configuration_To_v1alpha1_Kube Proxy Winkernel Configuration ( a . ( * config . Kube Proxy Winkernel Configuration ) , b . ( * v1alpha1 . Kube Proxy Winkernel } 
func Convert_v1alpha1_Kube Proxy Configuration_To_config_Kube Proxy Configuration ( in * v1alpha1 . Kube Proxy Configuration , out * config . Kube Proxy Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Proxy Configuration_To_config_Kube Proxy } 
func Convert_config_Kube Proxy Configuration_To_v1alpha1_Kube Proxy Configuration ( in * config . Kube Proxy Configuration , out * v1alpha1 . Kube Proxy Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Proxy Configuration_To_v1alpha1_Kube Proxy } 
func Convert_v1alpha1_Kube Proxy Conntrack Configuration_To_config_Kube Proxy Conntrack Configuration ( in * v1alpha1 . Kube Proxy Conntrack Configuration , out * config . Kube Proxy Conntrack Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Proxy Conntrack Configuration_To_config_Kube Proxy Conntrack } 
func Convert_config_Kube Proxy Conntrack Configuration_To_v1alpha1_Kube Proxy Conntrack Configuration ( in * config . Kube Proxy Conntrack Configuration , out * v1alpha1 . Kube Proxy Conntrack Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Proxy Conntrack Configuration_To_v1alpha1_Kube Proxy Conntrack } 
func Convert_v1alpha1_Kube Proxy IP Tables Configuration_To_config_Kube Proxy IP Tables Configuration ( in * v1alpha1 . Kube Proxy IP Tables Configuration , out * config . Kube Proxy IP Tables Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Proxy IP Tables Configuration_To_config_Kube Proxy IP Tables } 
func Convert_config_Kube Proxy IP Tables Configuration_To_v1alpha1_Kube Proxy IP Tables Configuration ( in * config . Kube Proxy IP Tables Configuration , out * v1alpha1 . Kube Proxy IP Tables Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Proxy IP Tables Configuration_To_v1alpha1_Kube Proxy IP Tables } 
func Convert_v1alpha1_Kube Proxy IPVS Configuration_To_config_Kube Proxy IPVS Configuration ( in * v1alpha1 . Kube Proxy IPVS Configuration , out * config . Kube Proxy IPVS Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Proxy IPVS Configuration_To_config_Kube Proxy IPVS } 
func Convert_config_Kube Proxy IPVS Configuration_To_v1alpha1_Kube Proxy IPVS Configuration ( in * config . Kube Proxy IPVS Configuration , out * v1alpha1 . Kube Proxy IPVS Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Proxy IPVS Configuration_To_v1alpha1_Kube Proxy IPVS } 
func Convert_v1alpha1_Kube Proxy Winkernel Configuration_To_config_Kube Proxy Winkernel Configuration ( in * v1alpha1 . Kube Proxy Winkernel Configuration , out * config . Kube Proxy Winkernel Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Proxy Winkernel Configuration_To_config_Kube Proxy Winkernel } 
func Convert_config_Kube Proxy Winkernel Configuration_To_v1alpha1_Kube Proxy Winkernel Configuration ( in * config . Kube Proxy Winkernel Configuration , out * v1alpha1 . Kube Proxy Winkernel Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Proxy Winkernel Configuration_To_v1alpha1_Kube Proxy Winkernel } 
func ( f * feature arr := strings . Split k := strings . Trim v := strings . Trim bool Value , err := strconv . Parse m [ k ] = bool return f . Set From } 
func ( f * feature Gate ) Set From // Copy existing state known := map [ Feature ] Feature for k , v := range f . known . Load ( ) . ( map [ Feature ] Feature feature if feature Spec . Lock To Default && feature Spec . Default != v { return fmt . Errorf ( " " , k , v , feature if feature Spec . Pre } else if feature Spec . Pre } 
func ( f * feature } 
func ( f * feature Gate ) Add ( features map [ Feature ] Feature // Copy existing state known := map [ Feature ] Feature for k , v := range f . known . Load ( ) . ( map [ Feature ] Feature for name , spec := range features { if existing Spec , found := known [ name ] ; found { if existing return fmt . Errorf ( " " , name , existing } 
func ( f * feature return f . known . Load ( ) . ( map [ Feature ] Feature } 
func ( f * feature Gate ) Add Flag ( fs * pflag . Flag // TODO(mtaufen): Shouldn't we just close it on the first Set/Set From Map instead? // Not all components expose a feature gates flag using this Add known := f . Known fs . Var ( f , flag } 
func ( f * feature Gate ) Known for k , v := range f . known . Load ( ) . ( map [ Feature ] Feature Spec ) { if v . Pre Release == GA || v . Pre known = append ( known , fmt . Sprintf ( " " , k , v . Pre } 
func ( f * feature Gate ) Deep Copy ( ) Mutable Feature Gate { // Copy existing state. known := map [ Feature ] Feature for k , v := range f . known . Load ( ) . ( map [ Feature ] Feature // Store copied state in new atomics. known known enabled enabled // Construct a new feature Gate around the copied state. // Note that special Features is treated as immutable by convention, // and we maintain the value of f.closed across the copy. return & feature Gate { special : special Features , known : known Value , enabled : enabled } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Priority Class ) ( nil ) , ( * scheduling . Priority Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Priority Class_To_scheduling_Priority Class ( a . ( * v1alpha1 . Priority Class ) , b . ( * scheduling . Priority if err := s . Add Generated Conversion Func ( ( * scheduling . Priority Class ) ( nil ) , ( * v1alpha1 . Priority Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheduling_Priority Class_To_v1alpha1_Priority Class ( a . ( * scheduling . Priority Class ) , b . ( * v1alpha1 . Priority if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Priority Class List ) ( nil ) , ( * scheduling . Priority Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Priority Class List_To_scheduling_Priority Class List ( a . ( * v1alpha1 . Priority Class List ) , b . ( * scheduling . Priority Class if err := s . Add Generated Conversion Func ( ( * scheduling . Priority Class List ) ( nil ) , ( * v1alpha1 . Priority Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheduling_Priority Class List_To_v1alpha1_Priority Class List ( a . ( * scheduling . Priority Class List ) , b . ( * v1alpha1 . Priority Class } 
func Convert_v1alpha1_Priority Class_To_scheduling_Priority Class ( in * v1alpha1 . Priority Class , out * scheduling . Priority Class , s conversion . Scope ) error { return auto Convert_v1alpha1_Priority Class_To_scheduling_Priority } 
func Convert_scheduling_Priority Class_To_v1alpha1_Priority Class ( in * scheduling . Priority Class , out * v1alpha1 . Priority Class , s conversion . Scope ) error { return auto Convert_scheduling_Priority Class_To_v1alpha1_Priority } 
func Convert_v1alpha1_Priority Class List_To_scheduling_Priority Class List ( in * v1alpha1 . Priority Class List , out * scheduling . Priority Class List , s conversion . Scope ) error { return auto Convert_v1alpha1_Priority Class List_To_scheduling_Priority Class } 
func Convert_scheduling_Priority Class List_To_v1alpha1_Priority Class List ( in * scheduling . Priority Class List , out * v1alpha1 . Priority Class List , s conversion . Scope ) error { return auto Convert_scheduling_Priority Class List_To_v1alpha1_Priority Class } 
func New Kubelet Command ( stop Ch <- chan struct { } ) * cobra . Command { clean Flag Set := pflag . New Flag Set ( component Kubelet , pflag . Continue On clean Flag Set . Set Normalize Func ( cliflag . Word Sep Normalize kubelet Flags := options . New Kubelet kubelet Config , err := options . New Kubelet cmd := & cobra . Command { Use : component The kubelet works in terms of a Pod Spec. A Pod that describes a pod. The kubelet takes a set of Pod described in those Pod Other than from an Pod (underspec'd currently) to submit a new manifest.` , // The Kubelet has special flag parsing requirements to enforce flag precedence rules, // so we do all our parsing manually in Run, below. // Disable Flag Parsing=true provides the full set of flags passed to the kubelet in the // `args` arg to Run, without Cobra's interference. Disable Flag Parsing : true , Run : func ( cmd * cobra . Command , args [ ] string ) { // initial flag parse, since we disable cobra's flag parsing if err := clean Flag // check if there are non-flag arguments in the command line cmds := clean Flag // short-circuit on help help , err := clean Flag Set . Get // short-circuit on verflag verflag . Print And Exit If utilflag . Print Flags ( clean Flag // set feature gates from initial flags-based config if err := utilfeature . Default Mutable Feature Gate . Set From Map ( kubelet Config . Feature // validate the initial Kubelet Flags if err := options . Validate Kubelet Flags ( kubelet if kubelet Flags . Container Runtime == " " && clean Flag // load kubelet config file, if provided if config File := kubelet Flags . Kubelet Config File ; len ( config File ) > 0 { kubelet Config , err = load Config File ( config // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubelet Config Flag Precedence ( kubelet // update feature gates based on new config if err := utilfeature . Default Mutable Feature Gate . Set From Map ( kubelet Config . Feature // We always validate the local configuration (command line + config file). // This is the default "last-known-good" config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation . Validate Kubelet Configuration ( kubelet // use dynamic kubelet config, if enabled var kubelet Config if dynamic Config Dir := kubelet Flags . Dynamic Config Dir . Value ( ) ; len ( dynamic Config Dir ) > 0 { var dynamic Kubelet Config * kubeletconfiginternal . Kubelet dynamic Kubelet Config , kubelet Config Controller , err = Bootstrap Kubelet Config Controller ( dynamic Config Dir , func ( kc * kubeletconfiginternal . Kubelet Configuration ) error { // Here, we enforce flag precedence inside the controller, prior to the controller's validation sequence, // so that we get a complete validation at the same point where we can decide to reject dynamic config. // This fixes the flag-precedence component of issue #63305. // See issue #56171 for general details on flag precedence. return kubelet Config Flag // If we should just use our existing, local config, the controller will return a nil config if dynamic Kubelet Config != nil { kubelet Config = dynamic Kubelet // Note: flag precedence was already enforced in the controller, prior to validation, // by our above transform function. Now we simply update feature gates from the new config. if err := utilfeature . Default Mutable Feature Gate . Set From Map ( kubelet Config . Feature // construct a Kubelet Server from kubelet Flags and kubelet Config kubelet Server := & options . Kubelet Server { Kubelet Flags : * kubelet Flags , Kubelet Configuration : * kubelet // use kubelet Server to construct the default Kubelet Deps kubelet Deps , err := Unsecured Dependencies ( kubelet // add the kubelet config controller to kubelet Deps kubelet Deps . Kubelet Config Controller = kubelet Config // start the experimental docker shim, if enabled if kubelet Server . Kubelet Flags . Experimental Dockershim { if err := Run Dockershim ( & kubelet Server . Kubelet Flags , kubelet Config , stop // run the kubelet klog . V ( 5 ) . Infof ( " " , kubelet Server . Kubelet if err := Run ( kubelet Server , kubelet Deps , stop // keep clean Flag Set separate, so Cobra doesn't pollute it with the global flags kubelet Flags . Add Flags ( clean Flag options . Add Kubelet Config Flags ( clean Flag Set , kubelet options . Add Global Flags ( clean Flag clean Flag Set . Bool // ugly, but necessary, because Cobra's default Usage Func and Help Func pollute the flagset with global flags const usage cmd . Set Usage Func ( func ( cmd * cobra . Command ) error { fmt . Fprintf ( cmd . Out Or Stderr ( ) , usage Fmt , cmd . Use Line ( ) , clean Flag Set . Flag Usages cmd . Set Help Func ( func ( cmd * cobra . Command , args [ ] string ) { fmt . Fprintf ( cmd . Out Or Stdout ( ) , " \n \n " + usage Fmt , cmd . Long , cmd . Use Line ( ) , clean Flag Set . Flag Usages } 
func new Flag Set With Globals ( ) * pflag . Flag Set { fs := pflag . New Flag Set ( " " , pflag . Exit On // set the normalize func, similar to k8s.io/component-base/cli//flags.go:Init Flags fs . Set Normalize Func ( cliflag . Word Sep Normalize // explicitly add flags from libs that register global flags options . Add Global } 
func new Fake Flag Set ( fs * pflag . Flag Set ) * pflag . Flag Set { ret := pflag . New Flag Set ( " " , pflag . Exit On ret . Set Normalize Func ( fs . Get Normalize fs . Visit All ( func ( f * pflag . Flag ) { ret . Var P ( cliflag . No } 
func kubelet Config Flag Precedence ( kc * kubeletconfiginternal . Kubelet Configuration , args [ ] string ) error { // We use a throwaway kubelet Flags and a fake global flagset to avoid double-parses, // as some Set implementations accumulate values from multiple flag invocations. fs := new Fake Flag Set ( new Flag Set With // register throwaway Kubelet Flags options . New Kubelet Flags ( ) . Add // register new Kubelet Configuration options . Add Kubelet Config // Remember original feature gates, so we can merge with flag gates later original := kc . Feature // Add back feature gates that were set in the original kc, but not in flags for k , v := range original { if _ , ok := kc . Feature Gates [ k ] ; ! ok { kc . Feature } 
func Unsecured Dependencies ( s * options . Kubelet Server ) ( * kubelet . Dependencies , error ) { // Initialize the TLS Options tls Options , err := Initialize TLS ( & s . Kubelet Flags , & s . Kubelet mounter := mount . New ( s . Experimental Mounter var plugin ne , err := nsenter . New Nsenter ( nsenter . Default Host Root Fs mounter = nsutil . New Mounter ( s . Root // N Senter only valid on Linux subpather = subpath . New NS Enter ( mounter , ne , s . Root // an exec interface which can use nsenter for flex plugin calls plugin Runner , err = nsenter . New Nsenter ( nsenter . Default Host Root Fs var docker Client Config * dockershim . Client if s . Container Runtime == kubetypes . Docker Container Runtime { docker Client Config = & dockershim . Client Config { Docker Endpoint : s . Docker Endpoint , Runtime Request Timeout : s . Runtime Request Timeout . Duration , Image Pull Progress Deadline : s . Image Pull Progress return & kubelet . Dependencies { Auth : nil , // default does not enforce auth[nz] C Advisor Interface : nil , // cadvisor.New launches background processes (bg http.Listen And Serve, and some bg cleaners), not set here Cloud : nil , // cloud provider might start background processes Container Manager : nil , Docker Client Config : docker Client Config , Kube Client : nil , Heartbeat Client : nil , Event Client : nil , Mounter : mounter , Subpather : subpather , OOM Adjuster : oom . New OOM Adjuster ( ) , OS Interface : kubecontainer . Real OS { } , Volume Plugins : Probe Volume Plugins ( ) , Dynamic Plugin Prober : Get Dynamic Plugin Prober ( s . Volume Plugin Dir , plugin Runner ) , TLS Options : tls } 
func Run ( s * options . Kubelet Server , kube Deps * kubelet . Dependencies , stop if err := init For OS ( s . Kubelet Flags . Windows if err := run ( s , kube Deps , stop } 
func make Event Recorder ( kube Deps * kubelet . Dependencies , node Name types . Node Name ) { if kube event Broadcaster := record . New kube Deps . Recorder = event Broadcaster . New Recorder ( legacyscheme . Scheme , v1 . Event Source { Component : component Kubelet , Host : string ( node event Broadcaster . Start if kube Deps . Event event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Deps . Event } 
func build Kubelet Client Config ( s * options . Kubelet Server , node Name types . Node Name ) ( * restclient . Config , func ( ) , error ) { if s . Rotate Certificates && utilfeature . Default Feature Gate . Enabled ( features . Rotate Kubelet Client cert Config , client Config , err := bootstrap . Load Client Config ( s . Kube Config , s . Bootstrap Kubeconfig , s . Cert // use the correct content type for cert rotation, but don't set QPS set Content Type For Client ( cert Config , s . Content kube Client Config Overrides ( s , client client Certificate Manager , err := build Client Certificate Manager ( cert Config , client Config , s . Cert Directory , node // the rotating transport will use the cert from the cert manager instead of these files transport Config := restclient . Anonymous Client Config ( client // we set exit After to five minutes because we use this client configuration to request new certs - if we are unable // to request new certs, we will be unable to continue normal operation. Exiting the process allows a wrapper // or the bootstrapping credentials to potentially lay down new initial config. close All Conns , err := kubeletcertificate . Update Transport ( wait . Never Stop , transport Config , client Certificate client Certificate return transport Config , close All if len ( s . Bootstrap Kubeconfig ) > 0 { if err := bootstrap . Load Client Cert ( s . Kube Config , s . Bootstrap Kubeconfig , s . Cert Directory , node client Config , err := clientcmd . New Non Interactive Deferred Loading Client Config ( & clientcmd . Client Config Loading Rules { Explicit Path : s . Kube Config } , & clientcmd . Config Overrides { } , ) . Client kube Client Config Overrides ( s , client return client } 
func build Client Certificate Manager ( cert Config , client Config * restclient . Config , cert Dir string , node Name types . Node Name ) ( certificate . Manager , error ) { new Client Fn := func ( current * tls . Certificate ) ( certificatesclient . Certificate Signing Request Interface , error ) { // If we have a valid certificate, use that to fetch CS Rs. Otherwise use the bootstrap // credentials. In the future it would be desirable to change the behavior of bootstrap // to always fall back to the external bootstrap credentials when such credentials are // provided by a fundamental trust system like cloud VM identity or an HSM module. config := cert if current != nil { config = client client , err := clientset . New For return client . Certificates V1beta1 ( ) . Certificate Signing return kubeletcertificate . New Kubelet Client Certificate Manager ( cert Dir , node Name , // this preserves backwards compatibility with kubeadm which passes // a high powered certificate to the kubelet as --kubeconfig and expects // it to be rotated out immediately client Config . Cert Data , client Config . Key Data , client Config . Cert File , client Config . Key File , new Client } 
func get Node Name ( cloud cloudprovider . Interface , hostname string ) ( types . Node Name , error ) { if cloud == nil { return types . Node node Name , err := instances . Current Node klog . V ( 2 ) . Infof ( " " , node return node } 
func Initialize TLS ( kf * options . Kubelet Flags , kc * kubeletconfiginternal . Kubelet Configuration ) ( * server . TLS Options , error ) { if ! kc . Server TLS Bootstrap && kc . TLS Cert File == " " && kc . TLS Private Key File == " " { kc . TLS Cert File = path . Join ( kf . Cert kc . TLS Private Key File = path . Join ( kf . Cert can Read Cert And Key , err := certutil . Can Read Cert And Key ( kc . TLS Cert File , kc . TLS Private Key if ! can Read Cert And Key { host Name , err := nodeutil . Get Hostname ( kf . Hostname cert , key , err := certutil . Generate Self Signed Cert Key ( host if err := certutil . Write Cert ( kc . TLS Cert if err := keyutil . Write Key ( kc . TLS Private Key klog . V ( 4 ) . Infof ( " " , kc . TLS Cert File , kc . TLS Private Key tls Cipher Suites , err := cliflag . TLS Cipher Suites ( kc . TLS Cipher min TLS Version , err := cliflag . TLS Version ( kc . TLS Min tls Options := & server . TLS Options { Config : & tls . Config { Min Version : min TLS Version , Cipher Suites : tls Cipher Suites , } , Cert File : kc . TLS Cert File , Key File : kc . TLS Private Key if len ( kc . Authentication . X509 . Client CA File ) > 0 { client C As , err := certutil . New Pool ( kc . Authentication . X509 . Client CA if err != nil { return nil , fmt . Errorf ( " " , kc . Authentication . X509 . Client CA // Specify allowed C As for client certificates tls Options . Config . Client C As = client C // Populate Peer Certificates in requests, but don't reject connections without verified certificates tls Options . Config . Client Auth = tls . Request Client return tls } 
func set Content Type For Client ( cfg * restclient . Config , content Type string ) { if len ( content cfg . Content Type = content switch content Type { case runtime . Content Type Protobuf : cfg . Accept Content Types = strings . Join ( [ ] string { runtime . Content Type Protobuf , runtime . Content Type } 
func Run Kubelet ( kube Server * options . Kubelet Server , kube Deps * kubelet . Dependencies , run Once bool ) error { hostname , err := nodeutil . Get Hostname ( kube Server . Hostname // Query the cloud provider for our node name, default to hostname if kube Deps.Cloud == nil node Name , err := get Node Name ( kube // Setup event recorder if required. make Event Recorder ( kube Deps , node // TODO(mtaufen): I moved the validation of these fields here, from Unsecured Kubelet Config, // so that I could remove the associated fields from Kubelet Configinternal. I would // prefer this to be done as part of an independent validation step on the // Kubelet Configuration. But as far as I can tell, we don't have an explicit // place for validation of the Kubelet Configuration yet. host Network Sources , err := kubetypes . Get Validated Sources ( kube Server . Host Network host PID Sources , err := kubetypes . Get Validated Sources ( kube Server . Host PID host IPC Sources , err := kubetypes . Get Validated Sources ( kube Server . Host IPC privileged Sources := capabilities . Privileged Sources { Host Network Sources : host Network Sources , Host PID Sources : host PID Sources , Host IPC Sources : host IPC capabilities . Setup ( kube Server . Allow Privileged , privileged credentialprovider . Set Preferred Dockercfg Path ( kube Server . Root klog . V ( 2 ) . Infof ( " " , kube Server . Root if kube Deps . OS Interface == nil { kube Deps . OS Interface = kubecontainer . Real k , err := create And Init Kubelet ( & kube Server . Kubelet Configuration , kube Deps , & kube Server . Container Runtime Options , kube Server . Container Runtime , kube Server . Runtime Cgroups , kube Server . Hostname Override , kube Server . Node IP , kube Server . Provider ID , kube Server . Cloud Provider , kube Server . Cert Directory , kube Server . Root Directory , kube Server . Register Node , kube Server . Register With Taints , kube Server . Allowed Unsafe Sysctls , kube Server . Remote Runtime Endpoint , kube Server . Remote Image Endpoint , kube Server . Experimental Mounter Path , kube Server . Experimental Kernel Memcg Notification , kube Server . Experimental Check Node Capabilities Before Mount , kube Server . Experimental Node Allocatable Ignore Eviction Threshold , kube Server . Minimum GC Age , kube Server . Max Per Pod Container Count , kube Server . Max Container Count , kube Server . Master Service Namespace , kube Server . Register Schedulable , kube Server . Non Masquerade CIDR , kube Server . Keep Terminated Pod Volumes , kube Server . Node Labels , kube Server . Seccomp Profile Root , kube Server . Bootstrap Checkpoint Path , kube Server . Node Status Max // New Main Kubelet should have set up a pod source config if one didn't exist // when the builder was run. This is just a precaution. if kube Deps . Pod pod Cfg := kube Deps . Pod rlimit . Rlimit Num Files ( uint64 ( kube Server . Max Open // process pods and exit. if run Once { if _ , err := k . Run Once ( pod } else { start Kubelet ( k , pod Cfg , & kube Server . Kubelet Configuration , kube Deps , kube Server . Enable } 
func parse Resource List ( m map [ string ] string ) ( v1 . Resource rl := make ( v1 . Resource for k , v := range m { switch v1 . Resource Name ( k ) { // CPU, memory, local storage, and PID resources are supported. case v1 . Resource CPU , v1 . Resource Memory , v1 . Resource Ephemeral Storage , pidlimit . PI Ds : if v1 . Resource Name ( k ) != pidlimit . PI Ds || utilfeature . Default Feature Gate . Enabled ( features . Support Node Pids Limit ) { q , err := resource . Parse rl [ v1 . Resource } 
func Bootstrap Kubelet Config Controller ( dynamic Config Dir string , transform dynamickubeletconfig . Transform Func ) ( * kubeletconfiginternal . Kubelet Configuration , * dynamickubeletconfig . Controller , error ) { if ! utilfeature . Default Feature Gate . Enabled ( features . Dynamic Kubelet if len ( dynamic Config // compute absolute path and bootstrap controller dir , err := filepath . Abs ( dynamic Config if err != nil { return nil , nil , fmt . Errorf ( " " , dynamic Config // get the latest Kubelet Configuration checkpoint from disk, or return the default config if no valid checkpoints exist c := dynamickubeletconfig . New } 
func Run Dockershim ( f * options . Kubelet Flags , c * kubeletconfiginternal . Kubelet Configuration , stop Ch <- chan struct { } ) error { r := & f . Container Runtime // Initialize docker client configuration. docker Client Config := & dockershim . Client Config { Docker Endpoint : r . Docker Endpoint , Runtime Request Timeout : c . Runtime Request Timeout . Duration , Image Pull Progress Deadline : r . Image Pull Progress // Initialize network plugin settings. plugin Settings := dockershim . Network Plugin Settings { Hairpin Mode : kubeletconfiginternal . Hairpin Mode ( c . Hairpin Mode ) , Non Masquerade CIDR : f . Non Masquerade CIDR , Plugin Name : r . Network Plugin Name , Plugin Conf Dir : r . CNI Conf Dir , Plugin Bin Dir String : r . CNI Bin Dir , MTU : int ( r . Network Plugin // Initialize streaming configuration. (Not using TLS now) streaming Config := & streaming . Config { // Use a relative redirect (no scheme or host). Base URL : & url . URL { Path : " " } , Stream Idle Timeout : c . Streaming Connection Idle Timeout . Duration , Stream Creation Timeout : streaming . Default Config . Stream Creation Timeout , Supported Remote Command Protocols : streaming . Default Config . Supported Remote Command Protocols , Supported Port Forward Protocols : streaming . Default Config . Supported Port Forward // Standalone dockershim will always start the local streaming server. ds , err := dockershim . New Docker Service ( docker Client Config , r . Pod Sandbox Image , streaming Config , & plugin Settings , f . Runtime Cgroups , c . Cgroup Driver , r . Dockershim Root Directory , true /*start Local Streaming server := dockerremote . New Docker Server ( f . Remote Runtime <- stop } 
func ( c * Clientset ) Discovery ( ) discovery . Discovery return c . Discovery } 
func New Fake Volume Manager ( initial Volumes [ ] v1 . Unique Volume Name ) * Fake Volume Manager { volumes := map [ v1 . Unique Volume for _ , v := range initial return & Fake Volume Manager { volumes : volumes , reported In Use : map [ v1 . Unique Volume } 
func ( f * Fake Volume Manager ) Get Volumes In Use ( ) [ ] v1 . Unique Volume Name { inuse := [ ] v1 . Unique Volume } 
func ( f * Fake Volume Manager ) Mark Volumes As Reported In Use ( volumes Reported As In Use [ ] v1 . Unique Volume Name ) { for _ , reported Volume := range volumes Reported As In Use { if _ , ok := f . volumes [ reported Volume ] ; ok { f . reported In Use [ reported } 
func ( f * Fake Volume Manager ) Get Volumes Reported In Use ( ) [ ] v1 . Unique Volume Name { inuse := [ ] v1 . Unique Volume for reported Volume := range f . reported In Use { inuse = append ( inuse , reported } 
func Get Generic } 
func Get Kubernetes Image ( image string , cfg * kubeadmapi . Cluster Configuration ) string { if cfg . Use Hyper Kube Image { image = constants . Hyper repo Prefix := cfg . Get Control Plane Image kubernetes Image Tag := kubeadmutil . Kubernetes Version To Image Tag ( cfg . Kubernetes return Get Generic Image ( repo Prefix , image , kubernetes Image } 
func Get DNS Image ( cfg * kubeadmapi . Cluster Configuration , image Name string ) string { // DNS uses default image repository by default dns Image Repository := cfg . Image // unless an override is specified if cfg . DNS . Image Repository != " " { dns Image Repository = cfg . DNS . Image // DNS uses an image Tag that corresponds to the DNS version matching the Kubernetes version dns Image Tag := constants . Get DNS // unless an override is specified if cfg . DNS . Image Tag != " " { dns Image Tag = cfg . DNS . Image return Get Generic Image ( dns Image Repository , image Name , dns Image } 
func Get Etcd Image ( cfg * kubeadmapi . Cluster Configuration ) string { // Etcd uses default image repository by default etcd Image Repository := cfg . Image // unless an override is specified if cfg . Etcd . Local != nil && cfg . Etcd . Local . Image Repository != " " { etcd Image Repository = cfg . Etcd . Local . Image // Etcd uses an image Tag that corresponds to the etcd version matching the Kubernetes version etcd Image Tag := constants . Default Etcd etcd Version , err := constants . Etcd Supported Version ( cfg . Kubernetes if err == nil { etcd Image Tag = etcd // unless an override is specified if cfg . Etcd . Local != nil && cfg . Etcd . Local . Image Tag != " " { etcd Image Tag = cfg . Etcd . Local . Image return Get Generic Image ( etcd Image Repository , constants . Etcd , etcd Image } 
func Get Pause Image ( cfg * kubeadmapi . Cluster Configuration ) string { return Get Generic Image ( cfg . Image Repository , " " , constants . Pause } 
func Get Control Plane Images ( cfg * kubeadmapi . Cluster // start with core kubernetes images if cfg . Use Hyper Kube Image { imgs = append ( imgs , Get Kubernetes Image ( constants . Hyper } else { imgs = append ( imgs , Get Kubernetes Image ( constants . Kube API imgs = append ( imgs , Get Kubernetes Image ( constants . Kube Controller imgs = append ( imgs , Get Kubernetes Image ( constants . Kube imgs = append ( imgs , Get Kubernetes Image ( constants . Kube // pause is not available on the ci image repository so use the default image repository. imgs = append ( imgs , Get Pause // if etcd is not external then add the image as it will be required if cfg . Etcd . Local != nil { imgs = append ( imgs , Get Etcd // Append the appropriate DNS images if cfg . DNS . Type == kubeadmapi . Core DNS { imgs = append ( imgs , Get DNS Image ( cfg , constants . Core DNS Image } else { imgs = append ( imgs , Get DNS Image ( cfg , constants . Kube DNS Kube DNS Image imgs = append ( imgs , Get DNS Image ( cfg , constants . Kube DNS Sidecar Image imgs = append ( imgs , Get DNS Image ( cfg , constants . Kube DNS Dns Masq Nanny Image } 
func monitor Resize Events ( fd uintptr , resize Events chan <- remotecommand . Terminal Size , stop chan struct { } ) { go func ( ) { defer runtime . Handle size := Get last size := Get if size . Height != last Size . Height || size . Width != last Size . Width { last last resize } 
func new Flunders ( c * Wardle V1beta1Client , namespace string ) * flunders { return & flunders { client : c . REST } 
} 
func ( c * flunders ) Update err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( flunder . Name ) . Sub } 
func ( pod Disruption Budget Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { pod Disruption Budget := obj . ( * policy . Pod Disruption // create cannot set status pod Disruption Budget . Status = policy . Pod Disruption Budget pod Disruption } 
func ( pod Disruption Budget Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Pod Disruption Budget := obj . ( * policy . Pod Disruption old Pod Disruption Budget := old . ( * policy . Pod Disruption // Update is not allowed to set status new Pod Disruption Budget . Status = old Pod Disruption // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. // See metav1.Object Meta description for more information on Generation. if ! apiequality . Semantic . Deep Equal ( old Pod Disruption Budget . Spec , new Pod Disruption Budget . Spec ) { new Pod Disruption Budget . Generation = old Pod Disruption } 
func ( pod Disruption Budget Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { pod Disruption Budget := obj . ( * policy . Pod Disruption return validation . Validate Pod Disruption Budget ( pod Disruption } 
func ( pod Disruption Budget Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { validation Error List := validation . Validate Pod Disruption Budget ( obj . ( * policy . Pod Disruption update Error List := validation . Validate Pod Disruption Budget Update ( obj . ( * policy . Pod Disruption Budget ) , old . ( * policy . Pod Disruption return append ( validation Error List , update Error } 
func ( pod Disruption Budget Status Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Pod Disruption Budget := obj . ( * policy . Pod Disruption old Pod Disruption Budget := old . ( * policy . Pod Disruption // status changes are not allowed to update spec new Pod Disruption Budget . Spec = old Pod Disruption } 
func ( in * Foo ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Foo ) Deep in . Deep Copy } 
func ( in * Foo ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Foo List ) Deep Copy Into ( out * Foo out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Foo List ) Deep Copy ( ) * Foo out := new ( Foo in . Deep Copy } 
func ( in * Foo List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Foo Spec ) Deep Copy Into ( out * Foo } 
func ( in * Foo Spec ) Deep Copy ( ) * Foo out := new ( Foo in . Deep Copy } 
func ( in * Foo Status ) Deep Copy ( ) * Foo out := new ( Foo in . Deep Copy } 
func ( v Volume Path Handler ) Map Device ( device Path string , map Path string , link Name string ) error { // Example of global map path: // global Map Path/link Name: plugins/kubernetes.io/{Plugin Name}/{Default Kubelet Volume Devices Dir Name}/{volume Plugin Dependent Path}/{pod Uid} // link Name: {pod Uid} // // Example of pod device map path: // pod Device Map Path/link Name: pods/{pod Uid}/{Default Kubelet Volume Devices Dir Name}/{escape Qualified Plugin Name}/{volume Name} // link Name: {volume Name} if len ( device if len ( map if ! filepath . Is Abs ( map Path ) { return fmt . Errorf ( " " , map klog . V ( 5 ) . Infof ( " " , device klog . V ( 5 ) . Infof ( " " , map klog . V ( 5 ) . Infof ( " " , link // Check and create map Path _ , err := os . Stat ( map if err != nil && ! os . Is Not Exist ( err ) { klog . Errorf ( " " , map if err = os . Mkdir All ( map Path , 0750 ) ; err != nil { return fmt . Errorf ( " " , map // Remove old symbolic link(or file) then create new one. // This should be done because current symbolic link is // stale across node reboot. link Path := filepath . Join ( map Path , string ( link if err = os . Remove ( link Path ) ; err != nil && ! os . Is Not err = os . Symlink ( device Path , link } 
func ( v Volume Path Handler ) Unmap Device ( map Path string , link Name string ) error { if len ( map klog . V ( 5 ) . Infof ( " " , map klog . V ( 5 ) . Infof ( " " , link // Check symbolic link exists link Path := filepath . Join ( map Path , string ( link if islink Exist , check Err := v . Is Symlink Exist ( link Path ) ; check Err != nil { return check } else if ! islink Exist { klog . Warningf ( " " , link err := os . Remove ( link } 
func ( v Volume Path Handler ) Remove Map Path ( map Path string ) error { if len ( map klog . V ( 5 ) . Infof ( " " , map err := os . Remove All ( map if err != nil && ! os . Is Not } 
func ( v Volume Path Handler ) Is Symlink Exist ( map Path string ) ( bool , error ) { fi , err := os . Lstat ( map if err == nil { // If file exits and it's symbolic link, return true and no error if fi . Mode ( ) & os . Mode Symlink == os . Mode // If file doesn't exist, return false and no error if os . Is Not } 
func ( v Volume Path Handler ) Get Device Symlink Refs ( dev Path string , map files , err := ioutil . Read Dir ( map for _ , file := range files { if file . Mode ( ) & os . Mode Symlink != os . Mode fp , err := os . Readlink ( filepath . Join ( map klog . V ( 5 ) . Infof ( " " , fp , dev if fp == dev Path { refs = append ( refs , filepath . Join ( map } 
func ( v Volume Path Handler ) Find Global Map Path UUID From Pod ( plugin Dir , map Path string , pod UID types . UID ) ( string , error ) { var global Map Path // Find symbolic link named pod uuid under plugin dir err := filepath . Walk ( plugin Dir , func ( path string , fi os . File if ( fi . Mode ( ) & os . Mode Symlink == os . Mode Symlink ) && ( fi . Name ( ) == string ( pod UID ) ) { klog . V ( 5 ) . Infof ( " " , path , map if res , err := compare Symlinks ( path , map Path ) ; err == nil && res { global Map Path klog . V ( 5 ) . Infof ( " " , global Map Path // Return path contains global map path + {pod uuid} return global Map Path } 
func ( in * SA Controller Configuration ) Deep Copy ( ) * SA Controller out := new ( SA Controller in . Deep Copy } 
func Namespaced for p , ns := range prefix Namespaces { if strings . Has return Unknown } 
func Load Translations ( root string , get Language Fn func ( ) string ) error { if get Language Fn == nil { get Language Fn = load System lang Str := find Language ( root , get Language translation Files := [ ] string { fmt . Sprintf ( " " , root , lang Str ) , fmt . Sprintf ( " " , root , lang klog . V ( 3 ) . Infof ( " " , lang w := zip . New // Make sure to check the error on Close. for _ , file := range translation gettext . Bind gettext . Set Locale ( lang } 
func T ( default Value string , args ... int ) string { if len ( args ) == 0 { return gettext . P Gettext ( " " , default return fmt . Sprintf ( gettext . PN Gettext ( " " , default Value , default } 
func Errorf ( default Value string , args ... int ) error { return errors . New ( T ( default } 
func ( Deduced Type Converter ) Object To Typed ( obj runtime . Object ) ( typed . Typed Value , error ) { u , err := runtime . Default Unstructured Converter . To return typed . Deduced Parseable Type { } . From } 
func ( Deduced Type Converter ) YAML To Typed ( from [ ] byte ) ( typed . Typed Value , error ) { return typed . Deduced Parseable Type { } . From YAML ( typed . YAML } 
func ( Deduced Type Converter ) Typed To Object ( value typed . Typed Value ) ( runtime . Object , error ) { return value To Object ( value . As } 
func New Type Converter ( models proto . Models ) ( Type Converter , error ) { parser , err := new GVK return & type } 
func ( gc * Garbage Collector ) resync Monitors ( deletable Resources map [ schema . Group Version Resource ] struct { } ) error { if err := gc . dependency Graph Builder . sync Monitors ( deletable gc . dependency Graph Builder . start } 
func ( gc * Garbage Collector ) Sync ( discovery Client discovery . Server Resources Interface , period time . Duration , stop Ch <- chan struct { } ) { old Resources := make ( map [ schema . Group Version wait . Until ( func ( ) { // Get the current resource list from discovery. new Resources := Get Deletable Resources ( discovery // This can occur if there is an internal error in Get Deletable Resources. if len ( new // Decide whether discovery has reported a change. if reflect . Deep Equal ( old Resources , new // Ensure workers are paused to avoid processing events before informers // have resynced. gc . worker defer gc . worker wait . Poll Immediate // On a reattempt, check if available resources have changed if attempt > 1 { new Resources = Get Deletable Resources ( discovery if len ( new klog . V ( 2 ) . Infof ( " " , attempt , print Diff ( old Resources , new // Resetting the REST mapper will also invalidate the underlying discovery // client. This is a leaky abstraction and assumes behavior about the REST // mapper, but we'll deal with it for now. gc . rest // Perform the monitor resync and wait for controllers to report cache sync. // // NOTE: It's possible that new Resources will diverge from the resources // discovered by rest Mapper during the call to Reset, since they are // distinct discovery clients invalidated at different times. For example, // new Resources may contain resources not returned in the rest Mapper's // discovery call if the resources appeared in-between the calls. In that // case, the rest Mapper will fail to map some of new Resources until the next // attempt. if err := gc . resync Monitors ( new Resources ) ; err != nil { utilruntime . Handle // wait for caches to fill for a while (our sync period) before attempting to rediscover resources and retry syncing. // this protects us from deadlocks where available resources changed and one of our informer caches will never fill. // informers keep attempting to sync in the background, so retrying doesn't interrupt them. // the call to resync Monitors on the reattempt will no-op for resources that still exist. // note that workers stay paused until we successfully resync. if ! controller . Wait For Cache Sync ( " " , wait For Stop Or Timeout ( stop Ch , period ) , gc . dependency Graph Builder . Is Synced ) { utilruntime . Handle } , stop // Finally, keep track of our new state. Do this after all preceding steps // have succeeded to ensure we'll retry on subsequent syncs if an error // occurred. old Resources = new } , period , stop } 
func print Diff ( old Resources , new Resources map [ schema . Group Version Resource ] struct { } ) string { removed := sets . New for old Resource := range old Resources { if _ , ok := new Resources [ old Resource ] ; ! ok { removed . Insert ( fmt . Sprintf ( " " , old added := sets . New for new Resource := range new Resources { if _ , ok := old Resources [ new Resource ] ; ! ok { added . Insert ( fmt . Sprintf ( " " , new } 
func wait For Stop Or Timeout ( stop Ch <- chan struct { } , timeout time . Duration ) <- chan struct { } { stop Ch With go func ( ) { select { case <- stop close ( stop Ch With return stop Ch With } 
func ( gc * Garbage Collector ) is Dangling ( reference metav1 . Owner Reference , item * node ) ( dangling bool , owner * unstructured . Unstructured , err error ) { if gc . absent Owner Cache . Has ( reference . UID ) { klog . V ( 5 ) . Infof ( " " , item . identity . UID , reference . API // TODO: we need to verify the reference resource is supported by the // system. If it's not a valid resource, the garbage collector should i) // ignore the reference when decide if the object should be deleted, and // ii) should update the object to remove such references. This is to // prevent objects having references to an old resource from being // deleted during a cluster upgrade. resource , namespaced , err := gc . api Resource ( reference . API // TODO: It's only necessary to talk to the API server if the owner node // is a "virtual" node. The local graph could lag behind the real // status, but in practice, the difference is small. owner , err = gc . dynamic Client . Resource ( resource ) . Namespace ( resource Default Namespace ( namespaced , item . identity . Namespace ) ) . Get ( reference . Name , metav1 . Get switch { case errors . Is Not Found ( err ) : gc . absent Owner klog . V ( 5 ) . Infof ( " " , item . identity . UID , reference . API if owner . Get UID ( ) != reference . UID { klog . V ( 5 ) . Infof ( " " , item . identity . UID , reference . API gc . absent Owner } 
func ( gc * Garbage Collector ) classify References ( item * node , latest References [ ] metav1 . Owner Reference ) ( solid , dangling , waiting For Dependents Deletion [ ] metav1 . Owner Reference , err error ) { for _ , reference := range latest References { is Dangling , owner , err := gc . is if is owner if owner Accessor . Get Deletion Timestamp ( ) != nil && has Delete Dependents Finalizer ( owner Accessor ) { waiting For Dependents Deletion = append ( waiting For Dependents return solid , dangling , waiting For Dependents } 
func ( gc * Garbage Collector ) process Deleting Dependents Item ( item * node ) error { blocking Dependents := item . blocking if len ( blocking return gc . remove Finalizer ( item , metav1 . Finalizer Delete for _ , dep := range blocking Dependents { if ! dep . is Deleting gc . attempt To } 
func ( gc * Garbage Collector ) orphan Dependents ( owner object Reference , dependents [ ] * node ) error { err wg := sync . Wait // the dependent.identity.UID is used as precondition patch := delete Owner Ref Strategic Merge _ , err := gc . patch ( dependent , patch , func ( n * node ) ( [ ] byte , error ) { return gc . delete Owner Ref JSON Merge // note that if the target owner Reference doesn't exist in the // dependent, strategic merge patch will NOT return an error. if err != nil && ! errors . Is Not Found ( err ) { err close ( err var errors for e := range err Ch { errors Slice = append ( errors if len ( errors Slice ) != 0 { return fmt . Errorf ( " " , owner , utilerrors . New Aggregate ( errors } 
func ( gc * Garbage Collector ) attempt To Orphan Worker ( ) bool { item , quit := gc . attempt To gc . worker Lock . R defer gc . worker Lock . R defer gc . attempt To if ! ok { utilruntime . Handle // we don't need to lock each element, because they never get updated owner . dependents Lock . R owner . dependents Lock . R err := gc . orphan if err != nil { utilruntime . Handle gc . attempt To Orphan . Add Rate // update the owner, remove "orphaning Finalizer" from its finalizers list err = gc . remove Finalizer ( owner , metav1 . Finalizer Orphan if err != nil { utilruntime . Handle gc . attempt To Orphan . Add Rate } 
func ( gc * Garbage Collector ) Graph Has UID ( UI Ds [ ] types . UID ) bool { for _ , u := range UI Ds { if _ , ok := gc . dependency Graph Builder . uid To } 
func Get Deletable Resources ( discovery Client discovery . Server Resources Interface ) map [ schema . Group Version Resource ] struct { } { preferred Resources , err := discovery Client . Server Preferred if err != nil { if discovery . Is Group Discovery Failed Error ( err ) { klog . Warningf ( " " , err . ( * discovery . Err Group Discovery if preferred Resources == nil { return map [ schema . Group Version // This is extracted from discovery.Group Version Resources to allow tolerating // failures on a per-resource basis. deletable Resources := discovery . Filtered By ( discovery . Supports All Verbs { Verbs : [ ] string { " " , " " , " " } } , preferred deletable Group Version Resources := map [ schema . Group Version for _ , rl := range deletable Resources { gv , err := schema . Parse Group Version ( rl . Group if err != nil { klog . Warningf ( " " , rl . Group for i := range rl . API Resources { deletable Group Version Resources [ schema . Group Version Resource { Group : gv . Group , Version : gv . Version , Resource : rl . API return deletable Group Version } 
func Is Allow All ( ipnets utilnet . IP Net Set ) bool { for _ , s := range ipnets . String } 
func Requests Only Local Traffic ( service * v1 . Service ) bool { if service . Spec . Type != v1 . Service Type Load Balancer && service . Spec . Type != v1 . Service Type Node return service . Spec . External Traffic Policy == v1 . Service External Traffic Policy Type } 
func Get Service Health Check Path Port ( service * v1 . Service ) ( string , int32 ) { if ! Needs Health port := service . Spec . Health Check Node } 
func New Allocation Map ( max int , range Spec string ) * Allocation Bitmap { a := Allocation Bitmap { strategy : random Scan Strategy { rand : rand . New ( rand . New Source ( time . Now ( ) . Unix Nano ( ) ) ) , } , allocated : big . New Int ( 0 ) , count : 0 , max : max , range Spec : range } 
func New Contiguous Allocation Map ( max int , range Spec string ) * Allocation Bitmap { a := Allocation Bitmap { strategy : contiguous Scan Strategy { } , allocated : big . New Int ( 0 ) , count : 0 , max : max , range Spec : range } 
func ( r * Allocation r . allocated = r . allocated . Set } 
func ( r * Allocation Bitmap ) Allocate next , ok := r . strategy . Allocate r . allocated = r . allocated . Set } 
func ( r * Allocation r . allocated = r . allocated . Set } 
func ( r * Allocation Bitmap ) For for word for word > 0 { if ( word & 1 ) != 0 { fn ( ( word Idx * word } 
func ( r * Allocation } 
func ( r * Allocation } 
func ( r * Allocation return r . range } 
func ( r * Allocation Bitmap ) Restore ( range if r . range Spec != range r . allocated = big . New Int ( 0 ) . Set r . count = count } 
func New Repair ( interval time . Duration , service Client corev1client . Services Getter , event Client corev1client . Events Getter , network * net . IP Net , alloc rangeallocation . Range Registry ) * Repair { event Broadcaster := record . New event Broadcaster . Start Recording To Sink ( & corev1client . Event Sink Impl { Interface : event recorder := event Broadcaster . New Recorder ( legacyscheme . Scheme , v1 . Event return & Repair { interval : interval , service Client : service } 
func ( c * Repair ) Run Until ( ch chan struct { } ) { wait . Until ( func ( ) { if err := c . Run Once ( ) ; err != nil { runtime . Handle } 
func ( c * Repair ) Run Once ( ) error { return retry . Retry On Conflict ( retry . Default Backoff , c . run } 
func ( c * Repair ) run Once ( ) error { // TODO: (per smarterclayton) if Get() or List Services() is a weak consistency read, // or if they are executed against different leaders, // the ordering guarantee required to ensure no IP is allocated twice is violated. // List Services must return a Resource Version higher than the etcd index Get triggers, // and the release code must not release services that have had I Ps allocated but not yet been created // See #8295 // If etcd server is not running we should wait for some time and fail only then. This is particularly // important when we start apiserver and etcd at the same time. var snapshot * api . Range err := wait . Poll // Create an allocator because it is easy to use. stored , err := ipallocator . New From // We explicitly send no resource version, since the resource version // of 'snapshot' is from a different collection, it's not comparable to // the service collection. The caching layer keeps per-collection R Vs, // and this is proper, since in theory the collections could be hosted // in separate etcd (or even non-etcd) instances. list , err := c . service Client . Services ( metav1 . Namespace All ) . List ( metav1 . List rebuilt := ipallocator . New CIDR // Check every Service's Cluster IP, and rebuild the state as we think it should be. for _ , svc := range list . Items { if ! helper . Is Service IP ip := net . Parse IP ( svc . Spec . Cluster if ip == nil { // cluster IP is corrupt c . recorder . Eventf ( & svc , v1 . Event Type Warning , " " , " " , svc . Spec . Cluster runtime . Handle Error ( fmt . Errorf ( " " , svc . Spec . Cluster } else { // cluster IP doesn't seem to be allocated c . recorder . Eventf ( & svc , v1 . Event Type runtime . Handle case ipallocator . Err Allocated : // cluster IP is duplicate c . recorder . Eventf ( & svc , v1 . Event Type runtime . Handle case err . ( * ipallocator . Err Not In Range ) : // cluster IP is out of range c . recorder . Eventf ( & svc , v1 . Event Type runtime . Handle case ipallocator . Err Full : // somehow we are out of I Ps c . recorder . Eventf ( & svc , v1 . Event Type default : c . recorder . Eventf ( & svc , v1 . Event Type // Check for I Ps that are left in the old set. They appear to have been leaked. stored . For switch { case ! found : // flag it to be cleaned up after any races (hopefully) are gone runtime . Handle count = num Repairs Before Leak if err := rebuilt . Allocate ( ip ) ; err != nil { runtime . Handle default : // do not add it to the rebuilt set, which means it will be available for reuse runtime . Handle if err := c . alloc . Create Or Update ( snapshot ) ; err != nil { if errors . Is } 
func Register ( plugins * admission . Plugins ) { plugins . Register ( Plugin Name , func ( config io . Reader ) ( admission . Interface , error ) { // the pods/status endpoint is ignored by this plugin since old kubelets // corrupt them. the pod status strategy ensures status updates cannot mutate // owner Ref. white List := [ ] white List Item { { group Resource : schema . Group return & gc Permissions Enforcement { Handler : admission . New Handler ( admission . Create , admission . Update ) , white List : white } 
func ( a * gc Permissions Enforcement ) is White Listed ( group Resource schema . Group Resource , subresource string ) bool { for _ , item := range a . white List { if item . group Resource == group } 
func ( a * gc Permissions Enforcement ) owner Ref To Delete Attribute Records ( ref metav1 . Owner Reference , attributes admission . Attributes ) ( [ ] authorizer . Attributes Record , error ) { var ret [ ] authorizer . Attributes group Version , err := schema . Parse Group Version ( ref . API mappings , err := a . rest Mapper . REST Mappings ( schema . Group Kind { Group : group Version . Group , Kind : ref . Kind } , group for _ , mapping := range mappings { ar := authorizer . Attributes Record { User : attributes . Get User Info ( ) , Verb : " " , API Group : mapping . Resource . Group , API Version : mapping . Resource . Version , Resource : mapping . Resource . Resource , Subresource : " " , Name : ref . Name , Resource if mapping . Scope . Name ( ) == meta . REST Scope Name Namespace { // if the owner is namespaced, it must be in the same namespace as the dependent is. ar . Namespace = attributes . Get } 
func blocking Owner Refs ( refs [ ] metav1 . Owner Reference ) [ ] metav1 . Owner Reference { var ret [ ] metav1 . Owner for _ , ref := range refs { if ref . Block Owner Deletion != nil && * ref . Block Owner } 
func new Blocking Owner Deletion Refs ( new Obj , old Obj runtime . Object ) [ ] metav1 . Owner Reference { new Meta , err := meta . Accessor ( new new Refs := new Meta . Get Owner blocking New Refs := blocking Owner Refs ( new if len ( blocking New if old Obj == nil { return blocking New old Meta , err := meta . Accessor ( old if err != nil { // if we don't have objectmeta, treat it as if all the owner Reference are newly created return blocking New var ret [ ] metav1 . Owner indexed Old Refs := index By UID ( old Meta . Get Owner for _ , ref := range blocking New Refs { old Ref , ok := indexed Old was Not Blocking := old Ref . Block Owner Deletion == nil || * old Ref . Block Owner if was Not } 
func New NS Enter ( mounter mount . Interface , ne * nsenter . Nsenter , root Dir string ) Interface { return & subpath NSE { mounter : mounter , ne : ne , root Dir : root } 
func check Device Inode ( fd int , path string ) error { var src Stat , dst err := unix . Fstat ( fd , & src err = unix . Stat ( path , & dst if src Stat . Dev != dst if src Stat . Ino != dst } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Test Type ) ( nil ) , ( * example . Test Type ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Test Type_To_example_Test Type ( a . ( * Test Type ) , b . ( * example . Test if err := s . Add Generated Conversion Func ( ( * example . Test Type ) ( nil ) , ( * Test Type ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_example_Test Type_To_v1_Test Type ( a . ( * example . Test Type ) , b . ( * Test if err := s . Add Generated Conversion Func ( ( * Test Type List ) ( nil ) , ( * example . Test Type List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Test Type List_To_example_Test Type List ( a . ( * Test Type List ) , b . ( * example . Test Type if err := s . Add Generated Conversion Func ( ( * example . Test Type List ) ( nil ) , ( * Test Type List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_example_Test Type List_To_v1_Test Type List ( a . ( * example . Test Type List ) , b . ( * Test Type if err := s . Add Generated Conversion Func ( ( * Test Type Status ) ( nil ) , ( * example . Test Type Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Test Type Status_To_example_Test Type Status ( a . ( * Test Type Status ) , b . ( * example . Test Type if err := s . Add Generated Conversion Func ( ( * example . Test Type Status ) ( nil ) , ( * Test Type Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_example_Test Type Status_To_v1_Test Type Status ( a . ( * example . Test Type Status ) , b . ( * Test Type } 
func ( s * Pod Disruption Budget V1Generator ) Structured Generate ( ) ( runtime . Object , error ) { if len ( s . Min Available ) == 0 { // defaulting behavior seen in Kubernetes 1.6 and below. s . Min selector , err := metav1 . Parse To Label min Available := intstr . Parse ( s . Min return & policy . Pod Disruption Budget { Object Meta : metav1 . Object Meta { Name : s . Name , } , Spec : policy . Pod Disruption Budget Spec { Min Available : & min } 
func ( s * Pod Disruption Budget if len ( s . Min } 
func ( s * Pod Disruption Budget V2Generator ) Structured selector , err := metav1 . Parse To Label if len ( s . Max Unavailable ) > 0 { max Unavailable := intstr . Parse ( s . Max return & policy . Pod Disruption Budget { Object Meta : metav1 . Object Meta { Name : s . Name , } , Spec : policy . Pod Disruption Budget Spec { Max Unavailable : & max if len ( s . Min Available ) > 0 { min Available := intstr . Parse ( s . Min return & policy . Pod Disruption Budget { Object Meta : metav1 . Object Meta { Name : s . Name , } , Spec : policy . Pod Disruption Budget Spec { Min Available : & min } 
func ( s * Pod Disruption Budget if len ( s . Max Unavailable ) == 0 && len ( s . Min if len ( s . Max Unavailable ) > 0 && len ( s . Min } 
func ( v * Schema Validation ) Validate gvk , errs := get Object if errs != nil { return utilerrors . New if ( gvk == schema . Group Version Kind { Version : " " , Kind : " " } ) { return utilerrors . New Aggregate ( v . validate return utilerrors . New Aggregate ( v . validate } 
func ( s * Server Run Options ) Apply To ( c * server . Config ) error { c . Cors Allowed Origin List = s . Cors Allowed Origin c . External Address = s . External c . Max Requests In Flight = s . Max Requests In c . Max Mutating Requests In Flight = s . Max Mutating Requests In c . Request Timeout = s . Request c . Min Request Timeout = s . Min Request c . JSON Patch Max Copy Bytes = s . JSON Patch Max Copy c . Max Request Body Bytes = s . Max Request Body c . Public Address = s . Advertise } 
func ( s * Server Run Options ) Default Advertise Address ( secure * Secure Serving if s . Advertise Address == nil || s . Advertise Address . Is Unspecified ( ) { host IP , err := secure . Default External s . Advertise Address = host } 
func ( s * Server Run if s . Target if s . Enable Infight Quota Handler { if ! utilfeature . Default Feature Gate . Enabled ( features . Request if s . Max Mutating Requests In Flight != 0 { errors = append ( errors , fmt . Errorf ( " " + " " , s . Max Mutating Requests In if s . Max Requests In Flight != 0 { errors = append ( errors , fmt . Errorf ( " " + " " , s . Max Requests In } else { if s . Max Requests In if s . Max Mutating Requests In if s . Request if s . Min Request if s . JSON Patch Max Copy if s . Max Request Body } 
func ( s * Server Run Options ) Add Universal Flags ( fs * pflag . Flag Set ) { // Note: the weird ""+ in below lines seems to be the only way to get gofmt to // arrange these text blocks sensibly. Grrr. fs . IP Var ( & s . Advertise Address , " " , s . Advertise fs . String Slice Var ( & s . Cors Allowed Origin List , " " , s . Cors Allowed Origin fs . Int Var ( & s . Target RAMMB , " " , s . Target fs . String Var ( & s . External Host , " " , s . External deprecated Master Service Namespace := metav1 . Namespace fs . String Var ( & deprecated Master Service Namespace , " " , deprecated Master Service fs . Int Var ( & s . Max Requests In Flight , " " , s . Max Requests In fs . Int Var ( & s . Max Mutating Requests In Flight , " " , s . Max Mutating Requests In fs . Duration Var ( & s . Request Timeout , " " , s . Request fs . Int Var ( & s . Min Request Timeout , " " , s . Min Request fs . Bool Var ( & s . Enable Infight Quota Handler , " " , s . Enable Infight Quota utilfeature . Default Mutable Feature Gate . Add } 
func ( sr default Service Resolver ) Resolve } 
func ( f * Identity Client ) Get Plugin Info ( ctx context . Context , in * csipb . Get Plugin Info Request , opts ... grpc . Call Option ) ( * csipb . Get Plugin Info } 
func ( f * Identity Client ) Get Plugin Capabilities ( ctx context . Context , in * csipb . Get Plugin Capabilities Request , opts ... grpc . Call Option ) ( * csipb . Get Plugin Capabilities } 
func ( f * Identity Client ) Probe ( ctx context . Context , in * csipb . Probe Request , opts ... grpc . Call Option ) ( * csipb . Probe } 
func New Node Client ( stage Unstage Set bool ) * Node Client { return & Node Client { node Published Volumes : make ( map [ string ] CSI Volume ) , node Staged Volumes : make ( map [ string ] CSI Volume ) , stage Unstage Set : stage Unstage } 
func ( f * Node Client ) Node Publish Volume ( ctx context . Context , req * csipb . Node Publish Volume Request , opts ... grpc . Call Option ) ( * csipb . Node Publish Volume Response , error ) { if f . next Err != nil { return nil , f . next if req . Get Volume if req . Get Target fs fs Type := req . Get Volume Capability ( ) . Get Mount ( ) . Get Fs if ! strings . Contains ( fs Types , fs f . node Published Volumes [ req . Get Volume Id ( ) ] = CSI Volume { Volume Handle : req . Get Volume Id ( ) , Path : req . Get Target Path ( ) , Device Mount Path : req . Get Staging Target Path ( ) , Volume Context : req . Get Volume Context ( ) , FS Type : req . Get Volume Capability ( ) . Get Mount ( ) . Get Fs Type ( ) , Mount Flags : req . Get Volume Capability ( ) . Get Mount ( ) . Mount return & csipb . Node Publish Volume } 
func ( f * Node Client ) Node Unpublish Volume ( ctx context . Context , req * csipb . Node Unpublish Volume Request , opts ... grpc . Call Option ) ( * csipb . Node Unpublish Volume Response , error ) { if f . next Err != nil { return nil , f . next if req . Get Volume if req . Get Target delete ( f . node Published Volumes , req . Get Volume return & csipb . Node Unpublish Volume } 
func ( f * Node Client ) Node Stage Volume ( ctx context . Context , req * csipb . Node Stage Volume Request , opts ... grpc . Call Option ) ( * csipb . Node Stage Volume Response , error ) { if f . next Err != nil { return nil , f . next if req . Get Volume if req . Get Staging Target fs fs mounted := req . Get Volume Capability ( ) . Get if mounted != nil { fs Type = mounted . Get Fs if ! strings . Contains ( fs Types , fs f . node Staged Volumes [ req . Get Volume Id ( ) ] = CSI Volume { Path : req . Get Staging Target Path ( ) , Volume Context : req . Get Volume return & csipb . Node Stage Volume } 
func ( f * Node Client ) Node Unstage Volume ( ctx context . Context , req * csipb . Node Unstage Volume Request , opts ... grpc . Call Option ) ( * csipb . Node Unstage Volume Response , error ) { if f . next Err != nil { return nil , f . next if req . Get Volume if req . Get Staging Target delete ( f . node Staged Volumes , req . Get Volume return & csipb . Node Unstage Volume } 
func ( f * Node Client ) Node Expand Volume ( ctx context . Context , req * csipb . Node Expand Volume Request , opts ... grpc . Call Option ) ( * csipb . Node Expand Volume Response , error ) { if f . next Err != nil { return nil , f . next if req . Get Volume if req . Get Volume if req . Get Capacity Range ( ) . Required resp := & csipb . Node Expand Volume Response { Capacity Bytes : req . Get Capacity Range ( ) . Required } 
func ( f * Node Client ) Node Get Info ( ctx context . Context , in * csipb . Node Get Info Request , opts ... grpc . Call Option ) ( * csipb . Node Get Info Response , error ) { if f . next Err != nil { return nil , f . next return f . node Get Info } 
func ( f * Node Client ) Node Get Capabilities ( ctx context . Context , in * csipb . Node Get Capabilities Request , opts ... grpc . Call Option ) ( * csipb . Node Get Capabilities Response , error ) { resp := & csipb . Node Get Capabilities Response { Capabilities : [ ] * csipb . Node Service if f . stage Unstage Set { resp . Capabilities = append ( resp . Capabilities , & csipb . Node Service Capability { Type : & csipb . Node Service Capability_Rpc { Rpc : & csipb . Node Service Capability_RPC { Type : csipb . Node Service if f . expansion Set { resp . Capabilities = append ( resp . Capabilities , & csipb . Node Service Capability { Type : & csipb . Node Service Capability_Rpc { Rpc : & csipb . Node Service Capability_RPC { Type : csipb . Node Service } 
func ( f * Node Client ) Node Get Volume Stats ( ctx context . Context , in * csipb . Node Get Volume Stats Request , opts ... grpc . Call Option ) ( * csipb . Node Get Volume Stats } 
func ( f * Controller Client ) Controller Get Capabilities ( ctx context . Context , in * csipb . Controller Get Capabilities Request , opts ... grpc . Call Option ) ( * csipb . Controller Get Capabilities Response , error ) { if f . next Err != nil { return nil , f . next if f . next Capabilities == nil { f . next Capabilities = [ ] * csipb . Controller Service Capability { { Type : & csipb . Controller Service Capability_Rpc { Rpc : & csipb . Controller Service Capability_RPC { Type : csipb . Controller Service return & csipb . Controller Get Capabilities Response { Capabilities : f . next } 
func ( f * Controller Client ) Create Volume ( ctx context . Context , in * csipb . Create Volume Request , opts ... grpc . Call Option ) ( * csipb . Create Volume } 
func ( f * Controller Client ) Delete Volume ( ctx context . Context , in * csipb . Delete Volume Request , opts ... grpc . Call Option ) ( * csipb . Delete Volume } 
func ( f * Controller Client ) Controller Publish Volume ( ctx context . Context , in * csipb . Controller Publish Volume Request , opts ... grpc . Call Option ) ( * csipb . Controller Publish Volume } 
func ( f * Controller Client ) Controller Unpublish Volume ( ctx context . Context , in * csipb . Controller Unpublish Volume Request , opts ... grpc . Call Option ) ( * csipb . Controller Unpublish Volume } 
func ( f * Controller Client ) Validate Volume Capabilities ( ctx context . Context , in * csipb . Validate Volume Capabilities Request , opts ... grpc . Call Option ) ( * csipb . Validate Volume Capabilities } 
func ( f * Controller Client ) List Volumes ( ctx context . Context , in * csipb . List Volumes Request , opts ... grpc . Call Option ) ( * csipb . List Volumes } 
func ( f * Controller Client ) Get Capacity ( ctx context . Context , in * csipb . Get Capacity Request , opts ... grpc . Call Option ) ( * csipb . Get Capacity } 
func compute Detached Sig ( content , token ID , token Secret string ) ( string , error ) { jwk := & jose . JSON Web Key { Key : [ ] byte ( token Secret ) , Key ID : token opts := & jose . Signer Options { // Since this is a symmetric key, go-jose doesn't automatically include // the Key ID as part of the protected header. We have to pass it here // explicitly. Extra Headers : map [ jose . Header Key ] interface { } { " " : token signer , err := jose . New Signer ( jose . Signing full Sig , err := jws . Compact return strip Content ( full } 
func strip Content ( full Sig string ) ( string , error ) { parts := strings . Split ( full } 
func Detached Token Is Valid ( detached Token , content , token ID , token Secret string ) bool { new Token , err := compute Detached Sig ( content , token ID , token return detached Token == new } 
func ( s * replica Set Lister ) Replica Sets ( namespace string ) Replica Set Namespace Lister { return replica Set Namespace } 
func ( s replica Set Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Replica Set , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Replica } 
func ( c * pods ) Get Logs ( name string , opts * v1 . Pod Log Options ) * restclient . Request { return c . client . Get ( ) . Namespace ( c . ns ) . Name ( name ) . Resource ( " " ) . Sub Resource ( " " ) . Versioned Params ( opts , scheme . Parameter } 
func New Cmd Config Current Context ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { options := & Current Context Options { Config Access : config cmd := & cobra . Command { Use : " " , Short : i18n . T ( " " ) , Long : current Context Long , Example : current Context Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check Err ( Run Current } 
func Run Current Context ( out io . Writer , options * Current Context Options ) error { config , err := options . Config Access . Get Starting if config . Current fmt . Fprintf ( out , " \n " , config . Current } 
func Parse Struct Tags ( tag string ) ( Struct Tags , error ) { tags := Struct tags = append ( tags , Struct } 
func ( csr Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { csr := obj . ( * certificates . Certificate Signing // Inject user.Info from request context if user , ok := genericapirequest . User From ( ctx ) ; ok { csr . Spec . Username = user . Get csr . Spec . UID = user . Get csr . Spec . Groups = user . Get if extra := user . Get Extra ( ) ; len ( extra ) > 0 { csr . Spec . Extra = map [ string ] certificates . Extra for k , v := range extra { csr . Spec . Extra [ k ] = certificates . Extra // Be explicit that users cannot create pre-approved certificate requests. csr . Status = certificates . Certificate Signing Request csr . Status . Conditions = [ ] certificates . Certificate Signing Request } 
func ( csr Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new CSR := obj . ( * certificates . Certificate Signing old CSR := old . ( * certificates . Certificate Signing new CSR . Spec = old new CSR . Status = old } 
func ( csr Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { csr := obj . ( * certificates . Certificate Signing return validation . Validate Certificate Signing } 
func ( csr Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { old CSR := old . ( * certificates . Certificate Signing new CSR := obj . ( * certificates . Certificate Signing return validation . Validate Certificate Signing Request Update ( new CSR , old } 
func ( csr Approval Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new CSR := obj . ( * certificates . Certificate Signing old CSR := old . ( * certificates . Certificate Signing // Updating the approval should only update the conditions. new CSR . Spec = old old CSR . Status . Conditions = new for i := range new CSR . Status . Conditions { // The Conditions are an array of values, some of which may be // pre-existing and unaltered by this update, so a Last Update Time is // added only if one isn't already set. if new CSR . Status . Conditions [ i ] . Last Update Time . Is Zero ( ) { new CSR . Status . Conditions [ i ] . Last Update new CSR . Status = old } 
func Register ( ) { register Metrics . Do ( func ( ) { prometheus . Must Register ( zone prometheus . Must Register ( zone prometheus . Must Register ( unhealthy prometheus . Must Register ( evictions } 
func ( qm * Quota Monitor ) Sync Monitors ( resources map [ schema . Group Version Resource ] struct { } ) error { qm . monitor defer qm . monitor to if to Remove == nil { to for resource := range resources { if _ , ok := qm . ignored Resources [ resource . Group if m , ok := to delete ( to c , err := qm . controller // check if we need to create an evaluator for this resource (if none previously registered) evaluator := qm . registry . Get ( resource . Group if evaluator == nil { lister Func := generic . Lister Func For Resource Func ( qm . informer Factory . For list Resource Func := generic . List Resource Using Lister Func ( lister evaluator = generic . New Object Count Evaluator ( resource . Group Resource ( ) , list Resource klog . Infof ( " " , resource . Group for _ , monitor := range to Remove { if monitor . stop Ch != nil { close ( monitor . stop klog . V ( 4 ) . Infof ( " " , added , kept , len ( to // New Aggregate returns nil if errs is 0-length return utilerrors . New } 
func ( qm * Quota Monitor ) Start Monitors ( ) { qm . monitor defer qm . monitor // we're waiting until after the informer start that happens once all the controllers are initialized. This ensures // that they don't get unexpected events on their work queues. <- qm . informers for _ , monitor := range monitors { if monitor . stop Ch == nil { monitor . stop qm . informer Factory . Start ( qm . stop } 
func ( qm * Quota Monitor ) Is Synced ( ) bool { qm . monitor Lock . R defer qm . monitor Lock . R for resource , monitor := range qm . monitors { if ! monitor . controller . Has } 
func ( qm * Quota Monitor ) Run ( stop // Set up the stop channel. qm . monitor qm . stop Ch = stop qm . monitor // Start monitors and begin change processing until the stop channel is // closed. qm . Start wait . Until ( qm . run Process Resource Changes , 1 * time . Second , stop // Stop any running monitors. qm . monitor defer qm . monitor for _ , monitor := range monitors { if monitor . stop close ( monitor . stop } 
func ( qm * Quota Monitor ) process Resource Changes ( ) bool { item , quit := qm . resource defer qm . resource if ! ok { utilruntime . Handle if err != nil { utilruntime . Handle klog . V ( 4 ) . Infof ( " " , event . gvr . String ( ) , accessor . Get Namespace ( ) , accessor . Get Name ( ) , string ( accessor . Get UID ( ) ) , event . event qm . replenishment Func ( event . gvr . Group Resource ( ) , accessor . Get } 
v , ok := acc . Get Annotations ( ) [ Revision return strconv . Parse } 
func Get All Replica Sets ( deployment * appsv1 . Deployment , c appsclient . Apps V1Interface ) ( [ ] * appsv1 . Replica Set , [ ] * appsv1 . Replica Set , * appsv1 . Replica Set , error ) { rs List , err := list Replica Sets ( deployment , rs List From old R Ses , all Old R Ses := find Old Replica Sets ( deployment , rs new RS := find New Replica Set ( deployment , rs return old R Ses , all Old R Ses , new } 
func rs List From Client ( c appsclient . Apps V1Interface ) rs List Func { return func ( namespace string , options metav1 . List Options ) ( [ ] * appsv1 . Replica Set , error ) { rs List , err := c . Replica var ret [ ] * appsv1 . Replica for i := range rs List . Items { ret = append ( ret , & rs } 
func list Replica Sets ( deployment * appsv1 . Deployment , get RS List rs List Func ) ( [ ] * appsv1 . Replica selector , err := metav1 . Label Selector As options := metav1 . List Options { Label all , err := get RS // Only include those whose Controller Ref matches the Deployment. owned := make ( [ ] * appsv1 . Replica for _ , rs := range all { if metav1 . Is Controlled } 
func equal Ignore Hash ( template1 , template2 * corev1 . Pod Template Spec ) bool { t1Copy := template1 . Deep t2Copy := template2 . Deep // Remove hash labels from template.Labels before comparing delete ( t1Copy . Labels , appsv1 . Default Deployment Unique Label delete ( t2Copy . Labels , appsv1 . Default Deployment Unique Label return apiequality . Semantic . Deep } 
func find New Replica Set ( deployment * appsv1 . Deployment , rs List [ ] * appsv1 . Replica Set ) * appsv1 . Replica Set { sort . Sort ( replica Sets By Creation Timestamp ( rs for i := range rs List { if equal Ignore Hash ( & rs List [ i ] . Spec . Template , & deployment . Spec . Template ) { // In rare cases, such as after cluster upgrades, Deployment may end up with // having more than one new Replica Sets that have the same template as its template, // see https://github.com/kubernetes/kubernetes/issues/40415 // We deterministically choose the oldest new Replica Set. return rs // new Replica } 
func find Old Replica Sets ( deployment * appsv1 . Deployment , rs List [ ] * appsv1 . Replica Set ) ( [ ] * appsv1 . Replica Set , [ ] * appsv1 . Replica Set ) { var required R Ss [ ] * appsv1 . Replica var all R Ss [ ] * appsv1 . Replica new RS := find New Replica Set ( deployment , rs for _ , rs := range rs List { // Filter out new replica set if new RS != nil && rs . UID == new all R Ss = append ( all R if * ( rs . Spec . Replicas ) != 0 { required R Ss = append ( required R return required R Ss , all R } 
func Resolve Fenceposts ( max Surge , max Unavailable * intstrutil . Int Or String , desired int32 ) ( int32 , int32 , error ) { surge , err := intstrutil . Get Value From Int Or Percent ( intstrutil . Value Or Default ( max Surge , intstrutil . From unavailable , err := intstrutil . Get Value From Int Or Percent ( intstrutil . Value Or Default ( max Unavailable , intstrutil . From if surge == 0 && unavailable == 0 { // Validation should never allow the user to explicitly use zero values for both max Surge // max Unavailable. Due to rounding down max Unavailable though, it may resolve to zero. // If both fenceposts resolve to zero, then we should set max } 
} 
func ( runner * runner ) Ensure Port Proxy out , err := runner . exec . Command ( cmd Netsh , args ... ) . Combined if ee , ok := err . ( utilexec . Exit Error ) ; ok { // netsh uses exit(0) to indicate a success of the operation, // as compared to a malformed commandline, for example. if ee . Exited ( ) && ee . Exit } 
func ( runner * runner ) Ensure IP Address ( args [ ] string , ip net . IP ) ( bool , error ) { // Check if the ip address exists int Name := runner . Get Interface To Add args Show Address := [ ] string { " " , " " , " " , " " , " " + int ip To exists , _ := check IP Exists ( ip To Check , args Show if exists == true { klog . V ( 4 ) . Infof ( " " , ip To out , err := runner . exec . Command ( cmd Netsh , args ... ) . Combined if err == nil { // Once the IP Address is added, it takes a bit to initialize and show up when querying for it // Query all the IP addresses and see if the one we added is present // PS: We are using netsh interface ipv4 show address here to query all the IP addresses, instead of // querying net.Interface Addrs() as it returns the IP address as soon as it is added even though it is uninitialized klog . V ( 3 ) . Infof ( " " , ip To for { if exists , _ := check IP Exists ( ip To Check , args Show if ee , ok := err . ( utilexec . Exit Error ) ; ok { // netsh uses exit(0) to indicate a success of the operation, // as compared to a malformed commandline, for example. if ee . Exited ( ) && ee . Exit } 
func ( runner * runner ) Get Interface To Add } 
func check IP Exists ( ip To Check string , args [ ] string , runner * runner ) ( bool , error ) { ip Address , err := runner . exec . Command ( cmd Netsh , args ... ) . Combined ip Address String := string ( ip klog . V ( 3 ) . Infof ( " " , ip To Check , ip Address show Address Array := strings . Split ( ip Address for _ , show Address := range show Address Array { if strings . Contains ( show Address , " " ) { ip From Netsh := get IP ( show if ip From Netsh == ip To } 
func get IP ( show Address string ) string { list := strings . Split N ( show return strings . Trim } 
func ( c * c SI Drivers ) Create ( c SI Driver * v1beta1 . CSI Driver ) ( result * v1beta1 . CSI Driver , err error ) { result = & v1beta1 . CSI err = c . client . Post ( ) . Resource ( " " ) . Body ( c SI } 
func ( c * c SI Drivers ) Update ( c SI Driver * v1beta1 . CSI Driver ) ( result * v1beta1 . CSI Driver , err error ) { result = & v1beta1 . CSI err = c . client . Put ( ) . Resource ( " " ) . Name ( c SI Driver . Name ) . Body ( c SI } 
func ( c * Apiregistration V1beta1Client ) REST return c . rest } 
func ( o * Namespace Controller Options ) Add Flags ( fs * pflag . Flag fs . Duration Var ( & o . Namespace Sync Period . Duration , " " , o . Namespace Sync fs . Int32Var ( & o . Concurrent Namespace Syncs , " " , o . Concurrent Namespace } 
func ( o * Namespace Controller Options ) Apply To ( cfg * namespaceconfig . Namespace Controller cfg . Namespace Sync Period = o . Namespace Sync cfg . Concurrent Namespace Syncs = o . Concurrent Namespace } 
func ( o * Namespace Controller } 
func New Cmd Create Quota ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Quota Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Aliases : [ ] string { " " } , Short : i18n . T ( " " ) , Long : quota Long , Example : quota Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Resource Quota V1Generator } 
func ( o * Quota Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Resource Quota V1Generator Name : generator = & generateversioned . Resource Quota Generator V1 { Name : name , Hard : cmdutil . Get Flag String ( cmd , " " ) , Scopes : cmdutil . Get Flag default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func ( c * Fake Fischers ) Get ( name string , options v1 . Get Options ) ( result * v1alpha1 . Fischer , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( fischers } 
func ( c * Fake Fischers ) List ( opts v1 . List Options ) ( result * v1alpha1 . Fischer List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( fischers Resource , fischers Kind , opts ) , & v1alpha1 . Fischer label , _ , _ := testing . Extract From List list := & v1alpha1 . Fischer List { List Meta : obj . ( * v1alpha1 . Fischer List ) . List for _ , item := range obj . ( * v1alpha1 . Fischer } 
func ( c * Fake Fischers ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( fischers } 
func ( c * Fake Fischers ) Create ( fischer * v1alpha1 . Fischer ) ( result * v1alpha1 . Fischer , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( fischers } 
func ( c * Fake Fischers ) Update ( fischer * v1alpha1 . Fischer ) ( result * v1alpha1 . Fischer , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( fischers } 
func ( c * Fake Fischers ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( fischers } 
func ( c * Fake Fischers ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( fischers Resource , list _ , err := c . Fake . Invokes ( action , & v1alpha1 . Fischer } 
func ( c * Fake Fischers ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1alpha1 . Fischer , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( fischers } 
func get Spam Key ( event * v1 . Event ) string { return strings . Join ( [ ] string { event . Source . Component , event . Source . Host , event . Involved Object . Kind , event . Involved Object . Namespace , event . Involved Object . Name , string ( event . Involved Object . UID ) , event . Involved Object . API } 
func New Event Source Object Spam Filter ( lru Cache Size , burst int , qps float32 , clock clock . Clock ) * Event Source Object Spam Filter { return & Event Source Object Spam Filter { cache : lru . New ( lru Cache } 
func ( f * Event Source Object Spam Filter ) Filter ( event * v1 . Event ) bool { var record spam // controls our cached information about this event (source+object) event Key := get Spam value , found := f . cache . Get ( event if found { record = value . ( spam // verify we have a rate limiter for this record if record . rate Limiter == nil { record . rate Limiter = flowcontrol . New Token Bucket Rate Limiter With // ensure we have available rate filter := ! record . rate Limiter . Try // update the cache f . cache . Add ( event } 
func Event Aggregator By Reason Func ( event * v1 . Event ) ( string , string ) { return strings . Join ( [ ] string { event . Source . Component , event . Source . Host , event . Involved Object . Kind , event . Involved Object . Namespace , event . Involved Object . Name , string ( event . Involved Object . UID ) , event . Involved Object . API } 
func New Event Aggregator ( lru Cache Size int , key Func Event Aggregator Key Func , message Func Event Aggregator Message Func , max Events int , max Interval In Seconds int , clock clock . Clock ) * Event Aggregator { return & Event Aggregator { cache : lru . New ( lru Cache Size ) , key Func : key Func , message Func : message Func , max Events : uint ( max Events ) , max Interval In Seconds : uint ( max Interval In } 
func ( e * Event Aggregator ) Event Aggregate ( new Event * v1 . Event ) ( * v1 . Event , string ) { now := metav1 . New var record aggregate // event Key is the full cache key for this event event Key := get Event Key ( new // aggregate Key is for the aggregate event, if one is needed. aggregate Key , local Key := e . key Func ( new value , found := e . cache . Get ( aggregate if found { record = value . ( aggregate // Is the previous record too old? If so, make a fresh one. Note: if we didn't // find a similar record, its last Timestamp will be the zero value, so we // create a new one in that case. max Interval := time . Duration ( e . max Interval In interval := now . Time . Sub ( record . last if interval > max Interval { record = aggregate Record { local Keys : sets . New // Write the new event into the aggregation record and put it on the cache record . local Keys . Insert ( local record . last e . cache . Add ( aggregate // If we are not yet over the threshold for unique events, don't correlate them if uint ( record . local Keys . Len ( ) ) < e . max Events { return new Event , event // do not grow our local key set any larger than max record . local Keys . Pop // create a new aggregate event, and return the aggregate Key as the cache key // (so that it can be overwritten.) event Copy := & v1 . Event { Object Meta : metav1 . Object Meta { Name : fmt . Sprintf ( " " , new Event . Involved Object . Name , now . Unix Nano ( ) ) , Namespace : new Event . Namespace , } , Count : 1 , First Timestamp : now , Involved Object : new Event . Involved Object , Last Timestamp : now , Message : e . message Func ( new Event ) , Type : new Event . Type , Reason : new Event . Reason , Source : new return event Copy , aggregate } 
func new Event Logger ( lru Cache Entries int , clock clock . Clock ) * event Logger { return & event Logger { cache : lru . New ( lru Cache } 
func ( e * event Logger ) event Observe ( new event Copy := * new event := & event // Check if there is an existing event we should update last Observation := e . last Event Observation From // If we found a result, prepare a patch if last Observation . count > 0 { // update the event based on the last observation so patch will work as desired event . Name = last event . Resource Version = last Observation . resource event . First Timestamp = last Observation . first event . Count = int32 ( last event event event Copy2 . Last Timestamp = metav1 . New event new old Data , _ := json . Marshal ( event patch , err = strategicpatch . Create Two Way Merge Patch ( old Data , new // record our new observation e . cache . Add ( key , event Log { count : uint ( event . Count ) , first Timestamp : event . First Timestamp , name : event . Name , resource Version : event . Resource } 
func ( e * event Logger ) update State ( event * v1 . Event ) { key := get Event // record our new observation e . cache . Add ( key , event Log { count : uint ( event . Count ) , first Timestamp : event . First Timestamp , name : event . Name , resource Version : event . Resource } 
func ( e * event Logger ) last Event Observation From Cache ( key string ) event if ok { observation Value , ok := value . ( event if ok { return observation return event } 
func New Event Correlator ( clock clock . Clock ) * Event Correlator { cache Size := max Lru Cache spam Filter := New Event Source Object Spam Filter ( cache Size , default Spam Burst , default Spam return & Event Correlator { filter Func : spam Filter . Filter , aggregator : New Event Aggregator ( cache Size , Event Aggregator By Reason Func , Event Aggregator By Reason Message Func , default Aggregate Max Events , default Aggregate Interval In Seconds , clock ) , logger : new Event Logger ( cache } 
func populate Defaults ( options Correlator Options ) Correlator Options { if options . LRU Cache Size == 0 { options . LRU Cache Size = max Lru Cache if options . Burst Size == 0 { options . Burst Size = default Spam if options . QPS == 0 { options . QPS = default Spam if options . Key Func == nil { options . Key Func = Event Aggregator By Reason if options . Message Func == nil { options . Message Func = Event Aggregator By Reason Message if options . Max Events == 0 { options . Max Events = default Aggregate Max if options . Max Interval In Seconds == 0 { options . Max Interval In Seconds = default Aggregate Interval In if options . Clock == nil { options . Clock = clock . Real } 
func ( c * Event Correlator ) Event Correlate ( new Event * v1 . Event ) ( * Event Correlate Result , error ) { if new aggregate Event , ckey := c . aggregator . Event Aggregate ( new observed Event , patch , err := c . logger . event Observe ( aggregate if c . filter Func ( observed Event ) { return & Event Correlate return & Event Correlate Result { Event : observed } 
func ( c * Event Correlator ) Update State ( event * v1 . Event ) { c . logger . update } 
func Validate ( spec Sys return errorsutil . New Aggregate ( warns ) , errorsutil . New } 
func Validate Spec ( spec Sys Spec , runtime string ) ( error , error ) { // OS-level validators. var os Validators = [ ] Validator { & OS Validator { Reporter : Default Reporter } , & Kernel Validator { Reporter : Default Reporter } , & Cgroups Validator { Reporter : Default Reporter } , & package Validator { reporter : Default // Docker-specific validators. var docker Validators = [ ] Validator { & Docker Validator { Reporter : Default validators := os switch runtime { case " " : validators = append ( validators , docker } 
func New Reconciler ( kube Client clientset . Interface , controller Attach Detach Enabled bool , loop Sleep Duration time . Duration , wait For Attach Timeout time . Duration , node Name types . Node Name , desired State Of World cache . Desired State Of World , actual State Of World cache . Actual State Of World , populator Has Added Pods func ( ) bool , operation Executor operationexecutor . Operation Executor , mounter mount . Interface , volume Plugin Mgr * volumepkg . Volume Plugin Mgr , kubelet Pods Dir string ) Reconciler { return & reconciler { kube Client : kube Client , controller Attach Detach Enabled : controller Attach Detach Enabled , loop Sleep Duration : loop Sleep Duration , wait For Attach Timeout : wait For Attach Timeout , node Name : node Name , desired State Of World : desired State Of World , actual State Of World : actual State Of World , populator Has Added Pods : populator Has Added Pods , operation Executor : operation Executor , mounter : mounter , volume Plugin Mgr : volume Plugin Mgr , kubelet Pods Dir : kubelet Pods Dir , time Of Last } 
func ( rc * reconciler ) sync States ( ) { // Get volumes information by reading the pod's directory pod Volumes , err := get Volumes From Pod Dir ( rc . kubelet Pods volumes Need Update := make ( map [ v1 . Unique Volume Name ] * reconstructed volume Need Report := [ ] v1 . Unique Volume for _ , volume := range pod Volumes { if rc . actual State Of World . Volume Exists With Spec Name ( volume . pod Name , volume . volume Spec Name ) { klog . V ( 4 ) . Infof ( " " , volume . volume Spec Name , volume . pod volume In DSW := rc . desired State Of World . Volume Exists With Spec Name ( volume . pod Name , volume . volume Spec reconstructed Volume , err := rc . reconstruct if err != nil { if volume In DSW { // Some pod needs the volume, don't clean it up and hope that // reconcile() calls Set Up and reconstructs the volume in ASW. klog . V ( 4 ) . Infof ( " " , volume . volume Spec Name , volume . pod // No pod needs the volume. klog . Warningf ( " " , volume . pod Name , volume . volume Spec rc . cleanup if volume In DSW { // Some pod needs the volume. And it exists on disk. Some previous // kubelet must have created the directory, therefore it must have // reported the volume as in use. Mark the volume as in use also in // this new kubelet so reconcile() calls Set Up and re-mounts the // volume if it's necessary. volume Need Report = append ( volume Need Report , reconstructed Volume . volume klog . V ( 4 ) . Infof ( " " , volume . volume Spec Name , volume . pod // There is no pod that uses the volume. if rc . operation Executor . Is Operation Pending ( reconstructed Volume . volume Name , nestedpendingoperations . Empty Unique Pod klog . V ( 2 ) . Infof ( " " , reconstructed volumes Need Update [ reconstructed Volume . volume Name ] = reconstructed if len ( volumes Need Update ) > 0 { if err = rc . update States ( volumes Need if len ( volume Need Report ) > 0 { rc . desired State Of World . Mark Volumes Reported In Use ( volume Need } 
func ( rc * reconciler ) reconstruct Volume ( volume pod Volume ) ( * reconstructed Volume , error ) { // plugin initializations plugin , err := rc . volume Plugin Mgr . Find Plugin By Name ( volume . plugin attachable Plugin , err := rc . volume Plugin Mgr . Find Attachable Plugin By Name ( volume . plugin device Mountable Plugin , err := rc . volume Plugin Mgr . Find Device Mountable Plugin By Name ( volume . plugin // Create pod object pod := & v1 . Pod { Object Meta : metav1 . Object Meta { UID : types . UID ( volume . pod mapper Plugin , err := rc . volume Plugin Mgr . Find Mapper Plugin By Name ( volume . plugin volume Spec , err := rc . operation Executor . Reconstruct Volume Operation ( volume . volume Mode , plugin , mapper Plugin , pod . UID , volume . pod Name , volume . volume Spec Name , volume . volume Path , volume . plugin var unique Volume Name v1 . Unique Volume if attachable Plugin != nil || device Mountable Plugin != nil { unique Volume Name , err = util . Get Unique Volume Name From Spec ( plugin , volume } else { unique Volume Name = util . Get Unique Volume Name From Spec With Pod ( volume . pod Name , plugin , volume volume Mounter , new Mounter Err := plugin . New Mounter ( volume Spec , pod , volumepkg . Volume if new Mounter Err != nil { return nil , fmt . Errorf ( " " , unique Volume Name , volume Spec . Name ( ) , volume . pod Name , pod . UID , new Mounter // Check existence of mount point for filesystem volume or symbolic link for block volume is Exist , check Err := rc . operation Executor . Check Volume Existence Operation ( volume Spec , volume Mounter . Get Path ( ) , volume Spec . Name ( ) , rc . mounter , unique Volume Name , volume . pod Name , pod . UID , attachable if check Err != nil { return nil , check // If mount or symlink doesn't exist, volume reconstruction should be failed if ! is Exist { return nil , fmt . Errorf ( " " , unique Volume // TODO: remove feature gate check after no longer needed var volume Mapper volumepkg . Block Volume if utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) && volume . volume Mode == v1 . Persistent Volume Block { var new Mapper if mapper Plugin != nil { volume Mapper , new Mapper Err = mapper Plugin . New Block Volume Mapper ( volume Spec , pod , volumepkg . Volume if new Mapper Err != nil { return nil , fmt . Errorf ( " " , unique Volume Name , volume Spec . Name ( ) , volume . pod Name , pod . UID , new Mapper reconstructed Volume := & reconstructed Volume { volume Name : unique Volume Name , pod Name : volume . pod Name , volume Spec : volume Spec , // volume.volume Spec Name is actually Inner Volume Spec Name. It will not be used // for volume cleanup. // TODO: in case pod is added back before reconciler starts to unmount, we can update this field from desired state information outer Volume Spec Name : volume . volume Spec Name , pod : pod , attachable Plugin : attachable Plugin , volume Gid Value : " " , // device Path is updated during update States() by checking node status's Volumes Attached data. // TODO: get device path directly from the volume mount path. device Path : " " , mounter : volume Mounter , block Volume Mapper : volume return reconstructed } 
func ( rc * reconciler ) update Device Path ( volumes Need Update map [ v1 . Unique Volume Name ] * reconstructed Volume ) { node , fetch Err := rc . kube Client . Core V1 ( ) . Nodes ( ) . Get ( string ( rc . node Name ) , metav1 . Get if fetch Err != nil { klog . Errorf ( " " , fetch } else { for _ , attached Volume := range node . Status . Volumes Attached { if volume , exists := volumes Need Update [ attached Volume . Name ] ; exists { volume . device Path = attached Volume . Device volumes Need Update [ attached klog . V ( 4 ) . Infof ( " " , attached Volume . Name , volume . device } 
func get Volumes From Pod Dir ( pod Dir string ) ( [ ] pod Volume , error ) { pods Dir Info , err := ioutil . Read Dir ( pod volumes := [ ] pod for i := range pods Dir Info { if ! pods Dir Info [ i ] . Is pod Name := pods Dir pod Dir := path . Join ( pod Dir , pod // Find filesystem volume information // ex. filesystem volume: /pods/{pod Uid}/volume/{escape Qualified Plugin Name}/{volume Name} volumes Dirs := map [ v1 . Persistent Volume Mode ] string { v1 . Persistent Volume Filesystem : path . Join ( pod Dir , config . Default Kubelet Volumes Dir // TODO: remove feature gate check after no longer needed if utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) { // Find block volume information // ex. block volume: /pods/{pod Uid}/volume Devices/{escape Qualified Plugin Name}/{volume Name} volumes Dirs [ v1 . Persistent Volume Block ] = path . Join ( pod Dir , config . Default Kubelet Volume Devices Dir for volume Mode , volumes Dir := range volumes Dirs { var volumes Dir Info [ ] os . File if volumes Dir Info , err = ioutil . Read Dir ( volumes Dir ) ; err != nil { // Just skip the loop because given volumes Dir doesn't exist depending on volume for _ , volume Dir := range volumes Dir Info { plugin Name := volume volume Plugin Path := path . Join ( volumes Dir , plugin volume Plugin Dirs , err := utilpath . Read Dir No Stat ( volume Plugin if err != nil { klog . Errorf ( " " , volume Plugin unescape Plugin Name := utilstrings . Unescape Qualified Name ( plugin for _ , volume Name := range volume Plugin Dirs { volume Path := path . Join ( volume Plugin Path , volume klog . V ( 5 ) . Infof ( " " , pod Name , volume volumes = append ( volumes , pod Volume { pod Name : volumetypes . Unique Pod Name ( pod Name ) , volume Spec Name : volume Name , volume Path : volume Path , plugin Name : unescape Plugin Name , volume Mode : volume klog . V ( 4 ) . Infof ( " " , pod } 
func ( s * pod Preset Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Pod Preset , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1alpha1 . Pod } 
func ( s * pod Preset Lister ) Pod Presets ( namespace string ) Pod Preset Namespace Lister { return pod Preset Namespace } 
func ( s pod Preset Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Pod Preset , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1alpha1 . Pod } 
func New Plugin ( ) * Plugin { return & Plugin { Handler : admission . New Handler ( admission . Create ) , features : utilfeature . Default Feature } 
func ( p * Plugin ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { // If Taint Nodes By Condition is not enabled, we don't need to do anything. if ! p . features . Enabled ( features . Taint Nodes By // Our job is just to taint nodes. if a . Get Resource ( ) . Group Resource ( ) != node Resource || a . Get node , ok := a . Get if ! ok { return admission . New Forbidden ( a , fmt . Errorf ( " " , a . Get // Taint node with Not Ready taint at creation if Taint Nodes By Condition is // enabled. This is needed to make sure that nodes are added to the cluster // with the Not Ready taint. Otherwise, a new node may receive the taint with // some delay causing pods to be scheduled on a not-ready node. // Node controller will remove the taint when the node becomes ready. add Not Ready } 
func New Daemon Set Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Daemon Set Informer ( client , namespace , resync } 
func New Client Backed Dry Run Getter ( config * rest . Config ) ( * Client Backed Dry Run Getter , error ) { client , err := clientset . New For dynamic Client , err := dynamic . New For return & Client Backed Dry Run Getter { client : client , dynamic Client : dynamic } 
func New Client Backed Dry Run Getter From Kubeconfig ( file string ) ( * Client Backed Dry Run Getter , error ) { config , err := clientcmd . Load From client Config , err := clientcmd . New Default Client Config ( * config , & clientcmd . Config Overrides { } ) . Client return New Client Backed Dry Run Getter ( client } 
func ( clg * Client Backed Dry Run Getter ) Handle Get Action ( action core . Get Action ) ( bool , runtime . Object , error ) { unstructured Obj , err := clg . dynamic Client . Resource ( action . Get Resource ( ) ) . Namespace ( action . Get Namespace ( ) ) . Get ( action . Get Name ( ) , metav1 . Get // Inform the user that the requested object wasn't found. print If Not new Obj , err := decode Unstructured Into API Object ( action , unstructured if err != nil { fmt . Printf ( " \n " , unstructured return true , new } 
func ( clg * Client Backed Dry Run Getter ) Handle List Action ( action core . List Action ) ( bool , runtime . Object , error ) { list Opts := metav1 . List Options { Label Selector : action . Get List Restrictions ( ) . Labels . String ( ) , Field Selector : action . Get List unstructured List , err := clg . dynamic Client . Resource ( action . Get Resource ( ) ) . Namespace ( action . Get Namespace ( ) ) . List ( list new Obj , err := decode Unstructured Into API Object ( action , unstructured if err != nil { fmt . Printf ( " \n " , unstructured return true , new } 
func decode Unstructured Into API Object ( action core . Action , unstructured Obj runtime . Unstructured ) ( runtime . Object , error ) { obj Bytes , err := json . Marshal ( unstructured new Obj , err := runtime . Decode ( clientsetscheme . Codecs . Universal Decoder ( action . Get Resource ( ) . Group Version ( ) ) , obj return new } 
func ( db * dbus Impl ) System Bus ( ) ( Connection , error ) { if db . system Bus == nil { bus , err := godbus . System db . system Bus = & conn return db . system } 
func ( db * dbus Impl ) Session Bus ( ) ( Connection , error ) { if db . session Bus == nil { bus , err := godbus . Session db . session Bus = & conn return db . session } 
func ( conn * conn Impl ) Object ( name , path string ) Object { return & object Impl { conn . conn . Object ( name , godbus . Object } 
func ( conn * conn } 
func ( obj * object Impl ) Call ( method string , flags godbus . Flags , args ... interface { } ) Call { return & call } 
func new Limit Ranges ( c * Core V1Client , namespace string ) * limit Ranges { return & limit Ranges { client : c . REST } 
func ( in * Replication Controller Dummy ) Deep Copy Into ( out * Replication Controller out . Type Meta = in . Type } 
func ( in * Replication Controller Dummy ) Deep Copy ( ) * Replication Controller out := new ( Replication Controller in . Deep Copy } 
func ( in * Replication Controller Dummy ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func Pod Running And Ready ( event watch . Event ) ( bool , error ) { switch event . Type { case watch . Deleted : return false , errors . New Not Found ( schema . Group switch t := event . Object . ( type ) { case * corev1 . Pod : switch t . Status . Phase { case corev1 . Pod Failed , corev1 . Pod Succeeded : return false , Err Pod case corev1 . Pod for i := range conditions { if conditions [ i ] . Type == corev1 . Pod Ready && conditions [ i ] . Status == corev1 . Condition } 
func ( db * Fake ) System Bus ( ) ( Connection , error ) { if db . system Bus != nil { return db . system } 
func ( db * Fake ) Session Bus ( ) ( Connection , error ) { if db . session Bus != nil { return db . session } 
func ( conn * Fake } 
func ( conn * Fake for i := range conn . signal Handlers { if conn . signal Handlers [ i ] == ch { conn . signal Handlers = append ( conn . signal Handlers [ : i ] , conn . signal conn . signal Handlers = append ( conn . signal } 
func ( conn * Fake Connection ) Add Object ( name , path string , handler Fake Handler ) { conn . objects [ name + path ] = & fake } 
func ( conn * Fake Connection ) Emit sig := & godbus . Signal { Sender : name , Path : godbus . Object for _ , ch := range conn . signal } 
func ( obj * fake return & fake } 
func ( call * fake } 
func ( t * Time ) Is return t . Time . Is } 
} 
} 
} 
} 
func ( t Time ) Marshal JSON ( ) ( [ ] byte , error ) { if t . Is // time cannot contain non escapable JSON characters buf = t . UTC ( ) . Append } 
func ( t Time ) Marshal Query Parameter ( ) ( string , error ) { if t . Is } 
} 
func New Basic Work Queue ( clock clock . Clock ) Work return & basic Work } 
func validate Host ( runtime string ) error { // Check feature-gates if ! utilfeature . Default Feature Gate . Enabled ( features . App // Check build support. if is Disabled // Check kernel support. if ! Is App Armor // Check runtime support. Currently only Docker is supported. if runtime != kubetypes . Docker Container Runtime && runtime != kubetypes . Remote Container } 
func validate Profile ( profile string , loaded Profiles map [ string ] bool ) error { if err := Validate Profile if strings . Has Prefix ( profile , Profile Name Prefix ) { profile Name := strings . Trim Prefix ( profile , Profile Name if ! loaded Profiles [ profile Name ] { return fmt . Errorf ( " " , profile } 
func parse Profile Name ( profile Line string ) string { mode Index := strings . Index Rune ( profile if mode return strings . Trim Space ( profile Line [ : mode } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . Deployment { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1beta1 . Deployment List { } , func ( obj interface { } ) { Set Object Defaults_Deployment List ( obj . ( * v1beta1 . Deployment scheme . Add Type Defaulting Func ( & v1beta1 . Stateful Set { } , func ( obj interface { } ) { Set Object Defaults_Stateful Set ( obj . ( * v1beta1 . Stateful scheme . Add Type Defaulting Func ( & v1beta1 . Stateful Set List { } , func ( obj interface { } ) { Set Object Defaults_Stateful Set List ( obj . ( * v1beta1 . Stateful Set } 
func ( in * Cross Version Object Reference ) Deep Copy ( ) * Cross Version Object out := new ( Cross Version Object in . Deep Copy } 
func ( in * External Metric Source ) Deep Copy Into ( out * External Metric if in . Metric Selector != nil { in , out := & in . Metric Selector , & out . Metric * out = new ( metav1 . Label ( * in ) . Deep Copy if in . Target Value != nil { in , out := & in . Target Value , & out . Target x := ( * in ) . Deep if in . Target Average Value != nil { in , out := & in . Target Average Value , & out . Target Average x := ( * in ) . Deep } 
func ( in * External Metric Source ) Deep Copy ( ) * External Metric out := new ( External Metric in . Deep Copy } 
func ( in * External Metric Status ) Deep Copy ( ) * External Metric out := new ( External Metric in . Deep Copy } 
func ( in * Horizontal Pod Autoscaler ) Deep Copy Into ( out * Horizontal Pod out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Horizontal Pod Autoscaler ) Deep Copy ( ) * Horizontal Pod out := new ( Horizontal Pod in . Deep Copy } 
func ( in * Horizontal Pod Autoscaler ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Horizontal Pod Autoscaler Condition ) Deep Copy Into ( out * Horizontal Pod Autoscaler in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Horizontal Pod Autoscaler Condition ) Deep Copy ( ) * Horizontal Pod Autoscaler out := new ( Horizontal Pod Autoscaler in . Deep Copy } 
func ( in * Horizontal Pod Autoscaler List ) Deep Copy Into ( out * Horizontal Pod Autoscaler out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Horizontal Pod for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Horizontal Pod Autoscaler List ) Deep Copy ( ) * Horizontal Pod Autoscaler out := new ( Horizontal Pod Autoscaler in . Deep Copy } 
func ( in * Horizontal Pod Autoscaler List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Horizontal Pod Autoscaler Spec ) Deep Copy Into ( out * Horizontal Pod Autoscaler out . Scale Target Ref = in . Scale Target if in . Min Replicas != nil { in , out := & in . Min Replicas , & out . Min if in . Target CPU Utilization Percentage != nil { in , out := & in . Target CPU Utilization Percentage , & out . Target CPU Utilization } 
func ( in * Horizontal Pod Autoscaler Spec ) Deep Copy ( ) * Horizontal Pod Autoscaler out := new ( Horizontal Pod Autoscaler in . Deep Copy } 
func ( in * Horizontal Pod Autoscaler Status ) Deep Copy Into ( out * Horizontal Pod Autoscaler if in . Observed Generation != nil { in , out := & in . Observed Generation , & out . Observed if in . Last Scale Time != nil { in , out := & in . Last Scale Time , & out . Last Scale * out = ( * in ) . Deep if in . Current CPU Utilization Percentage != nil { in , out := & in . Current CPU Utilization Percentage , & out . Current CPU Utilization } 
func ( in * Horizontal Pod Autoscaler Status ) Deep Copy ( ) * Horizontal Pod Autoscaler out := new ( Horizontal Pod Autoscaler in . Deep Copy } 
func ( in * Metric Spec ) Deep Copy Into ( out * Metric * out = new ( Object Metric ( * in ) . Deep Copy * out = new ( Pods Metric ( * in ) . Deep Copy * out = new ( Resource Metric ( * in ) . Deep Copy * out = new ( External Metric ( * in ) . Deep Copy } 
func ( in * Metric Spec ) Deep Copy ( ) * Metric out := new ( Metric in . Deep Copy } 
func ( in * Metric Status ) Deep Copy Into ( out * Metric * out = new ( Object Metric ( * in ) . Deep Copy * out = new ( Pods Metric ( * in ) . Deep Copy * out = new ( Resource Metric ( * in ) . Deep Copy * out = new ( External Metric ( * in ) . Deep Copy } 
func ( in * Metric Status ) Deep Copy ( ) * Metric out := new ( Metric in . Deep Copy } 
func ( in * Object Metric Source ) Deep Copy ( ) * Object Metric out := new ( Object Metric in . Deep Copy } 
func ( in * Object Metric Status ) Deep Copy ( ) * Object Metric out := new ( Object Metric in . Deep Copy } 
func ( in * Pods Metric Source ) Deep Copy ( ) * Pods Metric out := new ( Pods Metric in . Deep Copy } 
func ( in * Pods Metric Status ) Deep Copy ( ) * Pods Metric out := new ( Pods Metric in . Deep Copy } 
func ( in * Resource Metric Source ) Deep Copy Into ( out * Resource Metric if in . Target Average Utilization != nil { in , out := & in . Target Average Utilization , & out . Target Average if in . Target Average Value != nil { in , out := & in . Target Average Value , & out . Target Average x := ( * in ) . Deep } 
func ( in * Resource Metric Source ) Deep Copy ( ) * Resource Metric out := new ( Resource Metric in . Deep Copy } 
func ( in * Resource Metric Status ) Deep Copy Into ( out * Resource Metric if in . Current Average Utilization != nil { in , out := & in . Current Average Utilization , & out . Current Average out . Current Average Value = in . Current Average Value . Deep } 
func ( in * Resource Metric Status ) Deep Copy ( ) * Resource Metric out := new ( Resource Metric in . Deep Copy } 
func ( in * Scale ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object } 
func ( in * Scale ) Deep in . Deep Copy } 
func ( in * Scale ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Scale Spec ) Deep Copy ( ) * Scale out := new ( Scale in . Deep Copy } 
func ( in * Scale Status ) Deep Copy ( ) * Scale out := new ( Scale in . Deep Copy } 
func New Kube Version Getter ( client clientset . Interface , writer io . Writer ) Version Getter { return & Kube Version } 
func ( g * Kube Version Getter ) Cluster Version ( ) ( string , * versionutil . Version , error ) { cluster Version Info , err := g . client . Discovery ( ) . Server fmt . Fprintf ( g . w , " \n " , cluster Version cluster Version , err := versionutil . Parse Semantic ( cluster Version return cluster Version Info . String ( ) , cluster } 
func ( g * Kube Version Getter ) Kubeadm Version ( ) ( string , * versionutil . Version , error ) { kubeadm Version fmt . Fprintf ( g . w , " \n " , kubeadm Version kubeadm Version , err := versionutil . Parse Semantic ( kubeadm Version return kubeadm Version Info . String ( ) , kubeadm } 
func ( g * Kube Version Getter ) Version From CI Label ( ci Version Label , description string ) ( string , * versionutil . Version , error ) { version Str , err := kubeadmutil . Kubernetes Release Version ( ci Version if description != " " { fmt . Fprintf ( g . w , " \n " , description , version ver , err := versionutil . Parse Semantic ( version return version } 
func ( g * Kube Version Getter ) Kubelet Versions ( ) ( map [ string ] uint16 , error ) { nodes , err := g . client . Core V1 ( ) . Nodes ( ) . List ( metav1 . List return compute Kubelet } 
func compute Kubelet Versions ( nodes [ ] v1 . Node ) map [ string ] uint16 { kubelet for _ , node := range nodes { kver := node . Status . Node Info . Kubelet if _ , found := kubelet Versions [ kver ] ; ! found { kubelet kubelet return kubelet } 
func New Offline Version Getter ( version Getter Version Getter , version string ) Version Getter { return & Offline Version Getter { Version Getter : version } 
func ( o * Offline Version Getter ) Version From CI Label ( ci Version Label , description string ) ( string , * versionutil . Version , error ) { if o . version == " " { return o . Version Getter . Version From CI Label ( ci Version ver , err := versionutil . Parse } 
func count Intolerable Taints Prefer No Schedule ( taints [ ] v1 . Taint , tolerations [ ] v1 . Toleration ) ( intolerable Taints int ) { for _ , taint := range taints { // check only on taints that have effect Prefer No Schedule if taint . Effect != v1 . Taint Effect Prefer No if ! v1helper . Tolerations Tolerate Taint ( tolerations , & taint ) { intolerable } 
func get All Toleration Prefer No Schedule ( tolerations [ ] v1 . Toleration ) ( toleration List [ ] v1 . Toleration ) { for _ , toleration := range tolerations { // Empty effect means all effects which includes Prefer No Schedule, so we need to collect it as well. if len ( toleration . Effect ) == 0 || toleration . Effect == v1 . Taint Effect Prefer No Schedule { toleration List = append ( toleration } 
func Compute Taint Toleration Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { node := node if node == nil { return schedulerapi . Host // To hold all the tolerations with Effect Prefer No Schedule var tolerations Prefer No if priority Meta , ok := meta . ( * priority Metadata ) ; ok { tolerations Prefer No Schedule = priority Meta . pod } else { tolerations Prefer No Schedule = get All Toleration Prefer No return schedulerapi . Host Priority { Host : node . Name , Score : count Intolerable Taints Prefer No Schedule ( node . Spec . Taints , tolerations Prefer No } 
func New Cmd Create Cluster Role Binding ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := & Cluster Role Binding Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : cluster Role Binding Long , Example : cluster Role Binding Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check o . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Cluster Role Binding V1Generator cmd . Mark Flag cmd . Flags ( ) . String cmd . Flags ( ) . String cmd . Flags ( ) . String } 
func Lazy Provide ( creds Lazy Auth Configuration , image string ) Auth Config { if creds . Provider != nil { entry := * creds . Provider . Lazy return Docker Config Entry To Lazy Auth Configuration ( entry ) . Auth return creds . Auth } 
func ( d * default Docker Config Provider ) Provide ( image string ) Docker Config { // Read the standard Docker credentials from .dockercfg if cfg , err := Read Docker Config } else if ! os . Is Not return Docker } 
func ( d * Caching Docker Config Provider ) Provide ( image string ) Docker // If the cache hasn't expired, return our cache if time . Now ( ) . Before ( d . expiration ) { return d . cache Docker klog . V ( 2 ) . Infof ( " " , reflect . Type d . cache Docker return d . cache Docker } 
func ( dc * Driver } 
func ( dc * Driver Call ) Append Spec ( spec * volume . Spec , host volume . Volume Host , extra Options map [ string ] string ) error { options For Driver , err := New Options For Driver ( spec , host , extra json Bytes , err := json . Marshal ( options For dc . Append ( string ( json } 
func ( dc * Driver Call ) Run ( ) ( * Driver Status , error ) { if dc . plugin . is Unsupported ( dc . Command ) { return nil , errors . New ( Status Not exec Path := dc . plugin . get cmd := dc . plugin . runner . Command ( exec if dc . Timeout > 0 { timer := time . After output , exec Err := cmd . Combined if exec Err != nil { if timeout { return nil , err _ , err := handle Cmd if err == nil { klog . Errorf ( " " , exec Path , exec return nil , exec if is Cmd Not Supported } else { klog . Warningf ( " " , exec Path , dc . args , exec status , err := handle Cmd if err != nil { if is Cmd Not Supported } 
func New Options For Driver ( spec * volume . Spec , host volume . Volume Host , extra Options map [ string ] string ) ( Options For Driver , error ) { vol Source FS Type , err := get FS read Only , err := get Read vol Source Options , err := get options [ option FS Type ] = vol Source FS if read Only { options [ option Read } else { options [ option Read options [ option P Vor Volume for key , value := range extra for key , value := range vol Source return Options For } 
func is Cmd Not Supported Err ( err error ) bool { if err != nil && err . Error ( ) == Status Not } 
func handle Cmd Response ( cmd string , output [ ] byte ) ( * Driver Status , error ) { status := Driver Status { Capabilities : default } else if status . Status == Status Not } else if status . Status != Status Success { err klog . Errorf ( err return nil , fmt . Errorf ( " " , err } 
func Get Load Balancer Source Ranges ( service * api . Service ) ( utilnet . IP Net Set , error ) { var ipnets utilnet . IP Net // if Source Range field is specified, ignore source Range annotation if len ( service . Spec . Load Balancer Source Ranges ) > 0 { specs := service . Spec . Load Balancer Source ipnets , err = utilnet . Parse IP } else { val := service . Annotations [ api . Annotation Load Balancer Source Ranges val = strings . Trim if val == " " { val = default Load Balancer Source ipnets , err = utilnet . Parse IP if err != nil { return nil , fmt . Errorf ( " " , api . Annotation Load Balancer Source Ranges } 
func Requests Only Local Traffic ( service * api . Service ) bool { if service . Spec . Type != api . Service Type Load Balancer && service . Spec . Type != api . Service Type Node return service . Spec . External Traffic Policy == api . Service External Traffic Policy Type } 
func Needs Health Check ( service * api . Service ) bool { if service . Spec . Type != api . Service Type Load return Requests Only Local } 
func New Generic Controller Manager Configuration Options ( cfg * kubectrlmgrconfig . Generic Controller Manager Configuration ) * Generic Controller Manager Configuration Options { o := & Generic Controller Manager Configuration Options { Generic Controller Manager Configuration : cfg , Debugging : & Debugging Options { Debugging Configuration : & componentbaseconfig . Debugging } 
func ( o * Generic Controller Manager Configuration Options ) Add Flags ( fss * cliflag . Named Flag Sets , all Controllers , disabled By Default o . Debugging . Add Flags ( fss . Flag genericfs := fss . Flag genericfs . Duration Var ( & o . Min Resync Period . Duration , " " , o . Min Resync genericfs . String Var ( & o . Client Connection . Content Type , " " , o . Client Connection . Content genericfs . Float32Var ( & o . Client Connection . QPS , " " , o . Client genericfs . Int32Var ( & o . Client Connection . Burst , " " , o . Client genericfs . Duration Var ( & o . Controller Start Interval . Duration , " " , o . Controller Start genericfs . String Slice Var ( & o . Controllers , " " , o . Controllers , fmt . Sprintf ( " " + " " + " \n \n " , strings . Join ( all Controllers , " " ) , strings . Join ( disabled By Default leaderelectionconfig . Bind Flags ( & o . Leader } 
func ( o * Generic Controller Manager Configuration Options ) Apply To ( cfg * kubectrlmgrconfig . Generic Controller Manager if err := o . Debugging . Apply cfg . Min Resync Period = o . Min Resync cfg . Client Connection = o . Client cfg . Controller Start Interval = o . Controller Start cfg . Leader Election = o . Leader } 
func ( o * Generic Controller Manager Configuration Options ) Validate ( all Controllers [ ] string , disabled By Default all Controllers Set := sets . New String ( all if strings . Has if ! all Controllers } 
func ( c * Fake Daemon Sets ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( daemonsets } 
func ( c * Fake Daemon Sets ) Update Status ( daemon Set * appsv1 . Daemon Set ) ( * appsv1 . Daemon Set , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( daemonsets Resource , " " , c . ns , daemon Set ) , & appsv1 . Daemon return obj . ( * appsv1 . Daemon } 
func ( c * Fake Daemon Sets ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( daemonsets Resource , c . ns , name ) , & appsv1 . Daemon } 
func ( c * Fake Daemon Sets ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( daemonsets Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & appsv1 . Daemon Set } 
func New CSR Approving Controller ( client clientset . Interface , csr Informer certificatesinformers . Certificate Signing Request Informer ) * certificates . Certificate Controller { approver := & sar return certificates . New Certificate Controller ( client , csr } 
func ( No Double Key Schema ) Validate if err := validate No Duplicate if err := validate No Duplicate return utilerrors . New } 
func ( c Conjunctive Schema ) Validate for ix := range schemas { if err := schemas [ ix ] . Validate return utilerrors . New } 
func ( p REST Storage Provider ) New REST Storage ( api Resource Config Source serverstorage . API Resource Config Source , rest Options Getter generic . REST Options Getter ) ( genericapiserver . API Group Info , bool ) { api Group Info := genericapiserver . New Default API Group Info ( nodeinternal . Group Name , legacyscheme . Scheme , legacyscheme . Parameter if api Resource Config Source . Version Enabled ( nodev1alpha1 . Scheme Group Version ) { api Group Info . Versioned Resources Storage Map [ nodev1alpha1 . Scheme Group Version . Version ] = p . v1alpha1Storage ( api Resource Config Source , rest Options if api Resource Config Source . Version Enabled ( nodev1beta1 . Scheme Group Version ) { api Group Info . Versioned Resources Storage Map [ nodev1beta1 . Scheme Group Version . Version ] = p . v1beta1Storage ( api Resource Config Source , rest Options return api Group } 
func Build Handler Chain ( api Handler http . Handler , authorization Info * apiserver . Authorization Info , authentication Info * apiserver . Authentication Info ) http . Handler { request Info Resolver := & apirequest . Request Info failed handler := api if authorization Info != nil { handler = genericapifilters . With Authorization ( api Handler , authorization if authentication Info != nil { handler = genericapifilters . With Authentication ( handler , authentication Info . Authenticator , failed handler = genericapifilters . With Request Info ( handler , request Info handler = genericfilters . With Panic } 
func New Base Handler ( c * componentbaseconfig . Debugging Configuration , checks ... healthz . Healthz Checker ) * mux . Path Recorder Mux { mux := mux . New Path Recorder healthz . Install if c . Enable if c . Enable Contention Profiling { goruntime . Set Block Profile configz . Install } 
} 
func ( q * Type ) Shut q . shutting } 
func ( c * component Statuses ) Update ( component Status * v1 . Component Status ) ( result * v1 . Component Status , err error ) { result = & v1 . Component err = c . client . Put ( ) . Resource ( " " ) . Name ( component Status . Name ) . Body ( component } 
func New UID } 
func New Generic Webhook ( scheme * runtime . Scheme , codec Factory serializer . Codec Factory , kube Config File string , group Versions [ ] schema . Group Version , initial Backoff time . Duration ) ( * Generic Webhook , error ) { return new Generic Webhook ( scheme , codec Factory , kube Config File , group Versions , initial Backoff , default Request } 
func ( g * Generic Webhook ) With Exponential Backoff ( webhook With Exponential Backoff ( g . Initial Backoff , func ( ) error { result = webhook } 
func With Exponential Backoff ( initial Backoff time . Duration , webhook Fn func ( ) error ) error { backoff := wait . Backoff { Duration : initial wait . Exponential Backoff ( backoff , func ( ) ( bool , error ) { err = webhook // these errors indicate a transient error that should be retried. if net . Is Connection Reset ( err ) || apierrors . Is Internal Error ( err ) || apierrors . Is Timeout ( err ) || apierrors . Is Too Many // if the error sends the Retry-After header, we respect it as an explicit confirmation we should retry. if _ , should Retry := apierrors . Suggests Client Delay ( err ) ; should } 
func ( s * flunder Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Flunder , err error ) { err = cache . List } 
func ( s * flunder Lister ) Flunders ( namespace string ) Flunder Namespace Lister { return flunder Namespace } 
func ( s flunder Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Flunder , err error ) { err = cache . List All By } 
func Get Reference ( scheme * runtime . Scheme , obj runtime . Object ) ( * v1 . Object Reference , error ) { if obj == nil { return nil , Err Nil if ref , ok := obj . ( * v1 . Object gvk := obj . Get Object Kind ( ) . Group Version if len ( kind ) == 0 { // TODO: this is wrong gvks , _ , err := scheme . Object // An object that implements only List has enough metadata to build a reference var list object if err != nil { list Meta , err = meta . Common } else { list Meta = object // if the object referenced is actually persisted, we can also get version from meta version := gvk . Group if len ( version ) == 0 { self Link := list Meta . Get Self if len ( self Link ) == 0 { return nil , Err No Self self Link Url , err := url . Parse ( self // example paths: /<prefix>/<version>/* parts := strings . Split ( self Link if len ( parts ) < 4 { return nil , fmt . Errorf ( " " , self // only has list metadata if object Meta == nil { return & v1 . Object Reference { Kind : kind , API Version : version , Resource Version : list Meta . Get Resource return & v1 . Object Reference { Kind : kind , API Version : version , Name : object Meta . Get Name ( ) , Namespace : object Meta . Get Namespace ( ) , UID : object Meta . Get UID ( ) , Resource Version : object Meta . Get Resource } 
func Get Partial Reference ( scheme * runtime . Scheme , obj runtime . Object , field Path string ) ( * v1 . Object Reference , error ) { ref , err := Get ref . Field Path = field } 
func ( u * Upgrade ) Can Upgrade Kubelets ( ) bool { // If there are multiple different versions now, an upgrade is possible (even if only for a subset of the nodes) if len ( u . Before . Kubelet // Don't report something available for upgrade if we don't know the current state if len ( u . Before . Kubelet // if the same version number existed both before and after, we don't have to upgrade it _ , same Version Found := u . Before . Kubelet Versions [ u . After . Kube return ! same Version } 
func ( u * Upgrade ) Can Upgrade Etcd ( ) bool { return u . Before . Etcd Version != u . After . Etcd } 
func Get Available Upgrades ( version Getter Impl Version Getter , experimental Upgrades Allowed , rc Upgrades Allowed bool , etcd Client etcdutil . Cluster Interrogator , dns Type kubeadmapi . DNS Add On // Get the cluster version cluster Version Str , cluster Version , err := version Getter Impl . Cluster // Get current kubeadm CLI version kubeadm Version Str , kubeadm Version , err := version Getter Impl . Kubeadm // Get and output the current latest stable version stable Version Str , stable Version , err := version Getter Impl . Version From CI stable Version Str , stable Version = kubeadm Version Str , kubeadm // Get the kubelet versions in the cluster kubelet Versions , err := version Getter Impl . Kubelet // Get current etcd version etcd Version , err := etcd Client . Get current DNS Type , dns Version , err := dns . Deployed DNS // Construct a descriptor for the current state of the world before State := Cluster State { Kube Version : cluster Version Str , DNS Type : current DNS Type , DNS Version : dns Version , Kubeadm Version : kubeadm Version Str , Kubelet Versions : kubelet Versions , Etcd Version : etcd // Do a "dumb guess" that a new minor upgrade is available just because the latest stable version is higher than the cluster version // This guess will be corrected once we know if there is a patch version available can Do Minor Upgrade := cluster Version . Less Than ( stable // A patch version doesn't exist if the cluster version is higher than or equal to the current stable version // in the case that a user is trying to upgrade from, let's say, v1.8.0-beta.2 to v1.8.0-rc.1 (given we support such upgrades experimentally) // a stable-1.8 branch doesn't exist yet. Hence this check. if patch Version Branch Exists ( cluster Version , stable Version ) { current Branch := get Branch From Version ( cluster Version version Label := fmt . Sprintf ( " " , current description := fmt . Sprintf ( " " , current // Get and output the latest patch version for the cluster branch patch Version Str , patch Version , err := version Getter Impl . Version From CI Label ( version } else { // Check if a minor version upgrade is possible when a patch release exists // It's only possible if the latest patch version is higher than the current patch version // If that's the case, they must be on different branches => a newer minor version can be upgraded to can Do Minor Upgrade = minor Upgrade Possible With Patch Release ( stable Version , patch // If the cluster version is lower than the newest patch version, we should inform about the possible upgrade if patch Upgrade Possible ( cluster Version , patch Version ) { // The kubeadm version has to be upgraded to the latest patch version new Kubeadm Ver := patch Version if kubeadm Version . At Least ( patch Version ) { // In this case, the kubeadm CLI version is new enough. Don't display an update suggestion for kubeadm by making .New Kubeadm Version equal .Current Kubeadm Version new Kubeadm Ver = kubeadm Version upgrades = append ( upgrades , Upgrade { Description : description , Before : before State , After : Cluster State { Kube Version : patch Version Str , DNS Type : dns Type , DNS Version : kubeadmconstants . Get DNS Version ( dns Type ) , Kubeadm Version : new Kubeadm Ver , Etcd Version : get Suggested Etcd Version ( patch Version Str ) , // Kubelet if can Do Minor Upgrade { upgrades = append ( upgrades , Upgrade { Description : " " , Before : before State , After : Cluster State { Kube Version : stable Version Str , DNS Type : dns Type , DNS Version : kubeadmconstants . Get DNS Version ( dns Type ) , Kubeadm Version : stable Version Str , Etcd Version : get Suggested Etcd Version ( stable Version Str ) , // Kubelet if experimental Upgrades Allowed || rc Upgrades Allowed { // dl.k8s.io/release/latest.txt is ALWAYS an alpha.X version // dl.k8s.io/release/latest-1.X.txt is first v1.X.0-alpha.0 -> v1.X.0-alpha.Y, then v1.X.0-beta.0 to v1.X.0-beta.Z, then v1.X.0-rc.1 to v1.X.0-rc.W. // After the v1.X.0 release, latest-1.X.txt is always a beta.0 version. Let's say the latest stable version on the v1.7 branch is v1.7.3, then the // latest-1.7 version is v1.7.4-beta.0 // Worth noticing is that when the release-1.X branch is cut; there are two versions tagged: v1.X.0-beta.0 AND v1.(X+1).alpha.0 // The v1.(X+1).alpha.0 is pretty much useless and should just be ignored, as more betas may be released that have more features than the initial v1.(X+1).alpha.0 // So what we do below is getting the latest overall version, always an v1.X.0-alpha.Y version. Then we get latest-1.(X-1) version. This version may be anything // between v1.(X-1).0-beta.0 and v1.(X-1).Z-beta.0. At some point in time, latest-1.(X-1) will point to v1.(X-1).0-rc.1. Then we should show it. // The flow looks like this (with time on the X axis): // v1.8.0-alpha.1 -> v1.8.0-alpha.2 -> v1.8.0-alpha.3 | release-1.8 branch | v1.8.0-beta.0 -> v1.8.0-beta.1 -> v1.8.0-beta.2 -> v1.8.0-rc.1 -> v1.8.0 -> v1.8.1 // v1.9.0-alpha.0 -> v1.9.0-alpha.1 -> v1.9.0-alpha.2 // Get and output the current latest unstable version latest Version Str , latest Version , err := version Getter Impl . Version From CI minor Unstable := latest // Get and output the current latest unstable version previous Branch := fmt . Sprintf ( " " , minor previous Branch Latest Version Str , previous Branch Latest Version , err := version Getter Impl . Version From CI Label ( previous // If that previous latest version is an RC, R Cs are allowed and the cluster version is lower than the RC version, show the upgrade if rc Upgrades Allowed && rc Upgrade Possible ( cluster Version , previous Branch Latest Version ) { upgrades = append ( upgrades , Upgrade { Description : " " , Before : before State , After : Cluster State { Kube Version : previous Branch Latest Version Str , DNS Type : dns Type , DNS Version : kubeadmconstants . Get DNS Version ( dns Type ) , Kubeadm Version : previous Branch Latest Version Str , Etcd Version : get Suggested Etcd Version ( previous Branch Latest Version Str ) , // Kubelet // Show the possibility if experimental upgrades are allowed if experimental Upgrades Allowed && cluster Version . Less Than ( latest Version ) { // Default to assume that the experimental version to show is the unstable one unstable Kube Version := latest Version unstable Kube DNS Version := kubeadmconstants . Get DNS Version ( dns // e should not display alpha.0. The previous branch's beta/rc versions are more relevant due how the kube branching process works. if latest Version . Pre Release ( ) == " " { unstable Kube Version = previous Branch Latest Version unstable Kube DNS Version = kubeadmconstants . Get DNS Version ( dns upgrades = append ( upgrades , Upgrade { Description : " " , Before : before State , After : Cluster State { Kube Version : unstable Kube Version , DNS Type : dns Type , DNS Version : unstable Kube DNS Version , Kubeadm Version : unstable Kube Version , Etcd Version : get Suggested Etcd Version ( unstable Kube Version ) , // Kubelet } 
func ( c * cron Jobs ) Create ( cron Job * v1beta1 . Cron Job ) ( result * v1beta1 . Cron Job , err error ) { result = & v1beta1 . Cron err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( cron } 
func ( c * cron Jobs ) Update ( cron Job * v1beta1 . Cron Job ) ( result * v1beta1 . Cron Job , err error ) { result = & v1beta1 . Cron err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( cron Job . Name ) . Body ( cron } 
func ( c * Cache ) Assume Pod ( pod * v1 . Pod ) error { c . Assume } 
func ( c * Cache ) Forget Pod ( pod * v1 . Pod ) error { c . Forget } 
func ( c * Cache ) Is Assumed Pod ( pod * v1 . Pod ) ( bool , error ) { return c . Is Assumed Pod } 
func ( c * Cache ) Get Pod ( pod * v1 . Pod ) ( * v1 . Pod , error ) { return c . Get Pod } 
func ( c * Cache ) Filtered List ( filter algorithm . Pod } 
func New ( f internalinterfaces . Shared Informer Factory , namespace string , tweak List Options internalinterfaces . Tweak List Options Func ) Interface { return & version { factory : f , namespace : namespace , tweak List Options : tweak List } 
func ( v * version ) Cron Jobs ( ) Cron Job Informer { return & cron Job Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Namespace Controller Configuration ) ( nil ) , ( * config . Namespace Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Namespace Controller Configuration_To_config_Namespace Controller Configuration ( a . ( * v1alpha1 . Namespace Controller Configuration ) , b . ( * config . Namespace Controller if err := s . Add Generated Conversion Func ( ( * config . Namespace Controller Configuration ) ( nil ) , ( * v1alpha1 . Namespace Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Namespace Controller Configuration_To_v1alpha1_Namespace Controller Configuration ( a . ( * config . Namespace Controller Configuration ) , b . ( * v1alpha1 . Namespace Controller if err := s . Add Conversion Func ( ( * config . Namespace Controller Configuration ) ( nil ) , ( * v1alpha1 . Namespace Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Namespace Controller Configuration_To_v1alpha1_Namespace Controller Configuration ( a . ( * config . Namespace Controller Configuration ) , b . ( * v1alpha1 . Namespace Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Namespace Controller Configuration ) ( nil ) , ( * config . Namespace Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Namespace Controller Configuration_To_config_Namespace Controller Configuration ( a . ( * v1alpha1 . Namespace Controller Configuration ) , b . ( * config . Namespace Controller } 
func Convert_v1alpha1_Group Resource_To_v1_Group Resource ( in * v1alpha1 . Group Resource , out * v1 . Group Resource , s conversion . Scope ) error { return auto Convert_v1alpha1_Group Resource_To_v1_Group } 
func Convert_v1_Group Resource_To_v1alpha1_Group Resource ( in * v1 . Group Resource , out * v1alpha1 . Group Resource , s conversion . Scope ) error { return auto Convert_v1_Group Resource_To_v1alpha1_Group } 
func New Authorizer ( always Allow paths := sets . New for _ , p := range always Allow Paths { p = strings . Trim if strings . Contains if strings . Has return authorizer . Authorizer Func ( func ( a authorizer . Attributes ) ( authorizer . Decision , string , error ) { if a . Is Resource Request ( ) { return authorizer . Decision No pth := strings . Trim Prefix ( a . Get if paths . Has ( pth ) { return authorizer . Decision for _ , prefix := range prefixes { if strings . Has Prefix ( pth , prefix ) { return authorizer . Decision return authorizer . Decision No } 
func Controller Revision } 
func New Controller Revision ( parent metav1 . Object , parent Kind schema . Group Version Kind , template Labels map [ string ] string , data runtime . Raw Extension , revision int64 , collision Count * int32 ) ( * apps . Controller Revision , error ) { label for k , v := range template Labels { label cr := & apps . Controller Revision { Object Meta : metav1 . Object Meta { Labels : label Map , Owner References : [ ] metav1 . Owner Reference { * metav1 . New Controller Ref ( parent , parent hash := Hash Controller Revision ( cr , collision cr . Name = Controller Revision Name ( parent . Get cr . Labels [ Controller Revision Hash } 
func Hash Controller Revision ( revision * apps . Controller if revision . Data . Object != nil { hashutil . Deep Hash if probe != nil { hf . Write ( [ ] byte ( strconv . Format return rand . Safe Encode } 
func Equal Revision ( lhs * apps . Controller Revision , rhs * apps . Controller Revision ) bool { var lhs Hash , rhs if hs , found := lhs . Labels [ Controller Revision Hash Label ] ; found { hash , err := strconv . Parse if err == nil { lhs * lhs if hs , found := rhs . Labels [ Controller Revision Hash Label ] ; found { hash , err := strconv . Parse if err == nil { rhs * rhs if lhs Hash != nil && rhs Hash != nil && * lhs Hash != * rhs return bytes . Equal ( lhs . Data . Raw , rhs . Data . Raw ) && apiequality . Semantic . Deep } 
func Find Equal Revisions ( revisions [ ] * apps . Controller Revision , needle * apps . Controller Revision ) [ ] * apps . Controller Revision { var eq [ ] * apps . Controller for i := range revisions { if Equal } 
func ( br by Revision ) Less ( i , j int ) bool { if br [ i ] . Revision == br [ j ] . Revision { if br [ j ] . Creation Timestamp . Equal ( & br [ i ] . Creation return br [ j ] . Creation Timestamp . After ( br [ i ] . Creation } 
func New History ( client clientset . Interface , lister appslisters . Controller Revision Lister ) Interface { return & real } 
func New Fake History ( informer appsinformers . Controller Revision Informer ) Interface { return & fake History { informer . Informer ( ) . Get } 
func parse Percentage ( v string ) ( int64 , error ) { if ! strings . Has percentage , err := strconv . Parse Int ( strings . Trim } 
func Parse QOS Reserved ( m map [ string ] string ) ( * map [ v1 . Resource Name ] int64 , error ) { reservations := make ( map [ v1 . Resource for k , v := range m { switch v1 . Resource Name ( k ) { // Only memory resources are supported. case v1 . Resource Memory : q , err := parse reservations [ v1 . Resource } 
func New Network Policy Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Network Policy Informer ( client , namespace , resync } 
func new Extended Resource Toleration ( ) * plugin { return & plugin { Handler : admission . New } 
func ( p * plugin ) Admit ( attributes admission . Attributes , o admission . Object Interfaces ) error { // Ignore all calls to subresources or resources other than pods. if len ( attributes . Get Subresource ( ) ) != 0 || attributes . Get Resource ( ) . Group pod , ok := attributes . Get if ! ok { return errors . New Bad Request ( fmt . Sprintf ( " " , attributes . Get for _ , container := range pod . Spec . Containers { for resource Name := range container . Resources . Requests { if helper . Is Extended Resource Name ( resource Name ) { resources . Insert ( string ( resource for _ , container := range pod . Spec . Init Containers { for resource Name := range container . Resources . Requests { if helper . Is Extended Resource Name ( resource Name ) { resources . Insert ( string ( resource // Doing .List() so that we get a stable sorted list. // This allows us to test adding tolerations for multiple extended resources. for _ , resource := range resources . List ( ) { helper . Add Or Update Toleration In Pod ( pod , & core . Toleration { Key : resource , Operator : core . Toleration Op Exists , Effect : core . Taint Effect No } 
func ( Simple Meta Factory ) Interpret ( data [ ] byte ) ( * schema . Group Version Kind , error ) { find Kind := struct { // +optional API Version string `json:"api if err := json . Unmarshal ( data , & find gv , err := schema . Parse Group Version ( find Kind . API return & schema . Group Version Kind { Group : gv . Group , Version : gv . Version , Kind : find } 
func ( plugin * cinder Plugin ) New Block Volume Mapper ( spec * volume . Spec , pod * v1 . Pod , _ volume . Volume Options ) ( volume . Block Volume Mapper , error ) { // If this is called via Generate Unmap Device return plugin . new Block Volume Mapper Internal ( spec , uid , & Disk Util { } , plugin . host . Get Mounter ( plugin . Get Plugin } 
func ( cd * cinder Volume ) Get Global Map Path ( spec * volume . Spec ) ( string , error ) { pd Name , _ , _ , err := get Volume return filepath . Join ( cd . plugin . host . Get Volume Device Plugin Dir ( cinder Volume Plugin Name ) , pd } 
func ( cd * cinder Volume ) Get Pod Device Map Path ( ) ( string , string ) { name := cinder Volume Plugin return cd . plugin . host . Get Pod Volume Device Dir ( cd . pod UID , utilstrings . Escape Qualified Name ( name ) ) , cd . vol } 
func ( v * vsphere Volume ) Get Global Map Path ( spec * volume . Spec ) ( string , error ) { volume Source , _ , err := get Volume return filepath . Join ( v . plugin . host . Get Volume Device Plugin Dir ( vsphere Volume Plugin Name ) , string ( volume Source . Volume } 
func New Replica Set Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Replica Set Informer ( client , namespace , resync } 
func gen Status ( t * types . Type ) bool { // Default to true if we have a Status member has for _ , m := range t . Members { if m . Name == " " { has return has Status && ! util . Must Parse Client Gen Tags ( append ( t . Second Closest Comment Lines , t . Comment Lines ... ) ) . No } 
func ( g * gen Client For Type ) Generate Type ( c * generator . Context , t * types . Type , w io . Writer ) error { sw := generator . New Snippet tags , err := util . Parse Client Gen Tags ( append ( t . Second Closest Comment Lines , t . Comment type extended Interface extended Methods := [ ] extended Interface for _ , e := range tags . Extensions { input result // TODO: Extract this to some helper method as this code is copied into // 2 other places. if len ( e . Input Type Override ) > 0 { if name , pkg := e . Input ( ) ; len ( pkg ) > 0 { new input Type = * new } else { input Type . Name . Name = e . Input Type if len ( e . Result Type Override ) > 0 { if name , pkg := e . Result ( ) ; len ( pkg ) > 0 { new result Type = * new } else { result Type . Name . Name = e . Result Type var updated if _ , exists := subresource Default Verb Templates [ e . Verb Type ] ; e . Is Subresource ( ) && exists { updated Verbtemplate = e . Verb Name + " " + strings . Trim Prefix ( subresource Default Verb Templates [ e . Verb Type ] , strings . Title ( e . Verb } else { updated Verbtemplate = e . Verb Name + " " + strings . Trim Prefix ( default Verb Templates [ e . Verb Type ] , strings . Title ( e . Verb extended Methods = append ( extended Methods , extended Interface Method { template : updated Verbtemplate , args : map [ string ] interface { } { " " : t , " " : & input Type , " " : & result m := map [ string ] interface { } { " " : t , " " : t , " " : t , " " : pkg , " " : namer . IC ( pkg ) , " " : ! tags . Non Namespaced , " " : namer . IC ( g . group ) , " " : false , " " : " " , " " : g . group Go Name , " " : namer . IC ( g . version ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Variable ( types . Name { Package : filepath . Join ( g . clientset sw . Do ( getter if tags . Non Namespaced { sw . Do ( getter Non } else { sw . Do ( getter sw . Do ( interface if ! tags . No Verbs { if ! gen Status ( t ) { tags . Skip Verbs = append ( tags . Skip interface if len ( extended Methods ) > 0 { interface sw . Do ( " \n " + generate Interface ( tags ) + interface // add extended verbs into interface for _ , v := range extended Methods { sw . Do ( v . template + interface sw . Do ( interface if tags . Non Namespaced { sw . Do ( struct Non sw . Do ( new Struct Non } else { sw . Do ( struct sw . Do ( new Struct if tags . No if tags . Has Verb ( " " ) { sw . Do ( get if tags . Has Verb ( " " ) { sw . Do ( list if tags . Has Verb ( " " ) { sw . Do ( watch if tags . Has Verb ( " " ) { sw . Do ( create if tags . Has Verb ( " " ) { sw . Do ( update if tags . Has Verb ( " " ) { sw . Do ( update Status if tags . Has Verb ( " " ) { sw . Do ( delete if tags . Has Verb ( " " ) { sw . Do ( delete Collection if tags . Has Verb ( " " ) { sw . Do ( patch // generate expansion methods for _ , e := range tags . Extensions { input result if len ( e . Input Type Override ) > 0 { if name , pkg := e . Input ( ) ; len ( pkg ) > 0 { new input Type = * new } else { input Type . Name . Name = e . Input Type if len ( e . Result Type Override ) > 0 { if name , pkg := e . Result ( ) ; len ( pkg ) > 0 { new result Type = * new } else { result Type . Name . Name = e . Result Type m [ " " ] = & input m [ " " ] = & result m [ " " ] = e . Sub Resource if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , get Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , get if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , list Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , list // TODO: Figure out schemantic for watching a sub-resource. if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , watch if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , create Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , create if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , update Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , update // TODO: Figure out schemantic for deleting a sub-resource (what arguments // are passed, does it need two names? etc. if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , delete if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , patch } 
func adjust Template ( name , verb Type , template string ) string { return strings . Replace ( template , " " + strings . Title ( verb } 
func New For Config Or cs . example V1 = examplev1 . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . example cs . Discovery Client = discovery . New Discovery } 
func new Context ( ctx context . Context , resource , name string , gvk schema . Group Version Kind ) context . Context { old Info , found := genericapirequest . Request Info new Info := genericapirequest . Request Info { Is Resource Request : true , Verb : " " , Namespace : old Info . Namespace , Resource : resource , Name : name , Parts : [ ] string { resource , name } , API Group : gvk . Group , API return genericapirequest . With Request Info ( ctx , & new } 
func New For Config ( c * rest . Config ) ( * Clientset , error ) { config Shallow if config Shallow Copy . Rate Limiter == nil && config Shallow Copy . QPS > 0 { config Shallow Copy . Rate Limiter = flowcontrol . New Token Bucket Rate Limiter ( config Shallow Copy . QPS , config Shallow cs . apiregistration V1beta1 , err = apiregistrationv1beta1 . New For Config ( & config Shallow cs . apiregistration V1 , err = apiregistrationv1 . New For Config ( & config Shallow cs . Discovery Client , err = discovery . New Discovery Client For Config ( & config Shallow } 
func New For Config Or cs . apiregistration V1beta1 = apiregistrationv1beta1 . New For Config Or cs . apiregistration V1 = apiregistrationv1 . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . apiregistration cs . apiregistration cs . Discovery Client = discovery . New Discovery } 
func Default Throttle ( ) * auditregistrationv1alpha1 . Webhook Throttle Config { return & auditregistrationv1alpha1 . Webhook Throttle Config { QPS : utilpointer . Int64Ptr ( Default QPS ) , Burst : utilpointer . Int64Ptr ( Default } 
func Set Defaults_Audit Sink ( obj * auditregistrationv1alpha1 . Audit Sink ) { if obj . Spec . Webhook . Throttle != nil { if obj . Spec . Webhook . Throttle . QPS == nil { obj . Spec . Webhook . Throttle . QPS = utilpointer . Int64Ptr ( Default if obj . Spec . Webhook . Throttle . Burst == nil { obj . Spec . Webhook . Throttle . Burst = utilpointer . Int64Ptr ( Default } else { obj . Spec . Webhook . Throttle = Default } 
func Before Create ( strategy REST Create Strategy , ctx context . Context , obj runtime . Object ) error { object Meta , kind , kerr := object Meta And if strategy . Namespace Scoped ( ) { if ! Valid Namespace ( ctx , object Meta ) { return errors . New Bad } else if len ( object Meta . Get Namespace ( ) ) > 0 { object Meta . Set Namespace ( metav1 . Namespace object Meta . Set Deletion object Meta . Set Deletion Grace Period strategy . Prepare For Fill Object Meta System Fields ( object if len ( object Meta . Get Generate Name ( ) ) > 0 && len ( object Meta . Get Name ( ) ) == 0 { object Meta . Set Name ( strategy . Generate Name ( object Meta . Get Generate // Initializers are a deprecated alpha field and should not be saved object Meta . Set // Ensure managed Fields is not set unless the feature is enabled if ! utilfeature . Default Feature Gate . Enabled ( features . Server Side Apply ) { object Meta . Set Managed // Cluster Name is ignored and should not be saved if len ( object Meta . Get Cluster Name ( ) ) > 0 { object Meta . Set Cluster if errs := strategy . Validate ( ctx , obj ) ; len ( errs ) > 0 { return errors . New Invalid ( kind . Group Kind ( ) , object Meta . Get // Custom validation (including name validation) passed // Now run common validation on object meta // Do this *after* custom validation so that specific error messages are shown whenever possible if errs := genericvalidation . Validate Object Meta Accessor ( object Meta , strategy . Namespace Scoped ( ) , path . Validate Path Segment Name , field . New Path ( " " ) ) ; len ( errs ) > 0 { return errors . New Invalid ( kind . Group Kind ( ) , object Meta . Get } 
func Check Generated Name Error ( strategy REST Create Strategy , err error , obj runtime . Object ) error { if ! errors . Is Already object Meta , kind , kerr := object Meta And if len ( object Meta . Get Generate return errors . New Server Timeout For Kind ( kind . Group } 
func object Meta And Kind ( typer runtime . Object Typer , obj runtime . Object ) ( metav1 . Object , schema . Group Version Kind , error ) { object if err != nil { return nil , schema . Group Version Kind { } , errors . New Internal kinds , _ , err := typer . Object if err != nil { return nil , schema . Group Version Kind { } , errors . New Internal return object } 
func ( rs * REST ) List ( ctx context . Context , options * metainternalversion . List Options ) ( runtime . Object , error ) { servers := rs . Get Servers To wait := sync . Wait statuses := make ( chan api . Component status := rs . get Component reply := [ ] api . Component return & api . Component Status } 
func ( c * Fake Roles ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( roles } 
func New Tokens Controller ( service Accounts informers . Service Account Informer , secrets informers . Secret Informer , cl clientset . Interface , options Tokens Controller Options ) ( * Tokens Controller , error ) { max Retries := options . Max if max Retries == 0 { max e := & Tokens Controller { client : cl , token : options . Token Generator , root CA : options . Root CA , sync Service Account Queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , sync Secret Queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , max Retries : max if cl != nil && cl . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { if err := metrics . Register Metric And Track Rate Limiter Usage ( " " , cl . Core V1 ( ) . REST Client ( ) . Get Rate e . service Accounts = service e . service Account Synced = service Accounts . Informer ( ) . Has service Accounts . Informer ( ) . Add Event Handler With Resync Period ( cache . Resource Event Handler Funcs { Add Func : e . queue Service Account Sync , Update Func : e . queue Service Account Update Sync , Delete Func : e . queue Service Account Sync , } , options . Service Account secret Cache := secrets . Informer ( ) . Get e . updated Secrets = cache . New Integer Resource Version Mutation Cache ( secret Cache , secret e . secret Synced = secrets . Informer ( ) . Has secrets . Informer ( ) . Add Event Handler With Resync Period ( cache . Filtering Resource Event Handler { Filter Func : func ( obj interface { } ) bool { switch t := obj . ( type ) { case * v1 . Secret : return t . Type == v1 . Secret Type Service Account default : utilruntime . Handle } , Handler : cache . Resource Event Handler Funcs { Add Func : e . queue Secret Sync , Update Func : e . queue Secret Update Sync , Delete Func : e . queue Secret Sync , } , } , options . Secret } 
func ( e * Tokens Controller ) Run ( workers int , stop Ch <- chan struct { } ) { // Shut down queues defer utilruntime . Handle defer e . sync Service Account Queue . Shut defer e . sync Secret Queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , e . service Account Synced , e . secret for i := 0 ; i < workers ; i ++ { go wait . Until ( e . sync Service Account , 0 , stop go wait . Until ( e . sync Secret , 0 , stop <- stop } 
func ( e * Tokens Controller ) retry Or Forget ( queue workqueue . Rate Limiting requeue Count := queue . Num if requeue Count < e . max Retries { queue . Add Rate klog . V ( 4 ) . Infof ( " " , requeue } 
func ( e * Tokens Controller ) ensure Referenced Token ( service Account * v1 . Service Account ) ( /* retry */ bool , error ) { if has Token , err := e . has Referenced Token ( service } else if has // We don't want to update the cache's copy of the service account // so add the secret to a freshly retrieved copy of the service account service Accounts := e . client . Core V1 ( ) . Service Accounts ( service live Service Account , err := service Accounts . Get ( service Account . Name , metav1 . Get if err != nil { // Retry if we cannot fetch the live service account (for a Not if live Service Account . Resource Version != service Account . Resource Version { // Retry if our live Service Account doesn't match our cache's resource Version (either the live lookup or our cache are stale) klog . V ( 4 ) . Infof ( " " , live Service Account . Resource Version , service Account . Resource // Build the secret secret := & v1 . Secret { Object Meta : metav1 . Object Meta { Name : secret . Strategy . Generate Name ( fmt . Sprintf ( " " , service Account . Name ) ) , Namespace : service Account . Namespace , Annotations : map [ string ] string { v1 . Service Account Name Key : service Account . Name , v1 . Service Account UID Key : string ( service Account . UID ) , } , } , Type : v1 . Secret Type Service Account // Generate the token token , err := e . token . Generate Token ( serviceaccount . Legacy Claims ( * service secret . Data [ v1 . Service Account Token secret . Data [ v1 . Service Account Namespace Key ] = [ ] byte ( service if e . root CA != nil && len ( e . root CA ) > 0 { secret . Data [ v1 . Service Account Root CA Key ] = e . root // Save the secret created Token , err := e . client . Core V1 ( ) . Secrets ( service // Manually add the new token to the cache store. // This prevents the service account update (below) triggering another token creation, if the referenced token couldn't be found in the store e . updated Secrets . Mutation ( created // Try to add a reference to the newly created token to the service account added err = clientretry . Retry On Conflict ( clientretry . Default Retry , func ( ) error { // refresh live Service Account on every retry defer func ( ) { live Service // fetch the live service account if needed, and verify the UID matches and that we still need a token if live Service Account == nil { live Service Account , err = service Accounts . Get ( service Account . Name , metav1 . Get if live Service Account . UID != service if has Token , err := e . has Referenced Token ( live Service } else if has // Try to add a reference to the token live Service Account . Secrets = append ( live Service Account . Secrets , v1 . Object if _ , err := service Accounts . Update ( live Service added if ! added delete Opts := & metav1 . Delete Options { Preconditions : & metav1 . Preconditions { UID : & created if delete Err := e . client . Core V1 ( ) . Secrets ( created Token . Namespace ) . Delete ( created Token . Name , delete Opts ) ; delete Err != nil { klog . Error ( delete if err != nil { if apierrors . Is Conflict ( err ) || apierrors . Is Not Found ( err ) { // if we got a Conflict error, the service account was updated by someone else, and we'll get an update notification later // if we got a Not } 
func ( e * Tokens Controller ) has Referenced Token ( service Account * v1 . Service Account ) ( bool , error ) { if len ( service all Secrets , err := e . list Token Secrets ( service referenced Secrets := get Secret References ( service for _ , secret := range all Secrets { if referenced } 
func ( e * Tokens Controller ) generate Token If Needed ( service Account * v1 . Service Account , cached Secret * v1 . Secret ) ( /* retry */ bool , error ) { // Check the cached secret to see if changes are needed if needs CA , needs Namespace , needs Token := e . secret Update Needed ( cached Secret ) ; ! needs CA && ! needs Token && ! needs // We don't want to update the cache's copy of the secret // so add the token to a freshly retrieved copy of the secret secrets := e . client . Core V1 ( ) . Secrets ( cached live Secret , err := secrets . Get ( cached Secret . Name , metav1 . Get if err != nil { // Retry for any error other than a Not Found return ! apierrors . Is Not if live Secret . Resource Version != cached Secret . Resource Version { // our view of the secret is not up to date // we'll get notified of an update event later and get to try again klog . V ( 2 ) . Infof ( " " , live Secret . Namespace , live needs CA , needs Namespace , needs Token := e . secret Update Needed ( live if ! needs CA && ! needs Token && ! needs if live Secret . Annotations == nil { live if live Secret . Data == nil { live // Set the CA if needs CA { live Secret . Data [ v1 . Service Account Root CA Key ] = e . root // Set the namespace if needs Namespace { live Secret . Data [ v1 . Service Account Namespace Key ] = [ ] byte ( live // Generate the token if needs Token { token , err := e . token . Generate Token ( serviceaccount . Legacy Claims ( * service Account , * live live Secret . Data [ v1 . Service Account Token // Set annotations live Secret . Annotations [ v1 . Service Account Name Key ] = service live Secret . Annotations [ v1 . Service Account UID Key ] = string ( service // Save the secret _ , err = secrets . Update ( live if apierrors . Is Conflict ( err ) || apierrors . Is Not Found ( err ) { // if we got a Conflict error, the secret was updated by someone else, and we'll get an update notification later // if we got a Not } 
func ( e * Tokens Controller ) remove Secret Reference ( sa Namespace string , sa Name string , sa UID types . UID , secret Name string ) error { // We don't want to update the cache's copy of the service account // so remove the secret from a freshly retrieved copy of the service account service Accounts := e . client . Core V1 ( ) . Service Accounts ( sa service Account , err := service Accounts . Get ( sa Name , metav1 . Get // Ignore Not Found errors when attempting to remove a reference if apierrors . Is Not // Short-circuit if the UID doesn't match if len ( sa UID ) > 0 && sa UID != service // Short-circuit if the secret is no longer referenced if ! get Secret References ( service Account ) . Has ( secret // Remove the secret secrets := [ ] v1 . Object for _ , s := range service Account . Secrets { if s . Name != secret service _ , err = service Accounts . Update ( service // Ignore Not Found errors when attempting to remove a reference if apierrors . Is Not } 
func ( e * Tokens Controller ) list Token Secrets ( service Account * v1 . Service Account ) ( [ ] * v1 . Secret , error ) { namespace Secrets , err := e . updated Secrets . By Index ( " " , service for _ , obj := range namespace if serviceaccount . Is Service Account Token ( secret , service } 
func Print Model Description ( fields Path [ ] string , w io . Writer , schema proto . Schema , gvk schema . Group Version Kind , recursive bool ) error { field if len ( fields Path ) != 0 { field Name = fields Path [ len ( fields // Go down the fields Path to find what we're trying to explain schema , err := Lookup Schema For Field ( schema , fields b := fields Printer return Print Model ( field } 
func printable if ! v . Is if ! v . Type ( ) . Implements ( error Type ) && ! v . Type ( ) . Implements ( fmt Stringer Type ) { if v . Can Addr ( ) && ( reflect . Ptr To ( v . Type ( ) ) . Implements ( error Type ) || reflect . Ptr To ( v . Type ( ) ) . Implements ( fmt Stringer } 
func can Be } 
func is True ( val reflect . Value ) ( truth , ok bool ) { if ! val . Is case reflect . Chan , reflect . Func , reflect . Ptr , reflect . Interface : truth = ! val . Is } 
func New Device Allocator ( ) Device Allocator { possible Devices := make ( map [ mount for _ , first Char := range [ ] rune { 'b' , 'c' } { for i := 'a' ; i <= 'z' ; i ++ { dev := mount Device ( [ ] rune { first possible return & device Allocator { possible Devices : possible } 
func ( d * device Allocator ) Get Next ( existing Devices Existing Devices ) ( mount Device , error ) { for _ , device Pair := range d . sort By Count ( ) { if _ , found := existing Devices [ device Pair . device Name ] ; ! found { return device Pair . device } 
func ( d * device Allocator ) Deprioritize ( chosen mount Device ) { d . device defer d . device if _ , ok := d . possible d . possible } 
func Resize Instance Group ( asg ASG , instance Group Name string , size int ) error { request := & autoscaling . Update Auto Scaling Group Input { Auto Scaling Group Name : aws . String ( instance Group Name ) , Min Size : aws . Int64 ( int64 ( size ) ) , Max if _ , err := asg . Update Auto Scaling } 
func ( c * Cloud ) Resize Instance Group ( instance Group Name string , size int ) error { return Resize Instance Group ( c . asg , instance Group } 
func Describe Instance Group ( asg ASG , instance Group Name string ) ( Instance Group Info , error ) { request := & autoscaling . Describe Auto Scaling Groups Input { Auto Scaling Group Names : [ ] * string { aws . String ( instance Group response , err := asg . Describe Auto Scaling if err != nil { return nil , fmt . Errorf ( " " , instance Group if len ( response . Auto Scaling if len ( response . Auto Scaling Groups ) > 1 { klog . Warning ( " " , instance Group group := response . Auto Scaling return & aws Instance } 
func ( c * Cloud ) Describe Instance Group ( instance Group Name string ) ( Instance Group Info , error ) { return Describe Instance Group ( c . asg , instance Group } 
func ( g * aws Instance Group ) Current } 
func ( f * generic Informer ) Lister ( ) cache . Generic Lister { return cache . New Generic Lister ( f . Informer ( ) . Get } 
func ( f * shared Informer Factory ) For Resource ( resource schema . Group Version Resource ) ( Generic Informer , error ) { switch resource { // Group=apiregistration.k8s.io, Version=v1 case v1 . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group Resource ( ) , informer : f . Apiregistration ( ) . V1 ( ) . API // Group=apiregistration.k8s.io, Version=v1beta1 case v1beta1 . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group Resource ( ) , informer : f . Apiregistration ( ) . V1beta1 ( ) . API } 
func Patch Resource ( r rest . Patcher , scope * Request Scope , admit admission . Interface , patch Types [ ] string ) http . Handler Func { return func ( w http . Response defer trace . Log If if is Dry Run ( req . URL ) && ! utilfeature . Default Feature Gate . Enabled ( features . Dry Run ) { scope . err ( errors . New Bad // Do this first, otherwise name extraction can fail for unrecognized content types // TODO: handle this in negotiation content // Remove "; charset=" if included in header. if idx := strings . Index ( content Type , " " ) ; idx > 0 { content Type = content patch Type := types . Patch Type ( content // Ensure the patch Type is one we support if ! sets . New String ( patch Types ... ) . Has ( content Type ) { scope . err ( negotiation . New Unsupported Media Type Error ( patch // TODO: we either want to remove timeout or document it (if we // document, move timeout out of this function and declare it in // api_installer) timeout := parse ctx = request . With output Media Type , _ , err := negotiation . Negotiate Output Media patch Bytes , err := limited Read Body ( req , scope . Max Request Body options := & metav1 . Patch if err := metainternalversion . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , scope . Meta Group Version , options ) ; err != nil { err = errors . New Bad if errs := validation . Validate Patch Options ( options , patch Type ) ; len ( errs ) > 0 { err := errors . New Invalid ( schema . Group Kind { Group : metav1 . Group ae := request . Audit Event admit = admission . With audit . Log Request Patch ( ae , patch base Content Type := runtime . Content Type if patch Type == types . Apply Patch Type { base Content Type = runtime . Content Type s , ok := runtime . Serializer Info For Media Type ( scope . Serializer . Supported Media Types ( ) , base Content if ! ok { scope . err ( fmt . Errorf ( " " , base Content gv := scope . Kind . Group codec := runtime . New Codec ( scope . Serializer . Encoder For Version ( s . Serializer , gv ) , scope . Serializer . Decoder To Version ( s . Serializer , scope . Hub Group user Info , _ := request . User static Create Attributes := admission . New Attributes Record ( nil , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Create , dryrun . Is Dry Run ( options . Dry Run ) , user static Update Attributes := admission . New Attributes Record ( nil , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Update , dryrun . Is Dry Run ( options . Dry Run ) , user mutating Admission , _ := admit . ( admission . Mutation create Authorizer Attributes := authorizer . Attributes Record { User : user Info , Resource Request : true , Path : req . URL . Path , Verb : " " , API Group : scope . Resource . Group , API p := patcher { namer : scope . Namer , creater : scope . Creater , defaulter : scope . Defaulter , typer : scope . Typer , unsafe Convertor : scope . Unsafe Convertor , kind : scope . Kind , resource : scope . Resource , subresource : scope . Subresource , dry Run : dryrun . Is Dry Run ( options . Dry Run ) , object Interfaces : scope , hub Group Version : scope . Hub Group Version , create Validation : with Authorization ( rest . Admission To Validate Object Func ( admit , static Create Attributes , scope ) , scope . Authorizer , create Authorizer Attributes ) , update Validation : rest . Admission To Validate Object Update Func ( admit , static Update Attributes , scope ) , admission Check : mutating Admission , codec : codec , timeout : timeout , options : options , rest Patcher : r , name : name , patch Type : patch Type , patch Bytes : patch Bytes , user Agent : req . User result , was Created , err := p . patch request Info , ok := request . Request Info if err := set Self Link ( result , request status := http . Status if was Created { status = http . Status transform Response Object ( ctx , scope , trace , req , w , status , output Media } 
func ( p * json Patcher ) apply JS Patch ( versioned JS [ ] byte ) ( patched JS [ ] byte , ret Err error ) { switch p . patch Type { case types . JSON Patch Type : patch Obj , err := jsonpatch . Decode Patch ( p . patch if err != nil { return nil , errors . New Bad if len ( patch Obj ) > max JSON Patch Operations { return nil , errors . New Request Entity Too Large Error ( fmt . Sprintf ( " " , max JSON Patch Operations , len ( patch patched JS , err := patch Obj . Apply ( versioned if err != nil { return nil , errors . New Generic Server Response ( http . Status Unprocessable Entity , " " , schema . Group return patched case types . Merge Patch Type : return jsonpatch . Merge Patch ( versioned JS , p . patch default : // only here as a safety net - go-restful filters content-type return nil , fmt . Errorf ( " " , p . patch } 
func strategic Patch Object ( defaulter runtime . Object Defaulter , original Object runtime . Object , patch Bytes [ ] byte , obj To Update runtime . Object , schema Reference Obj runtime . Object , ) error { original Obj Map , err := runtime . Default Unstructured Converter . To Unstructured ( original patch if err := json . Unmarshal ( patch Bytes , & patch Map ) ; err != nil { return errors . New Bad if err := apply Patch To Object ( defaulter , original Obj Map , patch Map , obj To Update , schema Reference } 
func ( p * patcher ) apply Patch ( _ context . Context , _ , current Object runtime . Object ) ( obj To Update runtime . Object , patch Err error ) { // Make sure we actually have a persisted current current Object Has UID , err := has UID ( current } else if ! current Object Has UID { obj To Update , patch Err = p . mechanism . create New } else { obj To Update , patch Err = p . mechanism . apply Patch To Current Object ( current if patch Err != nil { return nil , patch obj To Update Has UID , err := has UID ( obj To if obj To Update Has UID && ! current Object Has UID { accessor , err := meta . Accessor ( obj To return nil , errors . New Conflict ( p . resource . Group Resource ( ) , p . name , fmt . Errorf ( " " , accessor . Get if err := check Name ( obj To return obj To } 
func ( p * patcher ) apply Admission ( ctx context . Context , patched Object runtime . Object , current if has UID , err := has UID ( current } else if ! has current if p . admission Check != nil && p . admission Check . Handles ( operation ) { attributes := p . admission Attributes ( ctx , patched Object , current return patched Object , p . admission Check . Admit ( attributes , p . object return patched } 
func ( p * patcher ) patch Resource ( ctx context . Context , scope * Request Scope ) ( runtime . Object , bool , error ) { p . namespace = request . Namespace switch p . patch Type { case types . JSON Patch Type , types . Merge Patch Type : p . mechanism = & json Patcher { patcher : p , field Manager : scope . Field case types . Strategic Merge Patch Type : schema Reference Obj , err := p . unsafe Convertor . Convert To Version ( p . rest Patcher . New ( ) , p . kind . Group p . mechanism = & smp Patcher { patcher : p , schema Reference Obj : schema Reference Obj , field Manager : scope . Field // this case is unreachable if Server Side Apply is not enabled because we will have already rejected the content type case types . Apply Patch Type : p . mechanism = & apply Patcher { field Manager : scope . Field Manager , patch : p . patch p . force Allow default : return nil , false , fmt . Errorf ( " " , p . patch was p . updated Object Info = rest . Default Updated Object Info ( nil , p . apply Patch , p . apply result , err := finish Request ( p . timeout , func ( ) ( runtime . Object , error ) { // TODO: Pass in Update Options to override Update Strategy.Allow Update On Create options , err := patch To Update update Object , created , update Err := p . rest Patcher . Update ( ctx , p . name , p . updated Object Info , p . create Validation , p . update Validation , p . force Allow was return update Object , update return result , was } 
func apply Patch To Object ( defaulter runtime . Object Defaulter , original Map map [ string ] interface { } , patch Map map [ string ] interface { } , obj To Update runtime . Object , schema Reference Obj runtime . Object , ) error { patched Obj Map , err := strategicpatch . Strategic Merge Map Patch ( original Map , patch Map , schema Reference if err != nil { return interpret Strategic Merge Patch // Rather than serialize the patched map to JSON, then decode it to an object, we go directly from a map to an object if err := runtime . Default Unstructured Converter . From Unstructured ( patched Obj Map , obj To // Decoding from JSON to a versioned object would apply defaults, so we do the same here defaulter . Default ( obj To } 
func interpret Strategic Merge Patch Error ( err error ) error { switch err { case mergepatch . Err Bad JSON Doc , mergepatch . Err Bad Patch Format For Primitive List , mergepatch . Err Bad Patch Format For Retain Keys , mergepatch . Err Bad Patch Format For Set Element Order List , mergepatch . Err Unsupported Strategic Merge Patch Format : return errors . New Bad case mergepatch . Err No List Of Lists , mergepatch . Err Patch Content Not Match Retain Keys : return errors . New Generic Server Response ( http . Status Unprocessable Entity , " " , schema . Group } 
func port Mapping To Hostport ( port Mapping * Port Mapping ) hostport { return hostport { port : port Mapping . Host Port , protocol : strings . To Lower ( string ( port } 
func ensure Kube Hostport Chains ( iptables utiliptables . Interface , nat Interface // Ensure kube Hostport Chain if _ , err := iptables . Ensure Chain ( utiliptables . Table NAT , kube Hostports Chain ) ; err != nil { return fmt . Errorf ( " " , utiliptables . Table NAT , kube Hostports table Chains Need Jump } { { utiliptables . Table NAT , utiliptables . Chain Output } , { utiliptables . Table NAT , utiliptables . Chain args := [ ] string { " " , " " , " " , " " , " " , " " , " " , " " , " " , string ( kube Hostports for _ , tc := range table Chains Need Jump Services { // KUBE-HOSTPORTS chain needs to be appended to the system chains. // This ensures KUBE-SERVICES chain gets processed first. // Since rules in KUBE-HOSTPORTS chain matches broader cases, allow the more specific rules to be processed first. if _ , err := iptables . Ensure Rule ( utiliptables . Append , tc . table , tc . chain , args ... ) ; err != nil { return fmt . Errorf ( " " , tc . table , tc . chain , kube Hostports // Need to SNAT traffic from localhost args = [ ] string { " " , " " , " " , " " , " " , nat Interface if _ , err := iptables . Ensure Rule ( utiliptables . Append , utiliptables . Table NAT , utiliptables . Chain Postrouting , args ... ) ; err != nil { return fmt . Errorf ( " " , utiliptables . Table NAT , utiliptables . Chain } 
func Set Join Dynamic Defaults ( cfg * kubeadmapi . Join Configuration ) error { add Control Plane if cfg . Control Plane != nil { add Control Plane if err := Set Node Registration Dynamic Defaults ( & cfg . Node Registration , add Control Plane return Set Join Control Plane Defaults ( cfg . Control } 
func Set Join Control Plane Defaults ( cfg * kubeadmapi . Join Control Plane ) error { if cfg != nil { if err := Set API Endpoint Dynamic Defaults ( & cfg . Local API } 
func Load Or Default Join Configuration ( cfg Path string , defaultversionedcfg * kubeadmapiv1beta2 . Join Configuration ) ( * kubeadmapi . Join Configuration , error ) { if cfg Path != " " { // Loads configuration from config file, if provided // Nb. --config overrides command line flags, TODO: fix this return Load Join Configuration From File ( cfg return Defaulted Join } 
func Load Join Configuration From File ( cfg Path string ) ( * kubeadmapi . Join Configuration , error ) { klog . V ( 1 ) . Infof ( " " , cfg b , err := ioutil . Read File ( cfg if err != nil { return nil , errors . Wrapf ( err , " " , cfg gvkmap , err := kubeadmutil . Split YAML return document Map To Join } 
func document Map To Join Configuration ( gvkmap map [ schema . Group Version Kind ] [ ] byte , allow Deprecated bool ) ( * kubeadmapi . Join Configuration , error ) { join for gvk , bytes := range gvkmap { // not interested in anything other than Join Configuration if gvk . Kind != constants . Join Configuration // check if this version is supported and possibly not deprecated if err := validate Supported Version ( gvk . Group Version ( ) , allow // verify the validity of the YAML strict . Verify Unmarshal join if len ( join Bytes ) == 0 { return nil , errors . Errorf ( " " , constants . Join Configuration internalcfg := & kubeadmapi . Join if err := runtime . Decode Into ( kubeadmscheme . Codecs . Universal Decoder ( ) , join // Applies dynamic defaults to settings not provided with flags if err := Set Join Dynamic // Validates cfg (flags/configs + defaults) if err := validation . Validate Join Configuration ( internalcfg ) . To } 
func Defaulted Join Configuration ( defaultversionedcfg * kubeadmapiv1beta2 . Join Configuration ) ( * kubeadmapi . Join Configuration , error ) { internalcfg := & kubeadmapi . Join // Applies dynamic defaults to settings not provided with flags if err := Set Join Dynamic // Validates cfg (flags/configs + defaults) if err := validation . Validate Join Configuration ( internalcfg ) . To } 
func ( c * Rbac V1beta1Client ) REST return c . rest } 
func New REST ( opts Getter generic . REST Options Getter ) ( * REST , * Status REST , * Approval REST ) { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & certificates . Certificate Signing Request { } } , New List Func : func ( ) runtime . Object { return & certificates . Certificate Signing Request List { } } , Default Qualified Resource : certificates . Resource ( " " ) , Create Strategy : csrregistry . Strategy , Update Strategy : csrregistry . Strategy , Delete Strategy : csrregistry . Strategy , Export Strategy : csrregistry . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts if err := store . Complete With // Subresources use the same store and creation strategy, which only // allows empty subs. Updates to an existing subresource are handled by // dedicated strategies. status status Store . Update Strategy = csrregistry . Status approval approval Store . Update Strategy = csrregistry . Approval return & REST { store } , & Status REST { store : & status Store } , & Approval REST { store : & approval } 
func Round Tripper For ( config * restclient . Config ) ( http . Round Tripper , Upgrader , error ) { tls Config , err := restclient . TLS Config upgrade Round Tripper := spdy . New Round Tripper ( tls wrapper , err := restclient . HTTP Wrappers For Config ( config , upgrade Round return wrapper , upgrade Round } 
func New } 
func Negotiate ( upgrader Upgrader , client * http . Client , req * http . Request , protocols ... string ) ( httpstream . Connection , string , error ) { for i := range protocols { req . Header . Add ( httpstream . Header Protocol conn , err := upgrader . New return conn , resp . Header . Get ( httpstream . Header Protocol } 
func ( os * Open compute , err := os . New Compute return & Instances { compute : compute , opts : os . metadata } 
func ( i * Instances ) Current Node Name ( ctx context . Context , hostname string ) ( types . Node Name , error ) { md , err := get Metadata ( i . opts . Search return types . Node } 
func ( i * Instances ) Add SSH Key To All Instances ( ctx context . Context , user string , key Data [ ] byte ) error { return cloudprovider . Not } 
func ( i * Instances ) Node Addresses ( ctx context . Context , name types . Node Name ) ( [ ] v1 . Node addrs , err := get Addresses By } 
func ( i * Instances ) Node Addresses By Provider ID ( ctx context . Context , provider ID string ) ( [ ] v1 . Node Address , error ) { instance ID , err := instance ID From Provider ID ( provider if err != nil { return [ ] v1 . Node server , err := servers . Get ( i . compute , instance if err != nil { return [ ] v1 . Node addresses , err := node if err != nil { return [ ] v1 . Node } 
func ( i * Instances ) Instance Exists By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { instance ID , err := instance ID From Provider ID ( provider _ , err = servers . Get ( i . compute , instance if err != nil { if is Not } 
func ( i * Instances ) Instance Shutdown By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { instance ID , err := instance ID From Provider ID ( provider server , err := servers . Get ( i . compute , instance // SHUTOFF is the only state where we can detach volumes immediately if server . Status == instance } 
func ( os * Open Stack ) Instance ID ( ) ( string , error ) { if len ( os . local Instance ID ) == 0 { id , err := read Instance ID ( os . metadata Opts . Search os . local Instance return os . local Instance } 
func ( i * Instances ) Instance ID ( ctx context . Context , name types . Node Name ) ( string , error ) { srv , err := get Server By if err != nil { if err == Err Not Found { return " " , cloudprovider . Instance Not } 
func ( i * Instances ) Instance Type By Provider ID ( ctx context . Context , provider ID string ) ( string , error ) { instance ID , err := instance ID From Provider ID ( provider server , err := servers . Get ( i . compute , instance return srv Instance } 
func ( i * Instances ) Instance Type ( ctx context . Context , name types . Node Name ) ( string , error ) { srv , err := get Server By return srv Instance } 
func instance ID From Provider ID ( provider ID string ) ( instance ID string , err error ) { // If Instances.Instance ID or cloudprovider.Get Instance Provider ID is changed, the regexp should be changed too. var provider ID Regexp = regexp . Must Compile ( `^` + Provider matches := provider ID Regexp . Find String Submatch ( provider if len ( matches ) != 2 { return " " , fmt . Errorf ( " \" \" \" \" " , provider } 
func With return & audit } 
func New Pod Template Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Pod Template Informer ( client , namespace , resync } 
} 
func ( gb * Graph Builder ) sync Monitors ( resources map [ schema . Group Version Resource ] struct { } ) error { gb . monitor defer gb . monitor to if to Remove == nil { to for resource := range resources { if _ , ok := gb . ignored Resources [ resource . Group if m , ok := to delete ( to kind , err := gb . rest Mapper . Kind c , s , err := gb . controller for _ , monitor := range to Remove { if monitor . stop Ch != nil { close ( monitor . stop klog . V ( 4 ) . Infof ( " " , added , kept , len ( to // New Aggregate returns nil if errs is 0-length return utilerrors . New } 
func ( gb * Graph Builder ) start Monitors ( ) { gb . monitor defer gb . monitor // we're waiting until after the informer start that happens once all the controllers are initialized. This ensures // that they don't get unexpected events on their work queues. <- gb . informers for _ , monitor := range monitors { if monitor . stop Ch == nil { monitor . stop gb . shared Informers . Start ( gb . stop } 
func ( gb * Graph Builder ) Is Synced ( ) bool { gb . monitor defer gb . monitor for resource , monitor := range gb . monitors { if ! monitor . controller . Has } 
func ( gb * Graph Builder ) Run ( stop // Set up the stop channel. gb . monitor gb . stop Ch = stop gb . monitor // Start monitors and begin change processing until the stop channel is // closed. gb . start wait . Until ( gb . run Process Graph Changes , 1 * time . Second , stop // Stop any running monitors. gb . monitor defer gb . monitor for _ , monitor := range monitors { if monitor . stop close ( monitor . stop } 
func ( gb * Graph Builder ) enqueue Virtual Delete Event ( ref object Reference ) { gb . graph Changes . Add ( & event { event Type : delete Event , obj : & metaonly . Metadata Only Object { Type Meta : metav1 . Type Meta { API Version : ref . API Version , Kind : ref . Kind } , Object Meta : metav1 . Object } 
func ( gb * Graph Builder ) add Dependent To Owners ( n * node , owners [ ] metav1 . Owner Reference ) { for _ , owner := range owners { owner Node , ok := gb . uid To if ! ok { // Create a "virtual" node in the graph for the owner if it doesn't // exist in the graph yet. owner Node = & node { identity : object Reference { Owner klog . V ( 5 ) . Infof ( " \n \n " , owner gb . uid To Node . Write ( owner owner Node . add if ! ok { // Enqueue the virtual node into attempt To Delete. // The garbage processor will enqueue a virtual delete // event to delete it from the graph if API server confirms this // owner doesn't exist. gb . attempt To Delete . Add ( owner } 
func ( gb * Graph Builder ) insert Node ( n * node ) { gb . uid To gb . add Dependent To } 
func ( gb * Graph Builder ) remove Dependent From Owners ( n * node , owners [ ] metav1 . Owner Reference ) { for _ , owner := range owners { owner Node , ok := gb . uid To owner Node . delete } 
func ( gb * Graph Builder ) remove Node ( n * node ) { gb . uid To gb . remove Dependent From } 
func references Diffs ( old [ ] metav1 . Owner Reference , new [ ] metav1 . Owner Reference ) ( added [ ] metav1 . Owner Reference , removed [ ] metav1 . Owner Reference , changed [ ] owner Ref Pair ) { old UID To Ref := make ( map [ string ] metav1 . Owner for _ , value := range old { old UID To old UID Set := sets . String Key Set ( old UID To new UID To Ref := make ( map [ string ] metav1 . Owner for _ , value := range new { new UID To new UID Set := sets . String Key Set ( new UID To added UID := new UID Set . Difference ( old UID removed UID := old UID Set . Difference ( new UID intersection := old UID Set . Intersection ( new UID for uid := range added UID { added = append ( added , new UID To for uid := range removed UID { removed = append ( removed , old UID To for uid := range intersection { if ! reflect . Deep Equal ( old UID To Ref [ uid ] , new UID To Ref [ uid ] ) { changed = append ( changed , owner Ref Pair { old Ref : old UID To Ref [ uid ] , new Ref : new UID To } 
func deletion Starts ( old Obj interface { } , new Accessor metav1 . Object ) bool { // The delta_fifo may combine the creation and update of the object into one // event, so if there is no old Obj, we just return if the new Obj (via // new Accessor) is being deleted. if old Obj == nil { if new Accessor . Get Deletion old Accessor , err := meta . Accessor ( old if err != nil { utilruntime . Handle return being Deleted ( new Accessor ) && ! being Deleted ( old } 
func starts Waiting For Dependents Deleted ( old Obj interface { } , new Accessor metav1 . Object ) bool { return deletion Starts ( old Obj , new Accessor ) && has Delete Dependents Finalizer ( new } 
func starts Waiting For Dependents Orphaned ( old Obj interface { } , new Accessor metav1 . Object ) bool { return deletion Starts ( old Obj , new Accessor ) && has Orphan Finalizer ( new } 
func ( gb * Graph Builder ) add Unblocked Owners To Delete Queue ( removed [ ] metav1 . Owner Reference , changed [ ] owner Ref Pair ) { for _ , ref := range removed { if ref . Block Owner Deletion != nil && * ref . Block Owner Deletion { node , found := gb . uid To gb . attempt To for _ , c := range changed { was Blocked := c . old Ref . Block Owner Deletion != nil && * c . old Ref . Block Owner is Unblocked := c . new Ref . Block Owner Deletion == nil || ( c . new Ref . Block Owner Deletion != nil && ! * c . new Ref . Block Owner if was Blocked && is Unblocked { node , found := gb . uid To Node . Read ( c . new if ! found { klog . V ( 5 ) . Infof ( " " , c . new gb . attempt To } 
func ( gb * Graph Builder ) process Graph Changes ( ) bool { item , quit := gb . graph defer gb . graph if ! ok { utilruntime . Handle if err != nil { utilruntime . Handle klog . V ( 5 ) . Infof ( " " , event . gvk . Group Version ( ) . String ( ) , event . gvk . Kind , accessor . Get Namespace ( ) , accessor . Get Name ( ) , string ( accessor . Get UID ( ) ) , event . event // Check if the node already exists existing Node , found := gb . uid To Node . Read ( accessor . Get if found { // this marks the node as having been observed via an informer event // 1. this depends on graph Changes only containing add/update events from the actual informer // 2. this allows things tracking virtual nodes' existence to stop polling and rely on informer events existing Node . mark switch { case ( event . event Type == add Event || event . event Type == update Event ) && ! found : new Node := & node { identity : object Reference { Owner Reference : metav1 . Owner Reference { API Version : event . gvk . Group Version ( ) . String ( ) , Kind : event . gvk . Kind , UID : accessor . Get UID ( ) , Name : accessor . Get Name ( ) , } , Namespace : accessor . Get Namespace ( ) , } , dependents : make ( map [ * node ] struct { } ) , owners : accessor . Get Owner References ( ) , deleting Dependents : being Deleted ( accessor ) && has Delete Dependents Finalizer ( accessor ) , being Deleted : being gb . insert Node ( new // the underlying delta_fifo may combine a creation and a deletion into // one event, so we need to further process the event. gb . process Transitions ( event . old Obj , accessor , new case ( event . event Type == add Event || event . event Type == update Event ) && found : // handle changes in owner References added , removed , changed := references Diffs ( existing Node . owners , accessor . Get Owner if len ( added ) != 0 || len ( removed ) != 0 || len ( changed ) != 0 { // check if the changed dependency graph unblock owners that are // waiting for the deletion of their dependents. gb . add Unblocked Owners To Delete // update the node itself existing Node . owners = accessor . Get Owner // Add the node to its new owners' dependent lists. gb . add Dependent To Owners ( existing // remove the node from the dependent list of node that are no longer in // the node's owners list. gb . remove Dependent From Owners ( existing if being Deleted ( accessor ) { existing Node . mark Being gb . process Transitions ( event . old Obj , accessor , existing case event . event Type == delete Event : if ! found { klog . V ( 5 ) . Infof ( " " , accessor . Get // remove Node updates the graph gb . remove Node ( existing existing Node . dependents Lock . R defer existing Node . dependents Lock . R if len ( existing Node . dependents ) > 0 { gb . absent Owner Cache . Add ( accessor . Get for dep := range existing Node . dependents { gb . attempt To for _ , owner := range existing Node . owners { owner Node , found := gb . uid To if ! found || ! owner Node . is Deleting // this is to let attemp To Delete Item check if all the owner's // dependents are deleted, if so, the owner will be deleted. gb . attempt To Delete . Add ( owner } 
func Common case metav1 . List case List Meta Accessor : if m := t . Get List return nil , err Not case metav1 . List Meta Accessor : if m := t . Get List return nil , err Not case metav1 . Object Meta Accessor : if m := t . Get Object return nil , err Not default : return nil , err Not } 
func List case metav1 . List case List Meta Accessor : if m := t . Get List return nil , err Not case metav1 . List Meta Accessor : if m := t . Get List return nil , err Not default : return nil , err Not } 
case metav1 . Object Meta Accessor : if m := t . Get Object return nil , err Not default : return nil , err Not } 
func As Partial Object Metadata ( m metav1 . Object ) * metav1beta1 . Partial Object Metadata { switch t := m . ( type ) { case * metav1 . Object Meta : return & metav1beta1 . Partial Object Metadata { Object default : return & metav1beta1 . Partial Object Metadata { Object Meta : metav1 . Object Meta { Name : m . Get Name ( ) , Generate Name : m . Get Generate Name ( ) , Namespace : m . Get Namespace ( ) , Self Link : m . Get Self Link ( ) , UID : m . Get UID ( ) , Resource Version : m . Get Resource Version ( ) , Generation : m . Get Generation ( ) , Creation Timestamp : m . Get Creation Timestamp ( ) , Deletion Timestamp : m . Get Deletion Timestamp ( ) , Deletion Grace Period Seconds : m . Get Deletion Grace Period Seconds ( ) , Labels : m . Get Labels ( ) , Annotations : m . Get Annotations ( ) , Owner References : m . Get Owner References ( ) , Finalizers : m . Get Finalizers ( ) , Cluster Name : m . Get Cluster Name ( ) , Initializers : m . Get Initializers ( ) , Managed Fields : m . Get Managed } 
func Type Accessor ( obj interface { } ) ( Type , error ) { if typed , ok := obj . ( runtime . Object ) ; ok { return object v , err := conversion . Enforce type Meta := v . Field By if ! type Meta . Is a := & generic if err := extract From Type Meta ( type Meta , a ) ; err != nil { return nil , fmt . Errorf ( " " , type } 
func extract From Owner Reference ( v reflect . Value , o * metav1 . Owner Reference ) error { if err := runtime . Field ( v , " " , & o . API var controller if err := runtime . Field ( v , " " , & controller if controller Ptr != nil { controller := * controller var block Owner Deletion if err := runtime . Field ( v , " " , & block Owner Deletion if block Owner Deletion Ptr != nil { block := * block Owner Deletion o . Block Owner } 
func set Owner Reference ( v reflect . Value , o * metav1 . Owner Reference ) error { if err := runtime . Set Field ( o . API if err := runtime . Set if err := runtime . Set if err := runtime . Set if err := runtime . Set if o . Block Owner Deletion != nil { block := * ( o . Block Owner if err := runtime . Set } 
func extract From Type Meta ( v reflect . Value , a * generic Accessor ) error { if err := runtime . Field Ptr ( v , " " , & a . api if err := runtime . Field } 
func New Admission Options ( ) * Admission Options { options := genericoptions . New Admission // register all admission plugins Register All Admission // set Recommended Plugin Order options . Recommended Plugin Order = All Ordered // set Default Off Plugins options . Default Off Plugins = Default Off Admission return & Admission Options { Generic } 
func ( a * Admission Options ) Add Flags ( fs * pflag . Flag Set ) { fs . String Slice Var ( & a . Plugin Names , " " , a . Plugin Names , " " + " " + " " + " " + " " + " " + " " + strings . Join ( a . Generic fs . Mark a . Generic Admission . Add } 
func ( a * Admission if a . Plugin Names != nil && ( a . Generic Admission . Enable Plugins != nil || a . Generic Admission . Disable registered Plugins := sets . New String ( a . Generic for _ , name := range a . Plugin Names { if ! registered errs = append ( errs , a . Generic } 
func ( a * Admission Options ) Apply To ( c * server . Config , informers informers . Shared Informer Factory , kube API Server Client Config * rest . Config , plugin Initializers ... admission . Plugin if a . Plugin Names != nil { // pass Plugin Names to generic Admission Options a . Generic Admission . Enable Plugins , a . Generic Admission . Disable Plugins = compute Plugin Names ( a . Plugin Names , a . Generic Admission . Recommended Plugin return a . Generic Admission . Apply To ( c , informers , kube API Server Client Config , plugin } 
func compute Plugin Names ( explicitly Enabled [ ] string , all [ ] string ) ( enabled [ ] string , disabled [ ] string ) { return explicitly Enabled , sets . New String ( all ... ) . Difference ( sets . New String ( explicitly } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1 . Cluster Role Binding { } , func ( obj interface { } ) { Set Object Defaults_Cluster Role Binding ( obj . ( * v1 . Cluster Role scheme . Add Type Defaulting Func ( & v1 . Cluster Role Binding List { } , func ( obj interface { } ) { Set Object Defaults_Cluster Role Binding List ( obj . ( * v1 . Cluster Role Binding scheme . Add Type Defaulting Func ( & v1 . Role Binding { } , func ( obj interface { } ) { Set Object Defaults_Role Binding ( obj . ( * v1 . Role scheme . Add Type Defaulting Func ( & v1 . Role Binding List { } , func ( obj interface { } ) { Set Object Defaults_Role Binding List ( obj . ( * v1 . Role Binding } 
func Can Use Win Kernel Proxier ( kcompat Kernel Compat Tester ) ( bool , error ) { // Check that the kernel supports what we need. if err := kcompat . Is } 
func ( lkct Windows Kernel Compat Tester ) Is Compatible ( ) error { _ , err := hcsshim . HNS List Policy List } 
func conjure Mac ( mac return fmt . Sprintf ( " " , mac } 
func new Service Info ( svc Port Name proxy . Service Port Name , port * v1 . Service Port , service * v1 . Service , hns Host Network Service ) * service Info { only Node Local if apiservice . Requests Only Local Traffic ( service ) { only Node Local // set default session sticky max age 180min=10800s sticky Max Age if service . Spec . Session Affinity == v1 . Service Affinity Client IP && service . Spec . Session Affinity Config != nil { sticky Max Age Seconds = int ( * service . Spec . Session Affinity Config . Client IP . Timeout info := & service Info { cluster IP : net . Parse IP ( service . Spec . Cluster IP ) , port : int ( port . Port ) , protocol : port . Protocol , node Port : int ( port . Node Port ) , // target Port is zero if it is specified as a name in port.Target Port. // Its real value would be got later from endpoints. target Port : port . Target Port . Int Value ( ) , // Deep-copy in case the service instance changes load Balancer Status : * service . Status . Load Balancer . Deep Copy ( ) , session Affinity Type : service . Spec . Session Affinity , sticky Max Age Seconds : sticky Max Age Seconds , load Balancer Source Ranges : make ( [ ] string , len ( service . Spec . Load Balancer Source Ranges ) ) , only Node Local Endpoints : only Node Local copy ( info . load Balancer Source Ranges , service . Spec . Load Balancer Source for _ , eip := range service . Spec . External I Ps { info . external I Ps = append ( info . external I Ps , & external IP for _ , ingress := range service . Status . Load Balancer . Ingress { info . load Balancer Ingress I Ps = append ( info . load Balancer Ingress I Ps , & load Balancer Ingress if apiservice . Needs Health Check ( service ) { p := service . Spec . Health Check Node if p == 0 { klog . Errorf ( " " , svc Port Name . Namespaced } else { info . health Check Node } 
func New Proxier ( sync Period time . Duration , min Sync Period time . Duration , masquerade All bool , masquerade Bit int , cluster CIDR string , hostname string , node IP net . IP , recorder record . Event Recorder , healthz Server healthcheck . Healthz Updater , config config . Kube Proxy Winkernel Configuration , ) ( * Proxier , error ) { masquerade Value := 1 << uint ( masquerade masquerade Mark := fmt . Sprintf ( " " , masquerade Value , masquerade if node node IP = net . Parse if len ( cluster health Checker := healthcheck . New var hns Host Network hns = hns supported Features := hcn . Get Supported if supported Features . Remote Subnet { hns = hns hns Network Name := config . Network if len ( hns Network hns Network if len ( hns Network hns Network Info , err := hns . get Network By Name ( hns Network if err != nil { klog . Errorf ( " " , hns Network klog . V ( 1 ) . Infof ( " " , hns Network is DSR := config . Enable if is DSR && ! utilfeature . Default Feature Gate . Enabled ( genericfeatures . Win err = hcn . DSR if is var source var host if hns Network Info . network Type == " " { if ! utilfeature . Default Feature Gate . Enabled ( genericfeatures . Win err = hcn . Remote Subnet source Vip = config . Source if len ( source for _ , addr := range addresses { addr IP , _ , _ := net . Parse if addr IP . String ( ) == node IP . String ( ) { klog . V ( 2 ) . Infof ( " " , inter . Hardware host Mac = inter . Hardware if len ( host Mac ) == 0 { return nil , fmt . Errorf ( " " , node existing Source Vip , _ := hns . get Endpoint By Ip Address ( source Vip , hns Network if existing Source Vip == nil { hns Endpoint := & endpoints Info { ip : source Vip , is Local : true , mac Address : host Mac , provider Address : node _ , err = hns . create Endpoint ( hns Endpoint , hns Network proxier := & Proxier { ports Map : make ( map [ local Port ] closeable ) , service Map : make ( proxy Service Map ) , service Changes : new Service Change Map ( ) , endpoints Map : make ( proxy Endpoints Map ) , endpoints Changes : new Endpoints Change Map ( hostname ) , masquerade All : masquerade All , masquerade Mark : masquerade Mark , cluster CIDR : cluster CIDR , hostname : hostname , node IP : node IP , recorder : recorder , health Checker : health Checker , healthz Server : healthz Server , hns : hns , network : * hns Network Info , source Vip : source Vip , host Mac : host Mac , is DSR : is burst klog . V ( 3 ) . Infof ( " " , min Sync Period , sync Period , burst proxier . sync Runner = async . New Bounded Frequency Runner ( " " , proxier . sync Proxy Rules , min Sync Period , sync Period , burst } 
func ( proxier * Proxier ) Sync Loop ( ) { // Update healthz timestamp at beginning in case Sync() never succeeds. if proxier . healthz Server != nil { proxier . healthz Server . Update proxier . sync Runner . Loop ( wait . Never } 
func ( proxier * Proxier ) update Service Map ( ) ( result update Service Map Result ) { result . stale Services = sets . New var service Map proxy Service Map = proxier . service var changes * service Change Map = & proxier . service for _ , change := range changes . items { existing Ports := service Map . merge ( change . current , proxier . endpoints service Map . unmerge ( change . previous , existing Ports , result . stale Services , proxier . endpoints changes . items = make ( map [ types . Namespaced Name ] * service // TODO: If this will appear to be computationally expensive, consider // computing this incrementally similarly to service Map. result . hc Services = make ( map [ types . Namespaced for svc Port Name , info := range service Map { if info . health Check Node Port != 0 { result . hc Services [ svc Port Name . Namespaced Name ] = uint16 ( info . health Check Node } 
func ( proxier * Proxier ) update Endpoints Map ( ) ( result update Endpoint Map Result ) { result . stale Endpoints = make ( map [ endpoint Service result . stale Service Names = make ( map [ proxy . Service Port var endpoints Map proxy Endpoints Map = proxier . endpoints var changes * endpoints Change Map = & proxier . endpoints for _ , change := range changes . items { endpoints Map . unmerge ( change . previous , proxier . service endpoints Map . merge ( change . current , proxier . service changes . items = make ( map [ types . Namespaced Name ] * endpoints // TODO: If this will appear to be computationally expensive, consider // computing this incrementally similarly to endpoints Map. result . hc Endpoints = make ( map [ types . Namespaced local I Ps := get Local I Ps ( endpoints for nsn , ips := range local I Ps { result . hc } 
func endpoints To Endpoints Map ( endpoints * v1 . Endpoints , hostname string , hns Host Network Service ) proxy Endpoints endpoints Map := make ( proxy Endpoints svc Port Name := proxy . Service Port Name { Namespaced Name : types . Namespaced is Local := addr . Node Name != nil && * addr . Node ep Info := new Endpoint Info ( addr . IP , uint16 ( port . Port ) , is endpoints Map [ svc Port Name ] = append ( endpoints Map [ svc Port Name ] , ep if klog . V ( 3 ) { new EP List := [ ] * endpoints for _ , ep := range endpoints Map [ svc Port Name ] { new EP List = append ( new EP klog . Infof ( " " , svc Port Name , new EP return endpoints } 
func service To Service Map ( service * v1 . Service , hns Host Network Service ) proxy Service svc Name := types . Namespaced if should Skip Service ( svc service Map := make ( proxy Service for i := range service . Spec . Ports { service svc Port Name := proxy . Service Port Name { Namespaced Name : svc Name , Port : service service Map [ svc Port Name ] = new Service Info ( svc Port Name , service return service } 
func ( proxier * Proxier ) sync Proxy defer func ( ) { Sync Proxy Rules Latency . Observe ( since In Deprecated Sync Proxy Rules Latency . Observe ( since In // don't sync rules till we've received services and endpoints if ! proxier . endpoints Synced || ! proxier . services // We assume that if this was called, we really want to sync them, // even if nothing changed in the meantime. In other words, callers are // responsible for detecting no-op changes and not calling this function. service Update Result := proxier . update Service endpoint Update Result := proxier . update Endpoints stale Services := service Update Result . stale // merge stale services gathered from update Endpoints Map for svc Port Name := range endpoint Update Result . stale Service Names { if svc Info , ok := proxier . service Map [ svc Port Name ] ; ok && svc Info != nil && svc Info . protocol == v1 . Protocol UDP { klog . V ( 2 ) . Infof ( " " , svc Port Name , svc Info . cluster stale Services . Insert ( svc Info . cluster // Program HNS by adding corresponding policies for each service. for svc Name , svc Info := range proxier . service Map { if svc Info . policy Applied { klog . V ( 4 ) . Infof ( " " , spew . Sdump ( svc hns Network if proxier . network . network Type == " " { service Vip Endpoint , _ := hns . get Endpoint By Ip Address ( svc Info . cluster IP . String ( ) , hns Network if service Vip Endpoint == nil { klog . V ( 4 ) . Infof ( " " , svc Info . cluster hns Endpoint := & endpoints Info { ip : svc Info . cluster IP . String ( ) , is Local : false , mac Address : proxier . host Mac , provider Address : proxier . node new Hns Endpoint , err := hns . create Endpoint ( hns Endpoint , hns Network new Hns Endpoint . ref svc Info . remote Endpoint = new Hns var hns Endpoints [ ] endpoints klog . V ( 4 ) . Infof ( " " , svc // Create Remote endpoints for every endpoint, corresponding to the service contains Public for _ , ep := range proxier . endpoints Map [ svc Name ] { var new Hns Endpoint * endpoints hns Network // target Port is zero if it is specified as a name in port.Target Port, so the real port should be got from endpoints. // Note that hcsshim.Add Load Balancer() doesn't support endpoints with different ports, so only port from first endpoint is used. // TODO(feiskyer): add support of different endpoint ports after hcsshim.Add Load Balancer() add that. if svc Info . target Port == 0 { svc Info . target if len ( ep . hns ID ) > 0 { new Hns Endpoint , err = hns . get Endpoint By ID ( ep . hns if new Hns Endpoint == nil { // First check if an endpoint resource exists for this IP, on the current host // A Local endpoint could exist here already // A remote endpoint was already created and proxy was restarted new Hns Endpoint , err = hns . get Endpoint By Ip Address ( ep . ip , hns Network if new Hns Endpoint == nil { if ep . is Local { klog . Errorf ( " " , ep . ip , err , hns Network if proxier . network . network network updated Network , err := hns . get Network By Name ( network if err != nil { klog . Fatalf ( " " , network proxier . network = * updated var provider for _ , rs := range proxier . network . remote Subnets { _ , ip Net , err := net . Parse CIDR ( rs . destination if ip Net . Contains ( net . Parse IP ( ep . ip ) ) { provider Address = rs . provider if ep . ip == rs . provider Address { provider Address = rs . provider if len ( provider provider Address = proxier . node contains Public hns Endpoint := & endpoints Info { ip : ep . ip , is Local : false , mac Address : conjure Mac ( " " , net . Parse IP ( ep . ip ) ) , provider Address : provider new Hns Endpoint , err = hns . create Endpoint ( hns Endpoint , hns Network if err != nil { klog . Errorf ( " " , err , spew . Sdump ( hns } else { hns Endpoint := & endpoints Info { ip : ep . ip , is Local : false , mac Address : ep . mac new Hns Endpoint , err = hns . create Endpoint ( hns Endpoint , hns Network // Save the hns Id for reference Log Json ( new Hns hns Endpoints = append ( hns Endpoints , * new Hns ep . hns ID = new Hns Endpoint . hns ep . ref klog . V ( 3 ) . Infof ( " " , spew . Sdump ( hns Endpoints ) , svc if len ( svc Info . hns ID ) > 0 { // This should not happen klog . Warningf ( " " , svc Info . hns if len ( hns Endpoints ) == 0 { klog . Errorf ( " " , svc klog . V ( 4 ) . Infof ( " " , spew . Sdump ( svc var hns Load Balancer * load Balancer var source Vip = proxier . source if contains Public IP { source Vip = proxier . node hns Load Balancer , err := hns . get Load Balancer ( hns Endpoints , false , proxier . is DSR , source Vip , svc Info . cluster IP . String ( ) , Enum ( svc Info . protocol ) , uint16 ( svc Info . target Port ) , uint16 ( svc svc Info . hns ID = hns Load Balancer . hns klog . V ( 3 ) . Infof ( " " , svc Info . cluster IP , hns Load Balancer . hns // If node Port is specified, user should be able to use node IP:node Port to reach the backend endpoints if svc Info . node Port > 0 { hns Load Balancer , err := hns . get Load Balancer ( hns Endpoints , false , false , source Vip , " " , Enum ( svc Info . protocol ) , uint16 ( svc Info . target Port ) , uint16 ( svc Info . node svc Info . node Porthns ID = hns Load Balancer . hns klog . V ( 3 ) . Infof ( " " , svc Info . cluster IP , hns Load Balancer . hns // Create a Load Balancer Policy for each external IP for _ , external Ip := range svc Info . external I Ps { // Try loading existing policies, if already available hns Load Balancer , err = hns . get Load Balancer ( hns Endpoints , false , false , source Vip , external Ip . ip , Enum ( svc Info . protocol ) , uint16 ( svc Info . target Port ) , uint16 ( svc external Ip . hns ID = hns Load Balancer . hns klog . V ( 3 ) . Infof ( " " , external Ip , hns Load Balancer . hns // Create a Load Balancer Policy for each loadbalancer ingress for _ , lb Ingress Ip := range svc Info . load Balancer Ingress I Ps { // Try loading existing policies, if already available hns Load Balancer , err := hns . get Load Balancer ( hns Endpoints , false , false , source Vip , lb Ingress Ip . ip , Enum ( svc Info . protocol ) , uint16 ( svc Info . target Port ) , uint16 ( svc lb Ingress Ip . hns ID = hns Load Balancer . hns klog . V ( 3 ) . Infof ( " " , lb Ingress svc Info . policy Log ( svc // Update healthz timestamp. if proxier . healthz Server != nil { proxier . healthz Server . Update // Update healthchecks. The endpoints list might include services that are // not "Only Local", but the services list will not, and the health Checker // will just drop those endpoints. if err := proxier . health Checker . Sync Services ( service Update Result . hc if err := proxier . health Checker . Sync Endpoints ( endpoint Update Result . hc // Finish housekeeping. // TODO: these could be made more consistent. for _ , svc IP := range stale Services . Unsorted List ( ) { // TODO : Check if this is required to cleanup stale services here klog . V ( 5 ) . Infof ( " " , svc } 
func New Authorizer ( graph * Graph , identifier nodeidentifier . Node Identifier , rules [ ] rbacv1 . Policy Rule ) authorizer . Authorizer { return & Node Authorizer { graph : graph , identifier : identifier , node Rules : rules , features : utilfeature . Default Feature } 
func ( r * Node Authorizer ) authorize Status Update ( node Name string , starting Type vertex Type , attrs authorizer . Attributes ) ( authorizer . Decision , string , error ) { switch attrs . Get Verb ( ) { case " " , " " : // ok default : klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No if attrs . Get Subresource ( ) != " " { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No return r . authorize ( node Name , starting } 
func ( r * Node Authorizer ) authorize Read Namespaced Object ( node Name string , starting Type vertex Type , attrs authorizer . Attributes ) ( authorizer . Decision , string , error ) { if attrs . Get Verb ( ) != " " && attrs . Get Verb ( ) != " " && attrs . Get Verb ( ) != " " { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No if len ( attrs . Get Subresource ( ) ) > 0 { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No if len ( attrs . Get Namespace ( ) ) == 0 { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No return r . authorize ( node Name , starting } 
func ( r * Node Authorizer ) authorize Lease ( node Name string , attrs authorizer . Attributes ) ( authorizer . Decision , string , error ) { // allowed verbs: get, create, update, patch, delete verb := attrs . Get if verb != " " && verb != " " && verb != " " && verb != " " && verb != " " { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No // the request must be against the system namespace reserved for node leases if attrs . Get Namespace ( ) != api . Namespace Node Lease { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No Opinion , fmt . Sprintf ( " " , api . Namespace Node // the request must come from a node with the same name as the lease // note we skip this check for create, since the authorizer doesn't know the name on create // the noderestriction admission plugin is capable of performing this check at create time if verb != " " && attrs . Get Name ( ) != node Name { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No return authorizer . Decision } 
func ( r * Node Authorizer ) authorize CSI Node ( node Name string , attrs authorizer . Attributes ) ( authorizer . Decision , string , error ) { // allowed verbs: get, create, update, patch, delete verb := attrs . Get if verb != " " && verb != " " && verb != " " && verb != " " && verb != " " { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No if len ( attrs . Get Subresource ( ) ) > 0 { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No // the request must come from a node with the same name as the CSI Node // note we skip this check for create, since the authorizer doesn't know the name on create // the noderestriction admission plugin is capable of performing this check at create time if verb != " " && attrs . Get Name ( ) != node Name { klog . V ( 2 ) . Infof ( " " , node return authorizer . Decision No return authorizer . Decision } 
func ( r * Node Authorizer ) has Path From ( node Name string , starting Type vertex Type , starting Namespace , starting Name string ) ( bool , error ) { r . graph . lock . R defer r . graph . lock . R node Vertex , exists := r . graph . get Vertex_rlocked ( node Vertex Type , " " , node if ! exists { return false , fmt . Errorf ( " " , node Name , vertex Types [ starting Type ] , starting Namespace , starting starting Vertex , exists := r . graph . get Vertex_rlocked ( starting Type , starting Namespace , starting if ! exists { return false , fmt . Errorf ( " " , node Name , vertex Types [ starting Type ] , starting Namespace , starting // Fast check to see if we know of a destination edge if r . graph . destination Edge Index [ starting Vertex . ID ( ) ] . has ( node traversal := & traverse . Visiting Depth First { Edge Filter : func ( edge graph . Edge ) bool { if destination Edge , ok := edge . ( * destination Edge ) ; ok { if destination Edge . Destination ID ( ) != node traversal . Walk ( r . graph . graph , starting Vertex , func ( n graph . Node ) bool { if n . ID ( ) == node if ! found { return false , fmt . Errorf ( " " , node Name , vertex Types [ starting Type ] , starting Namespace , starting } 
func ( f * recursive Fields Printer ) Visit Array ( a * proto . Array ) { a . Sub } 
func ( f * recursive Fields Printer ) Visit f . Writer . Write ( " \t " , key , Get Type sub Fields := & recursive Fields Printer { Writer : f . Writer . Indent ( indent Per if err := sub Fields . Print } 
func ( f * recursive Fields Printer ) Visit Map ( m * proto . Map ) { m . Sub } 
func ( f * recursive Fields Printer ) Visit Reference ( r proto . Reference ) { if _ , ok := visited visited r . Sub delete ( visited } 
func ( f * recursive Fields Printer ) Print } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Replica Set Controller Configuration ) ( nil ) , ( * config . Replica Set Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Replica Set Controller Configuration_To_config_Replica Set Controller Configuration ( a . ( * v1alpha1 . Replica Set Controller Configuration ) , b . ( * config . Replica Set Controller if err := s . Add Generated Conversion Func ( ( * config . Replica Set Controller Configuration ) ( nil ) , ( * v1alpha1 . Replica Set Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Replica Set Controller Configuration_To_v1alpha1_Replica Set Controller Configuration ( a . ( * config . Replica Set Controller Configuration ) , b . ( * v1alpha1 . Replica Set Controller if err := s . Add Conversion Func ( ( * config . Replica Set Controller Configuration ) ( nil ) , ( * v1alpha1 . Replica Set Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Replica Set Controller Configuration_To_v1alpha1_Replica Set Controller Configuration ( a . ( * config . Replica Set Controller Configuration ) , b . ( * v1alpha1 . Replica Set Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Replica Set Controller Configuration ) ( nil ) , ( * config . Replica Set Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Replica Set Controller Configuration_To_config_Replica Set Controller Configuration ( a . ( * v1alpha1 . Replica Set Controller Configuration ) , b . ( * config . Replica Set Controller } 
func ( in * Configuration ) Deep Copy out . Type Meta = in . Type for i := range * in { ( * in ) [ i ] . Deep Copy for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Configuration ) Deep in . Deep Copy } 
func ( in * Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func Register ( plugins * admission . Plugins ) { plugins . Register ( Plugin Name , func ( config io . Reader ) ( admission . Interface , error ) { // TODO move this to a versioned configuration file format. plugin Config := read plugin := New Pod Node Selector ( plugin Config . Pod Node Selector Plugin } 
func read Config ( config io . Reader ) * plugin Config { default Config := & plugin if config == nil || reflect . Value Of ( config ) . Is Nil ( ) { return default d := yaml . New YAML Or JSON for { if err := d . Decode ( default return default } 
func ( p * pod Node Selector ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { if should if ! p . Wait For Ready ( ) { return admission . New resource := a . Get Resource ( ) . Group pod := a . Get namespace Node Selector , err := p . get Namespace Node Selector Map ( a . Get if labels . Conflicts ( namespace Node Selector , labels . Set ( pod . Spec . Node Selector ) ) { return errors . New // Merge pod node selector = namespace node selector + current pod node selector // second selector wins pod Node Selector Labels := labels . Merge ( namespace Node Selector , pod . Spec . Node pod . Spec . Node Selector = map [ string ] string ( pod Node Selector } 
func ( p * pod Node Selector ) Validate ( a admission . Attributes , o admission . Object Interfaces ) error { if should if ! p . Wait For Ready ( ) { return admission . New resource := a . Get Resource ( ) . Group pod := a . Get namespace Node Selector , err := p . get Namespace Node Selector Map ( a . Get if labels . Conflicts ( namespace Node Selector , labels . Set ( pod . Spec . Node Selector ) ) { return errors . New // whitelist verification whitelist , err := labels . Convert Selector To Labels Map ( p . cluster Node Selectors [ a . Get if ! labels . Are Labels In White List ( pod . Spec . Node Selector , whitelist ) { return errors . New } 
func Get New Tab Writer ( output io . Writer ) * tabwriter . Writer { return tabwriter . New Writer ( output , tabwriter Min Width , tabwriter Width , tabwriter Padding , tabwriter Pad Char , tabwriter } 
func ( v * version ) Jobs ( ) Job Informer { return & job Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( in * Configuration ) Deep Copy out . Type Meta = in . Type if in . Limited Resources != nil { in , out := & in . Limited Resources , & out . Limited * out = make ( [ ] Limited for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Limited Resource ) Deep Copy Into ( out * Limited if in . Match Contains != nil { in , out := & in . Match Contains , & out . Match if in . Match Scopes != nil { in , out := & in . Match Scopes , & out . Match * out = make ( [ ] v1 . Scoped Resource Selector for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Limited Resource ) Deep Copy ( ) * Limited out := new ( Limited in . Deep Copy } 
func create Short Lived Bootstrap Token ( client clientset . Interface ) ( string , error ) { token Str , err := bootstraputil . Generate Bootstrap token , err := kubeadmapi . New Bootstrap Token String ( token tokens := [ ] kubeadmapi . Bootstrap Token { { Token : token , Description : " " , TTL : & metav1 . Duration { Duration : kubeadmconstants . Default Cert Token if err := nodebootstraptokenphase . Create New } 
func Create Certificate Key ( ) ( string , error ) { rand Bytes , err := cryptoutil . Create Rand Bytes ( kubeadmconstants . Certificate Key return hex . Encode To String ( rand } 
func Upload Certs ( client clientset . Interface , cfg * kubeadmapi . Init Configuration , key string ) error { fmt . Printf ( " \n " , kubeadmconstants . Kubeadm Certs Secret , metav1 . Namespace decoded Key , err := hex . Decode token ID , err := create Short Lived Bootstrap secret Data , err := get Data From Disk ( cfg , decoded ref , err := get Secret Owner Ref ( client , token err = apiclient . Create Or Update Secret ( client , & v1 . Secret { Object Meta : metav1 . Object Meta { Name : kubeadmconstants . Kubeadm Certs Secret , Namespace : metav1 . Namespace System , Owner References : ref , } , Data : secret return create } 
func Download Certs ( client clientset . Interface , cfg * kubeadmapi . Init Configuration , key string ) error { fmt . Printf ( " \n " , kubeadmconstants . Kubeadm Certs Secret , metav1 . Namespace decoded Key , err := hex . Decode secret , err := get secret Data , err := get Data From Secret ( secret , decoded for cert Or Key Name , cert Or Key Path := range certs To Transfer ( cfg ) { cert Or Key Data , found := secret Data [ cert Or Key Name To Secret Name ( cert Or Key if ! found { return errors . Errorf ( " " , cert Or Key Name , cert Or Key if len ( cert Or Key Data ) == 0 { klog . V ( 1 ) . Infof ( " \n " , cert Or Key Name , kubeadmconstants . Kubeadm Certs if err := write Cert Or Key ( cert Or Key Path , cert Or Key } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Custom Resource Column Definition ) ( nil ) , ( * apiextensions . Custom Resource Column Definition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Column Definition_To_apiextensions_Custom Resource Column Definition ( a . ( * Custom Resource Column Definition ) , b . ( * apiextensions . Custom Resource Column if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Column Definition ) ( nil ) , ( * Custom Resource Column Definition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Column Definition_To_v1beta1_Custom Resource Column Definition ( a . ( * apiextensions . Custom Resource Column Definition ) , b . ( * Custom Resource Column if err := s . Add Generated Conversion Func ( ( * Custom Resource Conversion ) ( nil ) , ( * apiextensions . Custom Resource Conversion ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Conversion_To_apiextensions_Custom Resource Conversion ( a . ( * Custom Resource Conversion ) , b . ( * apiextensions . Custom Resource if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Conversion ) ( nil ) , ( * Custom Resource Conversion ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Conversion_To_v1beta1_Custom Resource Conversion ( a . ( * apiextensions . Custom Resource Conversion ) , b . ( * Custom Resource if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition ) ( nil ) , ( * apiextensions . Custom Resource Definition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition_To_apiextensions_Custom Resource Definition ( a . ( * Custom Resource Definition ) , b . ( * apiextensions . Custom Resource if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition ) ( nil ) , ( * Custom Resource Definition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition_To_v1beta1_Custom Resource Definition ( a . ( * apiextensions . Custom Resource Definition ) , b . ( * Custom Resource if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition Condition ) ( nil ) , ( * apiextensions . Custom Resource Definition Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition Condition_To_apiextensions_Custom Resource Definition Condition ( a . ( * Custom Resource Definition Condition ) , b . ( * apiextensions . Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition Condition ) ( nil ) , ( * Custom Resource Definition Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition Condition_To_v1beta1_Custom Resource Definition Condition ( a . ( * apiextensions . Custom Resource Definition Condition ) , b . ( * Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition List ) ( nil ) , ( * apiextensions . Custom Resource Definition List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition List_To_apiextensions_Custom Resource Definition List ( a . ( * Custom Resource Definition List ) , b . ( * apiextensions . Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition List ) ( nil ) , ( * Custom Resource Definition List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition List_To_v1beta1_Custom Resource Definition List ( a . ( * apiextensions . Custom Resource Definition List ) , b . ( * Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition Names ) ( nil ) , ( * apiextensions . Custom Resource Definition Names ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition Names_To_apiextensions_Custom Resource Definition Names ( a . ( * Custom Resource Definition Names ) , b . ( * apiextensions . Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition Names ) ( nil ) , ( * Custom Resource Definition Names ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition Names_To_v1beta1_Custom Resource Definition Names ( a . ( * apiextensions . Custom Resource Definition Names ) , b . ( * Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition Spec ) ( nil ) , ( * apiextensions . Custom Resource Definition Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition Spec_To_apiextensions_Custom Resource Definition Spec ( a . ( * Custom Resource Definition Spec ) , b . ( * apiextensions . Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition Spec ) ( nil ) , ( * Custom Resource Definition Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition Spec_To_v1beta1_Custom Resource Definition Spec ( a . ( * apiextensions . Custom Resource Definition Spec ) , b . ( * Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition Status ) ( nil ) , ( * apiextensions . Custom Resource Definition Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition Status_To_apiextensions_Custom Resource Definition Status ( a . ( * Custom Resource Definition Status ) , b . ( * apiextensions . Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition Status ) ( nil ) , ( * Custom Resource Definition Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition Status_To_v1beta1_Custom Resource Definition Status ( a . ( * apiextensions . Custom Resource Definition Status ) , b . ( * Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * Custom Resource Definition Version ) ( nil ) , ( * apiextensions . Custom Resource Definition Version ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Definition Version_To_apiextensions_Custom Resource Definition Version ( a . ( * Custom Resource Definition Version ) , b . ( * apiextensions . Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Definition Version ) ( nil ) , ( * Custom Resource Definition Version ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Definition Version_To_v1beta1_Custom Resource Definition Version ( a . ( * apiextensions . Custom Resource Definition Version ) , b . ( * Custom Resource Definition if err := s . Add Generated Conversion Func ( ( * Custom Resource Subresource Scale ) ( nil ) , ( * apiextensions . Custom Resource Subresource Scale ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Subresource Scale_To_apiextensions_Custom Resource Subresource Scale ( a . ( * Custom Resource Subresource Scale ) , b . ( * apiextensions . Custom Resource Subresource if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Subresource Scale ) ( nil ) , ( * Custom Resource Subresource Scale ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Subresource Scale_To_v1beta1_Custom Resource Subresource Scale ( a . ( * apiextensions . Custom Resource Subresource Scale ) , b . ( * Custom Resource Subresource if err := s . Add Generated Conversion Func ( ( * Custom Resource Subresource Status ) ( nil ) , ( * apiextensions . Custom Resource Subresource Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Subresource Status_To_apiextensions_Custom Resource Subresource Status ( a . ( * Custom Resource Subresource Status ) , b . ( * apiextensions . Custom Resource Subresource if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Subresource Status ) ( nil ) , ( * Custom Resource Subresource Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Subresource Status_To_v1beta1_Custom Resource Subresource Status ( a . ( * apiextensions . Custom Resource Subresource Status ) , b . ( * Custom Resource Subresource if err := s . Add Generated Conversion Func ( ( * Custom Resource Subresources ) ( nil ) , ( * apiextensions . Custom Resource Subresources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Subresources_To_apiextensions_Custom Resource Subresources ( a . ( * Custom Resource Subresources ) , b . ( * apiextensions . Custom Resource if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Subresources ) ( nil ) , ( * Custom Resource Subresources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Subresources_To_v1beta1_Custom Resource Subresources ( a . ( * apiextensions . Custom Resource Subresources ) , b . ( * Custom Resource if err := s . Add Generated Conversion Func ( ( * Custom Resource Validation ) ( nil ) , ( * apiextensions . Custom Resource Validation ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Custom Resource Validation_To_apiextensions_Custom Resource Validation ( a . ( * Custom Resource Validation ) , b . ( * apiextensions . Custom Resource if err := s . Add Generated Conversion Func ( ( * apiextensions . Custom Resource Validation ) ( nil ) , ( * Custom Resource Validation ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Custom Resource Validation_To_v1beta1_Custom Resource Validation ( a . ( * apiextensions . Custom Resource Validation ) , b . ( * Custom Resource if err := s . Add Generated Conversion Func ( ( * External Documentation ) ( nil ) , ( * apiextensions . External Documentation ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_External Documentation_To_apiextensions_External Documentation ( a . ( * External Documentation ) , b . ( * apiextensions . External if err := s . Add Generated Conversion Func ( ( * apiextensions . External Documentation ) ( nil ) , ( * External Documentation ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_External Documentation_To_v1beta1_External Documentation ( a . ( * apiextensions . External Documentation ) , b . ( * External if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * JSON Schema Props ) ( nil ) , ( * apiextensions . JSON Schema Props ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_JSON Schema Props_To_apiextensions_JSON Schema Props ( a . ( * JSON Schema Props ) , b . ( * apiextensions . JSON Schema if err := s . Add Generated Conversion Func ( ( * apiextensions . JSON Schema Props ) ( nil ) , ( * JSON Schema Props ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_JSON Schema Props_To_v1beta1_JSON Schema Props ( a . ( * apiextensions . JSON Schema Props ) , b . ( * JSON Schema if err := s . Add Generated Conversion Func ( ( * JSON Schema Props Or Array ) ( nil ) , ( * apiextensions . JSON Schema Props Or Array ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_JSON Schema Props Or Array_To_apiextensions_JSON Schema Props Or Array ( a . ( * JSON Schema Props Or Array ) , b . ( * apiextensions . JSON Schema Props Or if err := s . Add Generated Conversion Func ( ( * apiextensions . JSON Schema Props Or Array ) ( nil ) , ( * JSON Schema Props Or Array ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_JSON Schema Props Or Array_To_v1beta1_JSON Schema Props Or Array ( a . ( * apiextensions . JSON Schema Props Or Array ) , b . ( * JSON Schema Props Or if err := s . Add Generated Conversion Func ( ( * JSON Schema Props Or Bool ) ( nil ) , ( * apiextensions . JSON Schema Props Or Bool ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_JSON Schema Props Or Bool_To_apiextensions_JSON Schema Props Or Bool ( a . ( * JSON Schema Props Or Bool ) , b . ( * apiextensions . JSON Schema Props Or if err := s . Add Generated Conversion Func ( ( * apiextensions . JSON Schema Props Or Bool ) ( nil ) , ( * JSON Schema Props Or Bool ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_JSON Schema Props Or Bool_To_v1beta1_JSON Schema Props Or Bool ( a . ( * apiextensions . JSON Schema Props Or Bool ) , b . ( * JSON Schema Props Or if err := s . Add Generated Conversion Func ( ( * JSON Schema Props Or String Array ) ( nil ) , ( * apiextensions . JSON Schema Props Or String Array ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_JSON Schema Props Or String Array_To_apiextensions_JSON Schema Props Or String Array ( a . ( * JSON Schema Props Or String Array ) , b . ( * apiextensions . JSON Schema Props Or String if err := s . Add Generated Conversion Func ( ( * apiextensions . JSON Schema Props Or String Array ) ( nil ) , ( * JSON Schema Props Or String Array ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_JSON Schema Props Or String Array_To_v1beta1_JSON Schema Props Or String Array ( a . ( * apiextensions . JSON Schema Props Or String Array ) , b . ( * JSON Schema Props Or String if err := s . Add Generated Conversion Func ( ( * Service Reference ) ( nil ) , ( * apiextensions . Service Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Service Reference_To_apiextensions_Service Reference ( a . ( * Service Reference ) , b . ( * apiextensions . Service if err := s . Add Generated Conversion Func ( ( * apiextensions . Service Reference ) ( nil ) , ( * Service Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Service Reference_To_v1beta1_Service Reference ( a . ( * apiextensions . Service Reference ) , b . ( * Service if err := s . Add Generated Conversion Func ( ( * Webhook Client Config ) ( nil ) , ( * apiextensions . Webhook Client Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Webhook Client Config_To_apiextensions_Webhook Client Config ( a . ( * Webhook Client Config ) , b . ( * apiextensions . Webhook Client if err := s . Add Generated Conversion Func ( ( * apiextensions . Webhook Client Config ) ( nil ) , ( * Webhook Client Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_Webhook Client Config_To_v1beta1_Webhook Client Config ( a . ( * apiextensions . Webhook Client Config ) , b . ( * Webhook Client if err := s . Add Conversion Func ( ( * apiextensions . JSON Schema Props ) ( nil ) , ( * JSON Schema Props ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiextensions_JSON Schema Props_To_v1beta1_JSON Schema Props ( a . ( * apiextensions . JSON Schema Props ) , b . ( * JSON Schema if err := s . Add Conversion if err := s . Add Conversion } 
func Convert_v1beta1_Custom Resource Column Definition_To_apiextensions_Custom Resource Column Definition ( in * Custom Resource Column Definition , out * apiextensions . Custom Resource Column Definition , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Column Definition_To_apiextensions_Custom Resource Column } 
func Convert_apiextensions_Custom Resource Column Definition_To_v1beta1_Custom Resource Column Definition ( in * apiextensions . Custom Resource Column Definition , out * Custom Resource Column Definition , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Column Definition_To_v1beta1_Custom Resource Column } 
func Convert_v1beta1_Custom Resource Conversion_To_apiextensions_Custom Resource Conversion ( in * Custom Resource Conversion , out * apiextensions . Custom Resource Conversion , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Conversion_To_apiextensions_Custom Resource } 
func Convert_apiextensions_Custom Resource Conversion_To_v1beta1_Custom Resource Conversion ( in * apiextensions . Custom Resource Conversion , out * Custom Resource Conversion , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Conversion_To_v1beta1_Custom Resource } 
func Convert_v1beta1_Custom Resource Definition_To_apiextensions_Custom Resource Definition ( in * Custom Resource Definition , out * apiextensions . Custom Resource Definition , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition_To_apiextensions_Custom Resource } 
func Convert_apiextensions_Custom Resource Definition_To_v1beta1_Custom Resource Definition ( in * apiextensions . Custom Resource Definition , out * Custom Resource Definition , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition_To_v1beta1_Custom Resource } 
func Convert_v1beta1_Custom Resource Definition Condition_To_apiextensions_Custom Resource Definition Condition ( in * Custom Resource Definition Condition , out * apiextensions . Custom Resource Definition Condition , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition Condition_To_apiextensions_Custom Resource Definition } 
func Convert_apiextensions_Custom Resource Definition Condition_To_v1beta1_Custom Resource Definition Condition ( in * apiextensions . Custom Resource Definition Condition , out * Custom Resource Definition Condition , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition Condition_To_v1beta1_Custom Resource Definition } 
func Convert_v1beta1_Custom Resource Definition List_To_apiextensions_Custom Resource Definition List ( in * Custom Resource Definition List , out * apiextensions . Custom Resource Definition List , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition List_To_apiextensions_Custom Resource Definition } 
func Convert_apiextensions_Custom Resource Definition List_To_v1beta1_Custom Resource Definition List ( in * apiextensions . Custom Resource Definition List , out * Custom Resource Definition List , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition List_To_v1beta1_Custom Resource Definition } 
func Convert_v1beta1_Custom Resource Definition Names_To_apiextensions_Custom Resource Definition Names ( in * Custom Resource Definition Names , out * apiextensions . Custom Resource Definition Names , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition Names_To_apiextensions_Custom Resource Definition } 
func Convert_apiextensions_Custom Resource Definition Names_To_v1beta1_Custom Resource Definition Names ( in * apiextensions . Custom Resource Definition Names , out * Custom Resource Definition Names , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition Names_To_v1beta1_Custom Resource Definition } 
func Convert_v1beta1_Custom Resource Definition Spec_To_apiextensions_Custom Resource Definition Spec ( in * Custom Resource Definition Spec , out * apiextensions . Custom Resource Definition Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition Spec_To_apiextensions_Custom Resource Definition } 
func Convert_apiextensions_Custom Resource Definition Spec_To_v1beta1_Custom Resource Definition Spec ( in * apiextensions . Custom Resource Definition Spec , out * Custom Resource Definition Spec , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition Spec_To_v1beta1_Custom Resource Definition } 
func Convert_v1beta1_Custom Resource Definition Status_To_apiextensions_Custom Resource Definition Status ( in * Custom Resource Definition Status , out * apiextensions . Custom Resource Definition Status , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition Status_To_apiextensions_Custom Resource Definition } 
func Convert_apiextensions_Custom Resource Definition Status_To_v1beta1_Custom Resource Definition Status ( in * apiextensions . Custom Resource Definition Status , out * Custom Resource Definition Status , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition Status_To_v1beta1_Custom Resource Definition } 
func Convert_v1beta1_Custom Resource Definition Version_To_apiextensions_Custom Resource Definition Version ( in * Custom Resource Definition Version , out * apiextensions . Custom Resource Definition Version , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Definition Version_To_apiextensions_Custom Resource Definition } 
func Convert_apiextensions_Custom Resource Definition Version_To_v1beta1_Custom Resource Definition Version ( in * apiextensions . Custom Resource Definition Version , out * Custom Resource Definition Version , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Definition Version_To_v1beta1_Custom Resource Definition } 
func Convert_v1beta1_Custom Resource Subresource Scale_To_apiextensions_Custom Resource Subresource Scale ( in * Custom Resource Subresource Scale , out * apiextensions . Custom Resource Subresource Scale , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Subresource Scale_To_apiextensions_Custom Resource Subresource } 
func Convert_apiextensions_Custom Resource Subresource Scale_To_v1beta1_Custom Resource Subresource Scale ( in * apiextensions . Custom Resource Subresource Scale , out * Custom Resource Subresource Scale , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Subresource Scale_To_v1beta1_Custom Resource Subresource } 
func Convert_v1beta1_Custom Resource Subresource Status_To_apiextensions_Custom Resource Subresource Status ( in * Custom Resource Subresource Status , out * apiextensions . Custom Resource Subresource Status , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Subresource Status_To_apiextensions_Custom Resource Subresource } 
func Convert_apiextensions_Custom Resource Subresource Status_To_v1beta1_Custom Resource Subresource Status ( in * apiextensions . Custom Resource Subresource Status , out * Custom Resource Subresource Status , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Subresource Status_To_v1beta1_Custom Resource Subresource } 
func Convert_v1beta1_Custom Resource Subresources_To_apiextensions_Custom Resource Subresources ( in * Custom Resource Subresources , out * apiextensions . Custom Resource Subresources , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Subresources_To_apiextensions_Custom Resource } 
func Convert_apiextensions_Custom Resource Subresources_To_v1beta1_Custom Resource Subresources ( in * apiextensions . Custom Resource Subresources , out * Custom Resource Subresources , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Subresources_To_v1beta1_Custom Resource } 
func Convert_v1beta1_Custom Resource Validation_To_apiextensions_Custom Resource Validation ( in * Custom Resource Validation , out * apiextensions . Custom Resource Validation , s conversion . Scope ) error { return auto Convert_v1beta1_Custom Resource Validation_To_apiextensions_Custom Resource } 
func Convert_apiextensions_Custom Resource Validation_To_v1beta1_Custom Resource Validation ( in * apiextensions . Custom Resource Validation , out * Custom Resource Validation , s conversion . Scope ) error { return auto Convert_apiextensions_Custom Resource Validation_To_v1beta1_Custom Resource } 
func Convert_v1beta1_External Documentation_To_apiextensions_External Documentation ( in * External Documentation , out * apiextensions . External Documentation , s conversion . Scope ) error { return auto Convert_v1beta1_External Documentation_To_apiextensions_External } 
func Convert_apiextensions_External Documentation_To_v1beta1_External Documentation ( in * apiextensions . External Documentation , out * External Documentation , s conversion . Scope ) error { return auto Convert_apiextensions_External Documentation_To_v1beta1_External } 
func Convert_v1beta1_JSON Schema Props_To_apiextensions_JSON Schema Props ( in * JSON Schema Props , out * apiextensions . JSON Schema Props , s conversion . Scope ) error { return auto Convert_v1beta1_JSON Schema Props_To_apiextensions_JSON Schema } 
func Convert_v1beta1_JSON Schema Props Or Array_To_apiextensions_JSON Schema Props Or Array ( in * JSON Schema Props Or Array , out * apiextensions . JSON Schema Props Or Array , s conversion . Scope ) error { return auto Convert_v1beta1_JSON Schema Props Or Array_To_apiextensions_JSON Schema Props Or } 
func Convert_apiextensions_JSON Schema Props Or Array_To_v1beta1_JSON Schema Props Or Array ( in * apiextensions . JSON Schema Props Or Array , out * JSON Schema Props Or Array , s conversion . Scope ) error { return auto Convert_apiextensions_JSON Schema Props Or Array_To_v1beta1_JSON Schema Props Or } 
func Convert_v1beta1_JSON Schema Props Or Bool_To_apiextensions_JSON Schema Props Or Bool ( in * JSON Schema Props Or Bool , out * apiextensions . JSON Schema Props Or Bool , s conversion . Scope ) error { return auto Convert_v1beta1_JSON Schema Props Or Bool_To_apiextensions_JSON Schema Props Or } 
func Convert_apiextensions_JSON Schema Props Or Bool_To_v1beta1_JSON Schema Props Or Bool ( in * apiextensions . JSON Schema Props Or Bool , out * JSON Schema Props Or Bool , s conversion . Scope ) error { return auto Convert_apiextensions_JSON Schema Props Or Bool_To_v1beta1_JSON Schema Props Or } 
func Convert_v1beta1_JSON Schema Props Or String Array_To_apiextensions_JSON Schema Props Or String Array ( in * JSON Schema Props Or String Array , out * apiextensions . JSON Schema Props Or String Array , s conversion . Scope ) error { return auto Convert_v1beta1_JSON Schema Props Or String Array_To_apiextensions_JSON Schema Props Or String } 
func Convert_apiextensions_JSON Schema Props Or String Array_To_v1beta1_JSON Schema Props Or String Array ( in * apiextensions . JSON Schema Props Or String Array , out * JSON Schema Props Or String Array , s conversion . Scope ) error { return auto Convert_apiextensions_JSON Schema Props Or String Array_To_v1beta1_JSON Schema Props Or String } 
func Convert_v1beta1_Service Reference_To_apiextensions_Service Reference ( in * Service Reference , out * apiextensions . Service Reference , s conversion . Scope ) error { return auto Convert_v1beta1_Service Reference_To_apiextensions_Service } 
func Convert_apiextensions_Service Reference_To_v1beta1_Service Reference ( in * apiextensions . Service Reference , out * Service Reference , s conversion . Scope ) error { return auto Convert_apiextensions_Service Reference_To_v1beta1_Service } 
func Convert_v1beta1_Webhook Client Config_To_apiextensions_Webhook Client Config ( in * Webhook Client Config , out * apiextensions . Webhook Client Config , s conversion . Scope ) error { return auto Convert_v1beta1_Webhook Client Config_To_apiextensions_Webhook Client } 
func Convert_apiextensions_Webhook Client Config_To_v1beta1_Webhook Client Config ( in * apiextensions . Webhook Client Config , out * Webhook Client Config , s conversion . Scope ) error { return auto Convert_apiextensions_Webhook Client Config_To_v1beta1_Webhook Client } 
func ( self * Resource List ) Cpu ( ) * resource . Quantity { if val , ok := ( * self ) [ Resource return & resource . Quantity { Format : resource . Decimal } 
func With Panic Recovery ( handler http . Handler ) http . Handler { return with Panic Recovery ( handler , func ( w http . Response Writer , req * http . Request , err interface { } ) { http . Error ( w , " " , http . Status Internal Server klog . Errorf ( " " , req . Method , req . Request } 
func new Service Accounts ( c * Core V1Client , namespace string ) * service Accounts { return & service Accounts { client : c . REST } 
func ( c * service Accounts ) Create ( service Account * v1 . Service Account ) ( result * v1 . Service Account , err error ) { result = & v1 . Service err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( service } 
func New Alpha Feature Gate ( features [ ] string ) * Alpha Feature Gate { feature for _ , name := range features { feature return & Alpha Feature Gate { feature } 
func ( jm * Job Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer jm . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , jm . pod Store Synced , jm . job Store for i := 0 ; i < workers ; i ++ { go wait . Until ( jm . worker , time . Second , stop <- stop } 
func ( jm * Job Controller ) get Pod Jobs ( pod * v1 . Pod ) [ ] * batch . Job { jobs , err := jm . job Lister . Get Pod if len ( jobs ) > 1 { // Controller Ref will ensure we don't do anything crazy, but more than one // item in this list nevertheless constitutes user error. utilruntime . Handle } 
func ( jm * Job Controller ) resolve Controller Ref ( namespace string , controller Ref * metav1 . Owner Reference ) * batch . Job { // We can't look up by UID, so look up by Name and then verify UID. // Don't even try to look up by Name if it's the wrong Kind. if controller Ref . Kind != controller job , err := jm . job Lister . Jobs ( namespace ) . Get ( controller if job . UID != controller Ref . UID { // The controller we found with this Name is not the same one that the // Controller } 
func ( jm * Job Controller ) add if pod . Deletion Timestamp != nil { // on a restart of the controller controller, it's possible a new pod shows up in a state that // is already pending deletion. Prevent the pod from being a creation observation. jm . delete // If it has a Controller Ref, that's all that matters. if controller Ref := metav1 . Get Controller Of ( pod ) ; controller Ref != nil { job := jm . resolve Controller Ref ( pod . Namespace , controller job Key , err := controller . Key jm . expectations . Creation Observed ( job jm . enqueue // Otherwise, it's an orphan. Get a list of all matching controllers and sync // them to see if anyone wants to adopt it. // DO NOT observe creation because no controller should be waiting for an // orphan. for _ , job := range jm . get Pod Jobs ( pod ) { jm . enqueue } 
func ( jm * Job Controller ) update Pod ( old , cur interface { } ) { cur old if cur Pod . Resource Version == old Pod . Resource Version { // Periodic resync will send update events for all known pods. // Two different versions of the same pod will always have different R if cur Pod . Deletion Timestamp != nil { // when a pod is deleted gracefully it's deletion timestamp is first modified to reflect a grace period, // and after such time has passed, the kubelet actually deletes it from the store. We receive an update // for modification of the deletion timestamp and expect an job to create more pods asap, not wait // until the kubelet actually deletes the pod. jm . delete Pod ( cur // the only time we want the backoff to kick-in, is when the pod failed immediate := cur Pod . Status . Phase != v1 . Pod cur Controller Ref := metav1 . Get Controller Of ( cur old Controller Ref := metav1 . Get Controller Of ( old controller Ref Changed := ! reflect . Deep Equal ( cur Controller Ref , old Controller if controller Ref Changed && old Controller Ref != nil { // The Controller Ref was changed. Sync the old controller, if any. if job := jm . resolve Controller Ref ( old Pod . Namespace , old Controller Ref ) ; job != nil { jm . enqueue // If it has a Controller Ref, that's all that matters. if cur Controller Ref != nil { job := jm . resolve Controller Ref ( cur Pod . Namespace , cur Controller jm . enqueue // Otherwise, it's an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. label Changed := ! reflect . Deep Equal ( cur Pod . Labels , old if label Changed || controller Ref Changed { for _ , job := range jm . get Pod Jobs ( cur Pod ) { jm . enqueue } 
func ( jm * Job Controller ) delete // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the pod // changed labels the new job will not be woken up till the periodic resync. if ! ok { tombstone , ok := obj . ( cache . Deleted Final State if ! ok { utilruntime . Handle if ! ok { utilruntime . Handle controller Ref := metav1 . Get Controller if controller job := jm . resolve Controller Ref ( pod . Namespace , controller job Key , err := controller . Key jm . expectations . Deletion Observed ( job jm . enqueue } 
func ( jm * Job Controller ) enqueue Controller ( obj interface { } , immediate bool ) { key , err := controller . Key if err != nil { utilruntime . Handle if ! immediate { backoff = get // TODO: Handle overlapping controllers better. Either disallow them at admission time or // deterministically avoid syncing controllers that fight over pods. Currently, we only // ensure that the same controller is synced for a given pod. When we periodically relist // all controllers there will still be some replica instability. One way to handle this is // by querying the store for all controllers that this rc overlaps, as well as all // controllers that overlap this rc, and sorting them. jm . queue . Add } 
func ( jm * Job Controller ) get Pods For Job ( j * batch . Job ) ( [ ] * v1 . Pod , error ) { selector , err := metav1 . Label Selector As // List all pods to include those that don't match the selector anymore // but have a Controller Ref pointing to this controller. pods , err := jm . pod // If any adoptions are attempted, we should first recheck for deletion // with an uncached quorum read sometime after listing Pods (see #42639). can Adopt Func := controller . Recheck Deletion Timestamp ( func ( ) ( metav1 . Object , error ) { fresh , err := jm . kube Client . Batch V1 ( ) . Jobs ( j . Namespace ) . Get ( j . Name , metav1 . Get cm := controller . New Pod Controller Ref Manager ( jm . pod Control , j , selector , controller Kind , can Adopt return cm . Claim } 
func ( jm * Job Controller ) sync Job ( key string ) ( bool , error ) { start defer func ( ) { klog . V ( 4 ) . Infof ( " " , key , time . Since ( start ns , name , err := cache . Split Meta Namespace shared Job , err := jm . job if err != nil { if errors . Is Not jm . expectations . Delete job := * shared // if job was finished previously, we don't want to redo the termination if Is Job // retrieve the previous number of retry previous Retry := jm . queue . Num // Check the expectations of the job before counting active pods, otherwise a new pod can sneak in // and update the expectations after we've retrieved active pods from the store. If a new pod enters // the store after we've checked the expectation, the job sync is just deferred till the next relist. job Needs Sync := jm . expectations . Satisfied pods , err := jm . get Pods For active Pods := controller . Filter Active active := int32 ( len ( active succeeded , failed := get // job first start if job . Status . Start job . Status . Start // enqueue a sync to check if job past Active Deadline Seconds if job . Spec . Active Deadline Seconds != nil { klog . V ( 4 ) . Infof ( " " , key , * job . Spec . Active Deadline jm . queue . Add After ( key , time . Duration ( * job . Spec . Active Deadline var manage Job job var failure var failure job Have New // new failures happen when status does not reflect the failures and active // is different than parallelism, otherwise the previous controller loop // failed updating status so even if we pick up failure it is not a new one exceeds Backoff Limit := job Have New Failure && ( active != * job . Spec . Parallelism ) && ( int32 ( previous Retry ) + 1 > * job . Spec . Backoff if exceeds Backoff Limit || past Backoff Limit On Failure ( & job , pods ) { // check if the number of pod restart exceeds backoff (for restart On Failure only) // OR if the number of failed jobs increased since the last sync Job job failure failure } else if past Active Deadline ( & job ) { job failure failure if job Failed { err jm . delete Job Pods ( & job , active Pods , err select { case manage Job Err = <- err Ch : if manage Job job . Status . Conditions = append ( job . Status . Conditions , new Condition ( batch . Job Failed , failure Reason , failure jm . recorder . Event ( & job , v1 . Event Type Warning , failure Reason , failure } else { if job Needs Sync && job . Deletion Timestamp == nil { active , manage Job Err = jm . manage Job ( active if active > 0 { jm . recorder . Event ( & job , v1 . Event Type if completions > * job . Spec . Completions { jm . recorder . Event ( & job , v1 . Event Type if complete { job . Status . Conditions = append ( job . Status . Conditions , new Condition ( batch . Job job . Status . Completion if err := jm . update if job Have New Failure && ! Is Job return forget , manage Job } 
func past Backoff Limit On Failure ( job * batch . Job , pods [ ] * v1 . Pod ) bool { if job . Spec . Template . Spec . Restart Policy != v1 . Restart Policy On if po . Status . Phase != v1 . Pod for j := range po . Status . Init Container Statuses { stat := po . Status . Init Container result += stat . Restart for j := range po . Status . Container Statuses { stat := po . Status . Container result += stat . Restart if * job . Spec . Backoff return result >= * job . Spec . Backoff } 
func past Active Deadline ( job * batch . Job ) bool { if job . Spec . Active Deadline Seconds == nil || job . Status . Start start := job . Status . Start allowed Duration := time . Duration ( * job . Spec . Active Deadline return duration >= allowed } 
func get Status ( pods [ ] * v1 . Pod ) ( succeeded , failed int32 ) { succeeded = int32 ( filter Pods ( pods , v1 . Pod failed = int32 ( filter Pods ( pods , v1 . Pod } 
func ( jm * Job Controller ) manage Job ( active Pods [ ] * v1 . Pod , succeeded int32 , job * batch . Job ) ( int32 , error ) { var active active := int32 ( len ( active job Key , err := controller . Key if err != nil { utilruntime . Handle var err err jm . expectations . Expect Deletions ( job klog . V ( 4 ) . Infof ( " " , job // Sort the pods in the order such that not-ready < ready, unscheduled // < scheduled, and pending < running. This ensures that we delete pods // in the earlier stages whenever possible. sort . Sort ( controller . Active Pods ( active wait := sync . Wait if err := jm . pod Control . Delete Pod ( job . Namespace , active Pods [ ix ] . Name , job ) ; err != nil { defer utilruntime . Handle // Decrement the expected number of deletes because the informer won't observe this deletion klog . V ( 2 ) . Infof ( " " , active jm . expectations . Deletion Observed ( job active active err } else if active < parallelism { want if job . Spec . Completions == nil { // Job does not specify a number of completions. Therefore, number active // should be equal to parallelism, unless the job has seen at least // once success, in which leave whatever is running, running. if succeeded > 0 { want } else { want } else { // Job specifies a specific number of completions. Therefore, number // active should not ever exceed number of remaining completions. want if want Active > parallelism { want diff := want if diff < 0 { utilruntime . Handle Error ( fmt . Errorf ( " " , job Key , want jm . expectations . Expect Creations ( job err klog . V ( 4 ) . Infof ( " " , job Key , want wait := sync . Wait // Batch the pod creates. Batch sizes start at Slow Start Initial Batch Size // and double with each successful iteration in a kind of "slow start". // This handles attempts to start large numbers of pods that would // likely all fail with the same error. For example a project with a // low quota that attempts to create a large number of pods will be // prevented from spamming the API service with the pod create requests // after one of its pods fails. Conveniently, this also prevents the // event spam that those failures would generate. for batch Size := int32 ( integer . Int Min ( int ( diff ) , controller . Slow Start Initial Batch Size ) ) ; diff > 0 ; batch Size = integer . Int32Min ( 2 * batch Size , diff ) { error Count := len ( err wait . Add ( int ( batch for i := int32 ( 0 ) ; i < batch err := jm . pod Control . Create Pods With Controller Ref ( job . Namespace , & job . Spec . Template , job , metav1 . New Controller Ref ( job , controller if err != nil && errors . Is if err != nil { defer utilruntime . Handle jm . expectations . Creation Observed ( job active active err // any skipped pods that we never attempted to start shouldn't be expected. skipped Pods := diff - batch if error Count < len ( err Ch ) && skipped Pods > 0 { klog . V ( 2 ) . Infof ( " " , skipped active -= skipped for i := int32 ( 0 ) ; i < skipped Pods ; i ++ { // Decrement the expected number of creates because the informer won't observe this pod jm . expectations . Creation Observed ( job diff -= batch select { case err := <- err } 
func filter Pods ( pods [ ] * v1 . Pod , phase v1 . Pod } 
func ( in * Cloud Controller Manager Configuration ) Deep Copy Into ( out * Cloud Controller Manager out . Type Meta = in . Type in . Generic . Deep Copy in . Kube Cloud Shared . Deep Copy Into ( & out . Kube Cloud out . Service Controller = in . Service out . Node Status Update Frequency = in . Node Status Update } 
func ( in * Cloud Controller Manager Configuration ) Deep Copy ( ) * Cloud Controller Manager out := new ( Cloud Controller Manager in . Deep Copy } 
func ( in * Cloud Controller Manager Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( v * version ) Audit Sinks ( ) Audit Sink Informer { return & audit Sink Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( t TTY ) Safe ( fn Safe Func ) error { in Fd , is Terminal := term . Get Fd if ! is Terminal && t . Try in is Terminal = term . Is Terminal ( in if ! is if t . Raw { state , err = term . Make Raw ( in } else { state , err = term . Save State ( in return interrupt . Chain ( t . Parent , func ( ) { if t . size Queue != nil { t . size term . Restore Terminal ( in } 
func Parse ( name , text string ) ( * Parser , error ) { p := New } 
func parse Action ( name , text string ) ( * Parser , error ) { p , err := Parse ( name , fmt . Sprintf ( " " , left Delim , text , right p . Root = p . Root . Nodes [ 0 ] . ( * List } 
func ( p * Parser ) consume } 
} 
func ( p * Parser ) parse Left Delim ( cur * List Node ) error { p . pos += len ( left p . consume new Node := new cur . append ( new cur = new return p . parse Inside } 
func ( p * Parser ) parse Right Delim ( cur * List Node ) error { p . pos += len ( right p . consume return p . parse } 
func ( p * Parser ) parse Identifier ( cur * List if is value := p . consume if is Bool ( value ) { v , err := strconv . Parse cur . append ( new } else { cur . append ( new return p . parse Inside } 
func ( p * Parser ) parse Recursive ( cur * List p . consume cur . append ( new if r := p . peek ( ) ; is Alpha Numeric ( r ) { return p . parse return p . parse Inside } 
func ( p * Parser ) parse Number ( cur * List if r != '.' && ! unicode . Is value := p . consume if err == nil { cur . append ( new return p . parse Inside d , err := strconv . Parse if err == nil { cur . append ( new return p . parse Inside } 
func ( p * Parser ) parse Array ( cur * List text := p . consume if len ( strs ) > 1 { union := [ ] * List for _ , str := range strs { parser , err := parse cur . append ( new return p . parse Inside // dict key value := dict Key Rex . Find String if value != nil { parser , err := parse return p . parse Inside //slice operator value = slice Operator Rex . Find String params := [ 3 ] Params cur . append ( new return p . parse Inside } 
func ( p * Parser ) parse Filter ( cur * List p . consume case ')' : //in right reg := regexp . Must text := p . consume value := reg . Find String if value == nil { parser , err := parse cur . append ( new Filter ( parser . Root , new } else { left Parser , err := parse right Parser , err := parse cur . append ( new Filter ( left Parser . Root , right return p . parse Inside } 
func ( p * Parser ) parse Quote ( cur * List value := p . consume s , err := Unquote cur . append ( new return p . parse Inside } 
func ( p * Parser ) parse Field ( cur * List Node ) error { p . consume value := p . consume if value == " " { cur . append ( new } else { cur . append ( new return p . parse Inside } 
} else if is } 
func is Terminator ( r rune ) bool { if is Space ( r ) || is End Of } 
func is Alpha Numeric ( r rune ) bool { return r == '_' || unicode . Is Letter ( r ) || unicode . Is } 
func Unquote if n < 2 { return " " , Err if quote != s [ n - 1 ] { return " " , Err if quote != '"' && quote != '\'' { return " " , Err var rune Tmp [ utf8 . UTF for len ( s ) > 0 { c , multibyte , ss , err := strconv . Unquote if c < utf8 . Rune } else { n := utf8 . Encode Rune ( rune buf = append ( buf , rune } 
func ( c * Config ) New ( proxy Transport * http . Transport , service Resolver webhook . Service Resolver ) ( [ ] admission . Plugin Initializer , server . Post Start Hook Func , error ) { webhook Auth Resolver Wrapper := webhook . New Default Authentication Info Resolver Wrapper ( proxy Transport , c . Loopback Client webhook Plugin Initializer := webhookinit . New Plugin Initializer ( webhook Auth Resolver Wrapper , service var cloud if c . Cloud Config cloud Config , err = ioutil . Read File ( c . Cloud Config if err != nil { klog . Fatalf ( " " , c . Cloud Config clientset , err := kubernetes . New For Config ( c . Loopback Client discovery Client := cacheddiscovery . New Mem Cache discovery REST Mapper := restmapper . New Deferred Discovery REST Mapper ( discovery kube Plugin Initializer := New Plugin Initializer ( cloud Config , discovery REST Mapper , quotainstall . New Quota Configuration For admission Post Start Hook := func ( context genericapiserver . Post Start Hook Context ) error { discovery REST go utilwait . Until ( discovery REST Mapper . Reset , 30 * time . Second , context . Stop return [ ] admission . Plugin Initializer { webhook Plugin Initializer , kube Plugin Initializer } , admission Post Start } 
func New Kube Config Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Short : " " , Long : cmdutil . Macro Command Long Description , Phases : [ ] workflow . Phase { { Name : " " , Short : " " , Inherit Flags : get Kube Config Phase Flags ( " " ) , Run All Siblings : true , } , New Kube Config File Phase ( kubeadmconstants . Admin Kube Config File Name ) , New Kube Config File Phase ( kubeadmconstants . Kubelet Kube Config File Name ) , New Kube Config File Phase ( kubeadmconstants . Controller Manager Kube Config File Name ) , New Kube Config File Phase ( kubeadmconstants . Scheduler Kube Config File Name ) , } , Run : run Kube } 
func New Kube Config File Phase ( kube Config File Name string ) workflow . Phase { return workflow . Phase { Name : kubeconfig File Phase Properties [ kube Config File Name ] . name , Short : kubeconfig File Phase Properties [ kube Config File Name ] . short , Long : fmt . Sprintf ( kubeconfig File Phase Properties [ kube Config File Name ] . long , kube Config File Name ) , Run : run Kube Config File ( kube Config File Name ) , Inherit Flags : get Kube Config Phase Flags ( kube Config File } 
func run Kube Config File ( kube Config File Name string ) func ( workflow . Run Data ) error { return func ( c workflow . Run Data ) error { data , ok := c . ( Init // if external CA mode, skip certificate authority generation if data . External CA ( ) { fmt . Printf ( " \n " , kube Config File cfg . Certificates Dir = data . Certificate Write defer func ( ) { cfg . Certificates Dir = data . Certificate // creates the Kube Config file (or use existing) return kubeconfigphase . Create Kube Config File ( kube Config File Name , data . Kube Config } 
func ( s * controller Revision Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Controller Revision , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Controller } 
func ( s * controller Revision Lister ) Controller Revisions ( namespace string ) Controller Revision Namespace Lister { return controller Revision Namespace } 
func ( s controller Revision Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Controller Revision , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Controller } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * AES Configuration ) ( nil ) , ( * config . AES Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_AES Configuration_To_config_AES Configuration ( a . ( * AES Configuration ) , b . ( * config . AES if err := s . Add Generated Conversion Func ( ( * config . AES Configuration ) ( nil ) , ( * AES Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_AES Configuration_To_v1_AES Configuration ( a . ( * config . AES Configuration ) , b . ( * AES if err := s . Add Generated Conversion Func ( ( * Encryption Configuration ) ( nil ) , ( * config . Encryption Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Encryption Configuration_To_config_Encryption Configuration ( a . ( * Encryption Configuration ) , b . ( * config . Encryption if err := s . Add Generated Conversion Func ( ( * config . Encryption Configuration ) ( nil ) , ( * Encryption Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Encryption Configuration_To_v1_Encryption Configuration ( a . ( * config . Encryption Configuration ) , b . ( * Encryption if err := s . Add Generated Conversion Func ( ( * Identity Configuration ) ( nil ) , ( * config . Identity Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Identity Configuration_To_config_Identity Configuration ( a . ( * Identity Configuration ) , b . ( * config . Identity if err := s . Add Generated Conversion Func ( ( * config . Identity Configuration ) ( nil ) , ( * Identity Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Identity Configuration_To_v1_Identity Configuration ( a . ( * config . Identity Configuration ) , b . ( * Identity if err := s . Add Generated Conversion Func ( ( * KMS Configuration ) ( nil ) , ( * config . KMS Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_KMS Configuration_To_config_KMS Configuration ( a . ( * KMS Configuration ) , b . ( * config . KMS if err := s . Add Generated Conversion Func ( ( * config . KMS Configuration ) ( nil ) , ( * KMS Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_KMS Configuration_To_v1_KMS Configuration ( a . ( * config . KMS Configuration ) , b . ( * KMS if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Provider Configuration ) ( nil ) , ( * config . Provider Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Provider Configuration_To_config_Provider Configuration ( a . ( * Provider Configuration ) , b . ( * config . Provider if err := s . Add Generated Conversion Func ( ( * config . Provider Configuration ) ( nil ) , ( * Provider Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Provider Configuration_To_v1_Provider Configuration ( a . ( * config . Provider Configuration ) , b . ( * Provider if err := s . Add Generated Conversion Func ( ( * Resource Configuration ) ( nil ) , ( * config . Resource Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Configuration_To_config_Resource Configuration ( a . ( * Resource Configuration ) , b . ( * config . Resource if err := s . Add Generated Conversion Func ( ( * config . Resource Configuration ) ( nil ) , ( * Resource Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Resource Configuration_To_v1_Resource Configuration ( a . ( * config . Resource Configuration ) , b . ( * Resource if err := s . Add Generated Conversion Func ( ( * Secretbox Configuration ) ( nil ) , ( * config . Secretbox Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secretbox Configuration_To_config_Secretbox Configuration ( a . ( * Secretbox Configuration ) , b . ( * config . Secretbox if err := s . Add Generated Conversion Func ( ( * config . Secretbox Configuration ) ( nil ) , ( * Secretbox Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Secretbox Configuration_To_v1_Secretbox Configuration ( a . ( * config . Secretbox Configuration ) , b . ( * Secretbox } 
func Convert_v1_AES Configuration_To_config_AES Configuration ( in * AES Configuration , out * config . AES Configuration , s conversion . Scope ) error { return auto Convert_v1_AES Configuration_To_config_AES } 
func Convert_config_AES Configuration_To_v1_AES Configuration ( in * config . AES Configuration , out * AES Configuration , s conversion . Scope ) error { return auto Convert_config_AES Configuration_To_v1_AES } 
func Convert_v1_Encryption Configuration_To_config_Encryption Configuration ( in * Encryption Configuration , out * config . Encryption Configuration , s conversion . Scope ) error { return auto Convert_v1_Encryption Configuration_To_config_Encryption } 
func Convert_config_Encryption Configuration_To_v1_Encryption Configuration ( in * config . Encryption Configuration , out * Encryption Configuration , s conversion . Scope ) error { return auto Convert_config_Encryption Configuration_To_v1_Encryption } 
func Convert_v1_Identity Configuration_To_config_Identity Configuration ( in * Identity Configuration , out * config . Identity Configuration , s conversion . Scope ) error { return auto Convert_v1_Identity Configuration_To_config_Identity } 
func Convert_config_Identity Configuration_To_v1_Identity Configuration ( in * config . Identity Configuration , out * Identity Configuration , s conversion . Scope ) error { return auto Convert_config_Identity Configuration_To_v1_Identity } 
func Convert_v1_KMS Configuration_To_config_KMS Configuration ( in * KMS Configuration , out * config . KMS Configuration , s conversion . Scope ) error { return auto Convert_v1_KMS Configuration_To_config_KMS } 
func Convert_config_KMS Configuration_To_v1_KMS Configuration ( in * config . KMS Configuration , out * KMS Configuration , s conversion . Scope ) error { return auto Convert_config_KMS Configuration_To_v1_KMS } 
func Convert_v1_Key_To_config_Key ( in * Key , out * config . Key , s conversion . Scope ) error { return auto } 
func Convert_config_Key_To_v1_Key ( in * config . Key , out * Key , s conversion . Scope ) error { return auto } 
func Convert_v1_Provider Configuration_To_config_Provider Configuration ( in * Provider Configuration , out * config . Provider Configuration , s conversion . Scope ) error { return auto Convert_v1_Provider Configuration_To_config_Provider } 
func Convert_config_Provider Configuration_To_v1_Provider Configuration ( in * config . Provider Configuration , out * Provider Configuration , s conversion . Scope ) error { return auto Convert_config_Provider Configuration_To_v1_Provider } 
func Convert_v1_Resource Configuration_To_config_Resource Configuration ( in * Resource Configuration , out * config . Resource Configuration , s conversion . Scope ) error { return auto Convert_v1_Resource Configuration_To_config_Resource } 
func Convert_config_Resource Configuration_To_v1_Resource Configuration ( in * config . Resource Configuration , out * Resource Configuration , s conversion . Scope ) error { return auto Convert_config_Resource Configuration_To_v1_Resource } 
func Convert_v1_Secretbox Configuration_To_config_Secretbox Configuration ( in * Secretbox Configuration , out * config . Secretbox Configuration , s conversion . Scope ) error { return auto Convert_v1_Secretbox Configuration_To_config_Secretbox } 
func Convert_config_Secretbox Configuration_To_v1_Secretbox Configuration ( in * config . Secretbox Configuration , out * Secretbox Configuration , s conversion . Scope ) error { return auto Convert_config_Secretbox Configuration_To_v1_Secretbox } 
func To Group Version ( gv string ) ( Group Version , error ) { // this can be the internal version for the legacy kube types // TODO once we've cleared the last uses as strings, this special case should be removed. if ( len ( gv ) == 0 ) || ( gv == " " ) { return Group switch strings . Count ( gv , " " ) { case 0 : return Group return Group default : return Group } 
func default Version ( versions [ ] Package Version ) Version { var version for _ , version := range versions { version Strings = append ( version sort . Sort ( sortable Slice Of Versions ( version return Version ( version Strings [ len ( version } 
func To Group Version Info ( groups [ ] Group Versions , group Go Names map [ Group Version ] string ) [ ] Group Version Info { var group Version Packages [ ] Group Version for _ , group := range groups { for _ , version := range group . Versions { group Go Name := group Go Names [ Group group Version Packages = append ( group Version Packages , Group Version Info { Group : Group ( namer . IC ( group . Group . Non Empty ( ) ) ) , Version : Version ( namer . IC ( version . Version . String ( ) ) ) , Package Alias : strings . To Lower ( group Go Name + version . Version . Non Empty ( ) ) , Group Go Name : group Go Name , Lower Case Group Go Name : namer . IL ( group Go return group Version } 
func New Controller Revision Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Controller Revision Informer ( client , namespace , resync } 
func New Object Cache ( list Object list Object Func , watch Object watch Object Func , new Object new Object Func , group Resource schema . Group Resource ) Store { return & object Cache { list Object : list Object , watch Object : watch Object , new Object : new Object , group Resource : group Resource , items : make ( map [ object Key ] * object Cache } 
func ( c * object } 
func New Watch Based Manager ( list Object list Object Func , watch Object watch Object Func , new Object new Object Func , group Resource schema . Group Resource , get Referenced Objects func ( * v1 . Pod ) sets . String ) Manager { object Store := New Object Cache ( list Object , watch Object , new Object , group return New Cache Based Manager ( object Store , get Referenced } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Priority Class { } , & Priority Class metav1 . Add To Group Version ( scheme , Scheme Group } 
func ( s * subject Access Review Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Subject Access Review , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Subject Access } 
func ( s * subject Access Review Lister ) Get ( name string ) ( * v1beta1 . Subject Access Review , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1beta1 . Subject Access } 
func ( v * version ) Pod Disruption Budgets ( ) Pod Disruption Budget Informer { return & pod Disruption Budget Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( v * version ) Pod Security Policies ( ) Pod Security Policy Informer { return & pod Security Policy Informer { factory : v . factory , tweak List Options : v . tweak List } 
func Walk Pkg ( pkg Name string , visit Visit Func ) error { // Visit the package itself. pkg , err := find Package ( pkg if err := visit ( pkg . Import Path , pkg . Dir ) ; err == Err Skip // Read all of the child dirents and find sub-packages. infos , err := read Dir for _ , info := range infos { if ! info . Is // Don't use path.Join() because it drops leading `./` via path.Clean(). err := Walk Pkg ( pkg } 
func find Package ( pkg Name string ) ( * build . Package , error ) { debug ( " " , pkg pkg , err := build . Import ( pkg Name , getwd ( ) , build . Find } 
func read Dir Infos ( dir Path string ) ( [ ] os . File Info , error ) { names , err := read Dir Names ( dir infos := make ( [ ] os . File for _ , n := range names { info , err := os . Stat ( path . Join ( dir } 
func read Dir Names ( dir Path string ) ( [ ] string , error ) { d , err := os . Open ( dir } 
func ( fs * Fake Subpath ) Prepare Safe Subpath ( sub Path Subpath ) ( new Host Path string , cleanup Action func ( ) , err error ) { return sub } 
func ( fs * Fake Subpath ) Safe Make Dir ( pathname string , base string , perm os . File } 
func new Ingresses ( c * Networking V1beta1Client , namespace string ) * ingresses { return & ingresses { client : c . REST } 
func New Available Condition Controller ( api Service Informer informers . API Service Informer , service Informer v1informers . Service Informer , endpoints Informer v1informers . Endpoints Informer , api Service Client apiregistrationclient . API Services Getter , proxy Transport * http . Transport , proxy Client Cert [ ] byte , proxy Client Key [ ] byte , service Resolver Service Resolver , ) ( * Available Condition Controller , error ) { c := & Available Condition Controller { api Service Client : api Service Client , api Service Lister : api Service Informer . Lister ( ) , api Service Synced : api Service Informer . Informer ( ) . Has Synced , service Lister : service Informer . Lister ( ) , services Synced : service Informer . Informer ( ) . Has Synced , endpoints Lister : endpoints Informer . Lister ( ) , endpoints Synced : endpoints Informer . Informer ( ) . Has Synced , service Resolver : service Resolver , queue : workqueue . New Named Rate Limiting Queue ( // We want a fairly tight requeue time. The controller listens to the API, but because it relies on the routability of the // service network, it is possible for an external, non-watchable factor to affect availability. This keeps // the maximum disruption time to a minimum, but it does prevent hot loops. workqueue . New Item Exponential Failure Rate // if a particular transport was specified, use that otherwise build one // construct an http client that will ignore TLS verification (if someone owns the network and messes with your status // that's not so bad) and sets a very short timeout. This is a best effort GET that provides no additional information rest Config := & rest . Config { TLS Client Config : rest . TLS Client Config { Insecure : true , Cert Data : proxy Client Cert , Key Data : proxy Client if proxy Transport != nil && proxy Transport . Dial Context != nil { rest Config . Dial = proxy Transport . Dial transport , err := rest . Transport For ( rest c . discovery // resync on this one because it is low cardinality and rechecking the actual discovery // allows us to detect health in a more timely fashion when network connectivity to // nodes is snipped, but the network still attempts to route there. See // https://github.com/openshift/origin/issues/17159#issuecomment-341798063 api Service Informer . Informer ( ) . Add Event Handler With Resync Period ( cache . Resource Event Handler Funcs { Add Func : c . add API Service , Update Func : c . update API Service , Delete Func : c . delete API service Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : c . add Service , Update Func : c . update Service , Delete Func : c . delete endpoints Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : c . add Endpoints , Update Func : c . update Endpoints , Delete Func : c . delete c . sync } 
func update API Service Status ( client apiregistrationclient . API Services Getter , original API Service , new API Service * apiregistration . API Service ) ( * apiregistration . API Service , error ) { if equality . Semantic . Deep Equal ( original API Service . Status , new API Service . Status ) { return new API new API Service , err := client . API Services ( ) . Update Status ( new API // update metrics was Available := apiregistration . Is API Service Condition True ( original API is Available := apiregistration . Is API Service Condition True ( new API if is Available != was Available { if is Available { unavailable Gauge . With Label Values ( new API } else { unavailable Gauge . With Label Values ( new API if new Condition := apiregistration . Get API Service Condition By Type ( new API Service , apiregistration . Available ) ; new Condition != nil { reason = new unavailable Counter . With Label Values ( new API return new API } 
func ( c * Available Condition Controller ) Run ( threadiness int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer c . queue . Shut if ! controllers . Wait For Cache Sync ( " " , stop Ch , c . api Service Synced , c . services Synced , c . endpoints for i := 0 ; i < threadiness ; i ++ { go wait . Until ( c . run Worker , time . Second , stop <- stop } 
func ( c * Available Condition Controller ) get API Services For ( obj runtime . Object ) [ ] * apiregistration . API if err != nil { utilruntime . Handle var ret [ ] * apiregistration . API api Service List , _ := c . api Service for _ , api Service := range api Service List { if api if api Service . Spec . Service . Namespace == metadata . Get Namespace ( ) && api Service . Spec . Service . Name == metadata . Get Name ( ) { ret = append ( ret , api } 
func ( c * Available Condition Controller ) add Service ( obj interface { } ) { for _ , api Service := range c . get API Services For ( obj . ( * v1 . Service ) ) { c . enqueue ( api } 
func Modify Config ( config Access Config Access , new Config clientcmdapi . Config , relativize Paths bool ) error { possible Sources := config Access . Get Loading // sort the possible kubeconfig files so we always "lock" in the same order // to avoid deadlock (note: this can fail w/ symlinks, but... come on). sort . Strings ( possible for _ , filename := range possible Sources { if err := lock defer unlock starting Config , err := config Access . Get Starting // We need to find all differences, locate their original files, read a partial config to modify only that stanza and write out the file. // Special case the test for current context and preferences since those always write to the default file. if reflect . Deep Equal ( * starting Config , new if starting Config . Current Context != new Config . Current Context { if err := write Current Context ( config Access , new Config . Current if ! reflect . Deep Equal ( starting Config . Preferences , new Config . Preferences ) { if err := write Preferences ( config Access , new // Search every cluster, auth Info, and context. First from new to old for differences, then from old to new for deletions for key , cluster := range new Config . Clusters { starting Cluster , exists := starting if ! reflect . Deep Equal ( cluster , starting Cluster ) || ! exists { destination File := cluster . Location Of if len ( destination File ) == 0 { destination File = config Access . Get Default config To Write , err := get Config From File ( destination config To config To Write . Clusters [ key ] . Location Of Origin = destination if relativize Paths { if err := Relativize Cluster Local Paths ( config To if err := Write To File ( * config To Write , destination // seen Configs stores a map of config source filenames to computed config objects seen for key , context := range new Config . Contexts { starting Context , exists := starting if ! reflect . Deep Equal ( context , starting Context ) || ! exists { destination File := context . Location Of if len ( destination File ) == 0 { destination File = config Access . Get Default // we only obtain a fresh config object from its source file // if we have not seen it already - this prevents us from // reading and writing to the same number of files repeatedly // when multiple / all contexts share the same destination file. config To Write , seen := seen Configs [ destination config To Write , err = get Config From File ( destination seen Configs [ destination File ] = config To config To // actually persist config object changes for destination File , config To Write := range seen Configs { if err := Write To File ( * config To Write , destination for key , auth Info := range new Config . Auth Infos { starting Auth Info , exists := starting Config . Auth if ! reflect . Deep Equal ( auth Info , starting Auth Info ) || ! exists { destination File := auth Info . Location Of if len ( destination File ) == 0 { destination File = config Access . Get Default config To Write , err := get Config From File ( destination t := * auth config To Write . Auth config To Write . Auth Infos [ key ] . Location Of Origin = destination if relativize Paths { if err := Relativize Auth Info Local Paths ( config To Write . Auth if err := Write To File ( * config To Write , destination for key , cluster := range starting Config . Clusters { if _ , exists := new Config . Clusters [ key ] ; ! exists { destination File := cluster . Location Of if len ( destination File ) == 0 { destination File = config Access . Get Default config To Write , err := get Config From File ( destination delete ( config To if err := Write To File ( * config To Write , destination for key , context := range starting Config . Contexts { if _ , exists := new Config . Contexts [ key ] ; ! exists { destination File := context . Location Of if len ( destination File ) == 0 { destination File = config Access . Get Default config To Write , err := get Config From File ( destination delete ( config To if err := Write To File ( * config To Write , destination for key , auth Info := range starting Config . Auth Infos { if _ , exists := new Config . Auth Infos [ key ] ; ! exists { destination File := auth Info . Location Of if len ( destination File ) == 0 { destination File = config Access . Get Default config To Write , err := get Config From File ( destination delete ( config To Write . Auth if err := Write To File ( * config To Write , destination } 
func get Config From File ( filename string ) ( * clientcmdapi . Config , error ) { config , err := Load From if err != nil && ! os . Is Not if config == nil { config = clientcmdapi . New } 
func Get Config From File Or Die ( filename string ) * clientcmdapi . Config { config , err := get Config From if err != nil { klog . Fatal } 
func ( a custom Resource Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { if utilfeature . Default Feature Gate . Enabled ( apiextensionsfeatures . Custom Resource Subresources ) && a . status != nil { custom Resource custom Resource := custom Resource Object . Unstructured // create cannot set status if _ , ok := custom Resource [ " " ] ; ok { delete ( custom accessor . Set } 
func ( a custom Resource Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Custom Resource old Custom Resource new Custom Resource := new Custom Resource Object . Unstructured old Custom Resource := old Custom Resource Object . Unstructured // If the /status subresource endpoint is installed, update is not allowed to set status. if utilfeature . Default Feature Gate . Enabled ( apiextensionsfeatures . Custom Resource Subresources ) && a . status != nil { _ , ok1 := new Custom _ , ok2 := old Custom switch { case ok2 : new Custom Resource [ " " ] = old Custom case ok1 : delete ( new Custom // except for the changes to `metadata`, any other changes // cause the generation to increment. new Copy Content := copy Non Metadata ( new Custom old Copy Content := copy Non Metadata ( old Custom if ! apiequality . Semantic . Deep Equal ( new Copy Content , old Copy Content ) { old Accessor , _ := meta . Accessor ( old Custom Resource new Accessor , _ := meta . Accessor ( new Custom Resource new Accessor . Set Generation ( old Accessor . Get } 
func ( a custom Resource Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return a . validator . Validate } 
func ( a custom Resource Strategy ) Get return labels . Set ( accessor . Get Labels ( ) ) , object Meta Fields Set ( accessor , a . namespace } 
func object Meta Fields Set ( object Meta metav1 . Object , namespace Scoped bool ) fields . Set { if namespace Scoped { return fields . Set { " " : object Meta . Get Name ( ) , " " : object Meta . Get return fields . Set { " " : object Meta . Get } 
func ( a custom Resource Strategy ) Match Custom Resource Definition Storage ( label labels . Selector , field fields . Selector ) apiserverstorage . Selection Predicate { return apiserverstorage . Selection Predicate { Label : label , Field : field , Get Attrs : a . Get } 
func ( b SA Controller Client Builder ) Config ( name string ) ( * restclient . Config , error ) { sa , err := get Or Create Service Account ( b . Core var client field Selector := fields . Selector From Set ( map [ string ] string { api . Secret Type Field : string ( v1 . Secret Type Service Account lw := & cache . List Watch { List Func : func ( options metav1 . List Options ) ( runtime . Object , error ) { options . Field Selector = field return b . Core } , Watch Func : func ( options metav1 . List Options ) ( watch . Interface , error ) { options . Field Selector = field return b . Core ctx , cancel := context . With _ , err = watchtools . Until With if ! serviceaccount . Is Service Account if len ( secret . Data [ v1 . Service Account Token valid Config , valid , err := b . get Authenticated Config ( sa , string ( secret . Data [ v1 . Service Account Token // try to delete the secret containing the invalid token if err := b . Core Client . Secrets ( secret . Namespace ) . Delete ( secret . Name , & metav1 . Delete Options { } ) ; err != nil && ! apierrors . Is Not client Config = valid return client } 
func ( plugin * kubenet Network Plugin ) ensure Masq Rule ( ) error { if plugin . non Masquerade CIDR != " " { if _ , err := plugin . iptables . Ensure Rule ( utiliptables . Append , utiliptables . Table NAT , utiliptables . Chain Postrouting , " " , " " , " " , " " , " " , " " , " " , " " , " " , " " , " " , plugin . non Masquerade CIDR , " " , " " ) ; err != nil { return fmt . Errorf ( " " , utiliptables . Table NAT , utiliptables . Chain } 
func ( plugin * kubenet Network Plugin ) setup ( namespace string , name string , id kubecontainer . Container ID , annotations map [ string ] string ) error { // Disable DAD so we skip the kernel delay on bringing up new interfaces. if err := plugin . disable Container // Bring up container loopback interface if _ , err := plugin . add Container To Network ( plugin . lo // Hook container up with our bridge res T , err := plugin . add Container To Network ( plugin . net Config , network . Default Interface // Coerce the CNI result version res , err := cnitypes020 . Get Result ( res // Put the container bridge into promiscuous mode to force it to accept hairpin packets. // TODO: Remove this once the kernel bug (#20096) is fixed. if plugin . hairpin Mode == kubeletconfig . Promiscuous Bridge { link , err := netlink . Link By Name ( Bridge if err != nil { return fmt . Errorf ( " " , Bridge if link . Attrs ( ) . Promisc != 1 { // promiscuous mode is not on, then turn it on. err := netlink . Set Promisc if err != nil { return fmt . Errorf ( " " , Bridge // configure the ebtables rules to eliminate duplicate packets by best effort plugin . sync Ebtables Dedup Rules ( link . Attrs ( ) . Hardware plugin . pod I // The first Set Up ingress , egress , err := bandwidth . Extract Pod Bandwidth if egress != nil || ingress != nil { if err := shaper . Reconcile // TODO: replace with CNI port-forwarding plugin port Mappings , err := plugin . host . Get Pod Port if port Mappings != nil && len ( port Mappings ) > 0 { if err := plugin . hostport Manager . Add ( id . ID , & hostport . Pod Port Mapping { Namespace : namespace , Name : name , Port Mappings : port Mappings , IP : ip4 , Host Network : false , } , Bridge } 
func ( plugin * kubenet Network Plugin ) teardown ( namespace string , name string , id kubecontainer . Container ID , pod IP string ) error { err if pod IP != " " { klog . V ( 5 ) . Infof ( " " , pod // shaper wants /32 if err := plugin . shaper ( ) . Reset ( fmt . Sprintf ( " " , pod IP ) ) ; err != nil { // Possible bandwidth shaping wasn't enabled for this pod anyways klog . V ( 4 ) . Infof ( " " , pod delete ( plugin . pod I if err := plugin . del Container From Network ( plugin . net Config , network . Default Interface Name , namespace , name , id ) ; err != nil { // This is to prevent returning error when Tear Down Pod is called twice on the same pod. This helps to reduce event pollution. if pod } else { err List = append ( err port Mappings , err := plugin . host . Get Pod Port if err != nil { err List = append ( err } else if port Mappings != nil && len ( port Mappings ) > 0 { if err = plugin . hostport Manager . Remove ( id . ID , & hostport . Pod Port Mapping { Namespace : namespace , Name : name , Port Mappings : port Mappings , Host Network : false , } ) ; err != nil { err List = append ( err return utilerrors . New Aggregate ( err } 
func ( plugin * kubenet Network Plugin ) Get Pod Network Status ( namespace string , name string , id kubecontainer . Container ID ) ( * network . Pod Network // Assuming the ip of pod does not change. Try to retrieve ip from kubenet map first. if pod IP , ok := plugin . pod I Ps [ id ] ; ok { return & network . Pod Network Status { IP : net . Parse IP ( pod netns Path , err := plugin . host . Get Net if netns ip , err := network . Get Pod IP ( plugin . execer , plugin . nsenter Path , netns Path , network . Default Interface plugin . pod I return & network . Pod Network } 
func ( plugin * kubenet Network Plugin ) check Required CNI Plugins ( ) bool { for _ , dir := range plugin . bin Dirs { if plugin . check Required CNI Plugins In One } 
func ( plugin * kubenet Network Plugin ) check Required CNI Plugins In One Dir ( dir string ) bool { files , err := ioutil . Read for _ , cni Plugin := range required CNI for _ , file := range files { if strings . Trim Space ( file . Name ( ) ) == cni } 
func ( plugin * kubenet Network Plugin ) shaper ( ) bandwidth . Bandwidth Shaper { if plugin . bandwidth Shaper == nil { plugin . bandwidth Shaper = bandwidth . New TC Shaper ( Bridge plugin . bandwidth Shaper . Reconcile return plugin . bandwidth } 
func ( plugin * kubenet Network Plugin ) sync Ebtables Dedup Rules ( mac Addr net . Hardware if err := plugin . ebtables . Flush Chain ( utilebtables . Table Filter , dedup _ , err := plugin . ebtables . Get klog . V ( 3 ) . Infof ( " " , mac Addr . String ( ) , plugin . gateway . String ( ) , plugin . pod _ , err = plugin . ebtables . Ensure Chain ( utilebtables . Table Filter , dedup if err != nil { klog . Errorf ( " " , utilebtables . Table Filter , dedup _ , err = plugin . ebtables . Ensure Rule ( utilebtables . Append , utilebtables . Table Filter , utilebtables . Chain Output , " " , string ( dedup if err != nil { klog . Errorf ( " " , utilebtables . Table Filter , utilebtables . Chain Output , dedup common Args := [ ] string { " " , " " , " " , mac _ , err = plugin . ebtables . Ensure Rule ( utilebtables . Prepend , utilebtables . Table Filter , dedup Chain , append ( common _ , err = plugin . ebtables . Ensure Rule ( utilebtables . Append , utilebtables . Table Filter , dedup Chain , append ( common Args , " " , plugin . pod } 
func ( plugin * kubenet Network Plugin ) disable Container DAD ( id kubecontainer . Container sysctl Bin , err := plugin . execer . Look netns Path , err := plugin . host . Get Net if netns // If the sysctl doesn't exist, it means ipv6 is disabled; log and move on if _ , err := plugin . sysctl . Get output , err := plugin . execer . Command ( plugin . nsenter Path , fmt . Sprintf ( " " , netns Path ) , " " , " " , sysctl Bin , " " , fmt . Sprintf ( " " , key , " " ) , ) . Combined } 
func new Plugin ( ) * priority Plugin { return & priority Plugin { Handler : admission . New } 
func ( p * priority Plugin ) Validate Initialization ( ) error { if p . client == nil { return fmt . Errorf ( " " , Plugin if p . lister == nil { return fmt . Errorf ( " " , Plugin } 
func ( p * priority Plugin ) Set External Kube Informer Factory ( f informers . Shared Informer Factory ) { priority Informer := f . Scheduling ( ) . V1 ( ) . Priority p . lister = priority p . Set Ready Func ( priority Informer . Informer ( ) . Has } 
func ( p * priority Plugin ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { if ! utilfeature . Default Feature Gate . Enabled ( features . Pod operation := a . Get // Ignore all calls to subresources if len ( a . Get switch a . Get Resource ( ) . Group Resource ( ) { case pod Resource : if operation == admission . Create || operation == admission . Update { return p . admit } 
func ( p * priority Plugin ) Validate ( a admission . Attributes , o admission . Object Interfaces ) error { operation := a . Get // Ignore all calls to subresources if len ( a . Get switch a . Get Resource ( ) . Group Resource ( ) { case priority Class Resource : if operation == admission . Create || operation == admission . Update { return p . validate Priority } 
func priority Class Permitted In Namespace ( priority Class Name string , namespace string ) bool { // Only allow system priorities in the system namespace. This is to prevent abuse or incorrect // usage of these priorities. Pods created at these priorities could preempt system critical // components. for _ , spc := range scheduling . System Priority Classes ( ) { if spc . Name == priority Class Name && namespace != metav1 . Namespace } 
func ( p * priority Plugin ) admit Pod ( a admission . Attributes ) error { operation := a . Get pod , ok := a . Get if ! ok { return errors . New Bad if operation == admission . Update { old Pod , ok := a . Get Old if ! ok { return errors . New Bad // This admission plugin set pod.Spec.Priority on create. // Ensure the existing priority is preserved on update. // API validation prevents mutations to Priority and Priority Class Name, so any other changes will fail update validation and not be persisted. if pod . Spec . Priority == nil && old Pod . Spec . Priority != nil { pod . Spec . Priority = old // TODO: @ravig - This is for backwards compatibility to ensure that critical pods with annotations just work fine. // Remove when no longer needed. if len ( pod . Spec . Priority Class Name ) == 0 && utilfeature . Default Feature Gate . Enabled ( features . Experimental Critical Pod Annotation ) && kubelettypes . Is Critical ( a . Get Namespace ( ) , pod . Annotations ) { pod . Spec . Priority Class Name = scheduling . System Cluster if len ( pod . Spec . Priority Class var pc pc Name , priority , err = p . get Default pod . Spec . Priority Class Name = pc } else { pc Name := pod . Spec . Priority Class if ! priority Class Permitted In Namespace ( pc Name , a . Get Namespace ( ) ) { return admission . New Forbidden ( a , fmt . Errorf ( " " , pc Name , a . Get // Try resolving the priority class name. pc , err := p . lister . Get ( pod . Spec . Priority Class if err != nil { if errors . Is Not Found ( err ) { return admission . New Forbidden ( a , fmt . Errorf ( " " , pod . Spec . Priority Class return fmt . Errorf ( " " , pod . Spec . Priority Class // if the pod contained a priority that differs from the one computed from the priority class, error if pod . Spec . Priority != nil && * pod . Spec . Priority != priority { return admission . New } 
func ( p * priority Plugin ) validate Priority Class ( a admission . Attributes ) error { operation := a . Get pc , ok := a . Get Object ( ) . ( * scheduling . Priority if ! ok { return errors . New Bad // If the new Priority Class tries to be the default priority, make sure that no other priority class is marked as default. if pc . Global Default { dpc , err := p . get Default Priority if dpc != nil { // Throw an error if a second default priority class is being created, or an existing priority class is being marked as default, while another default already exists. if operation == admission . Create || ( operation == admission . Update && dpc . Get Name ( ) != pc . Get Name ( ) ) { return admission . New Forbidden ( a , fmt . Errorf ( " " , dpc . Get } 
func New Configuration ( evaluators [ ] quota . Evaluator , ignored Resources map [ schema . Group Resource ] struct { } ) quota . Configuration { return & simple Configuration { evaluators : evaluators , ignored Resources : ignored } 
func ( c * Kubelet Client Config ) transport Config ( ) * transport . Config { cfg := & transport . Config { TLS : transport . TLS Config { CA File : c . CA File , CA Data : c . CA Data , Cert File : c . Cert File , Cert Data : c . Cert Data , Key File : c . Key File , Key Data : c . Key Data , } , Bearer Token : c . Bearer if c . Enable Https && ! cfg . Has } 
func ( r * Subject Access Evaluator ) Allowed Subjects ( request Attributes authorizer . Attributes ) ( [ ] rbacv1 . Subject , error ) { subjects := [ ] rbacv1 . Subject { { Kind : rbacv1 . Group Kind , API Group : rbacv1 . Group Name , Name : user . System Privileged if len ( r . super User ) > 0 { subjects = append ( subjects , rbacv1 . Subject { Kind : rbacv1 . User Kind , API Group : rbacv1 . Group Name , Name : r . super if cluster Role Bindings , err := r . cluster Role Binding Lister . List Cluster Role } else { for _ , cluster Role Binding := range cluster Role Bindings { rules , err := r . role To Rule Mapper . Get Role Reference Rules ( cluster Role Binding . Role if Rules Allow ( request Attributes , rules ... ) { subjects = append ( subjects , cluster Role if namespace := request Attributes . Get Namespace ( ) ; len ( namespace ) > 0 { if role Bindings , err := r . role Binding Lister . List Role } else { for _ , role Binding := range role Bindings { rules , err := r . role To Rule Mapper . Get Role Reference Rules ( role Binding . Role if Rules Allow ( request Attributes , rules ... ) { subjects = append ( subjects , role deduped for _ , curr := range deduped if ! found { deduped Subjects = append ( deduped return subjects , utilerrors . New } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . HTTP Ingress Path ) ( nil ) , ( * networking . HTTP Ingress Path ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_HTTP Ingress Path_To_networking_HTTP Ingress Path ( a . ( * v1beta1 . HTTP Ingress Path ) , b . ( * networking . HTTP Ingress if err := s . Add Generated Conversion Func ( ( * networking . HTTP Ingress Path ) ( nil ) , ( * v1beta1 . HTTP Ingress Path ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_HTTP Ingress Path_To_v1beta1_HTTP Ingress Path ( a . ( * networking . HTTP Ingress Path ) , b . ( * v1beta1 . HTTP Ingress if err := s . Add Generated Conversion Func ( ( * v1beta1 . HTTP Ingress Rule Value ) ( nil ) , ( * networking . HTTP Ingress Rule Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_HTTP Ingress Rule Value_To_networking_HTTP Ingress Rule Value ( a . ( * v1beta1 . HTTP Ingress Rule Value ) , b . ( * networking . HTTP Ingress Rule if err := s . Add Generated Conversion Func ( ( * networking . HTTP Ingress Rule Value ) ( nil ) , ( * v1beta1 . HTTP Ingress Rule Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_HTTP Ingress Rule Value_To_v1beta1_HTTP Ingress Rule Value ( a . ( * networking . HTTP Ingress Rule Value ) , b . ( * v1beta1 . HTTP Ingress Rule if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress Backend ) ( nil ) , ( * networking . Ingress Backend ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress Backend_To_networking_Ingress Backend ( a . ( * v1beta1 . Ingress Backend ) , b . ( * networking . Ingress if err := s . Add Generated Conversion Func ( ( * networking . Ingress Backend ) ( nil ) , ( * v1beta1 . Ingress Backend ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress Backend_To_v1beta1_Ingress Backend ( a . ( * networking . Ingress Backend ) , b . ( * v1beta1 . Ingress if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress List ) ( nil ) , ( * networking . Ingress List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress List_To_networking_Ingress List ( a . ( * v1beta1 . Ingress List ) , b . ( * networking . Ingress if err := s . Add Generated Conversion Func ( ( * networking . Ingress List ) ( nil ) , ( * v1beta1 . Ingress List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress List_To_v1beta1_Ingress List ( a . ( * networking . Ingress List ) , b . ( * v1beta1 . Ingress if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress Rule ) ( nil ) , ( * networking . Ingress Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress Rule_To_networking_Ingress Rule ( a . ( * v1beta1 . Ingress Rule ) , b . ( * networking . Ingress if err := s . Add Generated Conversion Func ( ( * networking . Ingress Rule ) ( nil ) , ( * v1beta1 . Ingress Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress Rule_To_v1beta1_Ingress Rule ( a . ( * networking . Ingress Rule ) , b . ( * v1beta1 . Ingress if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress Rule Value ) ( nil ) , ( * networking . Ingress Rule Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress Rule Value_To_networking_Ingress Rule Value ( a . ( * v1beta1 . Ingress Rule Value ) , b . ( * networking . Ingress Rule if err := s . Add Generated Conversion Func ( ( * networking . Ingress Rule Value ) ( nil ) , ( * v1beta1 . Ingress Rule Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress Rule Value_To_v1beta1_Ingress Rule Value ( a . ( * networking . Ingress Rule Value ) , b . ( * v1beta1 . Ingress Rule if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress Spec ) ( nil ) , ( * networking . Ingress Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress Spec_To_networking_Ingress Spec ( a . ( * v1beta1 . Ingress Spec ) , b . ( * networking . Ingress if err := s . Add Generated Conversion Func ( ( * networking . Ingress Spec ) ( nil ) , ( * v1beta1 . Ingress Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress Spec_To_v1beta1_Ingress Spec ( a . ( * networking . Ingress Spec ) , b . ( * v1beta1 . Ingress if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress Status ) ( nil ) , ( * networking . Ingress Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress Status_To_networking_Ingress Status ( a . ( * v1beta1 . Ingress Status ) , b . ( * networking . Ingress if err := s . Add Generated Conversion Func ( ( * networking . Ingress Status ) ( nil ) , ( * v1beta1 . Ingress Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress Status_To_v1beta1_Ingress Status ( a . ( * networking . Ingress Status ) , b . ( * v1beta1 . Ingress if err := s . Add Generated Conversion Func ( ( * v1beta1 . Ingress TLS ) ( nil ) , ( * networking . Ingress TLS ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Ingress TLS_To_networking_Ingress TLS ( a . ( * v1beta1 . Ingress TLS ) , b . ( * networking . Ingress if err := s . Add Generated Conversion Func ( ( * networking . Ingress TLS ) ( nil ) , ( * v1beta1 . Ingress TLS ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Ingress TLS_To_v1beta1_Ingress TLS ( a . ( * networking . Ingress TLS ) , b . ( * v1beta1 . Ingress } 
func Convert_v1beta1_HTTP Ingress Path_To_networking_HTTP Ingress Path ( in * v1beta1 . HTTP Ingress Path , out * networking . HTTP Ingress Path , s conversion . Scope ) error { return auto Convert_v1beta1_HTTP Ingress Path_To_networking_HTTP Ingress } 
func Convert_networking_HTTP Ingress Path_To_v1beta1_HTTP Ingress Path ( in * networking . HTTP Ingress Path , out * v1beta1 . HTTP Ingress Path , s conversion . Scope ) error { return auto Convert_networking_HTTP Ingress Path_To_v1beta1_HTTP Ingress } 
func Convert_v1beta1_HTTP Ingress Rule Value_To_networking_HTTP Ingress Rule Value ( in * v1beta1 . HTTP Ingress Rule Value , out * networking . HTTP Ingress Rule Value , s conversion . Scope ) error { return auto Convert_v1beta1_HTTP Ingress Rule Value_To_networking_HTTP Ingress Rule } 
func Convert_networking_HTTP Ingress Rule Value_To_v1beta1_HTTP Ingress Rule Value ( in * networking . HTTP Ingress Rule Value , out * v1beta1 . HTTP Ingress Rule Value , s conversion . Scope ) error { return auto Convert_networking_HTTP Ingress Rule Value_To_v1beta1_HTTP Ingress Rule } 
func Convert_v1beta1_Ingress_To_networking_Ingress ( in * v1beta1 . Ingress , out * networking . Ingress , s conversion . Scope ) error { return auto } 
func Convert_networking_Ingress_To_v1beta1_Ingress ( in * networking . Ingress , out * v1beta1 . Ingress , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Ingress Backend_To_networking_Ingress Backend ( in * v1beta1 . Ingress Backend , out * networking . Ingress Backend , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress Backend_To_networking_Ingress } 
func Convert_networking_Ingress Backend_To_v1beta1_Ingress Backend ( in * networking . Ingress Backend , out * v1beta1 . Ingress Backend , s conversion . Scope ) error { return auto Convert_networking_Ingress Backend_To_v1beta1_Ingress } 
func Convert_v1beta1_Ingress List_To_networking_Ingress List ( in * v1beta1 . Ingress List , out * networking . Ingress List , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress List_To_networking_Ingress } 
func Convert_networking_Ingress List_To_v1beta1_Ingress List ( in * networking . Ingress List , out * v1beta1 . Ingress List , s conversion . Scope ) error { return auto Convert_networking_Ingress List_To_v1beta1_Ingress } 
func Convert_v1beta1_Ingress Rule_To_networking_Ingress Rule ( in * v1beta1 . Ingress Rule , out * networking . Ingress Rule , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress Rule_To_networking_Ingress } 
func Convert_networking_Ingress Rule_To_v1beta1_Ingress Rule ( in * networking . Ingress Rule , out * v1beta1 . Ingress Rule , s conversion . Scope ) error { return auto Convert_networking_Ingress Rule_To_v1beta1_Ingress } 
func Convert_v1beta1_Ingress Rule Value_To_networking_Ingress Rule Value ( in * v1beta1 . Ingress Rule Value , out * networking . Ingress Rule Value , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress Rule Value_To_networking_Ingress Rule } 
func Convert_networking_Ingress Rule Value_To_v1beta1_Ingress Rule Value ( in * networking . Ingress Rule Value , out * v1beta1 . Ingress Rule Value , s conversion . Scope ) error { return auto Convert_networking_Ingress Rule Value_To_v1beta1_Ingress Rule } 
func Convert_v1beta1_Ingress Spec_To_networking_Ingress Spec ( in * v1beta1 . Ingress Spec , out * networking . Ingress Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress Spec_To_networking_Ingress } 
func Convert_networking_Ingress Spec_To_v1beta1_Ingress Spec ( in * networking . Ingress Spec , out * v1beta1 . Ingress Spec , s conversion . Scope ) error { return auto Convert_networking_Ingress Spec_To_v1beta1_Ingress } 
func Convert_v1beta1_Ingress Status_To_networking_Ingress Status ( in * v1beta1 . Ingress Status , out * networking . Ingress Status , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress Status_To_networking_Ingress } 
func Convert_networking_Ingress Status_To_v1beta1_Ingress Status ( in * networking . Ingress Status , out * v1beta1 . Ingress Status , s conversion . Scope ) error { return auto Convert_networking_Ingress Status_To_v1beta1_Ingress } 
func Convert_v1beta1_Ingress TLS_To_networking_Ingress TLS ( in * v1beta1 . Ingress TLS , out * networking . Ingress TLS , s conversion . Scope ) error { return auto Convert_v1beta1_Ingress TLS_To_networking_Ingress } 
func Convert_networking_Ingress TLS_To_v1beta1_Ingress TLS ( in * networking . Ingress TLS , out * v1beta1 . Ingress TLS , s conversion . Scope ) error { return auto Convert_networking_Ingress TLS_To_v1beta1_Ingress } 
func ( in * AES Configuration ) Deep Copy Into ( out * AES } 
func ( in * AES Configuration ) Deep Copy ( ) * AES out := new ( AES in . Deep Copy } 
func ( in * Encryption Configuration ) Deep Copy Into ( out * Encryption out . Type Meta = in . Type * out = make ( [ ] Resource for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Encryption Configuration ) Deep Copy ( ) * Encryption out := new ( Encryption in . Deep Copy } 
func ( in * Encryption Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Identity Configuration ) Deep Copy ( ) * Identity out := new ( Identity in . Deep Copy } 
func ( in * KMS Configuration ) Deep Copy Into ( out * KMS } 
func ( in * KMS Configuration ) Deep Copy ( ) * KMS out := new ( KMS in . Deep Copy } 
func ( in * Key ) Deep in . Deep Copy } 
func ( in * Provider Configuration ) Deep Copy Into ( out * Provider * out = new ( AES ( * in ) . Deep Copy * out = new ( AES ( * in ) . Deep Copy * out = new ( Secretbox ( * in ) . Deep Copy * out = new ( Identity * out = new ( KMS ( * in ) . Deep Copy } 
func ( in * Provider Configuration ) Deep Copy ( ) * Provider out := new ( Provider in . Deep Copy } 
func ( in * Resource Configuration ) Deep Copy Into ( out * Resource * out = make ( [ ] Provider for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Resource Configuration ) Deep Copy ( ) * Resource out := new ( Resource in . Deep Copy } 
func ( in * Secretbox Configuration ) Deep Copy ( ) * Secretbox out := new ( Secretbox in . Deep Copy } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & Configuration { } , func ( obj interface { } ) { Set Object } 
func New REST ( services Service Storage , endpoints Endpoints Storage , pods rest . Getter , service I Ps ipallocator . Interface , service Node Ports portallocator . Interface , proxy Transport http . Round Tripper , ) ( * REST , * registry . Proxy REST ) { rest := & REST { services : services , endpoints : endpoints , service I Ps : service I Ps , service Node Ports : service Node Ports , proxy Transport : proxy return rest , & registry . Proxy REST { Redirector : rest , Proxy Transport : proxy } 
func external Traffic Policy Update ( old Service , service * api . Service ) { var needed External Traffic , needs External if old Service . Spec . Type == api . Service Type Node Port || old Service . Spec . Type == api . Service Type Load Balancer { needed External if service . Spec . Type == api . Service Type Node Port || service . Spec . Type == api . Service Type Load Balancer { needs External if needed External Traffic && ! needs External Traffic { // Clear External Traffic Policy to prevent confusion from ineffective field. service . Spec . External Traffic Policy = api . Service External Traffic Policy } 
func ( rs * REST ) health Check Node Port Update ( old Service , service * api . Service , node Port Op * portallocator . Port Allocation Operation ) ( bool , error ) { needed Health Check Node Port := apiservice . Needs Health Check ( old old Health Check Node Port := old Service . Spec . Health Check Node needs Health Check Node Port := apiservice . Needs Health new Health Check Node Port := service . Spec . Health Check Node switch { // Case 1: Transition from don't need Health Check Node Port to needs Health Check Node Port. // Allocate a health check node port or attempt to reserve the user-specified one if provided. // Insert health check node port into the service's Health Check Node Port field if needed. case ! needed Health Check Node Port && needs Health Check Node if err := allocate Health Check Node Port ( service , node Port Op ) ; err != nil { return false , errors . New Internal // Case 2: Transition from needs Health Check Node Port to don't need Health Check Node Port. // Free the existing health Check Node Port and clear the Health Check Node Port field. case needed Health Check Node Port && ! needs Health Check Node klog . V ( 4 ) . Infof ( " " , old Health Check Node node Port Op . Release Deferred ( int ( old Health Check Node // Clear the Health Check Node Port field. service . Spec . Health Check Node // Case 3: Remain in needs Health Check Node Port. // Reject changing the value of the Health Check Node Port field. case needed Health Check Node Port && needs Health Check Node Port : if old Health Check Node Port != new Health Check Node fld Path := field . New el := field . Error List { field . Invalid ( fld Path , new Health Check Node return false , errors . New } 
func ( rs * REST ) Resource Location ( ctx context . Context , id string ) ( * url . URL , http . Round Tripper , error ) { // Allow ID as "svcname", "svcname:port", or "scheme:svcname:port". svc Scheme , svc Name , port Str , valid := utilnet . Split Scheme Name if ! valid { return nil , nil , errors . New Bad // If a port *number* was specified, find the corresponding service port name if port Num , err := strconv . Parse Int ( port Str , 10 , 64 ) ; err == nil { obj , err := rs . services . Get ( ctx , svc Name , & metav1 . Get for _ , svc Port := range svc . Spec . Ports { if int64 ( svc Port . Port ) == port Num { // use the declared port's name port Str = svc if ! found { return nil , nil , errors . New Service Unavailable ( fmt . Sprintf ( " " , port Num , svc obj , err := rs . endpoints . Get ( ctx , svc Name , & metav1 . Get if len ( eps . Subsets ) == 0 { return nil , nil , errors . New Service Unavailable ( fmt . Sprintf ( " " , svc // Pick a random Subset to start searching from. ss // Find a Subset that has the port. for ssi := 0 ; ssi < len ( eps . Subsets ) ; ssi ++ { ss := & eps . Subsets [ ( ss for i := range ss . Ports { if ss . Ports [ i ] . Name == port Str { addr // This is a little wonky, but it's expensive to test for the presence of a Pod // So we repeatedly try at random and validate it, this means that for an invalid // service with a lot of endpoints we're going to potentially make a lot of calls, // but in the expected case we'll only make one. for try := 0 ; try < len ( ss . Addresses ) ; try ++ { addr := ss . Addresses [ ( addr if err := is Valid Address ( ctx , & addr , rs . pods ) ; err != nil { utilruntime . Handle return & url . URL { Scheme : svc Scheme , Host : net . Join Host Port ( ip , strconv . Itoa ( port ) ) , } , rs . proxy utilruntime . Handle return nil , nil , errors . New Service } 
func contains } 
func contains Node Port ( service Node Ports [ ] Service Node Port , service Node Port Service Node Port ) bool { for _ , snp := range service Node Ports { if snp == service Node } 
func find Requested Node Port ( port int , service Ports [ ] api . Service Port ) int { for i := range service Ports { service Port := service if port == int ( service Port . Port ) && service Port . Node Port != 0 { return int ( service Port . Node } 
func allocate Health Check Node Port ( service * api . Service , node Port Op * portallocator . Port Allocation Operation ) error { health Check Node Port := service . Spec . Health Check Node if health Check Node Port != 0 { // If the request has a health check node Port in mind, attempt to reserve it. err := node Port Op . Allocate ( int ( health Check Node if err != nil { return fmt . Errorf ( " " , health Check Node klog . V ( 4 ) . Infof ( " " , health Check Node } else { // If the request has no health check node Port specified, allocate any. health Check Node Port , err := node Port Op . Allocate if err != nil { return fmt . Errorf ( " " , health Check Node service . Spec . Health Check Node Port = int32 ( health Check Node klog . V ( 4 ) . Infof ( " " , health Check Node } 
func init Cluster IP ( service * api . Service , service I Ps ipallocator . Interface ) ( bool , error ) { switch { case service . Spec . Cluster IP == " " : // Allocate next available. ip , err := service I Ps . Allocate if err != nil { // TODO: what error should be returned here? It's not a // field-level validation failure (the field is valid), and it's // not really an internal error. return false , errors . New Internal service . Spec . Cluster case service . Spec . Cluster IP != api . Cluster IP None && service . Spec . Cluster IP != " " : // Try to respect the requested IP. if err := service I Ps . Allocate ( net . Parse IP ( service . Spec . Cluster IP ) ) ; err != nil { // TODO: when validation becomes versioned, this gets more complicated. el := field . Error List { field . Invalid ( field . New Path ( " " , " " ) , service . Spec . Cluster return false , errors . New } 
func Default Controller Rate Limiter ( ) Rate Limiter { return New Max Of Rate Limiter ( New Item Exponential Failure Rate Limiter ( 5 * time . Millisecond , 1000 * time . Second ) , // 10 qps, 100 bucket size. This is only for retry speed and its only the overall factor (not per item) & Bucket Rate Limiter { Limiter : rate . New } 
func ( r * Request Info Factory ) New Request Info ( req * http . Request ) ( * Request Info , error ) { // start with a non-resource request until proven otherwise request Info := Request Info { Is Resource Request : false , Path : req . URL . Path , Verb : strings . To current Parts := split if len ( current Parts ) < 3 { // return a non-resource request return & request if ! r . API Prefixes . Has ( current Parts [ 0 ] ) { // return a non-resource request return & request request Info . API Prefix = current current Parts = current if ! r . Groupless API Prefixes . Has ( request Info . API Prefix ) { // one part (API Prefix) has already been consumed, so this is actually "do we have four parts?" if len ( current Parts ) < 3 { // return a non-resource request return & request request Info . API Group = current current Parts = current request Info . Is Resource request Info . API Version = current current Parts = current // handle input of form /{special Verb}/* if special Verbs . Has ( current Parts [ 0 ] ) { if len ( current Parts ) < 2 { return & request request Info . Verb = current current Parts = current } else { switch req . Method { case " " : request case " " , " " : request case " " : request case " " : request case " " : request default : request // URL forms: /namespaces/{namespace}/{kind}/*, where parts are adjusted to be relative to kind if current Parts [ 0 ] == " " { if len ( current Parts ) > 1 { request Info . Namespace = current // if there is another step after the namespace name and it is not a known namespace subresource // move current Parts to include it as a resource in its own right if len ( current Parts ) > 2 && ! namespace Subresources . Has ( current Parts [ 2 ] ) { current Parts = current } else { request Info . Namespace = metav1 . Namespace // parsing successful, so we now know the proper value for .Parts request Info . Parts = current // parts look like: resource/resource Name/subresource/other/stuff/we/don't/interpret switch { case len ( request Info . Parts ) >= 3 && ! special Verbs No Subresources . Has ( request Info . Verb ) : request Info . Subresource = request case len ( request Info . Parts ) >= 2 : request Info . Name = request case len ( request Info . Parts ) >= 1 : request Info . Resource = request // if there's no name on the request and we thought it was a get before, then the actual verb is a list or a watch if len ( request Info . Name ) == 0 && request Info . Verb == " " { opts := metainternalversion . List if err := metainternalversion . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , metav1 . Scheme Group // Reset opts to not rely on partial results from parsing. // However, if watch is set, let's report it. opts = metainternalversion . List if values := req . URL . Query ( ) [ " " ] ; len ( values ) > 0 { switch strings . To if opts . Watch { request } else { request if opts . Field Selector != nil { if name , ok := opts . Field Selector . Requires Exact Match ( " " ) ; ok { if len ( path . Is Valid Path Segment Name ( name ) ) == 0 { request // if there's no name on the request and we thought it was a delete before, then the actual verb is deletecollection if len ( request Info . Name ) == 0 && request Info . Verb == " " { request return & request } 
func With Request Info ( parent context . Context , info * Request Info ) context . Context { return With Value ( parent , request Info } 
func Request Info From ( ctx context . Context ) ( * Request Info , bool ) { info , ok := ctx . Value ( request Info Key ) . ( * Request } 
func split } 
func new V1Node Client ( addr csi Addr ) ( node Client csipbv1 . Node Client , closer io . Closer , err error ) { var conn * grpc . Client conn , err = new Grpc node Client = csipbv1 . New Node return node } 
func new V0Node Client ( addr csi Addr ) ( node Client csipbv0 . Node Client , closer io . Closer , err error ) { var conn * grpc . Client conn , err = new Grpc node Client = csipbv0 . New Node return node } 
func New Storage Class Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Storage Class Informer ( client , resync } 
func ( u * Undelta u . Push } 
func New Undelta Store ( push Func func ( [ ] interface { } ) , key Func Key Func ) * Undelta Store { return & Undelta Store { Store : New Store ( key Func ) , Push Func : push } 
func With Namespace ( parent context . Context , namespace string ) context . Context { return With Value ( parent , namespace } 
func Namespace From ( ctx context . Context ) ( string , bool ) { namespace , ok := ctx . Value ( namespace } 
func Namespace Value ( ctx context . Context ) string { namespace , _ := Namespace } 
func User From ( ctx context . Context ) ( user . Info , bool ) { user , ok := ctx . Value ( user } 
func With Audit Event ( parent context . Context , ev * audit . Event ) context . Context { return With Value ( parent , audit } 
func Audit Event From ( ctx context . Context ) * audit . Event { ev , _ := ctx . Value ( audit } 
func Register All Admission } 
func Build Insecure Handler Chain ( api Handler http . Handler , c * server . Config ) http . Handler { handler := api handler = genericapifilters . With Audit ( handler , c . Audit Backend , c . Audit Policy Checker , c . Long Running handler = genericapifilters . With Authentication ( handler , server . Insecure handler = genericfilters . With CORS ( handler , c . Cors Allowed Origin handler = genericfilters . With Timeout For Non Long Running Requests ( handler , c . Long Running Func , c . Request handler = genericfilters . With Max In Flight Limit ( handler , c . Max Requests In Flight , c . Max Mutating Requests In Flight , c . Long Running handler = genericfilters . With Wait Group ( handler , c . Long Running Func , c . Handler Chain Wait handler = genericapifilters . With Request Info ( handler , server . New Request Info handler = genericfilters . With Panic } 
func Convert_v1alpha1_Deployment Controller Configuration_To_config_Deployment Controller Configuration ( in * v1alpha1 . Deployment Controller Configuration , out * deploymentconfig . Deployment Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Deployment Controller Configuration_To_config_Deployment Controller } 
func Convert_config_Deployment Controller Configuration_To_v1alpha1_Deployment Controller Configuration ( in * deploymentconfig . Deployment Controller Configuration , out * v1alpha1 . Deployment Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Deployment Controller Configuration_To_v1alpha1_Deployment Controller } 
func Validate Configuration ( config * internalapi . Configuration ) error { all Errs := field . Error fldpath := field . New all Errs = append ( all Errs , validation . Validate all Errs = append ( all Errs , validation . Validate if len ( all Errs ) > 0 { return fmt . Errorf ( " " , all } 
func Handle Panic ( fn func ( ) ) func ( ) { return func ( ) { defer func ( ) { if r := recover ( ) ; r != nil { for _ , fn := range utilruntime . Panic } 
func find Service Port ( svc * v1 . Service , port int32 ) ( * v1 . Service Port , error ) { for _ , svc Port := range svc . Spec . Ports { if svc Port . Port == port { return & svc return nil , errors . New Service } 
func Resolve Endpoint ( services listersv1 . Service Lister , endpoints listersv1 . Endpoints svc Port , err := find Service switch { case svc . Spec . Type == v1 . Service Type Cluster IP , svc . Spec . Type == v1 . Service Type Load Balancer , svc . Spec . Type == v1 . Service Type Node if len ( eps . Subsets ) == 0 { return nil , errors . New Service // Pick a random Subset to start searching from. ss // Find a Subset that has the port. for ssi := 0 ; ssi < len ( eps . Subsets ) ; ssi ++ { ss := & eps . Subsets [ ( ss for i := range ss . Ports { if ss . Ports [ i ] . Name == svc return & url . URL { Scheme : " " , Host : net . Join Host return nil , errors . New Service } 
func ( m * csi Block Mapper ) Get Global Map Path ( spec * volume . Spec ) ( string , error ) { dir := get Volume Device Plugin } 
func ( m * csi Block Mapper ) get Staging Path ( ) string { sanitized Spec Vol ID := utilstrings . Escape Qualified Name ( m . spec return filepath . Join ( m . plugin . host . Get Volume Device Plugin Dir ( CSI Plugin Name ) , " " , sanitized Spec Vol } 
func ( m * csi Block Mapper ) Get Pod Device Map Path ( ) ( string , string ) { path := m . plugin . host . Get Pod Volume Device Dir ( m . pod UID , utilstrings . Escape Qualified Name ( CSI Plugin spec Name := m . spec klog . V ( 4 ) . Infof ( log ( " " , path , spec return path , spec } 
func ( m * csi Block Mapper ) stage Volume For Block ( ctx context . Context , csi csi Client , access Mode v1 . Persistent Volume Access Mode , csi Source * v1 . CSI Persistent Volume Source , attachment * storage . Volume staging Path := m . get Staging klog . V ( 4 ) . Infof ( log ( " " , staging // Check whether "STAGE_UNSTAGE_VOLUME" is set stage Unstage Set , err := csi . Node Supports Stage if ! stage Unstage publish Volume Info := attachment . Status . Attachment node Stage if csi Source . Node Stage Secret Ref != nil { node Stage Secrets , err = get Credentials From Secret ( m . k8s , csi Source . Node Stage Secret if err != nil { return " " , fmt . Errorf ( " " , csi Source . Node Stage Secret Ref . Namespace , csi Source . Node Stage Secret // Creating a staging Path directory before call to Node Stage Volume if err := os . Mkdir All ( staging Path , 0750 ) ; err != nil { klog . Error ( log ( " " , staging klog . V ( 4 ) . Info ( log ( " " , staging // Request to stage a block volume to staging Path. // Expected implementation for driver is creating driver specific resource on staging Path and // attaching the block volume to the node. err = csi . Node Stage Volume ( ctx , csi Source . Volume Handle , publish Volume Info , staging Path , fs Type Block Name , access Mode , node Stage Secrets , csi Source . Volume klog . V ( 4 ) . Infof ( log ( " " , staging return staging } 
func ( m * csi Block Mapper ) publish Volume For Block ( ctx context . Context , csi csi Client , access Mode v1 . Persistent Volume Access Mode , csi Source * v1 . CSI Persistent Volume Source , attachment * storage . Volume Attachment , staging publish Volume Info := attachment . Status . Attachment node Publish if csi Source . Node Publish Secret Ref != nil { node Publish Secrets , err = get Credentials From Secret ( m . k8s , csi Source . Node Publish Secret if err != nil { klog . Errorf ( " " , csi Source . Node Publish Secret Ref . Namespace , csi Source . Node Publish Secret publish Path := m . get Publish // Setup a parent directory for publish Path before call to Node Publish Volume publish Dir := filepath . Dir ( publish if err := os . Mkdir All ( publish Dir , 0750 ) ; err != nil { klog . Error ( log ( " " , publish klog . V ( 4 ) . Info ( log ( " " , publish // Request to publish a block volume to publish Path. // Expectation for driver is to place a block volume on the publish Path, by bind-mounting the device file on the publish Path or // creating device file on the publish Path. // Parent directory for publish Path is created by k8s, but driver is responsible for creating publish Path itself. // If driver doesn't implement Node Stage Volume, attaching the block volume to the node may be done, here. err = csi . Node Publish Volume ( ctx , m . volume ID , m . read Only , staging Path , publish Path , access Mode , publish Volume Info , csi Source . Volume Attributes , node Publish Secrets , fs Type Block return publish } 
func ( m * csi Block Mapper ) Set Up Device ( ) ( string , error ) { if ! m . plugin . block // Get csi csi Source , err := get CSI Source From // Search for attachment by Volume Attachment.Spec.Source.Persistent Volume Name node Name := string ( m . plugin . host . Get Node attach ID := get Attachment Name ( csi Source . Volume Handle , csi Source . Driver , node attachment , err := m . k8s . Storage V1 ( ) . Volume Attachments ( ) . Get ( attach ID , meta . Get if err != nil { klog . Error ( log ( " " , attach if attachment == nil { klog . Error ( log ( " " , attach //TODO (vladimirvivien) implement better Access Modes mapping between k8s and CSI access Mode := v1 . Read Write if m . spec . Persistent Volume . Spec . Access Modes != nil { access Mode = m . spec . Persistent Volume . Spec . Access ctx , cancel := context . With Timeout ( context . Background ( ) , csi csi Client , err := m . csi Client // Call Node Stage Volume staging Path , err := m . stage Volume For Block ( ctx , csi Client , access Mode , csi // Call Node Publish Volume publish Path , err := m . publish Volume For Block ( ctx , csi Client , access Mode , csi Source , attachment , staging return publish } 
func ( m * csi Block Mapper ) unpublish Volume For Block ( ctx context . Context , csi csi Client , publish Path string ) error { // Request to unpublish a block volume from publish Path. // Expectation for driver is to remove block volume from the publish Path, by unmounting bind-mounted device file // or deleting device file. // Driver is responsible for deleting publish Path itself. // If driver doesn't implement Node Unstage Volume, detaching the block volume from the node may be done, here. if err := csi . Node Unpublish Volume ( ctx , m . volume ID , publish klog . V ( 4 ) . Infof ( log ( " " , publish } 
func ( m * csi Block Mapper ) unstage Volume For Block ( ctx context . Context , csi csi Client , staging Path string ) error { // Check whether "STAGE_UNSTAGE_VOLUME" is set stage Unstage Set , err := csi . Node Supports Stage if ! stage Unstage // Request to unstage a block volume from staging Path. // Expected implementation for driver is removing driver specific resource in staging Path and // detaching the block volume from the node. if err := csi . Node Unstage Volume ( ctx , m . volume ID , staging klog . V ( 4 ) . Infof ( log ( " " , staging // Remove staging Path directory and its contents if err := os . Remove All ( staging Path ) ; err != nil { klog . Error ( log ( " " , staging } 
func ( m * csi Block Mapper ) Tear Down Device ( global Map Path , device Path string ) error { if ! m . plugin . block klog . V ( 4 ) . Infof ( log ( " " , global Map Path , device ctx , cancel := context . With Timeout ( context . Background ( ) , csi csi Client , err := m . csi Client // Call Node Unpublish Volume publish Path := m . get Publish if _ , err := os . Stat ( publish Path ) ; err != nil { if os . Is Not Exist ( err ) { klog . V ( 4 ) . Infof ( log ( " " , publish } else { err := m . unpublish Volume For Block ( ctx , csi Client , publish // Call Node Unstage Volume staging Path := m . get Staging if _ , err := os . Stat ( staging Path ) ; err != nil { if os . Is Not Exist ( err ) { klog . V ( 4 ) . Infof ( log ( " " , staging } else { err := m . unstage Volume For Block ( ctx , csi Client , staging } 
func New Controller Manager Command ( ) * cobra . Command { s , err := options . New Kube Controller Manager controller, and serviceaccounts controller.` , Run : func ( cmd * cobra . Command , args [ ] string ) { verflag . Print And Exit If utilflag . Print c , err := s . Config ( Known Controllers ( ) , Controllers Disabled By if err := Run ( c . Complete ( ) , wait . Never named Flag Sets := s . Flags ( Known Controllers ( ) , Controllers Disabled By verflag . Add Flags ( named Flag Sets . Flag globalflag . Add Global Flags ( named Flag Sets . Flag // hoist this flag from the global flagset to preserve the commandline until // the gce cloudprovider is removed. globalflag . Register ( named Flag Sets . Flag named Flag Sets . Flag Set ( " " ) . Mark for _ , f := range named Flag Sets . Flag Sets { fs . Add Flag usage cols , _ , _ := term . Terminal Size ( cmd . Out Or cmd . Set Usage Func ( func ( cmd * cobra . Command ) error { fmt . Fprintf ( cmd . Out Or Stderr ( ) , usage Fmt , cmd . Use cliflag . Print Sections ( cmd . Out Or Stderr ( ) , named Flag cmd . Set Help Func ( func ( cmd * cobra . Command , args [ ] string ) { fmt . Fprintf ( cmd . Out Or Stdout ( ) , " \n \n " + usage Fmt , cmd . Long , cmd . Use cliflag . Print Sections ( cmd . Out Or Stdout ( ) , named Flag } 
func Resync Period ( c * config . Completed return time . Duration ( float64 ( c . Component Config . Generic . Min Resync } 
func Run ( c * config . Completed Config , stop if cfgz , err := configz . New ( Configz Name ) ; err == nil { cfgz . Set ( c . Component // Setup any healthz checks we will want to use. var checks [ ] healthz . Healthz var election Checker * leaderelection . Healthz if c . Component Config . Generic . Leader Election . Leader Elect { election Checker = leaderelection . New Leader Healthz checks = append ( checks , election // Start the controller manager HTTP server // unsecured Mux is the handler for these controller *after* authn/authz filters have been applied var unsecured Mux * mux . Path Recorder if c . Secure Serving != nil { unsecured Mux = genericcontrollermanager . New Base Handler ( & c . Component handler := genericcontrollermanager . Build Handler Chain ( unsecured // TODO: handle stopped Ch returned by c.Secure Serving.Serve if _ , err := c . Secure Serving . Serve ( handler , 0 , stop if c . Insecure Serving != nil { unsecured Mux = genericcontrollermanager . New Base Handler ( & c . Component insecure Superuser Authn := server . Authentication Info { Authenticator : & server . Insecure handler := genericcontrollermanager . Build Handler Chain ( unsecured Mux , nil , & insecure Superuser if err := c . Insecure Serving . Serve ( handler , 0 , stop run := func ( ctx context . Context ) { root Client Builder := controller . Simple Controller Client Builder { Client var client Builder controller . Controller Client if c . Component Config . Kube Cloud Shared . Use Service Account Credentials { if len ( c . Component Config . SA Controller . Service Account Key if should Turn On Dynamic //Dynamic builder will use Token Request feature and refresh service account token periodically client Builder = controller . New Dynamic Client Builder ( restclient . Anonymous Client Config ( c . Kubeconfig ) , c . Client . Core client Builder = controller . SA Controller Client Builder { Client Config : restclient . Anonymous Client Config ( c . Kubeconfig ) , Core Client : c . Client . Core V1 ( ) , Authentication Client : c . Client . Authentication } else { client Builder = root Client controller Context , err := Create Controller Context ( c , root Client Builder , client sa Token Controller Init Func := service Account Token Controller Starter { root Client Builder : root Client Builder } . start Service Account Token if err := Start Controllers ( controller Context , sa Token Controller Init Func , New Controller Initializers ( controller Context . Loop Mode ) , unsecured controller Context . Informer Factory . Start ( controller controller Context . Generic Informer Factory . Start ( controller close ( controller Context . Informers if ! c . Component Config . Generic . Leader Election . Leader // add a uniquifier so that two processes on the same host don't accidentally both become active id = id + " " + string ( uuid . New rl , err := resourcelock . New ( c . Component Config . Generic . Leader Election . Resource Lock , " " , " " , c . Leader Election Client . Core V1 ( ) , c . Leader Election Client . Coordination V1 ( ) , resourcelock . Resource Lock Config { Identity : id , Event Recorder : c . Event leaderelection . Run Or Die ( context . TODO ( ) , leaderelection . Leader Election Config { Lock : rl , Lease Duration : c . Component Config . Generic . Leader Election . Lease Duration . Duration , Renew Deadline : c . Component Config . Generic . Leader Election . Renew Deadline . Duration , Retry Period : c . Component Config . Generic . Leader Election . Retry Period . Duration , Callbacks : leaderelection . Leader Callbacks { On Started Leading : run , On Stopped } , } , Watch Dog : election } 
func New Controller Initializers ( loop Mode Controller Loop Mode ) map [ string ] Init Func { controllers := map [ string ] Init controllers [ " " ] = start Endpoint controllers [ " " ] = start Replication controllers [ " " ] = start Pod GC controllers [ " " ] = start Resource Quota controllers [ " " ] = start Namespace controllers [ " " ] = start Service Account controllers [ " " ] = start Garbage Collector controllers [ " " ] = start Daemon Set controllers [ " " ] = start Job controllers [ " " ] = start Deployment controllers [ " " ] = start Replica Set controllers [ " " ] = start HPA controllers [ " " ] = start Disruption controllers [ " " ] = start Stateful Set controllers [ " " ] = start Cron Job controllers [ " " ] = start CSR Signing controllers [ " " ] = start CSR Approving controllers [ " " ] = start CSR Cleaner controllers [ " " ] = start TTL controllers [ " " ] = start Bootstrap Signer controllers [ " " ] = start Token Cleaner controllers [ " " ] = start Node Ipam controllers [ " " ] = start Node Lifecycle if loop Mode == Include Cloud Loops { controllers [ " " ] = start Service controllers [ " " ] = start Route controllers [ " " ] = start Cloud Node Lifecycle // TODO: volume controller into the Include Cloud controllers [ " " ] = start Persistent Volume Binder controllers [ " " ] = start Attach Detach controllers [ " " ] = start Volume Expand controllers [ " " ] = start Cluster Role Aggregration controllers [ " " ] = start PVC Protection controllers [ " " ] = start PV Protection controllers [ " " ] = start TTL After Finished controllers [ " " ] = start Root CA Cert } 
func Get Available Resources ( client Builder controller . Controller Client Builder ) ( map [ schema . Group Version Resource ] bool , error ) { client := client Builder . Client Or discovery resource Map , err := discovery Client . Server if err != nil { utilruntime . Handle if len ( resource all Resources := map [ schema . Group Version for _ , api Resource List := range resource Map { version , err := schema . Parse Group Version ( api Resource List . Group for _ , api Resource := range api Resource List . API Resources { all Resources [ version . With Resource ( api return all } 
func Create Controller Context ( s * config . Completed Config , root Client Builder , client Builder controller . Controller Client Builder , stop <- chan struct { } ) ( Controller Context , error ) { versioned Client := root Client Builder . Client Or shared Informers := informers . New Shared Informer Factory ( versioned Client , Resync dynamic Client := dynamic . New For Config Or Die ( root Client Builder . Config Or dynamic Informers := dynamicinformer . New Dynamic Shared Informer Factory ( dynamic Client , Resync // If apiserver is not running we should wait for some time and fail only then. This is particularly // important when we start apiserver and controller manager at the same time. if err := genericcontrollermanager . Wait For API Server ( versioned Client , 10 * time . Second ) ; err != nil { return Controller // Use a discovery client capable of being refreshed. discovery Client := root Client Builder . Client Or cached Client := cacheddiscovery . New Mem Cache Client ( discovery rest Mapper := restmapper . New Deferred Discovery REST Mapper ( cached go wait . Until ( func ( ) { rest available Resources , err := Get Available Resources ( root Client if err != nil { return Controller cloud , loop Mode , err := create Cloud Provider ( s . Component Config . Kube Cloud Shared . Cloud Provider . Name , s . Component Config . Kube Cloud Shared . External Cloud Volume Plugin , s . Component Config . Kube Cloud Shared . Cloud Provider . Cloud Config File , s . Component Config . Kube Cloud Shared . Allow Untagged Cloud , shared if err != nil { return Controller ctx := Controller Context { Client Builder : client Builder , Informer Factory : shared Informers , Generic Informer Factory : controller . New Informer Factory ( shared Informers , dynamic Informers ) , Component Config : s . Component Config , REST Mapper : rest Mapper , Available Resources : available Resources , Cloud : cloud , Loop Mode : loop Mode , Stop : stop , Informers Started : make ( chan struct { } ) , Resync Period : Resync } 
func ( s * non } 
func ( s * non Root ) Validate ( sc Path * field . Path , _ * api . Pod , _ * api . Container , run As Non Root * bool , run As User * int64 ) field . Error List { all Errs := field . Error if run As Non Root == nil && run As User == nil { all Errs = append ( all Errs , field . Required ( sc return all if run As Non Root != nil && * run As Non Root == false { all Errs = append ( all Errs , field . Invalid ( sc Path . Child ( " " ) , * run As Non return all if run As User != nil && * run As User == 0 { all Errs = append ( all Errs , field . Invalid ( sc Path . Child ( " " ) , * run As return all return all } 
func ( s * storage Leases ) List Leases ( ) ( [ ] string , error ) { ip Info List := & corev1 . Endpoints if err := s . storage . List ( apirequest . New Default Context ( ) , s . base Key , " " , storage . Everything , ip Info ip List := make ( [ ] string , len ( ip Info for i , ip := range ip Info List . Items { ip klog . V ( 6 ) . Infof ( " " , ip return ip } 
func ( s * storage Leases ) Update Lease ( ip string ) error { key := path . Join ( s . base return s . storage . Guaranteed Update ( apirequest . New Default Context ( ) , key , & corev1 . Endpoints { } , true , nil , func ( input kruntime . Object , resp Meta storage . Response existing . Subsets = [ ] corev1 . Endpoint Subset { { Addresses : [ ] corev1 . Endpoint // lease Time needs to be in seconds lease Time := uint64 ( s . lease // NB: Guaranteed klog . V ( 6 ) . Infof ( " " , ip , lease return existing , & lease } 
func ( s * storage Leases ) Remove Lease ( ip string ) error { return s . storage . Delete ( apirequest . New Default Context ( ) , s . base } 
func New Leases ( storage storage . Interface , base Key string , lease Time time . Duration ) Leases { return & storage Leases { storage : storage , base Key : base Key , lease Time : lease } 
func New Lease Endpoint Reconciler ( endpoint Client corev1client . Endpoints Getter , master Leases Leases ) Endpoint Reconciler { return & lease Endpoint Reconciler { endpoint Client : endpoint Client , master Leases : master Leases , stop Reconciling } 
func ( r * lease Endpoint Reconciler ) Reconcile Endpoints ( service Name string , ip net . IP , endpoint Ports [ ] corev1 . Endpoint Port , reconcile Ports bool ) error { r . reconciling defer r . reconciling if r . stop Reconciling // Refresh the TTL on our key, independently of whether any error or // update conflict happens below. This makes sure that at least some of // the masters will add our endpoint. if err := r . master Leases . Update return r . do Reconcile ( service Name , endpoint Ports , reconcile } 
func check Endpoint Subset Format With Lease ( e * corev1 . Endpoints , expected I Ps [ ] string , ports [ ] corev1 . Endpoint Port , reconcile Ports bool ) ( format Correct bool , ips Correct bool , ports ports if reconcile Ports { if len ( sub . Ports ) != len ( ports ) { ports } else { for i , port := range ports { if port != sub . Ports [ i ] { ports ips if len ( sub . Addresses ) != len ( expected I Ps ) { ips } else { // check the actual content of the addresses // present addrs is used as a set (the keys) and to indicate if a // value was already found (the values) present Addrs := make ( map [ string ] bool , len ( expected I for _ , ip := range expected I Ps { present // uniqueness is assumed amongst all Addresses. for _ , addr := range sub . Addresses { if already Seen , ok := present Addrs [ addr . IP ] ; already Seen || ! ok { ips present return true , ips Correct , ports } 
func Register ( plugins * admission . Plugins ) { plugins . Register ( Plugin Name , func ( config io . Reader ) ( admission . Interface , error ) { new Image Policy Webhook , err := New Image Policy return new Image Policy } 
func ( a * Plugin ) filter Annotations ( all for k , v := range all } 
func ( a * Plugin ) webhook if a . default Allow { attributes . Add Annotation ( Audit Key Prefix + Image Policy Failed Open Key // TODO(wteiken): Remove the annotation code for the 1.13 release annotations := pod . Get annotations [ api . Image Policy Failed Open pod . Object Meta . Set return admission . New } 
func ( a * Plugin ) Validate ( attributes admission . Attributes , o admission . Object Interfaces ) ( err error ) { // Ignore all calls to subresources or resources other than pods. if attributes . Get Subresource ( ) != " " || attributes . Get Resource ( ) . Group pod , ok := attributes . Get if ! ok { return apierrors . New Bad // Build list of Image Review Container Spec var image Review Container Specs [ ] v1alpha1 . Image Review Container containers := make ( [ ] api . Container , 0 , len ( pod . Spec . Containers ) + len ( pod . Spec . Init containers = append ( containers , pod . Spec . Init for _ , c := range containers { image Review Container Specs = append ( image Review Container Specs , v1alpha1 . Image Review Container image Review := v1alpha1 . Image Review { Spec : v1alpha1 . Image Review Spec { Containers : image Review Container Specs , Annotations : a . filter Annotations ( pod . Annotations ) , Namespace : attributes . Get if err := a . admit Pod ( pod , attributes , & image Review ) ; err != nil { return admission . New } 
func New Image Policy Webhook ( config File io . Reader ) ( * Plugin , error ) { if config // TODO: move this to a versioned configuration file format var config Admission d := yaml . New YAML Or JSON Decoder ( config wh Config := config . Image Policy if err := normalize Webhook Config ( & wh gw , err := webhook . New Generic Webhook ( legacyscheme . Scheme , legacyscheme . Codecs , wh Config . Kube Config File , group Versions , wh Config . Retry return & Plugin { Handler : admission . New Handler ( admission . Create , admission . Update ) , webhook : gw , response Cache : cache . New LRU Expire Cache ( 1024 ) , allow TTL : wh Config . Allow TTL , deny TTL : wh Config . Deny TTL , default Allow : wh Config . Default } 
func New Role Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Role Informer ( client , namespace , resync } 
func field Name ( field * ast . Field ) string { json if field . Tag != nil { json Tag = reflect . Struct if strings . Contains ( json json Tag = strings . Split ( json if json return json } 
func Parse Documentation From ( src string ) [ ] Kube Types { var doc For Types [ ] Kube pkg := ast for _ , kub Type := range pkg . Types { if struct Type , ok := kub Type . Decl . Specs [ 0 ] . ( * ast . Type Spec ) . Type . ( * ast . Struct Type ) ; ok { var ks Kube ks = append ( ks , Pair { kub Type . Name , fmt Raw Doc ( kub for _ , field := range struct Type . Fields . List { if n := field Name ( field ) ; n != " " { field Doc := fmt Raw ks = append ( ks , Pair { n , field doc For Types = append ( doc For return doc For } 
func Write Swagger Doc Func ( kube Types [ ] Kube Types , w io . Writer ) error { for _ , kube Type := range kube Types { struct Name := kube kube // Ignore empty documentation docful Types := make ( Kube Types , 0 , len ( kube for _ , pair := range kube Type { if pair . Doc != " " { docful Types = append ( docful if len ( docful buffer := new write Func Header ( buffer , struct write Map Body ( buffer , docful write Func Footer ( buffer , struct buffer . add if err := buffer . flush } 
func Verify Swagger Docs Exist ( kube Types [ ] Kube Types , w io . Writer ) ( int , error ) { missing buffer := new for _ , kube Type := range kube Types { struct Name := kube if kube s := fmt . Sprintf ( format , struct buffer . add missing kube Type = kube for _ , pair := range kube s := fmt . Sprintf ( format , struct buffer . add missing if err := buffer . flush return missing } 
func Listen And Serve Kubelet Server ( host Host Interface , resource Analyzer stats . Resource Analyzer , address net . IP , port uint , tls Options * TLS Options , auth Auth Interface , enable Debugging Handlers , enable Contention Profiling , redirect Container Streaming bool , cri handler := New Server ( host , resource Analyzer , auth , enable Debugging Handlers , enable Contention Profiling , redirect Container Streaming , cri s := & http . Server { Addr : net . Join Host Port ( address . String ( ) , strconv . Format Uint ( uint64 ( port ) , 10 ) ) , Handler : & handler , Max Header if tls Options != nil { s . TLS Config = tls // Passing empty strings as the cert and key files means no // cert/keys are specified and Get Certificate in the TLS Config // should be called instead. klog . Fatal ( s . Listen And Serve TLS ( tls Options . Cert File , tls Options . Key } else { klog . Fatal ( s . Listen And } 
func Listen And Serve Kubelet Read Only Server ( host Host Interface , resource Analyzer stats . Resource s := New Server ( host , resource server := & http . Server { Addr : net . Join Host Port ( address . String ( ) , strconv . Format Uint ( uint64 ( port ) , 10 ) ) , Handler : & s , Max Header klog . Fatal ( server . Listen And } 
func Listen And Serve Pod Resources ( socket string , pods Provider podresources . Pods Provider , devices Provider podresources . Devices Provider ) { server := grpc . New podresourcesapi . Register Pod Resources Lister Server ( server , podresources . New Pod Resources Server ( pods Provider , devices l , err := util . Create } 
func New Server ( host Host Interface , resource Analyzer stats . Resource Analyzer , auth Auth Interface , enable Debugging Handlers , enable Contention Profiling , redirect Container Streaming bool , cri Handler http . Handler ) Server { server := Server { host : host , resource Analyzer : resource Analyzer , auth : auth , restful Cont : & filtering Container { Container : restful . New Container ( ) } , redirect Container Streaming : redirect Container if auth != nil { server . Install Auth server . Install Default if enable Debugging Handlers { server . Install Debugging Handlers ( cri if enable Contention Profiling { goruntime . Set Block Profile } else { server . Install Debugging Disabled } 
func ( s * Server ) Install Auth Filter ( ) { s . restful Cont . Filter ( func ( req * restful . Request , resp * restful . Response , chain * restful . Filter Chain ) { // Authenticate info , ok , err := s . auth . Authenticate resp . Write Error String ( http . Status if ! ok { resp . Write Error String ( http . Status // Get authorization attributes attrs := s . auth . Get Request if err != nil { msg := fmt . Sprintf ( " " , attrs . Get User ( ) . Get Name ( ) , attrs . Get Verb ( ) , attrs . Get Resource ( ) , attrs . Get resp . Write Error String ( http . Status Internal Server if decision != authorizer . Decision Allow { msg := fmt . Sprintf ( " " , attrs . Get User ( ) . Get Name ( ) , attrs . Get Verb ( ) , attrs . Get Resource ( ) , attrs . Get resp . Write Error String ( http . Status // Continue chain . Process } 
func ( s * Server ) Install Default Handlers ( ) { healthz . Install Handler ( s . restful Cont , healthz . Ping Healthz , healthz . Log Healthz , healthz . Named Check ( " " , s . sync Loop Health ws := new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( s . get s . restful s . restful Cont . Add ( stats . Create Handlers ( stats Path , s . host , s . resource s . restful Cont . Handle ( metrics // c Advisor metrics are exposed under the secured handler as well r := prometheus . New included Metrics := cadvisormetrics . Metric Set { cadvisormetrics . Cpu Usage Metrics : struct { } { } , cadvisormetrics . Memory Usage Metrics : struct { } { } , cadvisormetrics . Cpu Load Metrics : struct { } { } , cadvisormetrics . Disk IO Metrics : struct { } { } , cadvisormetrics . Disk Usage Metrics : struct { } { } , cadvisormetrics . Network Usage Metrics : struct { } { } , cadvisormetrics . Accelerator Usage Metrics : struct { } { } , cadvisormetrics . App r . Must Register ( metrics . New Prometheus Collector ( prometheus Host Adapter { s . host } , container Prometheus Labels Func ( s . host ) , included s . restful Cont . Handle ( cadvisor Metrics Path , promhttp . Handler For ( r , promhttp . Handler Opts { Error Handling : promhttp . Continue On v1alpha1Resource Registry := prometheus . New v1alpha1Resource Registry . Must Register ( stats . New Prometheus Resource Metric Collector ( s . resource s . restful Cont . Handle ( path . Join ( resource Metrics Path Prefix , v1alpha1 . Version ) , promhttp . Handler For ( v1alpha1Resource Registry , promhttp . Handler Opts { Error Handling : promhttp . Continue On // prober metrics are exposed under a different endpoint p := prometheus . New p . Must Register ( prober . Prober s . restful Cont . Handle ( prober Metrics Path , promhttp . Handler For ( p , promhttp . Handler Opts { Error Handling : promhttp . Continue On ws = new ( restful . Web ws . Path ( spec ws . Route ( ws . GET ( " " ) . To ( s . get Spec ) . Operation ( " " ) . Writes ( cadvisorapi . Machine s . restful } 
func ( s * Server ) Install Debugging Handlers ( cri ws := new ( restful . Web ws . Route ( ws . POST ( " " ) . To ( s . get ws . Route ( ws . POST ( " " ) . To ( s . get s . restful ws = new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( s . get ws . Route ( ws . POST ( " " ) . To ( s . get ws . Route ( ws . GET ( " " ) . To ( s . get ws . Route ( ws . POST ( " " ) . To ( s . get s . restful ws = new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( s . get ws . Route ( ws . POST ( " " ) . To ( s . get ws . Route ( ws . GET ( " " ) . To ( s . get ws . Route ( ws . POST ( " " ) . To ( s . get s . restful ws = new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( s . get Port ws . Route ( ws . POST ( " " ) . To ( s . get Port ws . Route ( ws . GET ( " " ) . To ( s . get Port ws . Route ( ws . POST ( " " ) . To ( s . get Port s . restful ws = new ( restful . Web ws . Path ( logs ws . Route ( ws . GET ( " " ) . To ( s . get ws . Route ( ws . GET ( " " ) . To ( s . get Logs ) . Operation ( " " ) . Param ( ws . Path Parameter ( " " , " " ) . Data s . restful ws = new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( s . get Container s . restful configz . Install Handler ( s . restful handle Pprof Endpoint := func ( req * restful . Request , resp * restful . Response ) { name := strings . Trim Prefix ( req . Request . URL . Path , pprof Base // Setup pprof handlers. ws = new ( restful . Web Service ) . Path ( pprof Base ws . Route ( ws . GET ( " " ) . To ( func ( req * restful . Request , resp * restful . Response ) { handle Pprof s . restful // Setup flags handlers. // so far, only logging related endpoints are considered valid to add for these debug flags. s . restful Cont . Handle ( " " , routes . String Flag Put Handler ( logs . Glog // The /runningpods endpoint is used for testing only. ws = new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( s . get Running s . restful if cri Handler != nil { s . restful Cont . Handle ( " " , cri } 
func ( s * Server ) Install Debugging Disabled Handlers ( ) { h := http . Handler Func ( func ( w http . Response Writer , r * http . Request ) { http . Error ( w , " " , http . Status Method Not paths := [ ] string { " " , " " , " " , " " , " " , " " , pprof Base Path , logs for _ , p := range paths { s . restful } 
func ( s * Server ) sync Loop Health Check ( req * http . Request ) error { duration := s . host . Resync min if duration < min Duration { duration = min enter Loop Time := s . host . Latest Loop Entry if ! enter Loop Time . Is Zero ( ) && time . Now ( ) . After ( enter Loop } 
func ( s * Server ) get Container Logs ( request * restful . Request , response * restful . Response ) { pod Namespace := request . Path pod ID := request . Path container Name := request . Path if len ( pod ID ) == 0 { // TODO: Why return JSON when the rest return plaintext errors? // TODO: Why return plaintext errors? response . Write Error ( http . Status Bad Request , fmt . Errorf ( `{"message": "Missing pod if len ( container Name ) == 0 { // TODO: Why return JSON when the rest return plaintext errors? response . Write Error ( http . Status Bad if len ( pod Namespace ) == 0 { // TODO: Why return JSON when the rest return plaintext errors? response . Write Error ( http . Status Bad Request , fmt . Errorf ( `{"message": "Missing pod // backwards compatibility for the "tail" query parameter if tail := request . Query // container logs on the kubelet are locked to the v1 API version of Pod Log Options log Options := & v1 . Pod Log if err := legacyscheme . Parameter Codec . Decode Parameters ( query , v1 . Scheme Group Version , log Options ) ; err != nil { response . Write Error ( http . Status Bad log Options . Type Meta = metav1 . Type if errs := validation . Validate Pod Log Options ( log Options ) ; len ( errs ) > 0 { response . Write Error ( http . Status Unprocessable pod , ok := s . host . Get Pod By Name ( pod Namespace , pod if ! ok { response . Write Error ( http . Status Not Found , fmt . Errorf ( " " , pod // Check if container Name is valid. container for _ , container := range pod . Spec . Containers { if container . Name == container Name { container if ! container Exists { for _ , container := range pod . Spec . Init Containers { if container . Name == container Name { container if ! container Exists { response . Write Error ( http . Status Not Found , fmt . Errorf ( " " , container Name , pod if _ , ok := response . Response Writer . ( http . Flusher ) ; ! ok { response . Write Error ( http . Status Internal Server Error , fmt . Errorf ( " " , reflect . Type fw := flushwriter . Wrap ( response . Response if err := s . host . Get Kubelet Container Logs ( ctx , kubecontainer . Get Pod Full Name ( pod ) , container Name , log Options , fw , fw ) ; err != nil { response . Write Error ( http . Status Bad } 
func encode Pods ( pods [ ] * v1 . Pod ) ( data [ ] byte , err error ) { pod List := new ( v1 . Pod for _ , pod := range pods { pod List . Items = append ( pod // TODO: this needs to be parameterized to the kubelet, not hardcoded. Depends on Kubelet // as API server refactor. // TODO: Locked to v1, needs to be made generic codec := legacyscheme . Codecs . Legacy Codec ( schema . Group Version { Group : v1 . Group return runtime . Encode ( codec , pod } 
func ( s * Server ) get Pods ( request * restful . Request , response * restful . Response ) { pods := s . host . Get data , err := encode if err != nil { response . Write Error ( http . Status Internal Server write JSON } 
func ( s * Server ) get Running Pods ( request * restful . Request , response * restful . Response ) { pods , err := s . host . Get Running if err != nil { response . Write Error ( http . Status Internal Server data , err := encode if err != nil { response . Write Error ( http . Status Internal Server write JSON } 
func ( s * Server ) get Logs ( request * restful . Request , response * restful . Response ) { s . host . Serve } 
func ( s * Server ) get Spec ( request * restful . Request , response * restful . Response ) { info , err := s . host . Get Cached Machine if err != nil { response . Write Error ( http . Status Internal Server response . Write } 
func proxy Stream ( w http . Response Writer , r * http . Request , url * url . URL ) { // TODO(random-liu): Set Max Bytes Per Sec to throttle the stream. handler := proxy . New Upgrade Aware Handler ( url , nil /*transport*/ , false /*wrap Transport*/ , true /*upgrade handler . Serve } 
func ( s * Server ) get Attach ( request * restful . Request , response * restful . Response ) { params := get Exec Request stream Opts , err := remotecommandserver . New if err != nil { utilruntime . Handle response . Write Error ( http . Status Bad pod , ok := s . host . Get Pod By Name ( params . pod Namespace , params . pod if ! ok { response . Write Error ( http . Status Not pod Full Name := kubecontainer . Get Pod Full url , err := s . host . Get Attach ( pod Full Name , params . pod UID , params . container Name , * stream if err != nil { streaming . Write Error ( err , response . Response if s . redirect Container Streaming { http . Redirect ( response . Response Writer , request . Request , url . String ( ) , http . Status proxy Stream ( response . Response } 
func ( s * Server ) get Run ( request * restful . Request , response * restful . Response ) { params := get Exec Request pod , ok := s . host . Get Pod By Name ( params . pod Namespace , params . pod if ! ok { response . Write Error ( http . Status Not // For legacy reasons, run uses different query param than exec. params . cmd = strings . Split ( request . Query data , err := s . host . Run In Container ( kubecontainer . Get Pod Full Name ( pod ) , params . pod UID , params . container if err != nil { response . Write Error ( http . Status Internal Server write JSON } 
func write JSON Response ( response * restful . Response , data [ ] byte ) { if data == nil { response . Write Header ( http . Status response . Header ( ) . Set ( restful . HEADER_Content response . Write Header ( http . Status } 
func ( s * Server ) get Port Forward ( request * restful . Request , response * restful . Response ) { params := get Port Forward Request port Forward Options , err := portforward . New if err != nil { utilruntime . Handle response . Write Error ( http . Status Bad pod , ok := s . host . Get Pod By Name ( params . pod Namespace , params . pod if ! ok { response . Write Error ( http . Status Not if len ( params . pod UID ) > 0 && pod . UID != params . pod UID { response . Write Error ( http . Status Not url , err := s . host . Get Port Forward ( pod . Name , pod . Namespace , pod . UID , * port Forward if err != nil { streaming . Write Error ( err , response . Response if s . redirect Container Streaming { http . Redirect ( response . Response Writer , request . Request , url . String ( ) , http . Status proxy Stream ( response . Response } 
func trim URL Path ( path string ) string { parts := strings . Split N ( strings . Trim } 
func ( s * Server ) Serve HTTP ( w http . Response Writer , req * http . Request ) { defer httplog . New Logged ( req , & w ) . Stacktrace When ( httplog . Status Is Not ( http . Status OK , http . Status Found , http . Status Moved Permanently , http . Status Temporary Redirect , http . Status Bad Request , http . Status Not Found , http . Status Switching // monitor http requests var server if s . auth == nil { server } else { server method , path , host := req . Method , trim URL long Running := strconv . Format Bool ( is Long Running servermetrics . HTTP Requests . With Label Values ( method , path , host , server Type , long servermetrics . HTTP Inflight Requests . With Label Values ( method , path , host , server Type , long defer servermetrics . HTTP Inflight Requests . With Label Values ( method , path , host , server Type , long start defer servermetrics . HTTP Requests Duration . With Label Values ( method , path , host , server Type , long Running ) . Observe ( servermetrics . Since In Seconds ( start s . restful Cont . Serve } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Controller Revision ) ( nil ) , ( * apps . Controller Revision ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Controller Revision_To_apps_Controller Revision ( a . ( * v1beta1 . Controller Revision ) , b . ( * apps . Controller if err := s . Add Generated Conversion Func ( ( * apps . Controller Revision ) ( nil ) , ( * v1beta1 . Controller Revision ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Controller Revision_To_v1beta1_Controller Revision ( a . ( * apps . Controller Revision ) , b . ( * v1beta1 . Controller if err := s . Add Generated Conversion Func ( ( * v1beta1 . Controller Revision List ) ( nil ) , ( * apps . Controller Revision List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Controller Revision List_To_apps_Controller Revision List ( a . ( * v1beta1 . Controller Revision List ) , b . ( * apps . Controller Revision if err := s . Add Generated Conversion Func ( ( * apps . Controller Revision List ) ( nil ) , ( * v1beta1 . Controller Revision List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Controller Revision List_To_v1beta1_Controller Revision List ( a . ( * apps . Controller Revision List ) , b . ( * v1beta1 . Controller Revision if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Deployment Condition ) ( nil ) , ( * apps . Deployment Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Condition_To_apps_Deployment Condition ( a . ( * v1beta1 . Deployment Condition ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Condition ) ( nil ) , ( * v1beta1 . Deployment Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Condition_To_v1beta1_Deployment Condition ( a . ( * apps . Deployment Condition ) , b . ( * v1beta1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1beta1 . Deployment List ) ( nil ) , ( * apps . Deployment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment List_To_apps_Deployment List ( a . ( * v1beta1 . Deployment List ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment List ) ( nil ) , ( * v1beta1 . Deployment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment List_To_v1beta1_Deployment List ( a . ( * apps . Deployment List ) , b . ( * v1beta1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1beta1 . Deployment Rollback ) ( nil ) , ( * apps . Deployment Rollback ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Rollback_To_apps_Deployment Rollback ( a . ( * v1beta1 . Deployment Rollback ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Rollback ) ( nil ) , ( * v1beta1 . Deployment Rollback ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Rollback_To_v1beta1_Deployment Rollback ( a . ( * apps . Deployment Rollback ) , b . ( * v1beta1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1beta1 . Deployment Spec ) ( nil ) , ( * apps . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Spec_To_apps_Deployment Spec ( a . ( * v1beta1 . Deployment Spec ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Spec ) ( nil ) , ( * v1beta1 . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Spec_To_v1beta1_Deployment Spec ( a . ( * apps . Deployment Spec ) , b . ( * v1beta1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1beta1 . Deployment Status ) ( nil ) , ( * apps . Deployment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Status_To_apps_Deployment Status ( a . ( * v1beta1 . Deployment Status ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Status ) ( nil ) , ( * v1beta1 . Deployment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Status_To_v1beta1_Deployment Status ( a . ( * apps . Deployment Status ) , b . ( * v1beta1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1beta1 . Deployment Strategy ) ( nil ) , ( * apps . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Strategy_To_apps_Deployment Strategy ( a . ( * v1beta1 . Deployment Strategy ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Strategy ) ( nil ) , ( * v1beta1 . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Strategy_To_v1beta1_Deployment Strategy ( a . ( * apps . Deployment Strategy ) , b . ( * v1beta1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1beta1 . Rollback Config ) ( nil ) , ( * apps . Rollback Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Rollback Config_To_apps_Rollback Config ( a . ( * v1beta1 . Rollback Config ) , b . ( * apps . Rollback if err := s . Add Generated Conversion Func ( ( * apps . Rollback Config ) ( nil ) , ( * v1beta1 . Rollback Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rollback Config_To_v1beta1_Rollback Config ( a . ( * apps . Rollback Config ) , b . ( * v1beta1 . Rollback if err := s . Add Generated Conversion Func ( ( * v1beta1 . Rolling Update Deployment ) ( nil ) , ( * apps . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Rolling Update Deployment_To_apps_Rolling Update Deployment ( a . ( * v1beta1 . Rolling Update Deployment ) , b . ( * apps . Rolling Update if err := s . Add Generated Conversion Func ( ( * apps . Rolling Update Deployment ) ( nil ) , ( * v1beta1 . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Deployment_To_v1beta1_Rolling Update Deployment ( a . ( * apps . Rolling Update Deployment ) , b . ( * v1beta1 . Rolling Update if err := s . Add Generated Conversion Func ( ( * v1beta1 . Rolling Update Stateful Set Strategy ) ( nil ) , ( * apps . Rolling Update Stateful Set Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Rolling Update Stateful Set Strategy_To_apps_Rolling Update Stateful Set Strategy ( a . ( * v1beta1 . Rolling Update Stateful Set Strategy ) , b . ( * apps . Rolling Update Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Rolling Update Stateful Set Strategy ) ( nil ) , ( * v1beta1 . Rolling Update Stateful Set Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Stateful Set Strategy_To_v1beta1_Rolling Update Stateful Set Strategy ( a . ( * apps . Rolling Update Stateful Set Strategy ) , b . ( * v1beta1 . Rolling Update Stateful Set if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Scale Spec ) ( nil ) , ( * autoscaling . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Scale Spec_To_autoscaling_Scale Spec ( a . ( * v1beta1 . Scale Spec ) , b . ( * autoscaling . Scale if err := s . Add Generated Conversion Func ( ( * autoscaling . Scale Spec ) ( nil ) , ( * v1beta1 . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Scale Spec_To_v1beta1_Scale Spec ( a . ( * autoscaling . Scale Spec ) , b . ( * v1beta1 . Scale if err := s . Add Generated Conversion Func ( ( * v1beta1 . Scale Status ) ( nil ) , ( * autoscaling . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Scale Status_To_autoscaling_Scale Status ( a . ( * v1beta1 . Scale Status ) , b . ( * autoscaling . Scale if err := s . Add Generated Conversion Func ( ( * autoscaling . Scale Status ) ( nil ) , ( * v1beta1 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Scale Status_To_v1beta1_Scale Status ( a . ( * autoscaling . Scale Status ) , b . ( * v1beta1 . Scale if err := s . Add Generated Conversion Func ( ( * v1beta1 . Stateful Set ) ( nil ) , ( * apps . Stateful Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set_To_apps_Stateful Set ( a . ( * v1beta1 . Stateful Set ) , b . ( * apps . Stateful if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set ) ( nil ) , ( * v1beta1 . Stateful Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set_To_v1beta1_Stateful Set ( a . ( * apps . Stateful Set ) , b . ( * v1beta1 . Stateful if err := s . Add Generated Conversion Func ( ( * v1beta1 . Stateful Set Condition ) ( nil ) , ( * apps . Stateful Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set Condition_To_apps_Stateful Set Condition ( a . ( * v1beta1 . Stateful Set Condition ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Condition ) ( nil ) , ( * v1beta1 . Stateful Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Condition_To_v1beta1_Stateful Set Condition ( a . ( * apps . Stateful Set Condition ) , b . ( * v1beta1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1beta1 . Stateful Set List ) ( nil ) , ( * apps . Stateful Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set List_To_apps_Stateful Set List ( a . ( * v1beta1 . Stateful Set List ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set List ) ( nil ) , ( * v1beta1 . Stateful Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set List_To_v1beta1_Stateful Set List ( a . ( * apps . Stateful Set List ) , b . ( * v1beta1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1beta1 . Stateful Set Spec ) ( nil ) , ( * apps . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set Spec_To_apps_Stateful Set Spec ( a . ( * v1beta1 . Stateful Set Spec ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Spec ) ( nil ) , ( * v1beta1 . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Spec_To_v1beta1_Stateful Set Spec ( a . ( * apps . Stateful Set Spec ) , b . ( * v1beta1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1beta1 . Stateful Set Status ) ( nil ) , ( * apps . Stateful Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set Status_To_apps_Stateful Set Status ( a . ( * v1beta1 . Stateful Set Status ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Status ) ( nil ) , ( * v1beta1 . Stateful Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Status_To_v1beta1_Stateful Set Status ( a . ( * apps . Stateful Set Status ) , b . ( * v1beta1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1beta1 . Stateful Set Update Strategy ) ( nil ) , ( * apps . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set Update Strategy_To_apps_Stateful Set Update Strategy ( a . ( * v1beta1 . Stateful Set Update Strategy ) , b . ( * apps . Stateful Set Update if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Update Strategy ) ( nil ) , ( * v1beta1 . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Update Strategy_To_v1beta1_Stateful Set Update Strategy ( a . ( * apps . Stateful Set Update Strategy ) , b . ( * v1beta1 . Stateful Set Update if err := s . Add Conversion Func ( ( * apps . Deployment Spec ) ( nil ) , ( * v1beta1 . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Spec_To_v1beta1_Deployment Spec ( a . ( * apps . Deployment Spec ) , b . ( * v1beta1 . Deployment if err := s . Add Conversion Func ( ( * apps . Deployment Strategy ) ( nil ) , ( * v1beta1 . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Strategy_To_v1beta1_Deployment Strategy ( a . ( * apps . Deployment Strategy ) , b . ( * v1beta1 . Deployment if err := s . Add Conversion Func ( ( * apps . Rolling Update Deployment ) ( nil ) , ( * v1beta1 . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Deployment_To_v1beta1_Rolling Update Deployment ( a . ( * apps . Rolling Update Deployment ) , b . ( * v1beta1 . Rolling Update if err := s . Add Conversion Func ( ( * apps . Stateful Set Spec ) ( nil ) , ( * v1beta1 . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Spec_To_v1beta1_Stateful Set Spec ( a . ( * apps . Stateful Set Spec ) , b . ( * v1beta1 . Stateful Set if err := s . Add Conversion Func ( ( * apps . Stateful Set Update Strategy ) ( nil ) , ( * v1beta1 . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Update Strategy_To_v1beta1_Stateful Set Update Strategy ( a . ( * apps . Stateful Set Update Strategy ) , b . ( * v1beta1 . Stateful Set Update if err := s . Add Conversion Func ( ( * autoscaling . Scale Status ) ( nil ) , ( * v1beta1 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Scale Status_To_v1beta1_Scale Status ( a . ( * autoscaling . Scale Status ) , b . ( * v1beta1 . Scale if err := s . Add Conversion Func ( ( * v1beta1 . Deployment Spec ) ( nil ) , ( * apps . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Spec_To_apps_Deployment Spec ( a . ( * v1beta1 . Deployment Spec ) , b . ( * apps . Deployment if err := s . Add Conversion Func ( ( * v1beta1 . Deployment Strategy ) ( nil ) , ( * apps . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Deployment Strategy_To_apps_Deployment Strategy ( a . ( * v1beta1 . Deployment Strategy ) , b . ( * apps . Deployment if err := s . Add Conversion Func ( ( * v1beta1 . Rolling Update Deployment ) ( nil ) , ( * apps . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Rolling Update Deployment_To_apps_Rolling Update Deployment ( a . ( * v1beta1 . Rolling Update Deployment ) , b . ( * apps . Rolling Update if err := s . Add Conversion Func ( ( * v1beta1 . Scale Status ) ( nil ) , ( * autoscaling . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Scale Status_To_autoscaling_Scale Status ( a . ( * v1beta1 . Scale Status ) , b . ( * autoscaling . Scale if err := s . Add Conversion Func ( ( * v1beta1 . Stateful Set Spec ) ( nil ) , ( * apps . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set Spec_To_apps_Stateful Set Spec ( a . ( * v1beta1 . Stateful Set Spec ) , b . ( * apps . Stateful Set if err := s . Add Conversion Func ( ( * v1beta1 . Stateful Set Update Strategy ) ( nil ) , ( * apps . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Stateful Set Update Strategy_To_apps_Stateful Set Update Strategy ( a . ( * v1beta1 . Stateful Set Update Strategy ) , b . ( * apps . Stateful Set Update } 
func Convert_v1beta1_Controller Revision_To_apps_Controller Revision ( in * v1beta1 . Controller Revision , out * apps . Controller Revision , s conversion . Scope ) error { return auto Convert_v1beta1_Controller Revision_To_apps_Controller } 
func Convert_apps_Controller Revision_To_v1beta1_Controller Revision ( in * apps . Controller Revision , out * v1beta1 . Controller Revision , s conversion . Scope ) error { return auto Convert_apps_Controller Revision_To_v1beta1_Controller } 
func Convert_v1beta1_Controller Revision List_To_apps_Controller Revision List ( in * v1beta1 . Controller Revision List , out * apps . Controller Revision List , s conversion . Scope ) error { return auto Convert_v1beta1_Controller Revision List_To_apps_Controller Revision } 
func Convert_apps_Controller Revision List_To_v1beta1_Controller Revision List ( in * apps . Controller Revision List , out * v1beta1 . Controller Revision List , s conversion . Scope ) error { return auto Convert_apps_Controller Revision List_To_v1beta1_Controller Revision } 
func Convert_v1beta1_Deployment_To_apps_Deployment ( in * v1beta1 . Deployment , out * apps . Deployment , s conversion . Scope ) error { return auto } 
func Convert_apps_Deployment_To_v1beta1_Deployment ( in * apps . Deployment , out * v1beta1 . Deployment , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Deployment Condition_To_apps_Deployment Condition ( in * v1beta1 . Deployment Condition , out * apps . Deployment Condition , s conversion . Scope ) error { return auto Convert_v1beta1_Deployment Condition_To_apps_Deployment } 
func Convert_apps_Deployment Condition_To_v1beta1_Deployment Condition ( in * apps . Deployment Condition , out * v1beta1 . Deployment Condition , s conversion . Scope ) error { return auto Convert_apps_Deployment Condition_To_v1beta1_Deployment } 
func Convert_v1beta1_Deployment List_To_apps_Deployment List ( in * v1beta1 . Deployment List , out * apps . Deployment List , s conversion . Scope ) error { return auto Convert_v1beta1_Deployment List_To_apps_Deployment } 
func Convert_apps_Deployment List_To_v1beta1_Deployment List ( in * apps . Deployment List , out * v1beta1 . Deployment List , s conversion . Scope ) error { return auto Convert_apps_Deployment List_To_v1beta1_Deployment } 
func Convert_v1beta1_Deployment Rollback_To_apps_Deployment Rollback ( in * v1beta1 . Deployment Rollback , out * apps . Deployment Rollback , s conversion . Scope ) error { return auto Convert_v1beta1_Deployment Rollback_To_apps_Deployment } 
func Convert_apps_Deployment Rollback_To_v1beta1_Deployment Rollback ( in * apps . Deployment Rollback , out * v1beta1 . Deployment Rollback , s conversion . Scope ) error { return auto Convert_apps_Deployment Rollback_To_v1beta1_Deployment } 
func Convert_v1beta1_Deployment Status_To_apps_Deployment Status ( in * v1beta1 . Deployment Status , out * apps . Deployment Status , s conversion . Scope ) error { return auto Convert_v1beta1_Deployment Status_To_apps_Deployment } 
func Convert_apps_Deployment Status_To_v1beta1_Deployment Status ( in * apps . Deployment Status , out * v1beta1 . Deployment Status , s conversion . Scope ) error { return auto Convert_apps_Deployment Status_To_v1beta1_Deployment } 
func Convert_v1beta1_Rollback Config_To_apps_Rollback Config ( in * v1beta1 . Rollback Config , out * apps . Rollback Config , s conversion . Scope ) error { return auto Convert_v1beta1_Rollback Config_To_apps_Rollback } 
func Convert_apps_Rollback Config_To_v1beta1_Rollback Config ( in * apps . Rollback Config , out * v1beta1 . Rollback Config , s conversion . Scope ) error { return auto Convert_apps_Rollback Config_To_v1beta1_Rollback } 
func Convert_v1beta1_Rolling Update Stateful Set Strategy_To_apps_Rolling Update Stateful Set Strategy ( in * v1beta1 . Rolling Update Stateful Set Strategy , out * apps . Rolling Update Stateful Set Strategy , s conversion . Scope ) error { return auto Convert_v1beta1_Rolling Update Stateful Set Strategy_To_apps_Rolling Update Stateful Set } 
func Convert_apps_Rolling Update Stateful Set Strategy_To_v1beta1_Rolling Update Stateful Set Strategy ( in * apps . Rolling Update Stateful Set Strategy , out * v1beta1 . Rolling Update Stateful Set Strategy , s conversion . Scope ) error { return auto Convert_apps_Rolling Update Stateful Set Strategy_To_v1beta1_Rolling Update Stateful Set } 
func Convert_v1beta1_Scale_To_autoscaling_Scale ( in * v1beta1 . Scale , out * autoscaling . Scale , s conversion . Scope ) error { return auto } 
func Convert_autoscaling_Scale_To_v1beta1_Scale ( in * autoscaling . Scale , out * v1beta1 . Scale , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Scale Spec_To_autoscaling_Scale Spec ( in * v1beta1 . Scale Spec , out * autoscaling . Scale Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Scale Spec_To_autoscaling_Scale } 
func Convert_autoscaling_Scale Spec_To_v1beta1_Scale Spec ( in * autoscaling . Scale Spec , out * v1beta1 . Scale Spec , s conversion . Scope ) error { return auto Convert_autoscaling_Scale Spec_To_v1beta1_Scale } 
func Convert_v1beta1_Stateful Set_To_apps_Stateful Set ( in * v1beta1 . Stateful Set , out * apps . Stateful Set , s conversion . Scope ) error { return auto Convert_v1beta1_Stateful Set_To_apps_Stateful } 
func Convert_apps_Stateful Set_To_v1beta1_Stateful Set ( in * apps . Stateful Set , out * v1beta1 . Stateful Set , s conversion . Scope ) error { return auto Convert_apps_Stateful Set_To_v1beta1_Stateful } 
func Convert_v1beta1_Stateful Set Condition_To_apps_Stateful Set Condition ( in * v1beta1 . Stateful Set Condition , out * apps . Stateful Set Condition , s conversion . Scope ) error { return auto Convert_v1beta1_Stateful Set Condition_To_apps_Stateful Set } 
func Convert_apps_Stateful Set Condition_To_v1beta1_Stateful Set Condition ( in * apps . Stateful Set Condition , out * v1beta1 . Stateful Set Condition , s conversion . Scope ) error { return auto Convert_apps_Stateful Set Condition_To_v1beta1_Stateful Set } 
func Convert_v1beta1_Stateful Set List_To_apps_Stateful Set List ( in * v1beta1 . Stateful Set List , out * apps . Stateful Set List , s conversion . Scope ) error { return auto Convert_v1beta1_Stateful Set List_To_apps_Stateful Set } 
func Convert_apps_Stateful Set List_To_v1beta1_Stateful Set List ( in * apps . Stateful Set List , out * v1beta1 . Stateful Set List , s conversion . Scope ) error { return auto Convert_apps_Stateful Set List_To_v1beta1_Stateful Set } 
func Convert_v1beta1_Stateful Set Status_To_apps_Stateful Set Status ( in * v1beta1 . Stateful Set Status , out * apps . Stateful Set Status , s conversion . Scope ) error { return auto Convert_v1beta1_Stateful Set Status_To_apps_Stateful Set } 
func Convert_apps_Stateful Set Status_To_v1beta1_Stateful Set Status ( in * apps . Stateful Set Status , out * v1beta1 . Stateful Set Status , s conversion . Scope ) error { return auto Convert_apps_Stateful Set Status_To_v1beta1_Stateful Set } 
func Get Namespaces From Pod Affinity Term ( pod * v1 . Pod , pod Affinity Term * v1 . Pod Affinity if len ( pod Affinity } else { names . Insert ( pod Affinity } 
func Pod Matches Terms Namespace And } 
func Nodes Have Same Topology Key ( node A , node B * v1 . Node , topology Key string ) bool { if len ( topology if node A . Labels == nil || node node A Label , ok A := node A . Labels [ topology node B Label , ok B := node B . Labels [ topology // If found label in both nodes, check the label if ok B && ok A { return node A Label == node B } 
func ( tps * Topologies ) Nodes Have Same Topology Key ( node A , node B * v1 . Node , topology Key string ) bool { return Nodes Have Same Topology Key ( node A , node B , topology } 
func Register Metrics ( ) { prometheus . Must Register ( vsphere API prometheus . Must Register ( vsphere API Error prometheus . Must Register ( vsphere Operation prometheus . Must Register ( vsphere Operation Error } 
func Recordv Sphere Metric ( action Name string , request Time time . Time , err error ) { switch action Name { case API Create Volume , API Delete Volume , API Attach Volume , API Detach Volume : recordv Sphere API Metric ( action Name , request default : recordv Sphere Operation Metric ( action Name , request } 
func Record Create Volume Metric ( volume Options * Volume Options , request Time time . Time , err error ) { var action if volume Options . Storage Policy Name != " " { action Name = Operation Create Volume With } else if volume Options . VSAN Storage Profile Data != " " { action Name = Operation Create Volume With Raw VSAN } else { action Name = Operation Create Recordv Sphere Metric ( action Name , request } 
func New REST ( opts Getter generic . REST Options Getter ) ( * REST , * Status REST ) { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & batch . Job { } } , New List Func : func ( ) runtime . Object { return & batch . Job List { } } , Predicate Func : job . Match Job , Default Qualified Resource : batch . Resource ( " " ) , Create Strategy : job . Strategy , Update Strategy : job . Strategy , Delete Strategy : job . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts Getter , Attr Func : job . Get if err := store . Complete With status status Store . Update Strategy = job . Status return & REST { store } , & Status REST { store : & status } 
func Meta To Table Row ( obj runtime . Object , row Fn func ( obj runtime . Object , m metav1 . Object , name , age string ) ( [ ] interface { } , error ) ) ( [ ] metav1beta1 . Table Row , error ) { if meta . Is List Type ( obj ) { rows := make ( [ ] metav1beta1 . Table err := meta . Each List Item ( obj , func ( obj runtime . Object ) error { nested Rows , err := Meta To Table Row ( obj , row rows = append ( rows , nested rows := make ( [ ] metav1beta1 . Table row := metav1beta1 . Table Row { Object : runtime . Raw row . Cells , err = row Fn ( obj , m , m . Get Name ( ) , Convert To Human Readable Date Type ( m . Get Creation } 
func New Generic PLEG ( runtime kubecontainer . Runtime , channel Capacity int , relist Period time . Duration , cache kubecontainer . Cache , clock clock . Clock ) Pod Lifecycle Event Generator { return & Generic PLEG { relist Period : relist Period , runtime : runtime , event Channel : make ( chan * Pod Lifecycle Event , channel Capacity ) , pod Records : make ( pod } 
func ( g * Generic PLEG ) Start ( ) { go wait . Until ( g . relist , g . relist Period , wait . Never } 
func ( g * Generic PLEG ) Healthy ( ) ( bool , error ) { relist Time := g . get Relist if relist Time . Is elapsed := g . clock . Since ( relist if elapsed > relist Threshold { return false , fmt . Errorf ( " " , elapsed , relist } 
func ( g * Generic if last Relist Time := g . get Relist Time ( ) ; ! last Relist Time . Is Zero ( ) { metrics . PLEG Relist Interval . Observe ( metrics . Since In Seconds ( last Relist metrics . Deprecated PLEG Relist Interval . Observe ( metrics . Since In Microseconds ( last Relist defer func ( ) { metrics . PLEG Relist Duration . Observe ( metrics . Since In metrics . Deprecated PLEG Relist Latency . Observe ( metrics . Since In // Get all the pods. pod List , err := g . runtime . Get g . update Relist pods := kubecontainer . Pods ( pod g . pod Records . set // Compare the old and the current pods, and generate events. events By Pod ID := map [ types . UID ] [ ] * Pod Lifecycle for pid := range g . pod Records { old Pod := g . pod Records . get pod := g . pod Records . get // Get all containers in the old and the new pod. all Containers := get Containers From Pods ( old for _ , container := range all Containers { events := compute Events ( old for _ , e := range events { update Events ( events By Pod var needs if g . cache Enabled ( ) { needs // If there are events associated with a pod, we should update the // pod Cache. for pid , events := range events By Pod ID { pod := g . pod Records . get if g . cache Enabled ( ) { // update Cache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated pod Record of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the Pod Status to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err := g . update Cache ( pod , pid ) ; err != nil { // Rely on update Cache calling Get Pod // make sure we try to reinspect the pod during the next relisting needs } else if _ , found := g . pods To Reinspect [ pid ] ; found { // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don't want the reinspection code below to inspect it a second time in // this relist execution) delete ( g . pods To // Update the internal storage and send out the events. g . pod for i := range events { // Filter out events that are not reliable and no other components use yet. if events [ i ] . Type == Container select { case g . event Channel <- events [ i ] : default : metrics . PLEG Discard Events . With Label if g . cache Enabled ( ) { // reinspect any pods that failed inspection during the previous relist if len ( g . pods To for pid , pod := range g . pods To Reinspect { if err := g . update Cache ( pod , pid ) ; err != nil { // Rely on update Cache calling Get Pod needs // Update the cache timestamp. This needs to happen *after* // all pods have been properly updated in the cache. g . cache . Update // make sure we retain the list of pods that need reinspecting the next time relist is called g . pods To Reinspect = needs } 
func ( g * Generic PLEG ) get Pod IP ( pid types . UID , status * kubecontainer . Pod old if err != nil || old for _ , sandbox Status := range status . Sandbox Statuses { // If at least one sandbox is ready, then use this status update's pod IP if sandbox Status . State == runtimeapi . Pod Sandbox if len ( status . Sandbox Statuses ) == 0 { // Without sandboxes (which built-in runtimes like rkt don't report) // look at all the container statuses, and if any containers are // running then use the new pod IP for _ , container Status := range status . Container Statuses { if container Status . State == kubecontainer . Container State Created || container Status . State == kubecontainer . Container State // For pods with no ready containers or sandboxes (like exited pods) // use the old status' pod IP return old } 
} 
func New Handler ( ops ... Operation ) * Handler { operations := sets . New } 
func ( h * Handler ) Wait For Ready ( ) bool { // there is no ready func configured, so we return immediately if h . ready timeout := time . After ( time To Wait For for ! h . ready Func ( ) { select { case <- time . After ( 100 * time . Millisecond ) : case <- timeout : return h . ready } 
func ( r * REST ) Create ( ctx context . Context , obj runtime . Object , create Validation rest . Validate Object Func , options * metav1 . Create Options ) ( runtime . Object , error ) { self SRR , ok := obj . ( * authorizationapi . Self Subject Rules if ! ok { return nil , apierrors . New Bad user , ok := genericapirequest . User if ! ok { return nil , apierrors . New Bad namespace := self if namespace == " " { return nil , apierrors . New Bad if create Validation != nil { if err := create Validation ( obj . Deep Copy resource Info , non Resource Info , incomplete , err := r . rule Resolver . Rules ret := & authorizationapi . Self Subject Rules Review { Status : authorizationapi . Subject Rules Review Status { Resource Rules : get Resource Rules ( resource Info ) , Non Resource Rules : get Non Resource Rules ( non Resource if err != nil { ret . Status . Evaluation } 
func New Plugin ( node Identifier nodeidentifier . Node Identifier ) * node Plugin { return & node Plugin { Handler : admission . New Handler ( admission . Create , admission . Update , admission . Delete ) , node Identifier : node Identifier , features : utilfeature . Default Feature } 
func get Modified Labels ( a , b map [ string ] string ) sets . String { modified := sets . New } 
func ( c * node Plugin ) get Forbidden Create Labels ( modified Labels sets . String ) sets . String { if len ( modified forbidden Labels := sets . New for label := range modified Labels { namespace := get Label // forbid kubelets from setting node-restriction labels if namespace == v1 . Label Namespace Node Restriction || strings . Has Suffix ( namespace , " " + v1 . Label Namespace Node Restriction ) { forbidden return forbidden } 
func ( c * node Plugin ) get Forbidden Update Labels ( modified Labels sets . String ) sets . String { if len ( modified forbidden Labels := sets . New for label := range modified Labels { namespace := get Label // forbid kubelets from setting node-restriction labels if namespace == v1 . Label Namespace Node Restriction || strings . Has Suffix ( namespace , " " + v1 . Label Namespace Node Restriction ) { forbidden // forbid kubelets from setting unknown kubernetes.io and k8s.io labels on update if is Kubernetes Label ( label ) && ! kubeletapis . Is Kubelet Label ( label ) { // TODO: defer to label policy once available forbidden return forbidden } 
func ( t * Taint ) To } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cron Job ) ( nil ) , ( * batch . Cron Job ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cron Job_To_batch_Cron Job ( a . ( * v1beta1 . Cron Job ) , b . ( * batch . Cron if err := s . Add Generated Conversion Func ( ( * batch . Cron Job ) ( nil ) , ( * v1beta1 . Cron Job ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Cron Job_To_v1beta1_Cron Job ( a . ( * batch . Cron Job ) , b . ( * v1beta1 . Cron if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cron Job List ) ( nil ) , ( * batch . Cron Job List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cron Job List_To_batch_Cron Job List ( a . ( * v1beta1 . Cron Job List ) , b . ( * batch . Cron Job if err := s . Add Generated Conversion Func ( ( * batch . Cron Job List ) ( nil ) , ( * v1beta1 . Cron Job List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Cron Job List_To_v1beta1_Cron Job List ( a . ( * batch . Cron Job List ) , b . ( * v1beta1 . Cron Job if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cron Job Spec ) ( nil ) , ( * batch . Cron Job Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cron Job Spec_To_batch_Cron Job Spec ( a . ( * v1beta1 . Cron Job Spec ) , b . ( * batch . Cron Job if err := s . Add Generated Conversion Func ( ( * batch . Cron Job Spec ) ( nil ) , ( * v1beta1 . Cron Job Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Cron Job Spec_To_v1beta1_Cron Job Spec ( a . ( * batch . Cron Job Spec ) , b . ( * v1beta1 . Cron Job if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cron Job Status ) ( nil ) , ( * batch . Cron Job Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cron Job Status_To_batch_Cron Job Status ( a . ( * v1beta1 . Cron Job Status ) , b . ( * batch . Cron Job if err := s . Add Generated Conversion Func ( ( * batch . Cron Job Status ) ( nil ) , ( * v1beta1 . Cron Job Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Cron Job Status_To_v1beta1_Cron Job Status ( a . ( * batch . Cron Job Status ) , b . ( * v1beta1 . Cron Job if err := s . Add Generated Conversion Func ( ( * v1beta1 . Job Template ) ( nil ) , ( * batch . Job Template ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Job Template_To_batch_Job Template ( a . ( * v1beta1 . Job Template ) , b . ( * batch . Job if err := s . Add Generated Conversion Func ( ( * batch . Job Template ) ( nil ) , ( * v1beta1 . Job Template ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job Template_To_v1beta1_Job Template ( a . ( * batch . Job Template ) , b . ( * v1beta1 . Job if err := s . Add Generated Conversion Func ( ( * v1beta1 . Job Template Spec ) ( nil ) , ( * batch . Job Template Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Job Template Spec_To_batch_Job Template Spec ( a . ( * v1beta1 . Job Template Spec ) , b . ( * batch . Job Template if err := s . Add Generated Conversion Func ( ( * batch . Job Template Spec ) ( nil ) , ( * v1beta1 . Job Template Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job Template Spec_To_v1beta1_Job Template Spec ( a . ( * batch . Job Template Spec ) , b . ( * v1beta1 . Job Template } 
func Convert_v1beta1_Cron Job_To_batch_Cron Job ( in * v1beta1 . Cron Job , out * batch . Cron Job , s conversion . Scope ) error { return auto Convert_v1beta1_Cron Job_To_batch_Cron } 
func Convert_batch_Cron Job_To_v1beta1_Cron Job ( in * batch . Cron Job , out * v1beta1 . Cron Job , s conversion . Scope ) error { return auto Convert_batch_Cron Job_To_v1beta1_Cron } 
func Convert_v1beta1_Cron Job List_To_batch_Cron Job List ( in * v1beta1 . Cron Job List , out * batch . Cron Job List , s conversion . Scope ) error { return auto Convert_v1beta1_Cron Job List_To_batch_Cron Job } 
func Convert_batch_Cron Job List_To_v1beta1_Cron Job List ( in * batch . Cron Job List , out * v1beta1 . Cron Job List , s conversion . Scope ) error { return auto Convert_batch_Cron Job List_To_v1beta1_Cron Job } 
func Convert_v1beta1_Cron Job Spec_To_batch_Cron Job Spec ( in * v1beta1 . Cron Job Spec , out * batch . Cron Job Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Cron Job Spec_To_batch_Cron Job } 
func Convert_batch_Cron Job Spec_To_v1beta1_Cron Job Spec ( in * batch . Cron Job Spec , out * v1beta1 . Cron Job Spec , s conversion . Scope ) error { return auto Convert_batch_Cron Job Spec_To_v1beta1_Cron Job } 
func Convert_v1beta1_Cron Job Status_To_batch_Cron Job Status ( in * v1beta1 . Cron Job Status , out * batch . Cron Job Status , s conversion . Scope ) error { return auto Convert_v1beta1_Cron Job Status_To_batch_Cron Job } 
func Convert_batch_Cron Job Status_To_v1beta1_Cron Job Status ( in * batch . Cron Job Status , out * v1beta1 . Cron Job Status , s conversion . Scope ) error { return auto Convert_batch_Cron Job Status_To_v1beta1_Cron Job } 
func Convert_v1beta1_Job Template_To_batch_Job Template ( in * v1beta1 . Job Template , out * batch . Job Template , s conversion . Scope ) error { return auto Convert_v1beta1_Job Template_To_batch_Job } 
func Convert_batch_Job Template_To_v1beta1_Job Template ( in * batch . Job Template , out * v1beta1 . Job Template , s conversion . Scope ) error { return auto Convert_batch_Job Template_To_v1beta1_Job } 
func Convert_v1beta1_Job Template Spec_To_batch_Job Template Spec ( in * v1beta1 . Job Template Spec , out * batch . Job Template Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Job Template Spec_To_batch_Job Template } 
func Convert_batch_Job Template Spec_To_v1beta1_Job Template Spec ( in * batch . Job Template Spec , out * v1beta1 . Job Template Spec , s conversion . Scope ) error { return auto Convert_batch_Job Template Spec_To_v1beta1_Job Template } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . CSR Signing Controller Configuration ) ( nil ) , ( * config . CSR Signing Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_CSR Signing Controller Configuration_To_config_CSR Signing Controller Configuration ( a . ( * v1alpha1 . CSR Signing Controller Configuration ) , b . ( * config . CSR Signing Controller if err := s . Add Generated Conversion Func ( ( * config . CSR Signing Controller Configuration ) ( nil ) , ( * v1alpha1 . CSR Signing Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_CSR Signing Controller Configuration_To_v1alpha1_CSR Signing Controller Configuration ( a . ( * config . CSR Signing Controller Configuration ) , b . ( * v1alpha1 . CSR Signing Controller if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Conversion Func ( ( * config . CSR Signing Controller Configuration ) ( nil ) , ( * v1alpha1 . CSR Signing Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_CSR Signing Controller Configuration_To_v1alpha1_CSR Signing Controller Configuration ( a . ( * config . CSR Signing Controller Configuration ) , b . ( * v1alpha1 . CSR Signing Controller if err := s . Add Conversion Func ( ( * v1alpha1 . CSR Signing Controller Configuration ) ( nil ) , ( * config . CSR Signing Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_CSR Signing Controller Configuration_To_config_CSR Signing Controller Configuration ( a . ( * v1alpha1 . CSR Signing Controller Configuration ) , b . ( * config . CSR Signing Controller } 
func ( c * Scheduling V1alpha1Client ) REST return c . rest } 
func ( c * sio client , err := sio . New Client With Args ( c . gateway , " " , c . insecure , c . certs if _ , err = c . client . Authenticate ( & sio . Config // retrieve system if c . system , err = c . find System ( c . sys Name ) ; err != nil { klog . Error ( log ( " " , c . sys // retrieve protection domain if c . protection Domain , err = c . find Protection Domain ( c . pd Name ) ; err != nil { klog . Error ( log ( " " , c . protection // retrieve storage pool if c . storage Pool , err = c . find Storage Pool ( c . sp Name ) ; err != nil { klog . Error ( log ( " " , c . storage } 
func ( c * sio Client ) Attach Volume ( id sio Volume ID , multiple params := & siotypes . Map Volume Sdc Param { Sdc ID : iid , Allow Multiple Mappings : strconv . Format Bool ( multiple Mappings ) , All vol Client := sio . New vol if err := vol Client . Map Volume } 
func ( c * sio Client ) Detach Volume ( id sio Volume params := & siotypes . Unmap Volume Sdc Param { Sdc ID : " " , Ignore Scsi Initiators : " " , All vol Client := sio . New vol if err := vol Client . Unmap Volume } 
func ( c * sio Client ) Delete Volume ( id sio Volume vol Client := sio . New vol if err := vol Client . Remove } 
func ( c * sio // if instance ID not set, retrieve it if c . instance ID == " " { guid , err := c . get sdc , err := c . sys Client . Find c . instance klog . V ( 4 ) . Info ( log ( " " , c . instance return c . instance } 
func ( c * sio Client ) get GUID ( ) ( string , error ) { if c . sdc cmd := c . get Sdc c . sdc GUID = strings . Trim return c . sdc } 
func ( c * sio Client ) get Sio Disk Paths ( ) ( [ ] os . File Info , error ) { files , err := ioutil . Read Dir ( sio Disk ID if err != nil { if os . Is Not Exist ( err ) { // sio Disk ID Path may not exist yet which is fine return [ ] os . File klog . Error ( log ( " " , sio Disk ID result := [ ] os . File for _ , file := range files { if c . disk Regex . Match } 
func ( c * sio Client ) Get Volume Refs ( vol ID sio Volume ID ) ( refs int , err error ) { files , err := c . get Sio Disk for _ , file := range files { if strings . Contains ( file . Name ( ) , string ( vol } 
func ( c * sio Client ) Devs ( ) ( map [ string ] string , error ) { volume files , err := c . get Sio Disk for _ , f := range files { // split emc-vol-<mdm ID>-<volume ID> to pull out volume volume dev Path , err := filepath . Eval Symlinks ( fmt . Sprintf ( " " , sio Disk ID // map volume ID to device Path volume Map [ volume ID ] = dev return volume } 
func ( c * sio Client ) Wait For Detached // wait for attach.Token to show up in local device list ticker := time . New timer := time . New for { select { case <- ticker . C : dev // cant find vol id, then ok. if _ , ok := dev } 
func ( c * sio Client ) find System ( sysname string ) ( sys * siotypes . System , err error ) { if c . sys Client , err = c . client . Find systems , err := c . client . Get } 
func ( o group Resource Overrides ) Apply ( config * storagebackend . Config , options * Storage Codec Config ) { if len ( o . etcd Location ) > 0 { config . Transport . Server List = o . etcd if len ( o . etcd Prefix ) > 0 { config . Prefix = o . etcd if len ( o . media Type ) > 0 { options . Storage Media Type = o . media if o . serializer != nil { options . Storage if o . encoder Decorator Fn != nil { options . Encoder Decorator Fn = o . encoder Decorator if o . decoder Decorator Fn != nil { options . Decoder Decorator Fn = o . decoder Decorator if o . disable } 
func ( s * Default Storage Factory ) Set Disable API List Chunking ( group Resource schema . Group Resource ) { overrides := s . Overrides [ group overrides . disable s . Overrides [ group } 
func ( s * Default Storage Factory ) Set Resource Etcd Prefix ( group Resource schema . Group Resource , prefix string ) { overrides := s . Overrides [ group overrides . etcd Resource s . Overrides [ group } 
func ( s * Default Storage Factory ) Add Cohabitating Resources ( group Resources ... schema . Group Resource ) { for _ , group Resource := range group Resources { overrides := s . Overrides [ group overrides . cohabitating Resources = group s . Overrides [ group } 
func ( s * Default Storage Factory ) New Config ( group Resource schema . Group Resource ) ( * storagebackend . Config , error ) { chosen Storage Resource := s . get Storage Group Resource ( group // operate on copy storage Config := s . Storage codec Config := Storage Codec Config { Storage Media Type : s . Default Media Type , Storage Serializer : s . Default if override , ok := s . Overrides [ get All Resources Alias ( chosen Storage Resource ) ] ; ok { override . Apply ( & storage Config , & codec if override , ok := s . Overrides [ chosen Storage Resource ] ; ok { override . Apply ( & storage Config , & codec codec Config . Storage Version , err = s . Resource Encoding Config . Storage Encoding For ( chosen Storage codec Config . Memory Version , err = s . Resource Encoding Config . In Memory Encoding For ( group codec Config . Config = storage storage Config . Codec , storage Config . Encode Versioner , err = s . new Storage Codec Fn ( codec klog . V ( 3 ) . Infof ( " " , group Resource , codec Config . Storage Version , codec Config . Memory Version , codec return & storage } 
func ( s * Default Storage Factory ) Backends ( ) [ ] Backend { servers := sets . New String ( s . Storage Config . Transport . Server for _ , overrides := range s . Overrides { servers . Insert ( overrides . etcd tls Config := & tls . Config { Insecure Skip if len ( s . Storage Config . Transport . Cert File ) > 0 && len ( s . Storage Config . Transport . Key File ) > 0 { cert , err := tls . Load X509Key Pair ( s . Storage Config . Transport . Cert File , s . Storage Config . Transport . Key } else { tls if len ( s . Storage Config . Transport . CA File ) > 0 { if ca Cert , err := ioutil . Read File ( s . Storage Config . Transport . CA } else { ca Pool := x509 . New Cert ca Pool . Append Certs From PEM ( ca tls Config . Root C As = ca tls Config . Insecure Skip for server := range servers { backends = append ( backends , Backend { Server : server , // We can't share TLS Config across different backends to avoid races. // For more details see: http://pr.k8s.io/59338 TLS Config : tls } 
func Setup Signal Handler ( ) <- chan struct { } { close ( only One Signal shutdown signal . Notify ( shutdown Handler , shutdown go func ( ) { <- shutdown <- shutdown } 
func ( c * c SI Nodes ) List ( opts v1 . List Options ) ( result * v1beta1 . CSI Node if opts . Timeout Seconds != nil { timeout = time . Duration ( * opts . Timeout result = & v1beta1 . CSI Node err = c . client . Get ( ) . Resource ( " " ) . Versioned Params ( & opts , scheme . Parameter } 
func ( c * c SI Nodes ) Create ( c SI Node * v1beta1 . CSI Node ) ( result * v1beta1 . CSI Node , err error ) { result = & v1beta1 . CSI err = c . client . Post ( ) . Resource ( " " ) . Body ( c SI } 
func ( c * c SI Nodes ) Update ( c SI Node * v1beta1 . CSI Node ) ( result * v1beta1 . CSI Node , err error ) { result = & v1beta1 . CSI err = c . client . Put ( ) . Resource ( " " ) . Name ( c SI Node . Name ) . Body ( c SI } 
func ( c * c SI Nodes ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . CSI Node , err error ) { result = & v1beta1 . CSI err = c . client . Patch ( pt ) . Resource ( " " ) . Sub } 
func ( in * Allowed CSI Driver ) Deep Copy ( ) * Allowed CSI out := new ( Allowed CSI in . Deep Copy } 
func ( in * Allowed Flex Volume ) Deep Copy ( ) * Allowed Flex out := new ( Allowed Flex in . Deep Copy } 
func ( in * Allowed Host Path ) Deep Copy ( ) * Allowed Host out := new ( Allowed Host in . Deep Copy } 
func ( in * Eviction ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object if in . Delete Options != nil { in , out := & in . Delete Options , & out . Delete * out = new ( v1 . Delete ( * in ) . Deep Copy } 
func ( in * Eviction ) Deep in . Deep Copy } 
func ( in * Eviction ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * FS Group Strategy Options ) Deep Copy ( ) * FS Group Strategy out := new ( FS Group Strategy in . Deep Copy } 
func ( in * Host Port Range ) Deep Copy ( ) * Host Port out := new ( Host Port in . Deep Copy } 
func ( in * ID Range ) Deep Copy ( ) * ID out := new ( ID in . Deep Copy } 
func ( in * Pod Disruption Budget ) Deep Copy Into ( out * Pod Disruption out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Pod Disruption Budget ) Deep Copy ( ) * Pod Disruption out := new ( Pod Disruption in . Deep Copy } 
func ( in * Pod Disruption Budget ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Disruption Budget List ) Deep Copy Into ( out * Pod Disruption Budget out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Pod Disruption for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Disruption Budget List ) Deep Copy ( ) * Pod Disruption Budget out := new ( Pod Disruption Budget in . Deep Copy } 
func ( in * Pod Disruption Budget List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Disruption Budget Spec ) Deep Copy Into ( out * Pod Disruption Budget if in . Min Available != nil { in , out := & in . Min Available , & out . Min * out = new ( intstr . Int Or * out = new ( v1 . Label ( * in ) . Deep Copy if in . Max Unavailable != nil { in , out := & in . Max Unavailable , & out . Max * out = new ( intstr . Int Or } 
func ( in * Pod Disruption Budget Spec ) Deep Copy ( ) * Pod Disruption Budget out := new ( Pod Disruption Budget in . Deep Copy } 
func ( in * Pod Disruption Budget Status ) Deep Copy Into ( out * Pod Disruption Budget if in . Disrupted Pods != nil { in , out := & in . Disrupted Pods , & out . Disrupted for key , val := range * in { ( * out ) [ key ] = * val . Deep } 
func ( in * Pod Disruption Budget Status ) Deep Copy ( ) * Pod Disruption Budget out := new ( Pod Disruption Budget in . Deep Copy } 
func ( in * Pod Security Policy ) Deep Copy Into ( out * Pod Security out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Pod Security Policy ) Deep Copy ( ) * Pod Security out := new ( Pod Security in . Deep Copy } 
func ( in * Pod Security Policy ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Security Policy List ) Deep Copy Into ( out * Pod Security Policy out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Pod Security for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Security Policy List ) Deep Copy ( ) * Pod Security Policy out := new ( Pod Security Policy in . Deep Copy } 
func ( in * Pod Security Policy List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Security Policy Spec ) Deep Copy ( ) * Pod Security Policy out := new ( Pod Security Policy in . Deep Copy } 
func ( in * Run As Group Strategy Options ) Deep Copy Into ( out * Run As Group Strategy * out = make ( [ ] ID } 
func ( in * Run As Group Strategy Options ) Deep Copy ( ) * Run As Group Strategy out := new ( Run As Group Strategy in . Deep Copy } 
func ( in * Run As User Strategy Options ) Deep Copy ( ) * Run As User Strategy out := new ( Run As User Strategy in . Deep Copy } 
func ( in * Runtime Class Strategy Options ) Deep Copy Into ( out * Runtime Class Strategy if in . Allowed Runtime Class Names != nil { in , out := & in . Allowed Runtime Class Names , & out . Allowed Runtime Class if in . Default Runtime Class Name != nil { in , out := & in . Default Runtime Class Name , & out . Default Runtime Class } 
func ( in * Runtime Class Strategy Options ) Deep Copy ( ) * Runtime Class Strategy out := new ( Runtime Class Strategy in . Deep Copy } 
func ( in * SE Linux Strategy Options ) Deep Copy Into ( out * SE Linux Strategy if in . SE Linux Options != nil { in , out := & in . SE Linux Options , & out . SE Linux * out = new ( core . SE Linux } 
func ( in * SE Linux Strategy Options ) Deep Copy ( ) * SE Linux Strategy out := new ( SE Linux Strategy in . Deep Copy } 
func ( in * Supplemental Groups Strategy Options ) Deep Copy ( ) * Supplemental Groups Strategy out := new ( Supplemental Groups Strategy in . Deep Copy } 
func New Default Reader Protocols ( ) map [ string ] Reader Protocol Config { return map [ string ] Reader Protocol Config { " " : { Binary : true } , binary Web Socket Protocol : { Binary : true } , base64Binary Web Socket } 
func New Reader ( r io . Reader , ping bool , protocols map [ string ] Reader Protocol Config ) * Reader { return & Reader { r : r , err : make ( chan error ) , ping : ping , protocols : protocols , handle Crash : func ( ) { runtime . Handle } 
func ( r * Reader ) handle ( ws * websocket . Conn ) { // Close the connection when the client requests it, or when we finish streaming, whichever happens first close Conn close Conn := func ( ) { close Conn r . selected defer close go func ( ) { defer runtime . Handle // This blocks until the connection is closed. // Client should not send anything. Ignore // Once the client closes, we should also close close r . err <- message Copy ( ws , r . r , ! r . protocols [ r . selected } 
func ( s * API Enablement Options ) Add Flags ( fs * pflag . Flag Set ) { fs . Var ( & s . Runtime } 
func ( s * API Enablement Options ) Validate ( registries ... Group if s . Runtime Config [ " " ] == " " && len ( s . Runtime groups , err := resourceconfig . Parse Groups ( s . Runtime for _ , registry := range registries { // filter out known groups groups = unknown } 
func ( s * API Enablement Options ) Apply To ( c * server . Config , default Resource Config * serverstore . Resource Config , registry resourceconfig . Group Version merged Resource Config , err := resourceconfig . Merge API Resource Configs ( default Resource Config , s . Runtime c . Merged Resource Config = merged Resource } 
func ( h * empty Handle ) Ensure Address Bind ( address , dev } 
func ( h * empty Handle ) Ensure Dummy Device ( dev } 
func ( h * empty Handle ) List Bind Address ( dev } 
func ( h * empty Handle ) Get Local Addresses ( dev , filter } 
func Validate Event Type ( eventtype string ) bool { switch eventtype { case v1 . Event Type Normal , v1 . Event Type } 
func Is Key Not Found Error ( err error ) bool { status Err , _ := err . ( * errors . Status if status Err != nil && status Err . Status ( ) . Code == http . Status Not } 
func ( util * Photon Disk Util ) Create Volume ( p * photon Persistent Disk Provisioner ) ( pd ID string , capacity GB int , fstype string , err error ) { cloud , err := get Cloud Provider ( p . plugin . host . Get Cloud capacity := p . options . PVC . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource // Photon Controller works with Gi B, convert to Gi B with rounding up vol Size GB , err := volumehelpers . Round Up To Gi B name := volumeutil . Generate Volume Name ( p . options . Cluster Name , p . options . PV volume Options := & photon . Volume Options { Capacity GB : vol Size GB , Tags : * p . options . Cloud for parameter , value := range p . options . Parameters { switch strings . To Lower ( parameter ) { case " " : volume case volume . Volume Parameter FS default : klog . Errorf ( " " , parameter , p . plugin . Get Plugin return " " , 0 , " " , fmt . Errorf ( " " , parameter , p . plugin . Get Plugin pd ID , err = cloud . Create Disk ( volume return pd ID , vol Size } 
func ( util * Photon Disk Util ) Delete Volume ( pd * photon Persistent Disk Deleter ) error { cloud , err := get Cloud Provider ( pd . plugin . host . Get Cloud if err = cloud . Delete Disk ( pd . pd ID ) ; err != nil { klog . Errorf ( " " , pd . pd klog . V ( 4 ) . Infof ( " " , pd . pd } 
func ( v * version ) Storage Classes ( ) Storage Class Informer { return & storage Class Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( v * version ) Volume Attachments ( ) Volume Attachment Informer { return & volume Attachment Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( s * Priority Class V1Generator ) Structured Generate ( ) ( runtime . Object , error ) { return & scheduling . Priority Class { Object Meta : metav1 . Object Meta { Name : s . Name , } , Value : s . Value , Global Default : s . Global } 
func Register ( container Cache kubecontainer . Runtime Cache , collectors ... prometheus . Collector ) { // Register the metrics. register Metrics . Do ( func ( ) { prometheus . Must Register ( Node prometheus . Must Register ( Pod Worker prometheus . Must Register ( Pod Start prometheus . Must Register ( Cgroup Manager prometheus . Must Register ( Pod Worker Start prometheus . Must Register ( Containers Per Pod prometheus . Must Register ( new Pod And Container Collector ( container prometheus . Must Register ( PLEG Relist prometheus . Must Register ( PLEG Discard prometheus . Must Register ( PLEG Relist prometheus . Must Register ( Runtime prometheus . Must Register ( Runtime Operations prometheus . Must Register ( Runtime Operations prometheus . Must Register ( Eviction Stats prometheus . Must Register ( Device Plugin Registration prometheus . Must Register ( Device Plugin Allocation prometheus . Must Register ( Deprecated Pod Worker prometheus . Must Register ( Deprecated Pod Start prometheus . Must Register ( Deprecated Cgroup Manager prometheus . Must Register ( Deprecated Pod Worker Start prometheus . Must Register ( Deprecated PLEG Relist prometheus . Must Register ( Deprecated PLEG Relist prometheus . Must Register ( Deprecated Runtime prometheus . Must Register ( Deprecated Runtime Operations prometheus . Must Register ( Deprecated Runtime Operations prometheus . Must Register ( Deprecated Eviction Stats prometheus . Must Register ( Deprecated Device Plugin Registration prometheus . Must Register ( Deprecated Device Plugin Allocation if utilfeature . Default Feature Gate . Enabled ( features . Dynamic Kubelet Config ) { prometheus . Must Register ( Assigned prometheus . Must Register ( Active prometheus . Must Register ( Last Known Good prometheus . Must Register ( Config for _ , collector := range collectors { prometheus . Must } 
func Since In } 
func ( s * component Status Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Component Status , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Component } 
func ( s * component Status Lister ) Get ( name string ) ( * v1 . Component Status , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1 . Component } 
func ( f * Custom Columns Print Flags ) To Printer ( template Format string ) ( printers . Resource Printer , error ) { if len ( template Format ) == 0 { return nil , genericclioptions . No Compatible Printer template if len ( f . Template Argument ) == 0 { for format := range columns if strings . Has Prefix ( template Format , format ) { template Value = template template } else { template Value = f . Template if _ , supported Format := columns Formats [ template Format ] ; ! supported Format { return nil , genericclioptions . No Compatible Printer Error { Output Format : & template Format , Allowed Formats : f . Allowed if len ( template // Universal Decoder call must specify parameter versions; otherwise it will decode to internal versions. decoder := scheme . Codecs . Universal Decoder ( scheme . Scheme . Prioritized Versions All if template Format == " " { file , err := os . Open ( template if err != nil { return nil , fmt . Errorf ( " \n " , template p , err := New Custom Columns Printer From return New Custom Columns Printer From Spec ( template Value , decoder , f . No } 
func random Label Part ( c fuzz . Continue , can Be Empty bool ) string { valid Start End := [ ] char valid Middle := [ ] char part if ! can Be Empty { part runes := make ( [ ] rune , part if part runes [ 0 ] = valid Start End [ c . Rand . Intn ( len ( valid Start for i := range runes [ 1 : ] { runes [ i + 1 ] = valid Middle [ c . Rand . Intn ( len ( valid runes [ len ( runes ) - 1 ] = valid Start End [ c . Rand . Intn ( len ( valid Start } 
func new Pod Disruption Budgets ( c * Policy V1beta1Client , namespace string ) * pod Disruption Budgets { return & pod Disruption Budgets { client : c . REST } 
func ( c * pod Disruption Budgets ) Create ( pod Disruption Budget * v1beta1 . Pod Disruption Budget ) ( result * v1beta1 . Pod Disruption Budget , err error ) { result = & v1beta1 . Pod Disruption err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( pod Disruption } 
func ( c * pod Disruption Budgets ) Update ( pod Disruption Budget * v1beta1 . Pod Disruption Budget ) ( result * v1beta1 . Pod Disruption Budget , err error ) { result = & v1beta1 . Pod Disruption err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( pod Disruption Budget . Name ) . Body ( pod Disruption } 
func ( u * Unstructured List ) Unstructured for i , item := range u . Items { items [ i ] = item . Unstructured } 
func ( obj * Unstructured List ) Set Unstructured unstructured new unstructured Items = append ( unstructured new Items = append ( new obj . Items = unstructured obj . Object [ " " ] = new } 
func ( c * Fake Deployments ) Get ( name string , options v1 . Get Options ) ( result * appsv1 . Deployment , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( deployments } 
func ( c * Fake Deployments ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( deployments } 
func ( c * Fake Deployments ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( deployments Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & appsv1 . Deployment } 
func ( c * Fake Deployments ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * appsv1 . Deployment , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( deployments } 
func ( c * Fake Deployments ) Get Scale ( deployment Name string , options v1 . Get Options ) ( result * autoscalingv1 . Scale , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Subresource Action ( deployments Resource , c . ns , " " , deployment } 
func ( c * Metrics V1alpha1Client ) REST return c . rest } 
func map Based Selector For Object ( object runtime . Object ) ( string , error ) { // TODO: replace with a swagger schema based approach (identify pod selector via schema introspection) switch t := object . ( type ) { case * corev1 . Replication Controller : return generate . Make return generate . Make return generate . Make if t . Spec . Selector != nil { // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match labels = t . Spec . Selector . Match return generate . Make case * appsv1 . Deployment : // "apps" deployments must have the selector set. if t . Spec . Selector == nil || len ( t . Spec . Selector . Match // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match return generate . Make Labels ( t . Spec . Selector . Match case * appsv1beta2 . Deployment : // "apps" deployments must have the selector set. if t . Spec . Selector == nil || len ( t . Spec . Selector . Match // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match return generate . Make Labels ( t . Spec . Selector . Match case * appsv1beta1 . Deployment : // "apps" deployments must have the selector set. if t . Spec . Selector == nil || len ( t . Spec . Selector . Match // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match return generate . Make Labels ( t . Spec . Selector . Match case * extensionsv1beta1 . Replica if t . Spec . Selector != nil { // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match labels = t . Spec . Selector . Match return generate . Make case * appsv1 . Replica Set : // "apps" replicasets must have the selector set. if t . Spec . Selector == nil || len ( t . Spec . Selector . Match // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match return generate . Make Labels ( t . Spec . Selector . Match case * appsv1beta2 . Replica Set : // "apps" replicasets must have the selector set. if t . Spec . Selector == nil || len ( t . Spec . Selector . Match // TODO(madhusudancs): Make this smarter by admitting Match Expressions with Equals // operator, Double Equals operator and In operator with only one element in the set. if len ( t . Spec . Selector . Match Expressions ) > 0 { return " " , fmt . Errorf ( " \" \" " , t . Spec . Selector . Match return generate . Make Labels ( t . Spec . Selector . Match } 
func ( m * model Printer ) Print for i , desc := range append ( m . Descriptions , schema . Get if err := m . Writer . Indent ( description Indent Level ) . Write if empty { return m . Writer . Indent ( description Indent Level ) . Write } 
func ( m * model Printer ) Visit Array ( a * proto . Array ) { m . Descriptions = append ( m . Descriptions , a . Get if m . Type == " " { m . Type = Get Type a . Sub } 
func ( m * model Printer ) Visit Kind ( k * proto . Kind ) { if err := m . Print Kind And if m . Type == " " { m . Type = Get Type if err := m . Print m . Error = m . Builder . Build Fields Printer ( m . Writer . Indent ( field Indent Level ) ) . Print } 
func ( m * model Printer ) Visit Map ( om * proto . Map ) { m . Descriptions = append ( m . Descriptions , om . Get if m . Type == " " { m . Type = Get Type om . Sub } 
func ( m * model Printer ) Visit Primitive ( p * proto . Primitive ) { if err := m . Print Kind And if m . Type == " " { m . Type = Get Type m . Error = m . Print } 
func ( m * model Printer ) Visit Reference ( r proto . Reference ) { m . Descriptions = append ( m . Descriptions , r . Get r . Sub } 
func Print Model ( name string , writer * Formatter , builder fields Printer Builder , schema proto . Schema , gvk schema . Group Version Kind ) error { m := & model } 
func find Cluster I Ds ( tags [ ] * ec2 . Tag ) ( string , string , error ) { legacy Cluster new Cluster for _ , tag := range tags { tag Key := aws . String if strings . Has Prefix ( tag Key , Tag Name Kubernetes Cluster Prefix ) { id := strings . Trim Prefix ( tag Key , Tag Name Kubernetes Cluster if new Cluster ID != " " { return " " , " " , fmt . Errorf ( " " , Tag Name Kubernetes Cluster Prefix , new Cluster new Cluster if tag Key == Tag Name Kubernetes Cluster Legacy { id := aws . String if legacy Cluster ID != " " { return " " , " " , fmt . Errorf ( " " , Tag Name Kubernetes Cluster Legacy , legacy Cluster legacy Cluster return legacy Cluster ID , new Cluster } 
func ( t * aws Tagging ) read Repair Cluster Tags ( client EC2 , resource ID string , lifecycle Resource Lifecycle , additional Tags map [ string ] string , observed Tags [ ] * ec2 . Tag ) error { actual Tag for _ , tag := range observed Tags { actual Tag Map [ aws . String Value ( tag . Key ) ] = aws . String expected Tags := t . build Tags ( lifecycle , additional add for k , expected := range expected Tags { actual := actual Tag if actual == " " { klog . Warningf ( " " , resource add } else { return fmt . Errorf ( " " , resource if len ( add if err := t . create Tags ( client , resource ID , lifecycle , add Tags ) ; err != nil { return fmt . Errorf ( " " , resource } 
func ( t * aws Tagging ) create Tags ( client EC2 , resource ID string , lifecycle Resource Lifecycle , additional Tags map [ string ] string ) error { tags := t . build Tags ( lifecycle , additional var aws aws Tags = append ( aws backoff := wait . Backoff { Duration : create Tag Initial Delay , Factor : create Tag Factor , Steps : create Tag request := & ec2 . Create Tags request . Resources = [ ] * string { & resource request . Tags = aws var last err := wait . Exponential Backoff ( backoff , func ( ) ( bool , error ) { _ , err := client . Create // We could check that the error is retryable, but the error code changes based on what we are tagging // Security Group: Invalid Group.Not last if err == wait . Err Wait Timeout { // return real Create Tags error instead of timeout err = last } 
func ( t * aws Tagging ) add Filters ( filters [ ] * ec2 . Filter ) [ ] * ec2 . Filter { // if there are no cluster ID configured - no filtering by special tag names // should be applied to revert to legacy behaviour. if len ( t . Cluster // For 1.6, we always recognize the legacy tag, for the 1.5 -> 1.6 upgrade // There are no "or" filters by key, so we look for both the legacy and new key, and then we have to post-filter f := new Ec2Filter ( " " , Tag Name Kubernetes Cluster Legacy , t . cluster Tag } 
func New Docker Server ( endpoint string , s dockershim . CRI Service ) * Docker Server { return & Docker } 
func ( s * Docker l , err := util . Create // Create the grpc server and register runtime and image services. s . server = grpc . New Server ( grpc . Max Recv Msg Size ( max Msg Size ) , grpc . Max Send Msg Size ( max Msg runtimeapi . Register Runtime Service runtimeapi . Register Image Service } 
func New Priority Class Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Priority Class Informer ( client , resync } 
func ( c * Fake Pod Metricses ) Get ( name string , options v1 . Get Options ) ( result * v1alpha1 . Pod Metrics , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( podmetricses Resource , c . ns , name ) , & v1alpha1 . Pod return obj . ( * v1alpha1 . Pod } 
func ( c * Fake Pod Metricses ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( podmetricses } 
func ( c * Certificates V1beta1Client ) REST return c . rest } 
func make Global PD Path ( host volume . Volume Host , disk Uri string , is Managed bool ) ( string , error ) { disk Uri = libstrings . To Lower ( disk unique Disk Name hashed Disk Uri := azure . Make CRC32 ( disk if is // "{m for managed b for blob}{hashed disk Uri or Disk Id depending on disk kind }" disk Name := fmt . Sprintf ( unique Disk Name Template , prefix , hashed Disk pd Path := filepath . Join ( host . Get Plugin Dir ( azure Data Disk Plugin Name ) , util . Mounts In Global PD Path , disk return pd } 
func get Disk LUN ( device Info string ) ( int32 , error ) { var disk if len ( device Info ) <= 2 { disk LUN = device } else { // extract the LUN num from a device path matches := lun Path RE . Find String Submatch ( device if len ( matches ) == 2 { disk } else { return - 1 , fmt . Errorf ( " " , device lun , err := strconv . Atoi ( disk } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Container Metrics ) ( nil ) , ( * metrics . Container Metrics ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Container Metrics_To_metrics_Container Metrics ( a . ( * Container Metrics ) , b . ( * metrics . Container if err := s . Add Generated Conversion Func ( ( * metrics . Container Metrics ) ( nil ) , ( * Container Metrics ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_metrics_Container Metrics_To_v1alpha1_Container Metrics ( a . ( * metrics . Container Metrics ) , b . ( * Container if err := s . Add Generated Conversion Func ( ( * Node Metrics ) ( nil ) , ( * metrics . Node Metrics ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Node Metrics_To_metrics_Node Metrics ( a . ( * Node Metrics ) , b . ( * metrics . Node if err := s . Add Generated Conversion Func ( ( * metrics . Node Metrics ) ( nil ) , ( * Node Metrics ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_metrics_Node Metrics_To_v1alpha1_Node Metrics ( a . ( * metrics . Node Metrics ) , b . ( * Node if err := s . Add Generated Conversion Func ( ( * Node Metrics List ) ( nil ) , ( * metrics . Node Metrics List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Node Metrics List_To_metrics_Node Metrics List ( a . ( * Node Metrics List ) , b . ( * metrics . Node Metrics if err := s . Add Generated Conversion Func ( ( * metrics . Node Metrics List ) ( nil ) , ( * Node Metrics List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_metrics_Node Metrics List_To_v1alpha1_Node Metrics List ( a . ( * metrics . Node Metrics List ) , b . ( * Node Metrics if err := s . Add Generated Conversion Func ( ( * Pod Metrics ) ( nil ) , ( * metrics . Pod Metrics ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Pod Metrics_To_metrics_Pod Metrics ( a . ( * Pod Metrics ) , b . ( * metrics . Pod if err := s . Add Generated Conversion Func ( ( * metrics . Pod Metrics ) ( nil ) , ( * Pod Metrics ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_metrics_Pod Metrics_To_v1alpha1_Pod Metrics ( a . ( * metrics . Pod Metrics ) , b . ( * Pod if err := s . Add Generated Conversion Func ( ( * Pod Metrics List ) ( nil ) , ( * metrics . Pod Metrics List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Pod Metrics List_To_metrics_Pod Metrics List ( a . ( * Pod Metrics List ) , b . ( * metrics . Pod Metrics if err := s . Add Generated Conversion Func ( ( * metrics . Pod Metrics List ) ( nil ) , ( * Pod Metrics List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_metrics_Pod Metrics List_To_v1alpha1_Pod Metrics List ( a . ( * metrics . Pod Metrics List ) , b . ( * Pod Metrics } 
func Convert_v1alpha1_Container Metrics_To_metrics_Container Metrics ( in * Container Metrics , out * metrics . Container Metrics , s conversion . Scope ) error { return auto Convert_v1alpha1_Container Metrics_To_metrics_Container } 
func Convert_metrics_Container Metrics_To_v1alpha1_Container Metrics ( in * metrics . Container Metrics , out * Container Metrics , s conversion . Scope ) error { return auto Convert_metrics_Container Metrics_To_v1alpha1_Container } 
func Convert_v1alpha1_Node Metrics_To_metrics_Node Metrics ( in * Node Metrics , out * metrics . Node Metrics , s conversion . Scope ) error { return auto Convert_v1alpha1_Node Metrics_To_metrics_Node } 
func Convert_metrics_Node Metrics_To_v1alpha1_Node Metrics ( in * metrics . Node Metrics , out * Node Metrics , s conversion . Scope ) error { return auto Convert_metrics_Node Metrics_To_v1alpha1_Node } 
func Convert_v1alpha1_Node Metrics List_To_metrics_Node Metrics List ( in * Node Metrics List , out * metrics . Node Metrics List , s conversion . Scope ) error { return auto Convert_v1alpha1_Node Metrics List_To_metrics_Node Metrics } 
func Convert_metrics_Node Metrics List_To_v1alpha1_Node Metrics List ( in * metrics . Node Metrics List , out * Node Metrics List , s conversion . Scope ) error { return auto Convert_metrics_Node Metrics List_To_v1alpha1_Node Metrics } 
func Convert_v1alpha1_Pod Metrics_To_metrics_Pod Metrics ( in * Pod Metrics , out * metrics . Pod Metrics , s conversion . Scope ) error { return auto Convert_v1alpha1_Pod Metrics_To_metrics_Pod } 
func Convert_metrics_Pod Metrics_To_v1alpha1_Pod Metrics ( in * metrics . Pod Metrics , out * Pod Metrics , s conversion . Scope ) error { return auto Convert_metrics_Pod Metrics_To_v1alpha1_Pod } 
func Convert_v1alpha1_Pod Metrics List_To_metrics_Pod Metrics List ( in * Pod Metrics List , out * metrics . Pod Metrics List , s conversion . Scope ) error { return auto Convert_v1alpha1_Pod Metrics List_To_metrics_Pod Metrics } 
func Convert_metrics_Pod Metrics List_To_v1alpha1_Pod Metrics List ( in * metrics . Pod Metrics List , out * Pod Metrics List , s conversion . Scope ) error { return auto Convert_metrics_Pod Metrics List_To_v1alpha1_Pod Metrics } 
func New Routes ( compute * gophercloud . Service Client , network * gophercloud . Service Client , opts Router Opts ) ( cloudprovider . Routes , error ) { if opts . Router ID == " " { return nil , err No Router } 
func ( r * Routes ) List Routes ( ctx context . Context , cluster Name string ) ( [ ] * cloudprovider . Route , error ) { klog . V ( 4 ) . Infof ( " " , cluster node Names By Addr := make ( map [ string ] types . Node err := foreach Server ( r . compute , servers . List Opts { } , func ( srv * servers . Server ) ( bool , error ) { addrs , err := node name := map Server To Node for _ , addr := range addrs { node Names By router , err := routers . Get ( r . network , r . opts . Router for _ , item := range router . Routes { node Name , found Node := node Names By Addr [ item . Next if ! found Node { node Name = types . Node Name ( item . Next route := cloudprovider . Route { Name : item . Destination CIDR , Target Node : node Name , //contains the nexthop address if node was not found Blackhole : ! found Node , Destination CIDR : item . Destination } 
func ( r * Routes ) Create Route ( ctx context . Context , cluster Name string , name Hint string , route * cloudprovider . Route ) error { klog . V ( 4 ) . Infof ( " " , cluster Name , name on Failure := new ip , _ , _ := net . Parse CIDR ( route . Destination is CID addr , err := get Address By Name ( r . compute , route . Target Node , is CID klog . V ( 4 ) . Infof ( " " , addr , route . Target router , err := routers . Get ( r . network , r . opts . Router for _ , item := range routes { if item . Destination CIDR == route . Destination CIDR && item . Next routes = append ( routes , routers . Route { Destination CIDR : route . Destination CIDR , Next unwind , err := update defer on // get the port of addr on target node. port ID , err := get Port ID By IP ( r . compute , route . Target port , err := get Port By ID ( r . network , port for _ , item := range port . Allowed Address Pairs { if item . IP Address == route . Destination if ! found { new Pairs := append ( port . Allowed Address Pairs , neutronports . Address Pair { IP Address : route . Destination unwind , err := update Allowed Address Pairs ( r . network , port , new defer on on } 
func ( r * Routes ) Delete Route ( ctx context . Context , cluster Name string , route * cloudprovider . Route ) error { klog . V ( 4 ) . Infof ( " " , cluster on Failure := new ip , _ , _ := net . Parse CIDR ( route . Destination is CID // Blackhole routes are orphaned and have no counterpart in Open addr , err = get Address By Name ( r . compute , route . Target Node , is CID router , err := routers . Get ( r . network , r . opts . Router for i , item := range routes { if item . Destination CIDR == route . Destination CIDR && ( item . Next Hop == addr || route . Blackhole && item . Next Hop == string ( route . Target unwind , err := update defer on // get the port of addr on target node. port ID , err := get Port ID By IP ( r . compute , route . Target port , err := get Port By ID ( r . network , port addr Pairs := port . Allowed Address for i , item := range addr Pairs { if item . IP Address == route . Destination if index != - 1 { // Delete element `index` addr Pairs [ index ] = addr Pairs [ len ( addr addr Pairs = addr Pairs [ : len ( addr unwind , err := update Allowed Address Pairs ( r . network , port , addr defer on on } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & API Service { } , & API Service } 
func Set Init Dynamic Defaults ( cfg * kubeadmapi . Init Configuration ) error { if err := Set Bootstrap Tokens Dynamic Defaults ( & cfg . Bootstrap if err := Set Node Registration Dynamic Defaults ( & cfg . Node if err := Set API Endpoint Dynamic Defaults ( & cfg . Local API return Set Cluster Dynamic Defaults ( & cfg . Cluster Configuration , cfg . Local API Endpoint . Advertise Address , cfg . Local API Endpoint . Bind } 
func Set Bootstrap Tokens Dynamic Defaults ( cfg * [ ] kubeadmapi . Bootstrap token Str , err := bootstraputil . Generate Bootstrap token , err := kubeadmapi . New Bootstrap Token String ( token } 
func Set Node Registration Dynamic Defaults ( cfg * kubeadmapi . Node Registration Options , Control Plane cfg . Name , err = nodeutil . Get // Only if the slice is nil, we should append the control-plane taint. This allows the user to specify an empty slice for no default control-plane taint if Control Plane Taint && cfg . Taints == nil { cfg . Taints = [ ] v1 . Taint { kubeadmconstants . Control Plane if cfg . CRI Socket == " " { cfg . CRI Socket , err = kubeadmruntime . Detect CRI klog . V ( 1 ) . Infof ( " " , cfg . CRI } 
func Set API Endpoint Dynamic Defaults ( cfg * kubeadmapi . API Endpoint ) error { // validate cfg.API.Advertise Address. address IP := net . Parse IP ( cfg . Advertise if address IP == nil && cfg . Advertise Address != " " { return errors . Errorf ( " \" \" \" \" " , cfg . Advertise // This is the same logic as the API Server uses, except that if no interface is found the address is set to 0.0.0.0, which is invalid and cannot be used // for bootstrapping a cluster. ip , err := Choose API Server Bind Address ( address cfg . Advertise } 
func Set Cluster Dynamic Defaults ( cfg * kubeadmapi . Cluster Configuration , advertise Address string , bind Port int32 ) error { // Default all the embedded Component ip := net . Parse IP ( advertise if ip . To4 ( ) != nil { cfg . Component Configs . Kube Proxy . Bind Address = kubeadmapiv1beta2 . Default Proxy Bind } else { cfg . Component Configs . Kube Proxy . Bind Address = kubeadmapiv1beta2 . Default Proxy Bind // Resolve possible version labels and validate version string if err := Normalize Kubernetes // If Control Plane Endpoint is specified without a port number defaults it to // the bind Port number of the API Endpoint. // This will allow join of additional control plane instances with different bind Port number if cfg . Control Plane Endpoint != " " { host , port , err := kubeadmutil . Parse Host Port ( cfg . Control Plane if port == " " { cfg . Control Plane Endpoint = net . Join Host Port ( host , strconv . Format Int ( int64 ( bind // Downcase SA Ns. Some domain names (like EL Bs) have capitals in them. Lowercase SA Ns ( cfg . API Server . Cert SA } 
func Defaulted Init Configuration ( defaultversionedcfg * kubeadmapiv1beta2 . Init Configuration ) ( * kubeadmapi . Init Configuration , error ) { internalcfg := & kubeadmapi . Init // Applies dynamic defaults to settings not provided with flags if err := Set Init Dynamic // Validates cfg (flags/configs + defaults + dynamic defaults) if err := validation . Validate Init Configuration ( internalcfg ) . To } 
func Load Init Configuration From File ( cfg Path string ) ( * kubeadmapi . Init Configuration , error ) { klog . V ( 1 ) . Infof ( " " , cfg b , err := ioutil . Read File ( cfg if err != nil { return nil , errors . Wrapf ( err , " " , cfg return Bytes To Init } 
func Load Or Default Init Configuration ( cfg Path string , defaultversionedcfg * kubeadmapiv1beta2 . Init Configuration ) ( * kubeadmapi . Init Configuration , error ) { if cfg Path != " " { // Loads configuration from config file, if provided // Nb. --config overrides command line flags return Load Init Configuration From File ( cfg return Defaulted Init } 
func Bytes To Init Configuration ( b [ ] byte ) ( * kubeadmapi . Init Configuration , error ) { gvkmap , err := kubeadmutil . Split YAML return document Map To Init } 
func document Map To Init Configuration ( gvkmap map [ schema . Group Version Kind ] [ ] byte , allow Deprecated bool ) ( * kubeadmapi . Init Configuration , error ) { var initcfg * kubeadmapi . Init var clustercfg * kubeadmapi . Cluster decoded Component Config Objects := map [ componentconfigs . Registration for gvk , file Content := range gvkmap { // first, check if this GVK is supported and possibly not deprecated if err := validate Supported Version ( gvk . Group Version ( ) , allow // verify the validity of the YAML strict . Verify Unmarshal Strict ( file // Try to get the registration for the Component Config based on the kind reg Kind := componentconfigs . Registration if registration , found := componentconfigs . Known [ reg Kind ] ; found { // Unmarshal the bytes from the YAML document into a runtime.Object containing the Component Configuration struct obj , err := registration . Unmarshal ( file decoded Component Config Objects [ reg if kubeadmutil . Group Version Kinds Has Init Configuration ( gvk ) { // Set initcfg to an empty struct value the deserializer will populate initcfg = & kubeadmapi . Init // Decode the bytes into the internal struct. Under the hood, the bytes will be unmarshalled into the // right external version, defaulted, and converted into the internal version. if err := runtime . Decode Into ( kubeadmscheme . Codecs . Universal Decoder ( ) , file if kubeadmutil . Group Version Kinds Has Cluster Configuration ( gvk ) { // Set clustercfg to an empty struct value the deserializer will populate clustercfg = & kubeadmapi . Cluster // Decode the bytes into the internal struct. Under the hood, the bytes will be unmarshalled into the // right external version, defaulted, and converted into the internal version. if err := runtime . Decode Into ( kubeadmscheme . Codecs . Universal Decoder ( ) , file // Enforce that Init Configuration and/or Cluster // If Init Configuration wasn't given, default it by creating an external struct instance, default it and convert into the internal type if initcfg == nil { extinitcfg := & kubeadmapiv1beta2 . Init // Set initcfg to an empty struct value the deserializer will populate initcfg = & kubeadmapi . Init // If Cluster Configuration was given, populate it in the Init Configuration struct if clustercfg != nil { initcfg . Cluster // Save the loaded Component Config objects in the initcfg object for kind , obj := range decoded Component Config Objects { if registration , found := componentconfigs . Known [ kind ] ; found { if ok := registration . Set To Internal Config ( obj , & initcfg . Cluster // Applies dynamic defaults to settings not provided with flags if err := Set Init Dynamic // Validates cfg (flags/configs + defaults + dynamic defaults) if err := validation . Validate Init Configuration ( initcfg ) . To } 
func Marshal Init Configuration To Bytes ( cfg * kubeadmapi . Init Configuration , gv schema . Group Version ) ( [ ] byte , error ) { initbytes , err := kubeadmutil . Marshal To Yaml For all // Exception: If the specified groupversion is targeting the internal type, don't print embedded Cluster Configuration contents // This is mostly used for unit testing. In a real scenario the internal version of the API is never marshalled as-is. if gv . Version != runtime . API Version Internal { clusterbytes , err := Marshal Cluster Configuration To Bytes ( & cfg . Cluster all Files = append ( all return bytes . Join ( all Files , [ ] byte ( kubeadmconstants . YAML Document } 
func Marshal Cluster Configuration To Bytes ( clustercfg * kubeadmapi . Cluster Configuration , gv schema . Group Version ) ( [ ] byte , error ) { clusterbytes , err := kubeadmutil . Marshal To Yaml For all component Config defaultedcfg := defaulted Internal for kind , registration := range componentconfigs . Known { // If the Component Config struct for the current registration is nil, skip it when marshalling realobj , ok := registration . Get From Internal defaultedobj , ok := registration . Get From Internal // Invalid: The caller asked to not print the componentconfigs if defaulted, but default Component // If the real Component Config object differs from the default, print it out. If not, there's no need to print it out, so skip it if ! reflect . Deep Equal ( realobj , defaultedobj ) { content component Config Content [ string ( kind ) ] = content // Sort the Component Config files by kind when marshalling sorted Component Config Files := consistent Order Byte Slice ( component Config all Files = append ( all Files , sorted Component Config return bytes . Join ( all Files , [ ] byte ( kubeadmconstants . YAML Document } 
func consistent Order Byte sorted for _ , key := range keys { sorted Content = append ( sorted return sorted } 
func New Daemon Sets Controller ( daemon Set Informer appsinformers . Daemon Set Informer , history Informer appsinformers . Controller Revision Informer , pod Informer coreinformers . Pod Informer , node Informer coreinformers . Node Informer , kube Client clientset . Interface , failed Pods Backoff * flowcontrol . Backoff , ) ( * Daemon Sets Controller , error ) { event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Client . Core if kube Client != nil && kube Client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { if err := metrics . Register Metric And Track Rate Limiter Usage ( " " , kube Client . Core V1 ( ) . REST Client ( ) . Get Rate dsc := & Daemon Sets Controller { kube Client : kube Client , event Recorder : event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event Source { Component : " " } ) , pod Control : controller . Real Pod Control { Kube Client : kube Client , Recorder : event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event Source { Component : " " } ) , } , cr Control : controller . Real Controller Revision Control { Kube Client : kube Client , } , burst Replicas : Burst Replicas , expectations : controller . New Controller Expectations ( ) , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , suspended Daemon daemon Set Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { ds := obj . ( * apps . Daemon dsc . enqueue Daemon } , Update Func : func ( old , cur interface { } ) { old DS := old . ( * apps . Daemon cur DS := cur . ( * apps . Daemon klog . V ( 4 ) . Infof ( " " , old dsc . enqueue Daemon Set ( cur } , Delete Func : dsc . delete dsc . ds Lister = daemon Set dsc . ds Store Synced = daemon Set Informer . Informer ( ) . Has history Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : dsc . add History , Update Func : dsc . update History , Delete Func : dsc . delete dsc . history Lister = history dsc . history Store Synced = history Informer . Informer ( ) . Has // Watch for creation/deletion of pods. The reason we watch is that we don't want a daemon set to create/delete // more pods until all the effects (expectations) of a daemon set's create/delete have been observed. pod Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : dsc . add Pod , Update Func : dsc . update Pod , Delete Func : dsc . delete dsc . pod Lister = pod // This custom indexer will index pods based on their Node Name which will decrease the amount of pods we need to get in simulate() call. pod Informer . Informer ( ) . Get Indexer ( ) . Add Indexers ( cache . Indexers { " " : index By Pod Node dsc . pod Node Index = pod Informer . Informer ( ) . Get dsc . pod Store Synced = pod Informer . Informer ( ) . Has node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : dsc . add Node , Update Func : dsc . update dsc . node Store Synced = node Informer . Informer ( ) . Has dsc . node Lister = node dsc . sync Handler = dsc . sync Daemon dsc . enqueue Daemon dsc . enqueue Daemon Set Rate Limited = dsc . enqueue Rate dsc . failed Pods Backoff = failed Pods } 
func ( dsc * Daemon Sets Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer dsc . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , dsc . pod Store Synced , dsc . node Store Synced , dsc . history Store Synced , dsc . ds Store for i := 0 ; i < workers ; i ++ { go wait . Until ( dsc . run Worker , time . Second , stop go wait . Until ( dsc . failed Pods Backoff . GC , Backoff GC Interval , stop <- stop } 
func ( dsc * Daemon Sets Controller ) process Next Work Item ( ) bool { ds defer dsc . queue . Done ( ds err := dsc . sync Handler ( ds if err == nil { dsc . queue . Forget ( ds utilruntime . Handle Error ( fmt . Errorf ( " " , ds dsc . queue . Add Rate Limited ( ds } 
func ( dsc * Daemon Sets Controller ) get Daemon Sets For Pod ( pod * v1 . Pod ) [ ] * apps . Daemon Set { sets , err := dsc . ds Lister . Get Pod Daemon if len ( sets ) > 1 { // Controller Ref will ensure we don't do anything crazy, but more than one // item in this list nevertheless constitutes user error. utilruntime . Handle } 
func ( dsc * Daemon Sets Controller ) get Daemon Sets For History ( history * apps . Controller Revision ) [ ] * apps . Daemon Set { daemon Sets , err := dsc . ds Lister . Get History Daemon if err != nil || len ( daemon if len ( daemon Sets ) > 1 { // Controller return daemon } 
func ( dsc * Daemon Sets Controller ) add History ( obj interface { } ) { history := obj . ( * apps . Controller if history . Deletion Timestamp != nil { // On a restart of the controller manager, it's possible for an object to // show up in a state that is already pending deletion. dsc . delete // If it has a Controller Ref, that's all that matters. if controller Ref := metav1 . Get Controller Of ( history ) ; controller Ref != nil { ds := dsc . resolve Controller Ref ( history . Namespace , controller // Otherwise, it's an orphan. Get a list of all matching Daemon Sets and sync // them to see if anyone wants to adopt it. daemon Sets := dsc . get Daemon Sets For if len ( daemon for _ , ds := range daemon Sets { dsc . enqueue Daemon } 
func ( dsc * Daemon Sets Controller ) update History ( old , cur interface { } ) { cur History := cur . ( * apps . Controller old History := old . ( * apps . Controller if cur History . Resource Version == old History . Resource Version { // Periodic resync will send update events for all known Controller cur Controller Ref := metav1 . Get Controller Of ( cur old Controller Ref := metav1 . Get Controller Of ( old controller Ref Changed := ! reflect . Deep Equal ( cur Controller Ref , old Controller if controller Ref Changed && old Controller Ref != nil { // The Controller Ref was changed. Sync the old controller, if any. if ds := dsc . resolve Controller Ref ( old History . Namespace , old Controller Ref ) ; ds != nil { dsc . enqueue Daemon // If it has a Controller Ref, that's all that matters. if cur Controller Ref != nil { ds := dsc . resolve Controller Ref ( cur History . Namespace , cur Controller klog . V ( 4 ) . Infof ( " " , cur dsc . enqueue Daemon // Otherwise, it's an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. label Changed := ! reflect . Deep Equal ( cur History . Labels , old if label Changed || controller Ref Changed { daemon Sets := dsc . get Daemon Sets For History ( cur if len ( daemon klog . V ( 4 ) . Infof ( " " , cur for _ , ds := range daemon Sets { dsc . enqueue Daemon } 
func ( dsc * Daemon Sets Controller ) delete History ( obj interface { } ) { history , ok := obj . ( * apps . Controller // When a delete is dropped, the relist will notice a Controller Revision in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the Controller Revision // changed labels the new Daemon Set will not be woken up till the periodic resync. if ! ok { tombstone , ok := obj . ( cache . Deleted Final State if ! ok { utilruntime . Handle history , ok = tombstone . Obj . ( * apps . Controller if ! ok { utilruntime . Handle controller Ref := metav1 . Get Controller if controller ds := dsc . resolve Controller Ref ( history . Namespace , controller dsc . enqueue Daemon } 
func ( dsc * Daemon Sets Controller ) update Pod ( old , cur interface { } ) { cur old if cur Pod . Resource Version == old Pod . Resource Version { // Periodic resync will send update events for all known pods. // Two different versions of the same pod will always have different R cur Controller Ref := metav1 . Get Controller Of ( cur old Controller Ref := metav1 . Get Controller Of ( old controller Ref Changed := ! reflect . Deep Equal ( cur Controller Ref , old Controller if controller Ref Changed && old Controller Ref != nil { // The Controller Ref was changed. Sync the old controller, if any. if ds := dsc . resolve Controller Ref ( old Pod . Namespace , old Controller Ref ) ; ds != nil { dsc . enqueue Daemon // If it has a Controller Ref, that's all that matters. if cur Controller Ref != nil { ds := dsc . resolve Controller Ref ( cur Pod . Namespace , cur Controller klog . V ( 4 ) . Infof ( " " , cur dsc . enqueue Daemon changed To Ready := ! podutil . Is Pod Ready ( old Pod ) && podutil . Is Pod Ready ( cur // See https://github.com/kubernetes/kubernetes/pull/38076 for more details if changed To Ready && ds . Spec . Min Ready Seconds > 0 { // Add a second to avoid milliseconds skew in Add After. // See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info. dsc . enqueue Daemon Set After ( ds , ( time . Duration ( ds . Spec . Min Ready // Otherwise, it's an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. dss := dsc . get Daemon Sets For Pod ( cur klog . V ( 4 ) . Infof ( " " , cur label Changed := ! reflect . Deep Equal ( cur Pod . Labels , old if label Changed || controller Ref Changed { for _ , ds := range dss { dsc . enqueue Daemon } 
func ( dsc * Daemon Sets Controller ) list Suspended Daemon Pods ( node string ) ( dss [ ] string ) { dsc . suspended Daemon Pods defer dsc . suspended Daemon Pods if _ , found := dsc . suspended Daemon for k := range dsc . suspended Daemon } 
func ( dsc * Daemon Sets Controller ) requeue Suspended Daemon Pods ( node string ) { dss := dsc . list Suspended Daemon for _ , ds Key := range dss { if ns , name , err := cache . Split Meta Namespace Key ( ds Key ) ; err != nil { klog . Errorf ( " " , ds } else if ds , err := dsc . ds Lister . Daemon } else { dsc . enqueue Daemon Set Rate } 
func ( dsc * Daemon Sets Controller ) add Suspended Daemon Pods ( node , ds string ) { dsc . suspended Daemon Pods defer dsc . suspended Daemon Pods if _ , found := dsc . suspended Daemon Pods [ node ] ; ! found { dsc . suspended Daemon Pods [ node ] = sets . New dsc . suspended Daemon } 
func ( dsc * Daemon Sets Controller ) remove Suspended Daemon Pods ( node , ds string ) { dsc . suspended Daemon Pods defer dsc . suspended Daemon Pods if _ , found := dsc . suspended Daemon dsc . suspended Daemon if len ( dsc . suspended Daemon Pods [ node ] ) == 0 { delete ( dsc . suspended Daemon } 
func node In Same Condition ( old [ ] v1 . Node Condition , cur [ ] v1 . Node c1map := map [ v1 . Node Condition Type ] v1 . Condition for _ , c := range old { if c . Status == v1 . Condition for _ , c := range cur { if c . Status != v1 . Condition } 
func ( dsc * Daemon Sets Controller ) get Daemon Pods ( ds * apps . Daemon Set ) ( [ ] * v1 . Pod , error ) { selector , err := metav1 . Label Selector As // List all pods to include those that don't match the selector anymore but // have a Controller Ref pointing to this controller. pods , err := dsc . pod // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing Pods (see #42639). ds Not Deleted := controller . Recheck Deletion Timestamp ( func ( ) ( metav1 . Object , error ) { fresh , err := dsc . kube Client . Apps V1 ( ) . Daemon Sets ( ds . Namespace ) . Get ( ds . Name , metav1 . Get // Use Controller Ref Manager to adopt/orphan as needed. cm := controller . New Pod Controller Ref Manager ( dsc . pod Control , ds , selector , controller Kind , ds Not return cm . Claim } 
func ( dsc * Daemon Sets Controller ) get Nodes To Daemon Pods ( ds * apps . Daemon Set ) ( map [ string ] [ ] * v1 . Pod , error ) { claimed Pods , err := dsc . get Daemon // Group Pods by Node name. node To Daemon for _ , pod := range claimed Pods { node Name , err := util . Get Target Node node To Daemon Pods [ node Name ] = append ( node To Daemon Pods [ node return node To Daemon } 
func ( dsc * Daemon Sets Controller ) resolve Controller Ref ( namespace string , controller Ref * metav1 . Owner Reference ) * apps . Daemon Set { // We can't look up by UID, so look up by Name and then verify UID. // Don't even try to look up by Name if it's the wrong Kind. if controller Ref . Kind != controller ds , err := dsc . ds Lister . Daemon Sets ( namespace ) . Get ( controller if ds . UID != controller Ref . UID { // The controller we found with this Name is not the same one that the // Controller } 
func ( dsc * Daemon Sets Controller ) pods Should Be On Node ( node * v1 . Node , node To Daemon Pods map [ string ] [ ] * v1 . Pod , ds * apps . Daemon Set , ) ( nodes Needing Daemon Pods , pods To Delete [ ] string , failed Pods Observed int , err error ) { want To Run , should Schedule , should Continue Running , err := dsc . node Should Run Daemon daemon Pods , exists := node To Daemon ds Key , _ := cache . Meta Namespace Key dsc . remove Suspended Daemon Pods ( node . Name , ds switch { case want To Run && ! should Schedule : // If daemon pod is supposed to run, but can not be scheduled, add to suspended list. dsc . add Suspended Daemon Pods ( node . Name , ds case should Schedule && ! exists : // If daemon pod is supposed to be running on node, but isn't, create daemon pod. nodes Needing Daemon Pods = append ( nodes Needing Daemon case should Continue Running : // If a daemon pod failed, delete it // If there's non-daemon pods left on this node, we will create it in the next sync loop var daemon Pods for _ , pod := range daemon Pods { if pod . Deletion if pod . Status . Phase == v1 . Pod Failed { failed Pods // This is a critical place where DS is often fighting with kubelet that rejects pods. // We need to avoid hot looping and backoff. backoff Key := failed Pods Backoff now := dsc . failed Pods in Backoff := dsc . failed Pods Backoff . Is In Back Off Since Update ( backoff if in Backoff { delay := dsc . failed Pods Backoff . Get ( backoff dsc . enqueue Daemon Set dsc . failed Pods Backoff . Next ( backoff // Emit an event so that it's discoverable to users. dsc . event Recorder . Eventf ( ds , v1 . Event Type Warning , Failed Daemon Pod pods To Delete = append ( pods To } else { daemon Pods Running = append ( daemon Pods // If daemon pod is supposed to be running on node, but more than 1 daemon pod is running, delete the excess daemon pods. // Sort the daemon pods by creation time, so the oldest is preserved. if len ( daemon Pods Running ) > 1 { sort . Sort ( pod By Creation Timestamp And Phase ( daemon Pods for i := 1 ; i < len ( daemon Pods Running ) ; i ++ { pods To Delete = append ( pods To Delete , daemon Pods case ! should Continue Running && exists : // If daemon pod isn't supposed to run on node, but it is, delete all daemon pods on node. for _ , pod := range daemon Pods { pods To Delete = append ( pods To return nodes Needing Daemon Pods , pods To Delete , failed Pods } 
func ( dsc * Daemon Sets Controller ) manage ( ds * apps . Daemon Set , node List [ ] * v1 . Node , hash string ) error { // Find out the pods which are created for the nodes by Daemon Set. node To Daemon Pods , err := dsc . get Nodes To Daemon // For each node, if the node is running the daemon pod but isn't supposed to, kill the daemon // pod. If the node is supposed to run the daemon pod, but isn't, create the daemon pod on the node. var nodes Needing Daemon Pods , pods To var failed Pods for _ , node := range node List { nodes Needing Daemon Pods On Node , pods To Delete On Node , failed Pods Observed On Node , err := dsc . pods Should Be On Node ( node , node To Daemon nodes Needing Daemon Pods = append ( nodes Needing Daemon Pods , nodes Needing Daemon Pods On pods To Delete = append ( pods To Delete , pods To Delete On failed Pods Observed += failed Pods Observed On // Remove unscheduled pods assigned to not existing nodes when daemonset pods are scheduled by scheduler. // If node doesn't exist then pods are never scheduled and can't be deleted by Pod GC Controller. if utilfeature . Default Feature Gate . Enabled ( features . Schedule Daemon Set Pods ) { pods To Delete = append ( pods To Delete , get Unscheduled Pods Without Node ( node List , node To Daemon // Label new pods using the hash label value of the current history when creating them if err = dsc . sync Nodes ( ds , pods To Delete , nodes Needing Daemon // Throw an error when the daemon pods fail, to use ratelimiter to prevent kill-recreate hot loop if failed Pods Observed > 0 { return fmt . Errorf ( " " , failed Pods } 
func ( dsc * Daemon Sets Controller ) sync Nodes ( ds * apps . Daemon Set , pods To Delete , nodes Needing Daemon Pods [ ] string , hash string ) error { // We need to set expectations before creating/deleting pods to avoid race conditions. ds Key , err := controller . Key create Diff := len ( nodes Needing Daemon delete Diff := len ( pods To if create Diff > dsc . burst Replicas { create Diff = dsc . burst if delete Diff > dsc . burst Replicas { delete Diff = dsc . burst dsc . expectations . Set Expectations ( ds Key , create Diff , delete // error channel to communicate back failures. make the buffer big enough to avoid any blocking err Ch := make ( chan error , create Diff + delete klog . V ( 4 ) . Infof ( " " , ds . Name , nodes Needing Daemon Pods , create create Wait := sync . Wait // If the returned error is not nil we have a parse error. // The controller handles this via the hash. generation , err := util . Get Template template := util . Create Pod // Batch the pod creates. Batch sizes start at Slow Start Initial Batch Size // and double with each successful iteration in a kind of "slow start". // This handles attempts to start large numbers of pods that would // likely all fail with the same error. For example a project with a // low quota that attempts to create a large number of pods will be // prevented from spamming the API service with the pod create requests // after one of its pods fails. Conveniently, this also prevents the // event spam that those failures would generate. batch Size := integer . Int Min ( create Diff , controller . Slow Start Initial Batch for pos := 0 ; create Diff > pos ; batch Size , pos = integer . Int Min ( 2 * batch Size , create Diff - ( pos + batch Size ) ) , pos + batch Size { error Count := len ( err create Wait . Add ( batch for i := pos ; i < pos + batch Size ; i ++ { go func ( ix int ) { defer create pod Template := template . Deep if utilfeature . Default Feature Gate . Enabled ( features . Schedule Daemon Set Pods ) { // The pod's Node Affinity will be updated to make sure the Pod is bound // to the target node by default scheduler. It is safe to do so because there // should be no conflicting node affinity with the target node. pod Template . Spec . Affinity = util . Replace Daemon Set Pod Node Name Node Affinity ( pod Template . Spec . Affinity , nodes Needing Daemon err = dsc . pod Control . Create Pods With Controller Ref ( ds . Namespace , pod Template , ds , metav1 . New Controller Ref ( ds , controller } else { // If pod is scheduled by Daemon Set Controller, set its '.spec.schedule Name'. pod Template . Spec . Scheduler err = dsc . pod Control . Create Pods On Node ( nodes Needing Daemon Pods [ ix ] , ds . Namespace , pod Template , ds , metav1 . New Controller Ref ( ds , controller if err != nil && errors . Is dsc . expectations . Creation Observed ( ds err utilruntime . Handle create // any skipped pods that we never attempted to start shouldn't be expected. skipped Pods := create Diff - batch if error Count < len ( err Ch ) && skipped Pods > 0 { klog . V ( 2 ) . Infof ( " " , skipped for i := 0 ; i < skipped Pods ; i ++ { dsc . expectations . Creation Observed ( ds klog . V ( 4 ) . Infof ( " " , ds . Name , pods To Delete , delete delete Wait := sync . Wait delete Wait . Add ( delete for i := 0 ; i < delete Diff ; i ++ { go func ( ix int ) { defer delete if err := dsc . pod Control . Delete Pod ( ds . Namespace , pods To dsc . expectations . Deletion Observed ( ds err utilruntime . Handle delete close ( err for err := range err return utilerrors . New } 
func ( dsc * Daemon Sets Controller ) node Should Run Daemon Pod ( node * v1 . Node , ds * apps . Daemon Set ) ( want To Run , should Schedule , should Continue Running bool , err error ) { new Pod := New // Because these bools require an && of all their required conditions, we start // with all bools set to true and set a bool to false if a condition is not met. // A bool should probably not be set to true after this line. want To Run , should Schedule , should Continue // If the daemon set specifies a node name, check that it matches with node.Name. if ! ( ds . Spec . Template . Spec . Node Name == " " || ds . Spec . Template . Spec . Node reasons , node Info , err := dsc . simulate ( new if err != nil { klog . Warningf ( " " , node . Name , ds . Object Meta . Namespace , ds . Object // TODO(k82cn): When 'Schedule Daemon Set Pods' upgrade to beta or GA, remove unnecessary check on failure reason, // e.g. Insufficient Resource Error; and simplify "want To Run, should Schedule, should Continue Running" // into one result, e.g. selected Node. var insufficient Resource for _ , r := range reasons { klog . V ( 4 ) . Infof ( " " , node . Name , ds . Object Meta . Namespace , ds . Object Meta . Name , r . Get switch reason := r . ( type ) { case * predicates . Insufficient Resource Error : insufficient Resource case * predicates . Predicate Failure Error : var emit // we try to partition predicates into two partitions here: intentional on the part of the operator and not. switch reason { // intentional case predicates . Err Node Selector Not Match , predicates . Err Pod Not Match Host Name , predicates . Err Node Label Presence Violated , // this one is probably intentional since it's a workaround for not having // pod hard anti affinity. predicates . Err Pod Not Fits Host case predicates . Err Taints Tolerations Not Match : // Daemon Set is expected to respect taints and tolerations fits No Execute , _ , err := predicates . Pod Tolerates Node No Execute Taints ( new Pod , nil , node if ! fits No want To Run , should // unintentional case predicates . Err Disk Conflict , predicates . Err Volume Zone Conflict , predicates . Err Max Volume Count Exceeded , predicates . Err Node Under Memory Pressure , predicates . Err Node Under Disk Pressure : // want To Run and should Continue Running are likely true here. They are // absolutely true at the time of writing the comment. See first comment // of this method. should emit // unexpected case predicates . Err Pod Affinity Not Match , predicates . Err Service Affinity Violated : klog . Warningf ( " " , reason . Get return false , false , false , fmt . Errorf ( " " , reason . Get default : klog . V ( 4 ) . Infof ( " " , reason . Get want To Run , should Schedule , should Continue emit if emit Event { dsc . event Recorder . Eventf ( ds , v1 . Event Type Warning , Failed Placement Reason , " " , node . Object Meta . Name , reason . Get // only emit this event if insufficient resource is the only thing // preventing the daemon pod from scheduling if should Schedule && insufficient Resource Err != nil { dsc . event Recorder . Eventf ( ds , v1 . Event Type Warning , Failed Placement Reason , " " , node . Object Meta . Name , insufficient Resource should } 
func New Pod ( ds * apps . Daemon Set , node Name string ) * v1 . Pod { new Pod := & v1 . Pod { Spec : ds . Spec . Template . Spec , Object Meta : ds . Spec . Template . Object new new Pod . Spec . Node Name = node // Added default tolerations for Daemon Set pods. util . Add Or Update Daemon Pod Tolerations ( & new return new } 
func Predicates ( pod * v1 . Pod , node Info * schedulernodeinfo . Node Info ) ( bool , [ ] predicates . Predicate Failure Reason , error ) { var predicate Fails [ ] predicates . Predicate Failure // If Schedule Daemon Set Pods is enabled, only check node Selector, node Affinity and toleration/taint match. if utilfeature . Default Feature Gate . Enabled ( features . Schedule Daemon Set Pods ) { fit , reasons , err := check Node Fitness ( pod , nil , node if err != nil { return false , predicate if ! fit { predicate Fails = append ( predicate return len ( predicate Fails ) == 0 , predicate critical := kubelettypes . Is Critical fit , reasons , err := predicates . Pod Tolerates Node Taints ( pod , nil , node if err != nil { return false , predicate if ! fit { predicate Fails = append ( predicate if critical { // If the pod is marked as critical and support for critical pod annotations is enabled, // check predicates for critical pods only. fit , reasons , err = predicates . Essential Predicates ( pod , nil , node } else { fit , reasons , err = predicates . General Predicates ( pod , nil , node if err != nil { return false , predicate if ! fit { predicate Fails = append ( predicate return len ( predicate Fails ) == 0 , predicate } 
func get Unscheduled Pods Without Node ( running Nodes List [ ] * v1 . Node , node To Daemon is Node for _ , node := range running Nodes List { is Node for n , pods := range node To Daemon Pods { if ! is Node Running [ n ] { for _ , pod := range pods { if len ( pod . Spec . Node } 
func Should Isolated By Hyper V ( annotations map [ string ] string ) bool { if ! utilfeature . Default Feature Gate . Enabled ( features . Hyper V v , ok := annotations [ Hyperv Isolation Annotation return ok && v == Hyperv Isolation } 
func New Options ( req * http . Request ) ( * Options , error ) { tty := req . Form Value ( api . Exec TTY stdin := req . Form Value ( api . Exec Stdin stdout := req . Form Value ( api . Exec Stdout stderr := req . Form Value ( api . Exec Stderr } 
func v4Write Status Func ( stream io . Writer ) func ( status * apierrors . Status Error ) error { return func ( status * apierrors . Status } 
func With Custom Resync Config ( resync Config map [ v1 . Object ] time . Duration ) Shared Informer Option { return func ( factory * shared Informer Factory ) * shared Informer Factory { for k , v := range resync Config { factory . custom Resync [ reflect . Type } 
func With Tweak List Options ( tweak List Options internalinterfaces . Tweak List Options Func ) Shared Informer Option { return func ( factory * shared Informer Factory ) * shared Informer Factory { factory . tweak List Options = tweak List } 
func With Namespace ( namespace string ) Shared Informer Option { return func ( factory * shared Informer Factory ) * shared Informer } 
func New Filtered Shared Informer Factory ( client versioned . Interface , default Resync time . Duration , namespace string , tweak List Options internalinterfaces . Tweak List Options Func ) Shared Informer Factory { return New Shared Informer Factory With Options ( client , default Resync , With Namespace ( namespace ) , With Tweak List Options ( tweak List } 
func New Shared Informer Factory With Options ( client versioned . Interface , default Resync time . Duration , options ... Shared Informer Option ) Shared Informer Factory { factory := & shared Informer Factory { client : client , namespace : v1 . Namespace All , default Resync : default Resync , informers : make ( map [ reflect . Type ] cache . Shared Index Informer ) , started Informers : make ( map [ reflect . Type ] bool ) , custom } 
func ( f * shared Informer Factory ) Start ( stop for informer Type , informer := range f . informers { if ! f . started Informers [ informer Type ] { go informer . Run ( stop f . started Informers [ informer } 
func ( f * shared Informer Factory ) Wait For Cache Sync ( stop Ch <- chan struct { } ) map [ reflect . Type ] bool { informers := func ( ) map [ reflect . Type ] cache . Shared Index informers := map [ reflect . Type ] cache . Shared Index for informer Type , informer := range f . informers { if f . started Informers [ informer Type ] { informers [ informer for inform Type , informer := range informers { res [ inform Type ] = cache . Wait For Cache Sync ( stop Ch , informer . Has } 
func ( f * shared Informer Factory ) Informer For ( obj runtime . Object , new Func internalinterfaces . New Informer Func ) cache . Shared Index informer Type := reflect . Type informer , exists := f . informers [ informer resync Period , exists := f . custom Resync [ informer if ! exists { resync Period = f . default informer = new Func ( f . client , resync f . informers [ informer } 
func ( r * fake Resource Result ) Visit ( fn resource . Visitor } 
func Register All Admission label . Register ( plugins ) // DEPRECATED, future P } 
func Default Off Admission Plugins ( ) sets . String { default On Plugins := sets . New String ( lifecycle . Plugin Name , //Namespace Lifecycle limitranger . Plugin Name , //Limit Ranger serviceaccount . Plugin Name , //Service Account setdefault . Plugin Name , //Default Storage Class resize . Plugin Name , //Persistent Volume Claim Resize defaulttolerationseconds . Plugin Name , //Default Toleration Seconds mutatingwebhook . Plugin Name , //Mutating Admission Webhook validatingwebhook . Plugin Name , //Validating Admission Webhook resourcequota . Plugin Name , //Resource Quota storageobjectinuseprotection . Plugin Name , //Storage Object In Use if utilfeature . Default Feature Gate . Enabled ( features . Pod Priority ) { default On Plugins . Insert ( podpriority . Plugin Name ) //Pod if utilfeature . Default Feature Gate . Enabled ( features . Taint Nodes By Condition ) { default On Plugins . Insert ( nodetaint . Plugin Name ) //Taint Nodes By return sets . New String ( All Ordered Plugins ... ) . Difference ( default On } 
func New Command Start Aggregator ( defaults * Aggregator Options , stop cmd := & cobra . Command { Short : " " , Long : " " , Run if err := o . Run Aggregator ( stop o . Add } 
func ( o * Aggregator Options ) Add Flags ( fs * pflag . Flag Set ) { o . Recommended Options . Add o . API Enablement . Add fs . String Var ( & o . Proxy Client Cert File , " " , o . Proxy Client Cert fs . String Var ( & o . Proxy Client Key File , " " , o . Proxy Client Key } 
func New Default Options ( out , err io . Writer ) * Aggregator Options { o := & Aggregator Options { Recommended Options : genericoptions . New Recommended Options ( default Etcd Path Prefix , aggregatorscheme . Codecs . Legacy Codec ( v1beta1 . Scheme Group Version ) , genericoptions . New Process Info ( " " , " " ) , ) , API Enablement : genericoptions . New API Enablement Options ( ) , Std Out : out , Std } 
func ( o Aggregator errors = append ( errors , o . Recommended errors = append ( errors , o . API return utilerrors . New } 
func ( o Aggregator Options ) Run Aggregator ( stop Ch <- chan struct { } ) error { // TODO have a "real" external address if err := o . Recommended Options . Secure Serving . Maybe Default With Self Signed server Config := genericapiserver . New Recommended if err := o . Recommended Options . Apply To ( server if err := o . API Enablement . Apply To ( & server Config . Config , apiserver . Default API Resource Config server Config . Long Running Func = filters . Basic Long Running Request Check ( sets . New String ( " " , " " ) , sets . New service Resolver := apiserver . New Cluster IP Service Resolver ( server Config . Shared Informer config := apiserver . Config { Generic Config : server Config , Extra Config : apiserver . Extra Config { Service Resolver : service config . Extra Config . Proxy Client Cert , err = ioutil . Read File ( o . Proxy Client Cert config . Extra Config . Proxy Client Key , err = ioutil . Read File ( o . Proxy Client Key server , err := config . Complete ( ) . New With Delegate ( genericapiserver . New Empty return server . Generic API Server . Prepare Run ( ) . Run ( stop } 
func ( c * Fake Persistent Volume Claims ) Get ( name string , options v1 . Get Options ) ( result * corev1 . Persistent Volume Claim , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( persistentvolumeclaims Resource , c . ns , name ) , & corev1 . Persistent Volume return obj . ( * corev1 . Persistent Volume } 
func ( c * Fake Persistent Volume Claims ) List ( opts v1 . List Options ) ( result * corev1 . Persistent Volume Claim List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( persistentvolumeclaims Resource , persistentvolumeclaims Kind , c . ns , opts ) , & corev1 . Persistent Volume Claim label , _ , _ := testing . Extract From List list := & corev1 . Persistent Volume Claim List { List Meta : obj . ( * corev1 . Persistent Volume Claim List ) . List for _ , item := range obj . ( * corev1 . Persistent Volume Claim } 
func ( c * Fake Persistent Volume Claims ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( persistentvolumeclaims } 
func ( c * Fake Persistent Volume Claims ) Create ( persistent Volume Claim * corev1 . Persistent Volume Claim ) ( result * corev1 . Persistent Volume Claim , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( persistentvolumeclaims Resource , c . ns , persistent Volume Claim ) , & corev1 . Persistent Volume return obj . ( * corev1 . Persistent Volume } 
func ( c * Fake Persistent Volume Claims ) Update ( persistent Volume Claim * corev1 . Persistent Volume Claim ) ( result * corev1 . Persistent Volume Claim , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( persistentvolumeclaims Resource , c . ns , persistent Volume Claim ) , & corev1 . Persistent Volume return obj . ( * corev1 . Persistent Volume } 
func ( c * Fake Persistent Volume Claims ) Update Status ( persistent Volume Claim * corev1 . Persistent Volume Claim ) ( * corev1 . Persistent Volume Claim , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( persistentvolumeclaims Resource , " " , c . ns , persistent Volume Claim ) , & corev1 . Persistent Volume return obj . ( * corev1 . Persistent Volume } 
func ( c * Fake Persistent Volume Claims ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( persistentvolumeclaims Resource , c . ns , name ) , & corev1 . Persistent Volume } 
func ( c * Fake Persistent Volume Claims ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( persistentvolumeclaims Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & corev1 . Persistent Volume Claim } 
func ( c * Fake Persistent Volume Claims ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * corev1 . Persistent Volume Claim , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( persistentvolumeclaims Resource , c . ns , name , pt , data , subresources ... ) , & corev1 . Persistent Volume return obj . ( * corev1 . Persistent Volume } 
func ( s * replication Controller Lister ) Get Pod Controllers ( pod * v1 . Pod ) ( [ ] * v1 . Replication items , err := s . Replication var controllers [ ] * v1 . Replication selector := labels . Set ( rc . Spec . Selector ) . As Selector Pre } 
func ( t * Mutable } 
func New Prefix Transformers ( err error , transformers ... Prefix return & prefix } 
func ( t * prefix Transformers ) Transform From Storage ( data [ ] byte , context Context ) ( [ ] byte , bool , error ) { for i , transformer := range t . transformers { if bytes . Has Prefix ( data , transformer . Prefix ) { result , stale , err := transformer . Transformer . Transform From } 
func ( t * prefix Transformers ) Transform To prefixed copy ( prefixed result , err := transformer . Transformer . Transform To prefixed Data = append ( prefixed return prefixed } 
func New Cmd Rollout Pause ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := & Pause Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , IO valid cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : pause Long , Example : pause Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( o . Run } , Valid Args : valid o . Print Flags . Add cmdutil . Add Filename Option Flags ( cmd , & o . Filename } 
func ( o * Pause Options ) Run Pause ( ) error { r := o . Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . Filename Param ( o . Enforce Namespace , & o . Filename Options ) . Resource Type Or Name Args ( true , o . Resources ... ) . Continue On all if err != nil { // restore previous command behavior where // an error caused by retrieving infos due to // at least a single broken object did not result // in an immediate return, but rather an overall // aggregation of errors. all Errs = append ( all for _ , patch := range set . Calculate Patches ( infos , scheme . Default JSON Encoder ( ) , set . Patch if patch . Err != nil { resource if len ( info . Mapping . Resource . Group ) > 0 { resource String = resource all Errs = append ( all Errs , fmt . Errorf ( " " , resource if string ( patch . Patch ) == " " || len ( patch . Patch ) == 0 { printer , err := o . To if err != nil { all Errs = append ( all if err = printer . Print Obj ( info . Object , o . Out ) ; err != nil { all Errs = append ( all obj , err := resource . New Helper ( info . Client , info . Mapping ) . Patch ( info . Namespace , info . Name , types . Strategic Merge Patch if err != nil { all Errs = append ( all printer , err := o . To if err != nil { all Errs = append ( all if err = printer . Print Obj ( info . Object , o . Out ) ; err != nil { all Errs = append ( all return utilerrors . New Aggregate ( all } 
func Try Connect Endpoints ( service proxy . Service Port Name , src Addr net . Addr , protocol string , load Balancer Load Balancer ) ( out net . Conn , err error ) { session Affinity for _ , dial Timeout := range Endpoint Dial Timeouts { endpoint , err := load Balancer . Next Endpoint ( service , src Addr , session Affinity // TODO: This could spin up a new goroutine to make the outbound connection, // and keep accepting inbound traffic. out Conn , err := net . Dial Timeout ( protocol , endpoint , dial if err != nil { if is Too Many F Ds session Affinity return out } 
func ( udp * udp Proxy Socket ) proxy Client ( cli Addr net . Addr , svr Conn net . Conn , active Clients * Client Cache , timeout time . Duration ) { defer svr for { n , err := svr if err != nil { if ! log err = svr Conn . Set n , err = udp . Write To ( buffer [ 0 : n ] , cli if err != nil { if ! log active delete ( active Clients . Clients , cli active } 
func New For Config Or cs . apiregistration = apiregistrationinternalversion . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . Discovery Client = discovery . New Discovery } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . CSI Driver { } , func ( obj interface { } ) { Set Object Defaults_CSI Driver ( obj . ( * v1beta1 . CSI scheme . Add Type Defaulting Func ( & v1beta1 . CSI Driver List { } , func ( obj interface { } ) { Set Object Defaults_CSI Driver List ( obj . ( * v1beta1 . CSI Driver scheme . Add Type Defaulting Func ( & v1beta1 . Storage Class { } , func ( obj interface { } ) { Set Object Defaults_Storage Class ( obj . ( * v1beta1 . Storage scheme . Add Type Defaulting Func ( & v1beta1 . Storage Class List { } , func ( obj interface { } ) { Set Object Defaults_Storage Class List ( obj . ( * v1beta1 . Storage Class } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1alpha1 . Cluster Role Binding { } , func ( obj interface { } ) { Set Object Defaults_Cluster Role Binding ( obj . ( * v1alpha1 . Cluster Role scheme . Add Type Defaulting Func ( & v1alpha1 . Cluster Role Binding List { } , func ( obj interface { } ) { Set Object Defaults_Cluster Role Binding List ( obj . ( * v1alpha1 . Cluster Role Binding scheme . Add Type Defaulting Func ( & v1alpha1 . Role Binding { } , func ( obj interface { } ) { Set Object Defaults_Role Binding ( obj . ( * v1alpha1 . Role scheme . Add Type Defaulting Func ( & v1alpha1 . Role Binding List { } , func ( obj interface { } ) { Set Object Defaults_Role Binding List ( obj . ( * v1alpha1 . Role Binding } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Attach Detach Controller Configuration ) ( nil ) , ( * config . Attach Detach Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Attach Detach Controller Configuration_To_config_Attach Detach Controller Configuration ( a . ( * v1alpha1 . Attach Detach Controller Configuration ) , b . ( * config . Attach Detach Controller if err := s . Add Generated Conversion Func ( ( * config . Attach Detach Controller Configuration ) ( nil ) , ( * v1alpha1 . Attach Detach Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Attach Detach Controller Configuration_To_v1alpha1_Attach Detach Controller Configuration ( a . ( * config . Attach Detach Controller Configuration ) , b . ( * v1alpha1 . Attach Detach Controller if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Conversion Func ( ( * config . Attach Detach Controller Configuration ) ( nil ) , ( * v1alpha1 . Attach Detach Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Attach Detach Controller Configuration_To_v1alpha1_Attach Detach Controller Configuration ( a . ( * config . Attach Detach Controller Configuration ) , b . ( * v1alpha1 . Attach Detach Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Attach Detach Controller Configuration ) ( nil ) , ( * config . Attach Detach Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Attach Detach Controller Configuration_To_config_Attach Detach Controller Configuration ( a . ( * v1alpha1 . Attach Detach Controller Configuration ) , b . ( * config . Attach Detach Controller } 
func New Filtered Priority Class Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Scheduling V1alpha1 ( ) . Priority } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Scheduling V1alpha1 ( ) . Priority } , } , & schedulingv1alpha1 . Priority Class { } , resync } 
func Packages ( context * generator . Context , arguments * args . Generator Args ) generator . Packages { boilerplate , err := arguments . Load Go for _ , input Dir := range arguments . Input Dirs { pkg := context . Universe . Package ( input internal , err := is if err != nil { klog . V ( 5 ) . Infof ( " " , arguments . Output File Base if internal { klog . V ( 5 ) . Infof ( " \" \" " , arguments . Output File Base register File search Path := path . Join ( args . Default Source Tree ( ) , input Dir , register File if _ , err := os . Stat ( path . Join ( search Path ) ) ; err == nil { klog . V ( 5 ) . Infof ( " " , arguments . Output File Base Name , register File Name , search } else if err != nil && ! os . Is Not Exist ( err ) { klog . Fatalf ( " " , err , register File gv := clientgentypes . Group { path if len ( path gv . Group = clientgentypes . Group ( path Parts [ len ( path gv . Version = clientgentypes . Version ( path Parts [ len ( path // if there is a comment of the form "// +group Name=somegroup" or "// +group Name=somegroup.foo.bar.io", // extract the fully qualified API group name from it and overwrite the group inferred from the package path if override := types . Extract Comment Tags ( " " , pkg . Doc Comments ) [ " " ] ; override != nil { group klog . V ( 5 ) . Infof ( " " , group gv . Group = clientgentypes . Group ( group types To for _ , type Member := range t . Members { if type Member . Name == " " && type Member . Embedded == true { types To Register = append ( types To packages = append ( packages , & generator . Default Package { Package Name : pkg . Name , Package Path : pkg . Path , Header Text : boilerplate , Generator Func : func ( c * generator . Context ) ( generators [ ] generator . Generator ) { return [ ] generator . Generator { & register External Generator { Default Gen : generator . Default Gen { Optional Name : arguments . Output File Base Name , } , gv : gv , types To Generate : types To Register , output Package : pkg . Path , imports : generator . New Import } 
func is } 
func New Endpoint Controller ( pod Informer coreinformers . Pod Informer , service Informer coreinformers . Service Informer , endpoints Informer coreinformers . Endpoints Informer , client clientset . Interface ) * Endpoint Controller { broadcaster := record . New broadcaster . Start broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : client . Core recorder := broadcaster . New Recorder ( scheme . Scheme , v1 . Event if client != nil && client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { metrics . Register Metric And Track Rate Limiter Usage ( " " , client . Core V1 ( ) . REST Client ( ) . Get Rate e := & Endpoint Controller { client : client , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , worker Loop service Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : e . enqueue Service , Update Func : func ( old , cur interface { } ) { e . enqueue } , Delete Func : e . enqueue e . service Lister = service e . services Synced = service Informer . Informer ( ) . Has pod Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : e . add Pod , Update Func : e . update Pod , Delete Func : e . delete e . pod Lister = pod e . pods Synced = pod Informer . Informer ( ) . Has e . endpoints Lister = endpoints e . endpoints Synced = endpoints Informer . Informer ( ) . Has e . trigger Time Tracker = New Trigger Time e . event e . event } 
func ( e * Endpoint Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer e . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , e . pods Synced , e . services Synced , e . endpoints for i := 0 ; i < workers ; i ++ { go wait . Until ( e . worker , e . worker Loop Period , stop go func ( ) { defer utilruntime . Handle e . check Leftover <- stop } 
func ( e * Endpoint Controller ) add services , err := e . get Pod Service if err != nil { utilruntime . Handle } 
func ( e * Endpoint Controller ) update Pod ( old , cur interface { } ) { new old if new Pod . Resource Version == old Pod . Resource Version { // Periodic resync will send update events for all known pods. // Two different versions of the same pod will always have different R pod Changed Flag := pod Changed ( old Pod , new // Check if the pod labels have changed, indicating a possible // change in the service membership labels if ! reflect . Deep Equal ( new Pod . Labels , old Pod . Labels ) || ! host Name And Domain Are Equal ( new Pod , old Pod ) { labels // If both the pod and labels are unchanged, no update is needed if ! pod Changed Flag && ! labels services , err := e . get Pod Service Memberships ( new if err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , new Pod . Namespace , new if labels Changed { old Services , err := e . get Pod Service Memberships ( old if err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , old Pod . Namespace , old services = determine Needed Service Updates ( old Services , services , pod Changed } 
func ( e * Endpoint Controller ) delete Pod ( obj interface { } ) { if _ , ok := obj . ( * v1 . Pod ) ; ok { // Enqueue all the services that the pod used to be a member // of. This happens to be exactly the same thing we do when a // pod is added. e . add // If we reached here it means the pod was deleted but its final state is unrecorded. tombstone , ok := obj . ( cache . Deleted Final State if ! ok { utilruntime . Handle if ! ok { utilruntime . Handle e . add } 
func ( e * Endpoint Controller ) enqueue Service ( obj interface { } ) { key , err := controller . Key if err != nil { utilruntime . Handle } 
func ( e * Endpoint Controller ) check Leftover Endpoints ( ) { list , err := e . endpoints if err != nil { utilruntime . Handle for _ , ep := range list { if _ , ok := ep . Annotations [ resourcelock . Leader Election Record Annotation key , err := controller . Key if err != nil { utilruntime . Handle } 
func ( in * Replication Controller Configuration ) Deep Copy ( ) * Replication Controller out := new ( Replication Controller in . Deep Copy } 
func New REST ( scheme * runtime . Scheme , opts Getter generic . REST Options Getter ) ( * registry . REST , error ) { strategy := New store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & wardle . Fischer { } } , New List Func : func ( ) runtime . Object { return & wardle . Fischer List { } } , Predicate Func : Match Fischer , Default Qualified Resource : wardle . Resource ( " " ) , Create Strategy : strategy , Update Strategy : strategy , Delete options := & generic . Store Options { REST Options : opts Getter , Attr Func : Get if err := store . Complete With } 
func ( s * validating Webhook Configuration Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Validating Webhook Configuration , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Validating Webhook } 
func ( s * validating Webhook Configuration Lister ) Get ( name string ) ( * v1beta1 . Validating Webhook Configuration , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1beta1 . Validating Webhook } 
func ( s * event Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Event , err error ) { err = cache . List } 
func ( s * event Lister ) Events ( namespace string ) Event Namespace Lister { return event Namespace } 
func ( m * kube Generic Runtime Manager ) create Pod Sandbox ( pod * v1 . Pod , attempt uint32 ) ( string , string , error ) { pod Sandbox Config , err := m . generate Pod Sandbox // Create pod logs directory err = m . os Interface . Mkdir All ( pod Sandbox Config . Log runtime if utilfeature . Default Feature Gate . Enabled ( features . Runtime Class ) && m . runtime Class Manager != nil { runtime Handler , err = m . runtime Class Manager . Lookup Runtime Handler ( pod . Spec . Runtime Class if runtime Handler != " " { klog . V ( 2 ) . Infof ( " " , format . Pod ( pod ) , runtime pod Sand Box ID , err := m . runtime Service . Run Pod Sandbox ( pod Sandbox Config , runtime return pod Sand Box } 
func ( m * kube Generic Runtime Manager ) generate Pod Sandbox Config ( pod * v1 . Pod , attempt uint32 ) ( * runtimeapi . Pod Sandbox Config , error ) { // TODO: deprecating podsandbox resource requirements in favor of the pod level cgroup // Refer https://github.com/kubernetes/kubernetes/issues/29871 pod pod Sandbox Config := & runtimeapi . Pod Sandbox Config { Metadata : & runtimeapi . Pod Sandbox Metadata { Name : pod . Name , Namespace : pod . Namespace , Uid : pod UID , Attempt : attempt , } , Labels : new Pod Labels ( pod ) , Annotations : new Pod dns Config , err := m . runtime Helper . Get Pod pod Sandbox Config . Dns Config = dns if ! kubecontainer . Is Host Network Pod ( pod ) { // TODO: Add domain support in new runtime interface hostname , _ , err := m . runtime Helper . Generate Pod Host Name And pod Sandbox log Dir := Build Pod Logs pod Sandbox Config . Log Directory = log port Mappings := [ ] * runtimeapi . Port for _ , c := range pod . Spec . Containers { container Port Mappings := kubecontainer . Make Port for idx := range container Port Mappings { port := container Port host Port := int32 ( port . Host container Port := int32 ( port . Container protocol := to Runtime port Mappings = append ( port Mappings , & runtimeapi . Port Mapping { Host Ip : port . Host IP , Host Port : host Port , Container Port : container if len ( port Mappings ) > 0 { pod Sandbox Config . Port Mappings = port lc , err := m . generate Pod Sandbox Linux pod Sandbox return pod Sandbox } 
func ( m * kube Generic Runtime Manager ) generate Pod Sandbox Linux Config ( pod * v1 . Pod ) ( * runtimeapi . Linux Pod Sandbox Config , error ) { cgroup Parent := m . runtime Helper . Get Pod Cgroup lc := & runtimeapi . Linux Pod Sandbox Config { Cgroup Parent : cgroup Parent , Security Context : & runtimeapi . Linux Sandbox Security Context { Privileged : kubecontainer . Has Privileged Container ( pod ) , Seccomp Profile Path : m . get Seccomp Profile From if utilfeature . Default Feature Gate . Enabled ( features . Sysctls ) { if pod . Spec . Security Context != nil { for _ , c := range pod . Spec . Security if pod . Spec . Security Context != nil { sc := pod . Spec . Security if sc . Run As User != nil { lc . Security Context . Run As User = & runtimeapi . Int64Value { Value : int64 ( * sc . Run As if sc . Run As Group != nil { lc . Security Context . Run As Group = & runtimeapi . Int64Value { Value : int64 ( * sc . Run As lc . Security Context . Namespace Options = namespaces For if sc . FS Group != nil { lc . Security Context . Supplemental Groups = append ( lc . Security Context . Supplemental Groups , int64 ( * sc . FS if groups := m . runtime Helper . Get Extra Supplemental Groups For Pod ( pod ) ; len ( groups ) > 0 { lc . Security Context . Supplemental Groups = append ( lc . Security Context . Supplemental if sc . Supplemental Groups != nil { for _ , sg := range sc . Supplemental Groups { lc . Security Context . Supplemental Groups = append ( lc . Security Context . Supplemental if sc . SE Linux Options != nil { lc . Security Context . Selinux Options = & runtimeapi . SE Linux Option { User : sc . SE Linux Options . User , Role : sc . SE Linux Options . Role , Type : sc . SE Linux Options . Type , Level : sc . SE Linux } 
func ( m * kube Generic Runtime Manager ) get Kubelet Sandboxes ( all bool ) ( [ ] * runtimeapi . Pod Sandbox , error ) { var filter * runtimeapi . Pod Sandbox if ! all { ready State := runtimeapi . Pod Sandbox filter = & runtimeapi . Pod Sandbox Filter { State : & runtimeapi . Pod Sandbox State Value { State : ready resp , err := m . runtime Service . List Pod } 
func ( m * kube Generic Runtime Manager ) determine Pod Sandbox IP ( pod Namespace , pod Name string , pod Sandbox * runtimeapi . Pod Sandbox Status ) string { if pod ip := pod if len ( ip ) != 0 && net . Parse } 
func ( m * kube Generic Runtime Manager ) get Sandbox ID By Pod UID ( pod UID kubetypes . UID , state * runtimeapi . Pod Sandbox State ) ( [ ] string , error ) { filter := & runtimeapi . Pod Sandbox Filter { Label Selector : map [ string ] string { types . Kubernetes Pod UID Label : string ( pod if state != nil { filter . State = & runtimeapi . Pod Sandbox State sandboxes , err := m . runtime Service . List Pod if err != nil { klog . Errorf ( " " , pod // Sort with newest first. sandbox I sort . Sort ( pod Sandbox By for i , s := range sandboxes { sandbox I return sandbox I } 
func ( m * kube Generic Runtime Manager ) Get Port Forward ( pod Name , pod Namespace string , pod UID kubetypes . UID , ports [ ] int32 ) ( * url . URL , error ) { sandbox I Ds , err := m . get Sandbox ID By Pod UID ( pod if err != nil { return nil , fmt . Errorf ( " " , format . Pod Desc ( pod Name , pod Namespace , pod if len ( sandbox I Ds ) == 0 { return nil , fmt . Errorf ( " " , format . Pod Desc ( pod Name , pod Namespace , pod req := & runtimeapi . Port Forward Request { Pod Sandbox Id : sandbox I resp , err := m . runtime Service . Port } 
func ( c * Fake Cron Jobs ) List ( opts v1 . List Options ) ( result * v2alpha1 . Cron Job List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( cronjobs Resource , cronjobs Kind , c . ns , opts ) , & v2alpha1 . Cron Job label , _ , _ := testing . Extract From List list := & v2alpha1 . Cron Job List { List Meta : obj . ( * v2alpha1 . Cron Job List ) . List for _ , item := range obj . ( * v2alpha1 . Cron Job } 
func ( c * Fake Cron Jobs ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( cronjobs } 
func ( c * Fake Cron Jobs ) Create ( cron Job * v2alpha1 . Cron Job ) ( result * v2alpha1 . Cron Job , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( cronjobs Resource , c . ns , cron Job ) , & v2alpha1 . Cron return obj . ( * v2alpha1 . Cron } 
func ( c * Fake Cron Jobs ) Update ( cron Job * v2alpha1 . Cron Job ) ( result * v2alpha1 . Cron Job , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( cronjobs Resource , c . ns , cron Job ) , & v2alpha1 . Cron return obj . ( * v2alpha1 . Cron } 
func ( c * Fake Cron Jobs ) Update Status ( cron Job * v2alpha1 . Cron Job ) ( * v2alpha1 . Cron Job , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( cronjobs Resource , " " , c . ns , cron Job ) , & v2alpha1 . Cron return obj . ( * v2alpha1 . Cron } 
func new Role Bindings ( c * Rbac V1alpha1Client , namespace string ) * role Bindings { return & role Bindings { client : c . REST } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Pod GC Controller Configuration ) ( nil ) , ( * config . Pod GC Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Pod GC Controller Configuration_To_config_Pod GC Controller Configuration ( a . ( * v1alpha1 . Pod GC Controller Configuration ) , b . ( * config . Pod GC Controller if err := s . Add Generated Conversion Func ( ( * config . Pod GC Controller Configuration ) ( nil ) , ( * v1alpha1 . Pod GC Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Pod GC Controller Configuration_To_v1alpha1_Pod GC Controller Configuration ( a . ( * config . Pod GC Controller Configuration ) , b . ( * v1alpha1 . Pod GC Controller if err := s . Add Conversion Func ( ( * config . Pod GC Controller Configuration ) ( nil ) , ( * v1alpha1 . Pod GC Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Pod GC Controller Configuration_To_v1alpha1_Pod GC Controller Configuration ( a . ( * config . Pod GC Controller Configuration ) , b . ( * v1alpha1 . Pod GC Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Pod GC Controller Configuration ) ( nil ) , ( * config . Pod GC Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Pod GC Controller Configuration_To_config_Pod GC Controller Configuration ( a . ( * v1alpha1 . Pod GC Controller Configuration ) , b . ( * config . Pod GC Controller } 
func ( kl * Kubelet ) sync Network Util ( ) { if kl . iptables Masquerade Bit < 0 || kl . iptables Masquerade Bit > 31 { klog . Errorf ( " " , kl . iptables Masquerade if kl . iptables Drop Bit < 0 || kl . iptables Drop Bit > 31 { klog . Errorf ( " " , kl . iptables Drop if kl . iptables Drop Bit == kl . iptables Masquerade Bit { klog . Errorf ( " " , kl . iptables Masquerade Bit , kl . iptables Drop // Setup KUBE-MARK-DROP rules drop Mark := get IP Tables Mark ( kl . iptables Drop if _ , err := kl . ipt Client . Ensure Chain ( utiliptables . Table NAT , Kube Mark Drop Chain ) ; err != nil { klog . Errorf ( " " , utiliptables . Table NAT , Kube Mark Drop if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Append , utiliptables . Table NAT , Kube Mark Drop Chain , " " , " " , " " , drop Mark ) ; err != nil { klog . Errorf ( " " , Kube Mark Drop if _ , err := kl . ipt Client . Ensure Chain ( utiliptables . Table Filter , Kube Firewall Chain ) ; err != nil { klog . Errorf ( " " , utiliptables . Table Filter , Kube Firewall if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Append , utiliptables . Table Filter , Kube Firewall Chain , " " , " " , " " , " " , " " , " " , " " , drop Mark , " " , " " ) ; err != nil { klog . Errorf ( " " , Kube Mark Drop Chain , utiliptables . Table Filter , Kube Firewall if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Prepend , utiliptables . Table Filter , utiliptables . Chain Output , " " , string ( Kube Firewall Chain ) ) ; err != nil { klog . Errorf ( " " , utiliptables . Table Filter , utiliptables . Chain Output , Kube Firewall if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Prepend , utiliptables . Table Filter , utiliptables . Chain Input , " " , string ( Kube Firewall Chain ) ) ; err != nil { klog . Errorf ( " " , utiliptables . Table Filter , utiliptables . Chain Input , Kube Firewall // Setup KUBE-MARK-MASQ rules masquerade Mark := get IP Tables Mark ( kl . iptables Masquerade if _ , err := kl . ipt Client . Ensure Chain ( utiliptables . Table NAT , Kube Mark Masq Chain ) ; err != nil { klog . Errorf ( " " , utiliptables . Table NAT , Kube Mark Masq if _ , err := kl . ipt Client . Ensure Chain ( utiliptables . Table NAT , Kube Postrouting Chain ) ; err != nil { klog . Errorf ( " " , utiliptables . Table NAT , Kube Postrouting if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Append , utiliptables . Table NAT , Kube Mark Masq Chain , " " , " " , " " , masquerade Mark ) ; err != nil { klog . Errorf ( " " , Kube Mark Masq if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Prepend , utiliptables . Table NAT , utiliptables . Chain Postrouting , " " , " " , " " , " " , " " , string ( Kube Postrouting Chain ) ) ; err != nil { klog . Errorf ( " " , utiliptables . Table NAT , utiliptables . Chain Postrouting , Kube Postrouting if _ , err := kl . ipt Client . Ensure Rule ( utiliptables . Append , utiliptables . Table NAT , Kube Postrouting Chain , " " , " " , " " , " " , " " , " " , " " , masquerade Mark , " " , " " ) ; err != nil { klog . Errorf ( " " , Kube Mark Masq Chain , utiliptables . Table NAT , Kube Postrouting } 
func get IP Tables } 
func Image Locality Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { node := node if node == nil { return schedulerapi . Host if priority Meta , ok := meta . ( * priority Metadata ) ; ok { score = calculate Priority ( sum Image Scores ( node Info , pod . Spec . Containers , priority Meta . total Num return schedulerapi . Host } 
func calculate Priority ( sum Scores int64 ) int { if sum Scores < min Threshold { sum Scores = min } else if sum Scores > max Threshold { sum Scores = max return int ( int64 ( schedulerapi . Max Priority ) * ( sum Scores - min Threshold ) / ( max Threshold - min } 
func sum Image Scores ( node Info * schedulernodeinfo . Node Info , containers [ ] v1 . Container , total Num image States := node Info . Image for _ , container := range containers { if state , ok := image States [ normalized Image Name ( container . Image ) ] ; ok { sum += scaled Image Score ( state , total Num } 
func scaled Image Score ( image State * schedulernodeinfo . Image State Summary , total Num Nodes int ) int64 { spread := float64 ( image State . Num Nodes ) / float64 ( total Num return int64 ( float64 ( image } 
func normalized Image Name ( name string ) string { if strings . Last Index ( name , " " ) <= strings . Last Index ( name , " " ) { name = name + " " + parsers . Default Image } 
func New Cloud Node Controller ( node Informer coreinformers . Node Informer , kube Client clientset . Interface , cloud cloudprovider . Interface , node Status Update Frequency time . Duration ) * Cloud Node Controller { event Broadcaster := record . New recorder := event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event event Broadcaster . Start if kube event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Client . Core cnc := & Cloud Node Controller { node Informer : node Informer , kube Client : kube Client , recorder : recorder , cloud : cloud , node Status Update Frequency : node Status Update // Use shared informer to listen to add/update of nodes. Note that any nodes // that exist before node controller starts will show up in the update method cnc . node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : cnc . Add Cloud Node , Update Func : cnc . Update Cloud } 
func ( cnc * Cloud Node Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle // The following loops run communicate with the API Server with a worst case complexity // of O(num_nodes) per cycle. These functions are justified here because these events fire // very infrequently. DO NOT MODIFY this to perform frequent operations. // Start a loop to periodically update the node addresses obtained from the cloud wait . Until ( cnc . Update Node Status , cnc . node Status Update Frequency , stop } 
func ( cnc * Cloud Node Controller ) Update Node if ! ok { utilruntime . Handle nodes , err := cnc . kube Client . Core V1 ( ) . Nodes ( ) . List ( metav1 . List Options { Resource for i := range nodes . Items { cnc . update Node } 
func ( cnc * Cloud Node Controller ) update Node Address ( node * v1 . Node , instances cloudprovider . Instances ) { // Do not process nodes that are still tainted cloud Taint := get Cloud if cloud // Node that isn't present according to the cloud provider shouldn't have its address updated exists , err := ensure Node Exists By Provider node Addresses , err := get Node Addresses By Provider ID Or if len ( node // Check if a hostname address exists in the cloud provided addresses hostname for i := range node Addresses { if node Addresses [ i ] . Type == v1 . Node Host Name { hostname // If hostname was not present in cloud provided addresses, use the hostname // from the existing node (populated by kubelet) if ! hostname Exists { for _ , addr := range node . Status . Addresses { if addr . Type == v1 . Node Host Name { node Addresses = append ( node // If node IP was suggested by user, ensure that // it can be found in the cloud as well (consistent with the behaviour in kubelet) if node IP , ok := ensure Node Provided IP Exists ( node , node Addresses ) ; ok { if node if ! node Addresses Change Detected ( node . Status . Addresses , node new Node := node . Deep new Node . Status . Addresses = node _ , _ , err = nodeutil . Patch Node Status ( cnc . kube Client . Core V1 ( ) , types . Node Name ( node . Name ) , node , new } 
func ( cnc * Cloud Node Controller ) Add Cloud cloud Taint := get Cloud if cloud cnc . initialize } 
func ( cnc * Cloud Node Controller ) initialize if ! ok { utilruntime . Handle err := clientretry . Retry On Conflict ( Update Node Spec Backoff , func ( ) error { // TODO(wlan0): Move this logic to the route controller using the node taint instead of condition // Since there are node taints, do we still need this? // This condition marks the node as unusable until routes are initialized in the cloud provider if cnc . cloud . Provider Name ( ) == " " { if err := nodeutil . Set Node Condition ( cnc . kube Client , types . Node Name ( node . Name ) , v1 . Node Condition { Type : v1 . Node Network Unavailable , Status : v1 . Condition True , Reason : " " , Message : " " , Last Transition cur Node , err := cnc . kube Client . Core V1 ( ) . Nodes ( ) . Get ( node . Name , metav1 . Get if cur Node . Spec . Provider ID == " " { provider ID , err := cloudprovider . Get Instance Provider ID ( context . TODO ( ) , cnc . cloud , types . Node Name ( cur if err == nil { cur Node . Spec . Provider ID = provider } else { // we should attempt to set provider ID on cur Node, but // we can continue if we fail since we will attempt to set // node addresses given the node name in get Node Addresses By Provider ID Or node Addresses , err := get Node Addresses By Provider ID Or Name ( instances , cur // If user provided an IP address, ensure that IP address is found // in the cloud provider before removing the taint on the node if node IP , ok := ensure Node Provided IP Exists ( cur Node , node Addresses ) ; ok { if node if instance Type , err := get Instance Type By Provider ID Or Name ( instances , cur } else if instance Type != " " { klog . V ( 2 ) . Infof ( " " , v1 . Label Instance Type , instance cur Node . Object Meta . Labels [ v1 . Label Instance Type ] = instance if zones , ok := cnc . cloud . Zones ( ) ; ok { zone , err := get Zone By Provider ID Or Name ( zones , cur if zone . Failure Domain != " " { klog . V ( 2 ) . Infof ( " " , v1 . Label Zone Failure Domain , zone . Failure cur Node . Object Meta . Labels [ v1 . Label Zone Failure Domain ] = zone . Failure if zone . Region != " " { klog . V ( 2 ) . Infof ( " " , v1 . Label Zone cur Node . Object Meta . Labels [ v1 . Label Zone cur Node . Spec . Taints = exclude Cloud Taint ( cur _ , err = cnc . kube Client . Core V1 ( ) . Nodes ( ) . Update ( cur // After adding, call Update Node Address to set the Cloud Provider provided IP Addresses // So that users do not see any significant delay in IP addresses being filled into the node cnc . update Node Address ( cur if err != nil { utilruntime . Handle } 
func ensure Node Exists By Provider ID ( instances cloudprovider . Instances , node * v1 . Node ) ( bool , error ) { provider ID := node . Spec . Provider if provider provider ID , err = instances . Instance ID ( context . TODO ( ) , types . Node if err != nil { if err == cloudprovider . Instance Not if provider return instances . Instance Exists By Provider ID ( context . TODO ( ) , provider } 
func get Zone By Provider ID Or Name ( zones cloudprovider . Zones , node * v1 . Node ) ( cloudprovider . Zone , error ) { zone , err := zones . Get Zone By Provider ID ( context . TODO ( ) , node . Spec . Provider if err != nil { provider ID zone , err = zones . Get Zone By Node Name ( context . TODO ( ) , types . Node if err != nil { return cloudprovider . Zone { } , fmt . Errorf ( " " , provider ID } 
func ( s * foo Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Foo , err error ) { err = cache . List } 
func ( s * foo Lister ) Foos ( namespace string ) Foo Namespace Lister { return foo Namespace } 
func ( s foo Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Foo , err error ) { err = cache . List All By } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * API Endpoint ) ( nil ) , ( * kubeadm . API Endpoint ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_API Endpoint_To_kubeadm_API Endpoint ( a . ( * API Endpoint ) , b . ( * kubeadm . API if err := s . Add Generated Conversion Func ( ( * kubeadm . API Endpoint ) ( nil ) , ( * API Endpoint ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_API Endpoint_To_v1beta2_API Endpoint ( a . ( * kubeadm . API Endpoint ) , b . ( * API if err := s . Add Generated Conversion Func ( ( * API Server ) ( nil ) , ( * kubeadm . API Server ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_API Server_To_kubeadm_API Server ( a . ( * API Server ) , b . ( * kubeadm . API if err := s . Add Generated Conversion Func ( ( * kubeadm . API Server ) ( nil ) , ( * API Server ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_API Server_To_v1beta2_API Server ( a . ( * kubeadm . API Server ) , b . ( * API if err := s . Add Generated Conversion Func ( ( * Bootstrap Token ) ( nil ) , ( * kubeadm . Bootstrap Token ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Bootstrap Token_To_kubeadm_Bootstrap Token ( a . ( * Bootstrap Token ) , b . ( * kubeadm . Bootstrap if err := s . Add Generated Conversion Func ( ( * kubeadm . Bootstrap Token ) ( nil ) , ( * Bootstrap Token ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Bootstrap Token_To_v1beta2_Bootstrap Token ( a . ( * kubeadm . Bootstrap Token ) , b . ( * Bootstrap if err := s . Add Generated Conversion Func ( ( * Bootstrap Token Discovery ) ( nil ) , ( * kubeadm . Bootstrap Token Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Bootstrap Token Discovery_To_kubeadm_Bootstrap Token Discovery ( a . ( * Bootstrap Token Discovery ) , b . ( * kubeadm . Bootstrap Token if err := s . Add Generated Conversion Func ( ( * kubeadm . Bootstrap Token Discovery ) ( nil ) , ( * Bootstrap Token Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Bootstrap Token Discovery_To_v1beta2_Bootstrap Token Discovery ( a . ( * kubeadm . Bootstrap Token Discovery ) , b . ( * Bootstrap Token if err := s . Add Generated Conversion Func ( ( * Bootstrap Token String ) ( nil ) , ( * kubeadm . Bootstrap Token String ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Bootstrap Token String_To_kubeadm_Bootstrap Token String ( a . ( * Bootstrap Token String ) , b . ( * kubeadm . Bootstrap Token if err := s . Add Generated Conversion Func ( ( * kubeadm . Bootstrap Token String ) ( nil ) , ( * Bootstrap Token String ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Bootstrap Token String_To_v1beta2_Bootstrap Token String ( a . ( * kubeadm . Bootstrap Token String ) , b . ( * Bootstrap Token if err := s . Add Generated Conversion Func ( ( * Cluster Configuration ) ( nil ) , ( * kubeadm . Cluster Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Cluster Configuration_To_kubeadm_Cluster Configuration ( a . ( * Cluster Configuration ) , b . ( * kubeadm . Cluster if err := s . Add Generated Conversion Func ( ( * kubeadm . Cluster Configuration ) ( nil ) , ( * Cluster Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Cluster Configuration_To_v1beta2_Cluster Configuration ( a . ( * kubeadm . Cluster Configuration ) , b . ( * Cluster if err := s . Add Generated Conversion Func ( ( * Cluster Status ) ( nil ) , ( * kubeadm . Cluster Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Cluster Status_To_kubeadm_Cluster Status ( a . ( * Cluster Status ) , b . ( * kubeadm . Cluster if err := s . Add Generated Conversion Func ( ( * kubeadm . Cluster Status ) ( nil ) , ( * Cluster Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Cluster Status_To_v1beta2_Cluster Status ( a . ( * kubeadm . Cluster Status ) , b . ( * Cluster if err := s . Add Generated Conversion Func ( ( * Control Plane Component ) ( nil ) , ( * kubeadm . Control Plane Component ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Control Plane Component_To_kubeadm_Control Plane Component ( a . ( * Control Plane Component ) , b . ( * kubeadm . Control Plane if err := s . Add Generated Conversion Func ( ( * kubeadm . Control Plane Component ) ( nil ) , ( * Control Plane Component ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Control Plane Component_To_v1beta2_Control Plane Component ( a . ( * kubeadm . Control Plane Component ) , b . ( * Control Plane if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * External Etcd ) ( nil ) , ( * kubeadm . External Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_External Etcd_To_kubeadm_External Etcd ( a . ( * External Etcd ) , b . ( * kubeadm . External if err := s . Add Generated Conversion Func ( ( * kubeadm . External Etcd ) ( nil ) , ( * External Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_External Etcd_To_v1beta2_External Etcd ( a . ( * kubeadm . External Etcd ) , b . ( * External if err := s . Add Generated Conversion Func ( ( * File Discovery ) ( nil ) , ( * kubeadm . File Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_File Discovery_To_kubeadm_File Discovery ( a . ( * File Discovery ) , b . ( * kubeadm . File if err := s . Add Generated Conversion Func ( ( * kubeadm . File Discovery ) ( nil ) , ( * File Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_File Discovery_To_v1beta2_File Discovery ( a . ( * kubeadm . File Discovery ) , b . ( * File if err := s . Add Generated Conversion Func ( ( * Host Path Mount ) ( nil ) , ( * kubeadm . Host Path Mount ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Host Path Mount_To_kubeadm_Host Path Mount ( a . ( * Host Path Mount ) , b . ( * kubeadm . Host Path if err := s . Add Generated Conversion Func ( ( * kubeadm . Host Path Mount ) ( nil ) , ( * Host Path Mount ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Host Path Mount_To_v1beta2_Host Path Mount ( a . ( * kubeadm . Host Path Mount ) , b . ( * Host Path if err := s . Add Generated Conversion Func ( ( * Image Meta ) ( nil ) , ( * kubeadm . Image Meta ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Image Meta_To_kubeadm_Image Meta ( a . ( * Image Meta ) , b . ( * kubeadm . Image if err := s . Add Generated Conversion Func ( ( * kubeadm . Image Meta ) ( nil ) , ( * Image Meta ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Image Meta_To_v1beta2_Image Meta ( a . ( * kubeadm . Image Meta ) , b . ( * Image if err := s . Add Generated Conversion Func ( ( * Init Configuration ) ( nil ) , ( * kubeadm . Init Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Init Configuration_To_kubeadm_Init Configuration ( a . ( * Init Configuration ) , b . ( * kubeadm . Init if err := s . Add Generated Conversion Func ( ( * kubeadm . Init Configuration ) ( nil ) , ( * Init Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Init Configuration_To_v1beta2_Init Configuration ( a . ( * kubeadm . Init Configuration ) , b . ( * Init if err := s . Add Generated Conversion Func ( ( * Join Configuration ) ( nil ) , ( * kubeadm . Join Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Join Configuration_To_kubeadm_Join Configuration ( a . ( * Join Configuration ) , b . ( * kubeadm . Join if err := s . Add Generated Conversion Func ( ( * kubeadm . Join Configuration ) ( nil ) , ( * Join Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Join Configuration_To_v1beta2_Join Configuration ( a . ( * kubeadm . Join Configuration ) , b . ( * Join if err := s . Add Generated Conversion Func ( ( * Join Control Plane ) ( nil ) , ( * kubeadm . Join Control Plane ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Join Control Plane_To_kubeadm_Join Control Plane ( a . ( * Join Control Plane ) , b . ( * kubeadm . Join Control if err := s . Add Generated Conversion Func ( ( * kubeadm . Join Control Plane ) ( nil ) , ( * Join Control Plane ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Join Control Plane_To_v1beta2_Join Control Plane ( a . ( * kubeadm . Join Control Plane ) , b . ( * Join Control if err := s . Add Generated Conversion Func ( ( * Local Etcd ) ( nil ) , ( * kubeadm . Local Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Local Etcd_To_kubeadm_Local Etcd ( a . ( * Local Etcd ) , b . ( * kubeadm . Local if err := s . Add Generated Conversion Func ( ( * kubeadm . Local Etcd ) ( nil ) , ( * Local Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Local Etcd_To_v1beta2_Local Etcd ( a . ( * kubeadm . Local Etcd ) , b . ( * Local if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Node Registration Options ) ( nil ) , ( * kubeadm . Node Registration Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Node Registration Options_To_kubeadm_Node Registration Options ( a . ( * Node Registration Options ) , b . ( * kubeadm . Node Registration if err := s . Add Generated Conversion Func ( ( * kubeadm . Node Registration Options ) ( nil ) , ( * Node Registration Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Node Registration Options_To_v1beta2_Node Registration Options ( a . ( * kubeadm . Node Registration Options ) , b . ( * Node Registration } 
func Convert_v1beta2_API Endpoint_To_kubeadm_API Endpoint ( in * API Endpoint , out * kubeadm . API Endpoint , s conversion . Scope ) error { return auto Convert_v1beta2_API Endpoint_To_kubeadm_API } 
func Convert_kubeadm_API Endpoint_To_v1beta2_API Endpoint ( in * kubeadm . API Endpoint , out * API Endpoint , s conversion . Scope ) error { return auto Convert_kubeadm_API Endpoint_To_v1beta2_API } 
func Convert_v1beta2_API Server_To_kubeadm_API Server ( in * API Server , out * kubeadm . API Server , s conversion . Scope ) error { return auto Convert_v1beta2_API Server_To_kubeadm_API } 
func Convert_kubeadm_API Server_To_v1beta2_API Server ( in * kubeadm . API Server , out * API Server , s conversion . Scope ) error { return auto Convert_kubeadm_API Server_To_v1beta2_API } 
func Convert_v1beta2_Bootstrap Token_To_kubeadm_Bootstrap Token ( in * Bootstrap Token , out * kubeadm . Bootstrap Token , s conversion . Scope ) error { return auto Convert_v1beta2_Bootstrap Token_To_kubeadm_Bootstrap } 
func Convert_kubeadm_Bootstrap Token_To_v1beta2_Bootstrap Token ( in * kubeadm . Bootstrap Token , out * Bootstrap Token , s conversion . Scope ) error { return auto Convert_kubeadm_Bootstrap Token_To_v1beta2_Bootstrap } 
func Convert_v1beta2_Bootstrap Token Discovery_To_kubeadm_Bootstrap Token Discovery ( in * Bootstrap Token Discovery , out * kubeadm . Bootstrap Token Discovery , s conversion . Scope ) error { return auto Convert_v1beta2_Bootstrap Token Discovery_To_kubeadm_Bootstrap Token } 
func Convert_kubeadm_Bootstrap Token Discovery_To_v1beta2_Bootstrap Token Discovery ( in * kubeadm . Bootstrap Token Discovery , out * Bootstrap Token Discovery , s conversion . Scope ) error { return auto Convert_kubeadm_Bootstrap Token Discovery_To_v1beta2_Bootstrap Token } 
func Convert_v1beta2_Bootstrap Token String_To_kubeadm_Bootstrap Token String ( in * Bootstrap Token String , out * kubeadm . Bootstrap Token String , s conversion . Scope ) error { return auto Convert_v1beta2_Bootstrap Token String_To_kubeadm_Bootstrap Token } 
func Convert_kubeadm_Bootstrap Token String_To_v1beta2_Bootstrap Token String ( in * kubeadm . Bootstrap Token String , out * Bootstrap Token String , s conversion . Scope ) error { return auto Convert_kubeadm_Bootstrap Token String_To_v1beta2_Bootstrap Token } 
func Convert_v1beta2_Cluster Configuration_To_kubeadm_Cluster Configuration ( in * Cluster Configuration , out * kubeadm . Cluster Configuration , s conversion . Scope ) error { return auto Convert_v1beta2_Cluster Configuration_To_kubeadm_Cluster } 
func Convert_kubeadm_Cluster Configuration_To_v1beta2_Cluster Configuration ( in * kubeadm . Cluster Configuration , out * Cluster Configuration , s conversion . Scope ) error { return auto Convert_kubeadm_Cluster Configuration_To_v1beta2_Cluster } 
func Convert_v1beta2_Cluster Status_To_kubeadm_Cluster Status ( in * Cluster Status , out * kubeadm . Cluster Status , s conversion . Scope ) error { return auto Convert_v1beta2_Cluster Status_To_kubeadm_Cluster } 
func Convert_kubeadm_Cluster Status_To_v1beta2_Cluster Status ( in * kubeadm . Cluster Status , out * Cluster Status , s conversion . Scope ) error { return auto Convert_kubeadm_Cluster Status_To_v1beta2_Cluster } 
func Convert_v1beta2_Control Plane Component_To_kubeadm_Control Plane Component ( in * Control Plane Component , out * kubeadm . Control Plane Component , s conversion . Scope ) error { return auto Convert_v1beta2_Control Plane Component_To_kubeadm_Control Plane } 
func Convert_kubeadm_Control Plane Component_To_v1beta2_Control Plane Component ( in * kubeadm . Control Plane Component , out * Control Plane Component , s conversion . Scope ) error { return auto Convert_kubeadm_Control Plane Component_To_v1beta2_Control Plane } 
func Convert_v1beta2_DNS_To_kubeadm_DNS ( in * DNS , out * kubeadm . DNS , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_DNS_To_v1beta2_DNS ( in * kubeadm . DNS , out * DNS , s conversion . Scope ) error { return auto } 
func Convert_v1beta2_Discovery_To_kubeadm_Discovery ( in * Discovery , out * kubeadm . Discovery , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_Discovery_To_v1beta2_Discovery ( in * kubeadm . Discovery , out * Discovery , s conversion . Scope ) error { return auto } 
func Convert_v1beta2_Etcd_To_kubeadm_Etcd ( in * Etcd , out * kubeadm . Etcd , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_Etcd_To_v1beta2_Etcd ( in * kubeadm . Etcd , out * Etcd , s conversion . Scope ) error { return auto } 
func Convert_v1beta2_External Etcd_To_kubeadm_External Etcd ( in * External Etcd , out * kubeadm . External Etcd , s conversion . Scope ) error { return auto Convert_v1beta2_External Etcd_To_kubeadm_External } 
func Convert_kubeadm_External Etcd_To_v1beta2_External Etcd ( in * kubeadm . External Etcd , out * External Etcd , s conversion . Scope ) error { return auto Convert_kubeadm_External Etcd_To_v1beta2_External } 
func Convert_v1beta2_File Discovery_To_kubeadm_File Discovery ( in * File Discovery , out * kubeadm . File Discovery , s conversion . Scope ) error { return auto Convert_v1beta2_File Discovery_To_kubeadm_File } 
func Convert_kubeadm_File Discovery_To_v1beta2_File Discovery ( in * kubeadm . File Discovery , out * File Discovery , s conversion . Scope ) error { return auto Convert_kubeadm_File Discovery_To_v1beta2_File } 
func Convert_v1beta2_Host Path Mount_To_kubeadm_Host Path Mount ( in * Host Path Mount , out * kubeadm . Host Path Mount , s conversion . Scope ) error { return auto Convert_v1beta2_Host Path Mount_To_kubeadm_Host Path } 
func Convert_kubeadm_Host Path Mount_To_v1beta2_Host Path Mount ( in * kubeadm . Host Path Mount , out * Host Path Mount , s conversion . Scope ) error { return auto Convert_kubeadm_Host Path Mount_To_v1beta2_Host Path } 
func Convert_v1beta2_Image Meta_To_kubeadm_Image Meta ( in * Image Meta , out * kubeadm . Image Meta , s conversion . Scope ) error { return auto Convert_v1beta2_Image Meta_To_kubeadm_Image } 
func Convert_kubeadm_Image Meta_To_v1beta2_Image Meta ( in * kubeadm . Image Meta , out * Image Meta , s conversion . Scope ) error { return auto Convert_kubeadm_Image Meta_To_v1beta2_Image } 
func Convert_v1beta2_Init Configuration_To_kubeadm_Init Configuration ( in * Init Configuration , out * kubeadm . Init Configuration , s conversion . Scope ) error { return auto Convert_v1beta2_Init Configuration_To_kubeadm_Init } 
func Convert_kubeadm_Init Configuration_To_v1beta2_Init Configuration ( in * kubeadm . Init Configuration , out * Init Configuration , s conversion . Scope ) error { return auto Convert_kubeadm_Init Configuration_To_v1beta2_Init } 
func Convert_v1beta2_Join Configuration_To_kubeadm_Join Configuration ( in * Join Configuration , out * kubeadm . Join Configuration , s conversion . Scope ) error { return auto Convert_v1beta2_Join Configuration_To_kubeadm_Join } 
func Convert_kubeadm_Join Configuration_To_v1beta2_Join Configuration ( in * kubeadm . Join Configuration , out * Join Configuration , s conversion . Scope ) error { return auto Convert_kubeadm_Join Configuration_To_v1beta2_Join } 
func Convert_v1beta2_Join Control Plane_To_kubeadm_Join Control Plane ( in * Join Control Plane , out * kubeadm . Join Control Plane , s conversion . Scope ) error { return auto Convert_v1beta2_Join Control Plane_To_kubeadm_Join Control } 
func Convert_kubeadm_Join Control Plane_To_v1beta2_Join Control Plane ( in * kubeadm . Join Control Plane , out * Join Control Plane , s conversion . Scope ) error { return auto Convert_kubeadm_Join Control Plane_To_v1beta2_Join Control } 
func Convert_v1beta2_Local Etcd_To_kubeadm_Local Etcd ( in * Local Etcd , out * kubeadm . Local Etcd , s conversion . Scope ) error { return auto Convert_v1beta2_Local Etcd_To_kubeadm_Local } 
func Convert_kubeadm_Local Etcd_To_v1beta2_Local Etcd ( in * kubeadm . Local Etcd , out * Local Etcd , s conversion . Scope ) error { return auto Convert_kubeadm_Local Etcd_To_v1beta2_Local } 
func Convert_v1beta2_Networking_To_kubeadm_Networking ( in * Networking , out * kubeadm . Networking , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_Networking_To_v1beta2_Networking ( in * kubeadm . Networking , out * Networking , s conversion . Scope ) error { return auto } 
func Convert_v1beta2_Node Registration Options_To_kubeadm_Node Registration Options ( in * Node Registration Options , out * kubeadm . Node Registration Options , s conversion . Scope ) error { return auto Convert_v1beta2_Node Registration Options_To_kubeadm_Node Registration } 
func Convert_kubeadm_Node Registration Options_To_v1beta2_Node Registration Options ( in * kubeadm . Node Registration Options , out * Node Registration Options , s conversion . Scope ) error { return auto Convert_kubeadm_Node Registration Options_To_v1beta2_Node Registration } 
func Validate Network Policy Name ( name string , prefix bool ) [ ] string { return apimachineryvalidation . Name Is DNS } 
func Validate Network Policy Port ( port * networking . Network Policy Port , port Path * field . Path ) field . Error List { all Errs := field . Error if port . Protocol != nil && * port . Protocol != api . Protocol TCP && * port . Protocol != api . Protocol UDP && * port . Protocol != api . Protocol SCTP { all Errs = append ( all Errs , field . Not Supported ( port Path . Child ( " " ) , * port . Protocol , [ ] string { string ( api . Protocol TCP ) , string ( api . Protocol UDP ) , string ( api . Protocol if port . Port != nil { if port . Port . Type == intstr . Int { for _ , msg := range validation . Is Valid Port Num ( int ( port . Port . Int Val ) ) { all Errs = append ( all Errs , field . Invalid ( port Path . Child ( " " ) , port . Port . Int } else { for _ , msg := range validation . Is Valid Port Name ( port . Port . Str Val ) { all Errs = append ( all Errs , field . Invalid ( port Path . Child ( " " ) , port . Port . Str return all } 
func Validate Network Policy Peer ( peer * networking . Network Policy Peer , peer Path * field . Path ) field . Error List { all Errs := field . Error num if peer . Pod Selector != nil { num all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( peer . Pod Selector , peer if peer . Namespace Selector != nil { num all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( peer . Namespace Selector , peer if peer . IP Block != nil { num all Errs = append ( all Errs , Validate IP Block ( peer . IP Block , peer if num Peers == 0 { all Errs = append ( all Errs , field . Required ( peer } else if num Peers > 1 && peer . IP Block != nil { all Errs = append ( all Errs , field . Forbidden ( peer return all } 
func Validate Network Policy Spec ( spec * networking . Network Policy Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( & spec . Pod Selector , fld // Validate ingress rules. for i , ingress := range spec . Ingress { ingress Path := fld for i , port := range ingress . Ports { port Path := ingress all Errs = append ( all Errs , Validate Network Policy Port ( & port , port for i , from := range ingress . From { from Path := ingress all Errs = append ( all Errs , Validate Network Policy Peer ( & from , from // Validate egress rules for i , egress := range spec . Egress { egress Path := fld for i , port := range egress . Ports { port Path := egress all Errs = append ( all Errs , Validate Network Policy Port ( & port , port for i , to := range egress . To { to Path := egress all Errs = append ( all Errs , Validate Network Policy Peer ( & to , to // Validate Policy Types allowed := sets . New String ( string ( networking . Policy Type Ingress ) , string ( networking . Policy Type if len ( spec . Policy Types ) > len ( allowed ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , & spec . Policy return all for i , p Type := range spec . Policy Types { policy Path := fld for _ , p := range spec . Policy Types { if ! allowed . Has ( string ( p ) ) { all Errs = append ( all Errs , field . Not Supported ( policy Path , p Type , [ ] string { string ( networking . Policy Type Ingress ) , string ( networking . Policy Type return all } 
func Validate Network Policy ( np * networking . Network Policy ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & np . Object Meta , true , Validate Network Policy Name , field . New all Errs = append ( all Errs , Validate Network Policy Spec ( & np . Spec , field . New return all } 
func Validate Network Policy Update ( update , old * networking . Network Policy ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Object Meta Update ( & update . Object Meta , & old . Object Meta , field . New all Errs = append ( all Errs , Validate Network Policy Spec ( & update . Spec , field . New return all } 
func Validate IP Block ( ipb * networking . IP Block , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( ipb . CIDR ) == 0 || ipb . CIDR == " " { all Errs = append ( all Errs , field . Required ( fld return all cidr IP Net , err := apivalidation . Validate if err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all except for i , except IP := range except CIDR { except Path := fld except CIDR , err := apivalidation . Validate CIDR ( except if err != nil { all Errs = append ( all Errs , field . Invalid ( except Path , except return all if ! cidr IP Net . Contains ( except CIDR . IP ) { all Errs = append ( all Errs , field . Invalid ( except Path , except return all } 
func Validate Ingress ( ingress * networking . Ingress ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & ingress . Object Meta , true , Validate Ingress Name , field . New all Errs = append ( all Errs , Validate Ingress Spec ( & ingress . Spec , field . New return all } 
func Validate Ingress Spec ( spec * networking . Ingress Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error // TODO: Is a default backend mandatory? if spec . Backend != nil { all Errs = append ( all Errs , validate Ingress Backend ( spec . Backend , fld } else if len ( spec . Rules ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld if len ( spec . Rules ) > 0 { all Errs = append ( all Errs , validate Ingress Rules ( spec . Rules , fld if len ( spec . TLS ) > 0 { all Errs = append ( all Errs , validate Ingress TLS ( spec , fld return all } 
func Validate Ingress Update ( ingress , old Ingress * networking . Ingress ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & ingress . Object Meta , & old Ingress . Object Meta , field . New all Errs = append ( all Errs , Validate Ingress Spec ( & ingress . Spec , field . New return all } 
func validate Ingress Backend ( backend * networking . Ingress Backend , fld Path * field . Path ) field . Error List { all Errs := field . Error // All backends must reference a single local service by name, and a single service port by name or number. if len ( backend . Service Name ) == 0 { return append ( all Errs , field . Required ( fld for _ , msg := range apivalidation . Validate Service Name ( backend . Service Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , backend . Service all Errs = append ( all Errs , apivalidation . Validate Port Num Or Name ( backend . Service Port , fld return all } 
func New Cmd Alpha ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : i18n . T ( " " ) , Long : templates . Long // Alpha commands should be added here. As features graduate from alpha they should move // from here to the Command Groups defined by New Kubelet Command() in cmd.go. //cmd.Add Command(New Cmd Debug(f, in, out, err)) // New Kubelet Command() will hide the alpha command if it has no subcommands. Overriding // the help function ensures a reasonable message if someone types the hidden command anyway. if ! cmd . Has Sub Commands ( ) { cmd . Set Help } 
func ( c * Fake Examples ) Get ( name string , options v1 . Get Options ) ( result * crv1 . Example , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( examples } 
func ( c * Fake Examples ) List ( opts v1 . List Options ) ( result * crv1 . Example List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( examples Resource , examples Kind , c . ns , opts ) , & crv1 . Example label , _ , _ := testing . Extract From List list := & crv1 . Example List { List Meta : obj . ( * crv1 . Example List ) . List for _ , item := range obj . ( * crv1 . Example } 
func ( c * Fake Examples ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( examples } 
func ( c * Fake Examples ) Create ( example * crv1 . Example ) ( result * crv1 . Example , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( examples } 
func ( c * Fake Examples ) Update ( example * crv1 . Example ) ( result * crv1 . Example , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( examples } 
func ( c * Fake Examples ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( examples } 
func ( c * Fake Examples ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( examples Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & crv1 . Example } 
func ( c * Fake Examples ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * crv1 . Example , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( examples } 
func New Defaults ( ) ( * args . Generator Args , * Custom Args ) { generic Args := args . Default ( ) . Without Default Flag custom Args := & Custom Args { Single generic Args . Custom Args = custom if pkg := codegenutil . Current Package ( ) ; len ( pkg ) != 0 { generic Args . Output Package custom Args . Versioned Client Set custom Args . Internal Client Set custom Args . Listers return generic Args , custom } 
func ( ca * Custom Args ) Add Flags ( fs * pflag . Flag Set ) { fs . String Var ( & ca . Internal Client Set Package , " " , ca . Internal Client Set fs . String Var ( & ca . Versioned Client Set Package , " " , ca . Versioned Client Set fs . String Var ( & ca . Listers Package , " " , ca . Listers fs . Bool Var ( & ca . Single Directory , " " , ca . Single } 
func Validate ( generic Args * args . Generator Args ) error { custom Args := generic Args . Custom Args . ( * Custom if len ( generic Args . Output Package if len ( custom Args . Versioned Client Set if len ( custom Args . Listers } 
func ( c * Fake Volume Attachments ) Get ( name string , options v1 . Get Options ) ( result * storagev1 . Volume Attachment , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( volumeattachments Resource , name ) , & storagev1 . Volume return obj . ( * storagev1 . Volume } 
func ( c * Fake Volume Attachments ) List ( opts v1 . List Options ) ( result * storagev1 . Volume Attachment List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( volumeattachments Resource , volumeattachments Kind , opts ) , & storagev1 . Volume Attachment label , _ , _ := testing . Extract From List list := & storagev1 . Volume Attachment List { List Meta : obj . ( * storagev1 . Volume Attachment List ) . List for _ , item := range obj . ( * storagev1 . Volume Attachment } 
func ( c * Fake Volume Attachments ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( volumeattachments } 
func ( c * Fake Volume Attachments ) Create ( volume Attachment * storagev1 . Volume Attachment ) ( result * storagev1 . Volume Attachment , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( volumeattachments Resource , volume Attachment ) , & storagev1 . Volume return obj . ( * storagev1 . Volume } 
func ( c * Fake Volume Attachments ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( volumeattachments Resource , list _ , err := c . Fake . Invokes ( action , & storagev1 . Volume Attachment } 
func ( s * job Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Job , err error ) { err = cache . List } 
func ( s * job Lister ) Jobs ( namespace string ) Job Namespace Lister { return job Namespace } 
func ( s job Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Job , err error ) { err = cache . List All By } 
func Set CRD Condition ( crd * Custom Resource Definition , new Condition Custom Resource Definition Condition ) { existing Condition := Find CRD Condition ( crd , new if existing Condition == nil { new Condition . Last Transition Time = metav1 . New crd . Status . Conditions = append ( crd . Status . Conditions , new if existing Condition . Status != new Condition . Status { existing Condition . Status = new existing Condition . Last Transition Time = new Condition . Last Transition existing Condition . Reason = new existing Condition . Message = new } 
func Remove CRD Condition ( crd * Custom Resource Definition , condition Type Custom Resource Definition Condition Type ) { new Conditions := [ ] Custom Resource Definition for _ , condition := range crd . Status . Conditions { if condition . Type != condition Type { new Conditions = append ( new crd . Status . Conditions = new } 
func Find CRD Condition ( crd * Custom Resource Definition , condition Type Custom Resource Definition Condition Type ) * Custom Resource Definition Condition { for i := range crd . Status . Conditions { if crd . Status . Conditions [ i ] . Type == condition } 
func Is CRD Condition True ( crd * Custom Resource Definition , condition Type Custom Resource Definition Condition Type ) bool { return Is CRD Condition Present And Equal ( crd , condition Type , Condition } 
func Is CRD Condition False ( crd * Custom Resource Definition , condition Type Custom Resource Definition Condition Type ) bool { return Is CRD Condition Present And Equal ( crd , condition Type , Condition } 
func Is CRD Condition Present And Equal ( crd * Custom Resource Definition , condition Type Custom Resource Definition Condition Type , status Condition Status ) bool { for _ , condition := range crd . Status . Conditions { if condition . Type == condition } 
func Is CRD Condition Equivalent ( lhs , rhs * Custom Resource Definition } 
func CRD Has Finalizer ( crd * Custom Resource } 
func CRD Remove Finalizer ( crd * Custom Resource Definition , needle string ) { new for _ , finalizer := range crd . Finalizers { if finalizer != needle { new Finalizers = append ( new crd . Finalizers = new } 
func Get CRD Storage Version ( crd * Custom Resource } 
func Is Stored Version ( crd * Custom Resource Definition , version string ) bool { for _ , v := range crd . Status . Stored } 
func Get Schema For Version ( crd * Custom Resource Definition , version string ) ( * Custom Resource Validation , error ) { if ! Has Per Version } 
func Get Subresources For Version ( crd * Custom Resource Definition , version string ) ( * Custom Resource Subresources , error ) { if ! Has Per Version } 
func Get Columns For Version ( crd * Custom Resource Definition , version string ) ( [ ] Custom Resource Column Definition , error ) { if ! Has Per Version Columns ( crd . Spec . Versions ) { return serve Default Columns If Empty ( crd . Spec . Additional Printer if len ( crd . Spec . Additional Printer for _ , v := range crd . Spec . Versions { if version == v . Name { return serve Default Columns If Empty ( v . Additional Printer } 
func Has Per Version Schema ( versions [ ] Custom Resource Definition } 
func Has Per Version Subresources ( versions [ ] Custom Resource Definition } 
func Has Per Version Columns ( versions [ ] Custom Resource Definition Version ) bool { for _ , v := range versions { if len ( v . Additional Printer } 
func serve Default Columns If Empty ( columns [ ] Custom Resource Column Definition ) [ ] Custom Resource Column return [ ] Custom Resource Column Definition { { Name : " " , Type : " " , Description : swagger Metadata Descriptions [ " " ] , JSON } 
func Has Version Served ( crd * Custom Resource } 
func ( gcc * Pod GC Controller ) gc // We want to get list of Nodes from the etcd, to make sure that it's as fresh as possible. nodes , err := gcc . kube Client . Core V1 ( ) . Nodes ( ) . List ( metav1 . List node Names := sets . New for i := range nodes . Items { node for _ , pod := range pods { if pod . Spec . Node if node Names . Has ( pod . Spec . Node klog . V ( 2 ) . Infof ( " " , pod . Namespace , pod . Name , pod . Spec . Node if err := gcc . delete Pod ( pod . Namespace , pod . Name ) ; err != nil { utilruntime . Handle } 
func ( gcc * Pod GC Controller ) gc Unscheduled for _ , pod := range pods { if pod . Deletion Timestamp == nil || len ( pod . Spec . Node if err := gcc . delete Pod ( pod . Namespace , pod . Name ) ; err != nil { utilruntime . Handle } 
func new Leases ( c * Coordination V1beta1Client , namespace string ) * leases { return & leases { client : c . REST } 
} 
} 
func New REST ( opts Getter generic . REST Options opts , err := opts Getter . Get REST store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Event { } } , New List Func : func ( ) runtime . Object { return & api . Event List { } } , Predicate Func : event . Match Event , TTL } , Default Qualified Resource : resource , Create Strategy : event . Strategy , Update Strategy : event . Strategy , Delete Strategy : event . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts , Attr Func : event . Get Attrs } // Pass in opts to use Undecorated if err := store . Complete With } 
func New Fs Loader ( fs utilfs . Filesystem , kubelet File string ) ( Loader , error ) { _ , kubelet Codecs , err := kubeletscheme . New Scheme And return & fs Loader { fs : fs , kubelet Codecs : kubelet Codecs , kubelet File : kubelet } 
func resolve Relative Paths ( paths [ ] * string , root string ) { for _ , path := range paths { // leave empty paths alone, "no path" is a valid input // do not attempt to resolve paths that are already absolute if len ( * path ) > 0 && ! filepath . Is } 
func Create PKI Assets ( cfg * kubeadmapi . Init // This structure cannot handle multilevel CA hierarchies. // This isn't a problem right now, but may become one in the future. var cert if cfg . Etcd . Local == nil { cert List = Get Certs Without } else { cert List = Get Default Cert cert Tree , err := cert List . As Map ( ) . Cert if err := cert Tree . Create fmt . Printf ( " \n " , cfg . Certificates // Service accounts are not x509 certs, so handled separately return Create Service Account Key And Public Key Files ( cfg . Certificates } 
func Create Service Account Key And Public Key Files ( certs _ , err := keyutil . Private Key From File ( filepath . Join ( certs Dir , kubeadmconstants . Service Account Private Key if err == nil { // kubeadm doesn't validate the existing certificate key more than this; // Basically, if we find a key file with the same path kubeadm thinks those files // are equal and doesn't bother writing a new file fmt . Printf ( " \n " , kubeadmconstants . Service Account Key Base } else if ! os . Is Not Exist ( err ) { return errors . Wrapf ( err , " " , kubeadmconstants . Service Account Private Key // The key does NOT exist, let's generate it now key , err := pkiutil . New Private // Write .key and .pub files to disk fmt . Printf ( " \n " , kubeadmconstants . Service Account Key Base if err := pkiutil . Write Key ( certs Dir , kubeadmconstants . Service Account Key Base return pkiutil . Write Public Key ( certs Dir , kubeadmconstants . Service Account Key Base } 
func Create CA Cert And Key Files ( cert Spec * Kubeadm Cert , cfg * kubeadmapi . Init Configuration ) error { if cert Spec . CA Name != " " { return errors . Errorf ( " " , cert Spec . Name , cert Spec . CA klog . V ( 1 ) . Infof ( " " , cert cert Config , err := cert Spec . Get ca Cert , ca Key , err := pkiutil . New Certificate Authority ( cert return write Certificate Authorithy Files If Not Exist ( cfg . Certificates Dir , cert Spec . Base Name , ca Cert , ca } 
func New CSR ( cert Spec * Kubeadm Cert , cfg * kubeadmapi . Init Configuration ) ( * x509 . Certificate Request , crypto . Signer , error ) { cert Config , err := cert Spec . Get return pkiutil . New CSR And Key ( cert } 
func Create CSR ( cert Spec * Kubeadm Cert , cfg * kubeadmapi . Init Configuration , path string ) error { csr , key , err := New CSR ( cert return write CSR Files If Not Exist ( path , cert Spec . Base } 
func Create Cert And Key Files With CA ( cert Spec * Kubeadm Cert , ca Cert Spec * Kubeadm Cert , cfg * kubeadmapi . Init Configuration ) error { if cert Spec . CA Name != ca Cert Spec . Name { return errors . Errorf ( " " , cert Spec . Name , cert Spec . CA Name , ca Cert ca Cert , ca Key , err := Load Certificate Authority ( cfg . Certificates Dir , ca Cert Spec . Base if err != nil { return errors . Wrapf ( err , " " , ca Cert return cert Spec . Create From CA ( cfg , ca Cert , ca } 
func Load Certificate Authority ( pki Dir string , base Name string ) ( * x509 . Certificate , crypto . Signer , error ) { // Checks if certificate authority exists in the PKI directory if ! pkiutil . Cert Or Key Exist ( pki Dir , base Name ) { return nil , nil , errors . Errorf ( " " , base Name , pki // Try to load certificate authority .crt and .key from the PKI directory ca Cert , ca Key , err := pkiutil . Try Load Cert And Key From Disk ( pki Dir , base if err != nil { return nil , nil , errors . Wrapf ( err , " " , base // Make sure the loaded CA cert actually is a CA if ! ca Cert . Is CA { return nil , nil , errors . Errorf ( " " , base return ca Cert , ca } 
func write Certificate Authorithy Files If Not Exist ( pki Dir string , base Name string , ca Cert * x509 . Certificate , ca Key crypto . Signer ) error { // If cert or key exists, we should try to load them if pkiutil . Cert Or Key Exist ( pki Dir , base Name ) { // Try to load .crt and .key from the PKI directory ca Cert , _ , err := pkiutil . Try Load Cert And Key From Disk ( pki Dir , base if err != nil { return errors . Wrapf ( err , " " , base // Check if the existing cert is a CA if ! ca Cert . Is CA { return errors . Errorf ( " " , base // kubeadm doesn't validate the existing certificate Authority more than this; // Basically, if we find a certificate file with the same path; and it is a CA // kubeadm thinks those files are equal and doesn't bother writing a new file fmt . Printf ( " \n " , base } else { // Write .crt and .key files to disk fmt . Printf ( " \n " , base if err := pkiutil . Write Cert And Key ( pki Dir , base Name , ca Cert , ca Key ) ; err != nil { return errors . Wrapf ( err , " " , base } 
func write Certificate Files If Not Exist ( pki Dir string , base Name string , signing Cert * x509 . Certificate , cert * x509 . Certificate , key crypto . Signer , cfg * certutil . Config ) error { // Checks if the signed certificate exists in the PKI directory if pkiutil . Cert Or Key Exist ( pki Dir , base Name ) { // Try to load signed certificate .crt and .key from the PKI directory signed Cert , _ , err := pkiutil . Try Load Cert And Key From Disk ( pki Dir , base if err != nil { return errors . Wrapf ( err , " " , base // Check if the existing cert is signed by the given CA if err := signed Cert . Check Signature From ( signing Cert ) ; err != nil { return errors . Errorf ( " " , base // Check if the certificate has the correct attributes if err := validate Certificate With Config ( signed Cert , base fmt . Printf ( " \n " , base } else { // Write .crt and .key files to disk fmt . Printf ( " \n " , base if err := pkiutil . Write Cert And Key ( pki Dir , base Name , cert , key ) ; err != nil { return errors . Wrapf ( err , " " , base if pkiutil . Has Server Auth ( cert ) { fmt . Printf ( " \n " , base Name , cert . DNS Names , cert . IP } 
func write CSR Files If Not Exist ( csr Dir string , base Name string , csr * x509 . Certificate Request , key crypto . Signer ) error { if pkiutil . CSR Or Key Exist ( csr Dir , base Name ) { _ , _ , err := pkiutil . Try Load CSR And Key From Disk ( csr Dir , base if err != nil { return errors . Wrapf ( err , " " , base fmt . Printf ( " \n " , base } else { // Write .key and .csr files to disk fmt . Printf ( " \n " , base if err := pkiutil . Write Key ( csr Dir , base Name , key ) ; err != nil { return errors . Wrapf ( err , " " , base if err := pkiutil . Write CSR ( csr Dir , base Name , csr ) ; err != nil { return errors . Wrapf ( err , " " , base } 
func Shared Certificate Exists ( cfg * kubeadmapi . Cluster Configuration ) ( bool , error ) { if err := validate CA Cert And Key ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . CA Cert And Key Base if err := validate Private Public Key ( cert Key Location { cfg . Certificates Dir , " " , kubeadmconstants . Service Account Key Base if err := validate CA Cert And Key ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . Front Proxy CA Cert And Key Base // in case of local/stacked etcd if cfg . Etcd . External == nil { if err := validate CA Cert And Key ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . Etcd CA Cert And Key Base } 
func Using External CA ( cfg * kubeadmapi . Cluster Configuration ) ( bool , error ) { if err := validate CA Cert ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . CA Cert And Key Base ca Key Path := filepath . Join ( cfg . Certificates Dir , kubeadmconstants . CA Key if _ , err := os . Stat ( ca Key Path ) ; ! os . Is Not if err := validate Signed Cert ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . CA Cert And Key Base Name , kubeadmconstants . API Server Cert And Key Base if err := validate Signed Cert ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . CA Cert And Key Base Name , kubeadmconstants . API Server Kubelet Client Cert And Key Base } 
func Using External Front Proxy CA ( cfg * kubeadmapi . Cluster Configuration ) ( bool , error ) { if err := validate CA Cert ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . Front Proxy CA Cert And Key Base front Proxy CA Key Path := filepath . Join ( cfg . Certificates Dir , kubeadmconstants . Front Proxy CA Key if _ , err := os . Stat ( front Proxy CA Key Path ) ; ! os . Is Not if err := validate Signed Cert ( cert Key Location { cfg . Certificates Dir , kubeadmconstants . Front Proxy CA Cert And Key Base Name , kubeadmconstants . Front Proxy Client Cert And Key Base } 
func validate CA Cert ( l cert Key Location ) error { // Check CA Cert ca Cert , err := pkiutil . Try Load Cert From Disk ( l . pki Dir , l . ca Base if err != nil { return errors . Wrapf ( err , " " , l . ux // Check if cert is a CA if ! ca Cert . Is CA { return errors . Errorf ( " " , l . ux } 
func validate CA Cert And Key ( l cert Key Location ) error { if err := validate CA _ , err := pkiutil . Try Load Key From Disk ( l . pki Dir , l . ca Base if err != nil { return errors . Wrapf ( err , " " , l . ux } 
func validate Signed Cert ( l cert Key Location ) error { // Try to load CA ca Cert , err := pkiutil . Try Load Cert From Disk ( l . pki Dir , l . ca Base if err != nil { return errors . Wrapf ( err , " " , l . ux return validate Signed Cert With CA ( l , ca } 
func validate Signed Cert With CA ( l cert Key Location , ca Cert * x509 . Certificate ) error { // Try to load key and signed certificate signed Cert , _ , err := pkiutil . Try Load Cert And Key From Disk ( l . pki Dir , l . base if err != nil { return errors . Wrapf ( err , " " , l . ux // Check if the cert is signed by the CA if err := signed Cert . Check Signature From ( ca Cert ) ; err != nil { return errors . Wrapf ( err , " " , l . ux } 
func validate Private Public Key ( l cert Key Location ) error { // Try to load key _ , _ , err := pkiutil . Try Load Private Public Key From Disk ( l . pki Dir , l . base if err != nil { return errors . Wrapf ( err , " " , l . ux } 
func validate Certificate With Config ( cert * x509 . Certificate , base Name string , cfg * certutil . Config ) error { for _ , dns Name := range cfg . Alt Names . DNS Names { if err := cert . Verify Hostname ( dns Name ) ; err != nil { return errors . Wrapf ( err , " " , base for _ , ip Address := range cfg . Alt Names . I Ps { if err := cert . Verify Hostname ( ip Address . String ( ) ) ; err != nil { return errors . Wrapf ( err , " " , base } 
func New Cmd Options ( out io . Writer ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : i18n . T ( " " ) , Long : " " , Example : options // The `options` command needs write its output to the `out` stream // (typically stdout). Without calling Set Output here, the Usage() // function call will fall back to stderr. // // See https://github.com/kubernetes/kubernetes/pull/46394 for details. cmd . Set templates . Use Options } 
func new Builder ( client Config Fn Client Config Func , rest Mapper REST Mapper Func , category Expander Category Expander Func ) * Builder { return & Builder { client Config Fn : client Config Fn , rest Mapper Fn : rest Mapper , category Expander Fn : category Expander , require } 
func ( b * Builder ) Filename Param ( enforce Namespace bool , filename Options * Filename Options ) * Builder { if errs := filename recursive := filename paths := filename b . URL ( default Http Get default : if ! recursive { b . single Item if filename Options . Kustomize != " " { b . paths = append ( b . paths , & Kustomize Visitor { filename Options . Kustomize , New Stream Visitor ( nil , b . mapper , filename if enforce Namespace { b . Require } 
b . object Typer = unstructuredscheme . New Unstructured Object b . mapper = & mapper { local Fn : b . is Local , rest Mapper Fn : b . rest Mapper Fn , client Fn : b . get Client , decoder : unstructured . Unstructured JSON } 
func ( b * Builder ) With Scheme ( scheme * runtime . Scheme , decoding Versions ... schema . Group b . object codec Factory := serializer . New Codec negotiated Serializer := runtime . Negotiated Serializer ( codec // if you specified versions, you're specifying a desire for external types, which you don't want to round-trip through // internal types if len ( decoding Versions ) > 0 { negotiated Serializer = codec Factory . Without b . negotiated Serializer = negotiated b . mapper = & mapper { local Fn : b . is Local , rest Mapper Fn : b . rest Mapper Fn , client Fn : b . get Client , decoder : codec Factory . Universal Decoder ( decoding } 
func ( b * Builder ) Local } 
func ( b * Builder ) URL ( http Attempt Count int , urls ... * url . URL ) * Builder { for _ , u := range urls { b . paths = append ( b . paths , & URL Visitor { URL : u , Stream Visitor : New Stream Visitor ( nil , b . mapper , u . String ( ) , b . schema ) , Http Attempt Count : http Attempt } 
b . paths = append ( b . paths , File Visitor For } 
b . paths = append ( b . paths , New Stream } 
func ( b * Builder ) Resource } 
func ( b * Builder ) Resource Names ( resource string , names ... string ) * Builder { for _ , name := range names { // See if this input string is of type/name format tuple , ok , err := split Resource Type if ok { b . resource Tuples = append ( b . resource // Use the given default type to create a resource tuple b . resource Tuples = append ( b . resource Tuples , resource } 
func ( b * Builder ) Label Selector Param ( s string ) * Builder { selector := strings . Trim if b . select return b . Label } 
func ( b * Builder ) Label b . label } 
func ( b * Builder ) Field Selector Param ( s string ) * Builder { s = strings . Trim if b . select b . field } 
func ( b * Builder ) Export } 
func ( b * Builder ) Namespace } 
func ( b * Builder ) All Namespaces ( all Namespace bool ) * Builder { if all Namespace { b . namespace = metav1 . Namespace b . all Namespace = all } 
func ( b * Builder ) Request Chunks Of ( chunk Size int64 ) * Builder { b . limit Chunks = chunk } 
func ( b * Builder ) Transform Requests ( opts ... Request Transform ) * Builder { b . request } 
func ( b * Builder ) Select All Param ( select All bool ) * Builder { if select All && ( b . label Selector != nil || b . field b . select All = select } 
func ( b * Builder ) Replace for _ , arg := range strings . Split ( input , " " ) { if b . category Expander category Expander , err := b . category Expander if err != nil { b . Add if resources , ok := category Expander . Expand ( arg ) ; ok { as for _ , resource := range resources { if len ( resource . Group ) == 0 { as Strings = append ( as as Strings = append ( as arg = strings . Join ( as } 
func normalize Multiple Resources resources = append ( resources , Split Resource new for _ , resource := range resources { for _ , name := range names { new Args = append ( new return new } 
func split Resource Type Name ( s string ) ( resource Tuple , bool , error ) { if ! strings . Contains ( s , " " ) { return resource if len ( seg ) != 2 { return resource if len ( resource ) == 0 || len ( name ) == 0 || len ( Split Resource Argument ( resource ) ) != 1 { return resource return resource } 
func ( b * Builder ) Require Object ( require bool ) * Builder { b . require } 
func ( b * Builder ) mapping For ( resource Or Kind Arg string ) ( * meta . REST Mapping , error ) { fully Specified GVR , group Resource := schema . Parse Resource Arg ( resource Or Kind gvk := schema . Group Version rest Mapper , err := b . rest Mapper if fully Specified GVR != nil { gvk , _ = rest Mapper . Kind For ( * fully Specified if gvk . Empty ( ) { gvk , _ = rest Mapper . Kind For ( group Resource . With if ! gvk . Empty ( ) { return rest Mapper . REST Mapping ( gvk . Group fully Specified GVK , group Kind := schema . Parse Kind Arg ( resource Or Kind if fully Specified GVK == nil { gvk := group Kind . With fully Specified if ! fully Specified GVK . Empty ( ) { if mapping , err := rest Mapper . REST Mapping ( fully Specified GVK . Group Kind ( ) , fully Specified mapping , err := rest Mapper . REST Mapping ( group if err != nil { // if we error out here, it is because we could not match a resource or a kind // for the given argument. To maintain consistency with previous behavior, // announce that a resource type could not be found. // if the error is _not_ a *meta.No Kind Match Error, then we had trouble doing discovery, // so we should return the original error since it may help a user diagnose what is actually wrong if meta . Is No Match Error ( err ) { return nil , fmt . Errorf ( " " , group } 
func ( b * Builder ) Do ( ) * Result { r := b . visitor if b . flatten { r . visitor = New Flatten List Visitor ( r . visitor , b . object helpers := [ ] Visitor if b . default Namespace { helpers = append ( helpers , Set if b . require Namespace { helpers = append ( helpers , Require helpers = append ( helpers , Filter if b . require Object { helpers = append ( helpers , Retrieve if b . continue On Error { r . visitor = New Decorated Visitor ( Continue On Error } else { r . visitor = New Decorated } 
func Split Resource set := sets . New } 
func Has Names ( args [ ] string ) ( bool , error ) { args = normalize Multiple Resources has Combined Types , err := has Combined Type return has Combined } 
func ( o * Replica Set Controller Options ) Add Flags ( fs * pflag . Flag fs . Int32Var ( & o . Concurrent RS Syncs , " " , o . Concurrent RS } 
func ( o * Replica Set Controller Options ) Apply To ( cfg * replicasetconfig . Replica Set Controller cfg . Concurrent RS Syncs = o . Concurrent RS } 
func ( o * Replica Set Controller } 
func new Pod Metricses ( c * Metrics V1beta1Client , namespace string ) * pod Metricses { return & pod Metricses { client : c . REST } 
func New Endpoints Config ( endpoints Informer coreinformers . Endpoints Informer , resync Period time . Duration ) * Endpoints Config { result := & Endpoints Config { lister : endpoints Informer . Lister ( ) , lister Synced : endpoints Informer . Informer ( ) . Has endpoints Informer . Informer ( ) . Add Event Handler With Resync Period ( cache . Resource Event Handler Funcs { Add Func : result . handle Add Endpoints , Update Func : result . handle Update Endpoints , Delete Func : result . handle Delete Endpoints , } , resync } 
func ( c * Endpoints Config ) Register Event Handler ( handler Endpoints Handler ) { c . event Handlers = append ( c . event } 
func ( c * Endpoints Config ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle if ! controller . Wait For Cache Sync ( " " , stop Ch , c . lister for i := range c . event c . event Handlers [ i ] . On Endpoints <- stop } 
func New Service Config ( service Informer coreinformers . Service Informer , resync Period time . Duration ) * Service Config { result := & Service Config { lister : service Informer . Lister ( ) , lister Synced : service Informer . Informer ( ) . Has service Informer . Informer ( ) . Add Event Handler With Resync Period ( cache . Resource Event Handler Funcs { Add Func : result . handle Add Service , Update Func : result . handle Update Service , Delete Func : result . handle Delete Service , } , resync } 
func ( c * Service Config ) Register Event Handler ( handler Service Handler ) { c . event Handlers = append ( c . event } 
func ( c * Service Config ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle if ! controller . Wait For Cache Sync ( " " , stop Ch , c . lister for i := range c . event c . event Handlers [ i ] . On Service <- stop } 
func New Custom Resource Definition Informer ( client internalclientset . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Custom Resource Definition Informer ( client , resync } 
func New Filtered Custom Resource Definition Informer ( client internalclientset . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Apiextensions ( ) . Custom Resource } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Apiextensions ( ) . Custom Resource } , } , & apiextensions . Custom Resource Definition { } , resync } 
func New Cmd Rollout ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : rollout Long , Example : rollout Example , Run : cmdutil . Default Sub Command // subcommands cmd . Add Command ( New Cmd Rollout cmd . Add Command ( New Cmd Rollout cmd . Add Command ( New Cmd Rollout cmd . Add Command ( New Cmd Rollout cmd . Add Command ( New Cmd Rollout cmd . Add Command ( New Cmd Rollout } 
func ( o * Kernel Validator Helper Impl ) Get Kernel Release release return strings . Trim Space ( string ( release } 
func New Secure Serving Options ( ) * genericoptions . Secure Serving Options With Loopback { o := genericoptions . Secure Serving Options { Bind Address : net . Parse IP ( " " ) , Bind Port : 6443 , Required : true , Server Cert : genericoptions . Generatable Key Cert { Pair Name : " " , Cert return o . With } 
func New Insecure Serving Options ( ) * genericoptions . Deprecated Insecure Serving Options With Loopback { o := genericoptions . Deprecated Insecure Serving Options { Bind Address : net . Parse IP ( " " ) , Bind return o . With } 
func Default Advertise Address ( s * genericoptions . Server Run Options , insecure * genericoptions . Deprecated Insecure Serving if s . Advertise Address == nil || s . Advertise Address . Is Unspecified ( ) { host IP , err := utilnet . Choose Bind Address ( insecure . Bind s . Advertise Address = host } 
func ( c * Rbac V1alpha1Client ) REST return c . rest } 
func ( f * Human Print Flags ) Ensure With Kind ( ) error { show f . Show Kind = & show } 
func ( f * Human Print Flags ) To Printer ( output Format string ) ( printers . Resource Printer , error ) { if len ( output Format ) > 0 && output Format != " " { return nil , genericclioptions . No Compatible Printer Error { Options : f , Allowed Formats : f . Allowed show if f . Show Kind != nil { show Kind = * f . Show show if f . Show Labels != nil { show Labels = * f . Show column if f . Column Labels != nil { column Labels = * f . Column p := printers . New Table Printer ( printers . Print Options { Kind : f . Kind , With Kind : show Kind , No Headers : f . No Headers , Wide : output Format == " " , With Namespace : f . With Namespace , Column Labels : column Labels , Show Labels : show printersinternal . Add } 
func ( f * Human Print Flags ) Add Flags ( c * cobra . Command ) { if f . Show Labels != nil { c . Flags ( ) . Bool Var ( f . Show Labels , " " , * f . Show if f . Sort By != nil { c . Flags ( ) . String Var ( f . Sort By , " " , * f . Sort if f . Column Labels != nil { c . Flags ( ) . String Slice Var P ( f . Column Labels , " " , " " , * f . Column if f . Show Kind != nil { c . Flags ( ) . Bool Var ( f . Show Kind , " " , * f . Show } 
func New Human Print Flags ( ) * Human Print Flags { show sort show column return & Human Print Flags { No Headers : false , With Namespace : false , Absolute Timestamps : false , Column Labels : & column Labels , Kind : schema . Group Kind { } , Show Labels : & show Labels , Sort By : & sort By , Show Kind : & show } 
func ( record * attributes Record ) get Annotations ( ) map [ string ] string { record . annotations Lock . R defer record . annotations Lock . R } 
func ( c * cluster Roles ) Update ( cluster Role * v1alpha1 . Cluster Role ) ( result * v1alpha1 . Cluster Role , err error ) { result = & v1alpha1 . Cluster err = c . client . Put ( ) . Resource ( " " ) . Name ( cluster Role . Name ) . Body ( cluster } 
func ( c * Fake Stateful Sets ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( statefulsets } 
func ( c * Fake Stateful Sets ) Update ( stateful Set * appsv1 . Stateful Set ) ( result * appsv1 . Stateful Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( statefulsets Resource , c . ns , stateful Set ) , & appsv1 . Stateful return obj . ( * appsv1 . Stateful } 
func ( fs * fake Fs ) Stat ( name string ) ( os . File } 
func ( fs * fake return & fake } 
func ( fs * fake } 
func ( fs * fake Fs ) Mkdir All ( path string , perm os . File Mode ) error { return fs . a . Fs . Mkdir } 
func ( fs * fake } 
func ( fs * fake Fs ) Read File ( filename string ) ( [ ] byte , error ) { return fs . a . Read } 
func ( fs * fake Fs ) Temp Dir ( dir , prefix string ) ( string , error ) { return fs . a . Temp } 
func ( fs * fake Fs ) Temp File ( dir , prefix string ) ( File , error ) { file , err := fs . a . Temp return & fake } 
func ( fs * fake Fs ) Read Dir ( dirname string ) ( [ ] os . File Info , error ) { return fs . a . Read } 
func ( fs * fake Fs ) Walk ( root string , walk Fn filepath . Walk Func ) error { return fs . a . Walk ( root , walk } 
func ( fs * fake Fs ) Remove All ( path string ) error { return fs . a . Remove } 
func ( fs * fake } 
func ( file * fake } 
func Round Up To GB ( size resource . Quantity ) int64 { request return round Up Size ( request } 
func Round Up To Gi B ( size resource . Quantity ) int64 { request return round Up Size ( request Bytes , Gi } 
func Round Up To MB ( size resource . Quantity ) int64 { request return round Up Size ( request } 
func Round Up To Mi B ( size resource . Quantity ) int64 { request return round Up Size ( request Bytes , Mi } 
func Round Up To KB ( size resource . Quantity ) int64 { request return round Up Size ( request } 
func Round Up To Ki B ( size resource . Quantity ) int64 { request return round Up Size ( request Bytes , Ki } 
func Round Up To GB Int ( size resource . Quantity ) ( int , error ) { request return round Up Size Int ( request } 
func Round Up To Gi B Int ( size resource . Quantity ) ( int , error ) { request return round Up Size Int ( request Bytes , Gi } 
func Round Up To MB Int ( size resource . Quantity ) ( int , error ) { request return round Up Size Int ( request } 
func Round Up To Mi B Int ( size resource . Quantity ) ( int , error ) { request return round Up Size Int ( request Bytes , Mi } 
func Round Up To KB Int ( size resource . Quantity ) ( int , error ) { request return round Up Size Int ( request } 
func Round Up To Ki B Int ( size resource . Quantity ) ( int , error ) { request return round Up Size Int ( request Bytes , Ki } 
func round Up Size Int ( volume Size Bytes int64 , allocation Unit Bytes int64 ) ( int , error ) { rounded Up := round Up Size ( volume Size Bytes , allocation Unit rounded Up Int := int ( rounded if int64 ( rounded Up Int ) != rounded Up { return 0 , fmt . Errorf ( " " , rounded return rounded Up } 
func round Up Size ( volume Size Bytes int64 , allocation Unit Bytes int64 ) int64 { rounded Up := volume Size Bytes / allocation Unit if volume Size Bytes % allocation Unit Bytes > 0 { rounded return rounded } 
func Is Certificate Request Approved ( csr * certificates . Certificate Signing Request ) bool { approved , denied := Get Cert Approval } 
func Hook Client Config For Webhook ( w * v1beta1 . Webhook ) webhook . Client Config { ret := webhook . Client Config { Name : w . Name , CA Bundle : w . Client Config . CA if w . Client Config . URL != nil { ret . URL = * w . Client if w . Client Config . Service != nil { ret . Service = & webhook . Client Config Service { Name : w . Client Config . Service . Name , Namespace : w . Client if w . Client Config . Service . Port != nil { ret . Service . Port = * w . Client if w . Client Config . Service . Path != nil { ret . Service . Path = * w . Client } 
func Has Admission Review Version ( a string , w * v1beta1 . Webhook ) bool { for _ , b := range w . Admission Review } 
func ( in * Pod Preset ) Deep Copy Into ( out * Pod out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Pod Preset ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod Preset ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Preset List ) Deep Copy Into ( out * Pod Preset out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Pod for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Preset List ) Deep Copy ( ) * Pod Preset out := new ( Pod Preset in . Deep Copy } 
func ( in * Pod Preset List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Preset Spec ) Deep Copy ( ) * Pod Preset out := new ( Pod Preset in . Deep Copy } 
func ( s * c SI Driver Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . CSI Driver , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . CSI } 
func ( s * c SI Driver Lister ) Get ( name string ) ( * v1beta1 . CSI Driver , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1beta1 . CSI } 
func Copy } 
func ( g * group ) Internal Version ( ) internalversion . Interface { return internalversion . New ( g . factory , g . namespace , g . tweak List } 
func ( s * pod Disruption Budget Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Pod Disruption Budget , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Pod Disruption } 
func ( s * pod Disruption Budget Lister ) Pod Disruption Budgets ( namespace string ) Pod Disruption Budget Namespace Lister { return pod Disruption Budget Namespace } 
func ( s pod Disruption Budget Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Pod Disruption Budget , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Pod Disruption } 
func Mark Control Plane ( client clientset . Interface , control Plane Name string , taints [ ] v1 . Taint ) error { fmt . Printf ( " \" \" \n " , control Plane Name , constants . Label Node Role if len ( taints ) > 0 { taint for _ , taint := range taints { taint Strs = append ( taint Strs , taint . To fmt . Printf ( " \n " , control Plane Name , taint return apiclient . Patch Node ( client , control Plane Name , func ( n * v1 . Node ) { mark Control Plane } 
func ( in * Auth Info ) Deep Copy Into ( out * Auth if in . Client Certificate Data != nil { in , out := & in . Client Certificate Data , & out . Client Certificate if in . Client Key Data != nil { in , out := & in . Client Key Data , & out . Client Key if in . Impersonate Groups != nil { in , out := & in . Impersonate Groups , & out . Impersonate if in . Impersonate User Extra != nil { in , out := & in . Impersonate User Extra , & out . Impersonate User for key , val := range * in { var out } else { in , out := & val , & out ( * out ) [ key ] = out if in . Auth Provider != nil { in , out := & in . Auth Provider , & out . Auth * out = new ( Auth Provider ( * in ) . Deep Copy * out = new ( Exec ( * in ) . Deep Copy * out = make ( [ ] Named for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Auth Info ) Deep Copy ( ) * Auth out := new ( Auth in . Deep Copy } 
func ( in * Auth Provider Config ) Deep Copy Into ( out * Auth Provider } 
func ( in * Auth Provider Config ) Deep Copy ( ) * Auth Provider out := new ( Auth Provider in . Deep Copy } 
func ( in * Cluster ) Deep Copy if in . Certificate Authority Data != nil { in , out := & in . Certificate Authority Data , & out . Certificate Authority * out = make ( [ ] Named for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Cluster ) Deep in . Deep Copy } 
func ( in * Config ) Deep Copy in . Preferences . Deep Copy * out = make ( [ ] Named for i := range * in { ( * in ) [ i ] . Deep Copy if in . Auth Infos != nil { in , out := & in . Auth Infos , & out . Auth * out = make ( [ ] Named Auth for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Named for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Named for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Config ) Deep in . Deep Copy } 
func ( in * Config ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Context ) Deep in . Deep Copy } 
func ( in * Exec Config ) Deep Copy Into ( out * Exec * out = make ( [ ] Exec Env } 
func ( in * Exec Config ) Deep Copy ( ) * Exec out := new ( Exec in . Deep Copy } 
func ( in * Exec Env Var ) Deep Copy ( ) * Exec Env out := new ( Exec Env in . Deep Copy } 
func ( in * Named Auth Info ) Deep Copy Into ( out * Named Auth in . Auth Info . Deep Copy Into ( & out . Auth } 
func ( in * Named Auth Info ) Deep Copy ( ) * Named Auth out := new ( Named Auth in . Deep Copy } 
func ( in * Named Cluster ) Deep Copy Into ( out * Named in . Cluster . Deep Copy } 
func ( in * Named Cluster ) Deep Copy ( ) * Named out := new ( Named in . Deep Copy } 
func ( in * Named Context ) Deep Copy Into ( out * Named in . Context . Deep Copy } 
func ( in * Named Context ) Deep Copy ( ) * Named out := new ( Named in . Deep Copy } 
func ( in * Named Extension ) Deep Copy Into ( out * Named in . Extension . Deep Copy } 
func ( in * Named Extension ) Deep Copy ( ) * Named out := new ( Named in . Deep Copy } 
func ( in * Preferences ) Deep Copy * out = make ( [ ] Named for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Preferences ) Deep in . Deep Copy } 
func New Default Client Config ( config clientcmdapi . Config , overrides * Config Overrides ) Client Config { return & Direct Client Config { config , config . Current Context , overrides , nil , New Default Client Config Loading Rules ( ) , prompted } 
func New Non Interactive Client Config ( config clientcmdapi . Config , context Name string , overrides * Config Overrides , config Access Config Access ) Client Config { return & Direct Client Config { config , context Name , overrides , nil , config Access , prompted } 
func New Client Config From Bytes ( config Bytes [ ] byte ) ( Client Config , error ) { config , err := Load ( config return & Direct Client Config { * config , " " , & Config Overrides { } , nil , nil , prompted } 
func REST Config From Kube Config ( config Bytes [ ] byte ) ( * restclient . Config , error ) { client Config , err := New Client Config From Bytes ( config return client Config . Client } 
func ( config * Direct Client Config ) Client Config ( ) ( * restclient . Config , error ) { // check that get Auth Info, get Context, and get Cluster do not return an error. // Do this before checking if the current config is usable in the event that an // Auth Info, Context, or Cluster config with user-defined names are not found. // This provides a user with the immediate cause for error if one is found config Auth Info , err := config . get Auth _ , err = config . get config Cluster Info , err := config . get if err := config . Confirm client client Config . Host = config Cluster if len ( config . overrides . Timeout ) > 0 { timeout , err := Parse client if u , err := url . Parse Request URI ( client Config . Host ) ; err == nil && u . Opaque == " " && len ( u . Path ) > 1 { u . Raw client if len ( config Auth Info . Impersonate ) > 0 { client Config . Impersonate = restclient . Impersonation Config { User Name : config Auth Info . Impersonate , Groups : config Auth Info . Impersonate Groups , Extra : config Auth Info . Impersonate User // only try to read the auth information if we are secure if restclient . Is Config Transport TLS ( * client var persister restclient . Auth Provider Config if config . config Access != nil { auth Info Name , _ := config . get Auth Info persister = Persister For User ( config . config Access , auth Info user Auth Partial Config , err := config . get User Identification Partial Config ( config Auth Info , config . fallback mergo . Merge With Overwrite ( client Config , user Auth Partial server Auth Partial Config , err := get Server Identification Partial Config ( config Auth Info , config Cluster mergo . Merge With Overwrite ( client Config , server Auth Partial return client } 
func get Server Identification Partial Config ( config Auth Info clientcmdapi . Auth Info , config Cluster Info clientcmdapi . Cluster ) ( * restclient . Config , error ) { merged // config Cluster Info holds the information identify the server provided by .kubeconfig config Client config Client Config . CA File = config Cluster Info . Certificate config Client Config . CA Data = config Cluster Info . Certificate Authority config Client Config . Insecure = config Cluster Info . Insecure Skip TLS mergo . Merge With Overwrite ( merged Config , config Client return merged } 
func ( config * Direct Client Config ) get User Identification Partial Config ( config Auth Info clientcmdapi . Auth Info , fallback Reader io . Reader , persist Auth Config restclient . Auth Provider Config Persister ) ( * restclient . Config , error ) { merged // blindly overwrite existing values based on precedence if len ( config Auth Info . Token ) > 0 { merged Config . Bearer Token = config Auth } else if len ( config Auth Info . Token File ) > 0 { token Bytes , err := ioutil . Read File ( config Auth Info . Token merged Config . Bearer Token = string ( token merged Config . Bearer Token File = config Auth Info . Token if len ( config Auth Info . Impersonate ) > 0 { merged Config . Impersonate = restclient . Impersonation Config { User Name : config Auth Info . Impersonate , Groups : config Auth Info . Impersonate Groups , Extra : config Auth Info . Impersonate User if len ( config Auth Info . Client Certificate ) > 0 || len ( config Auth Info . Client Certificate Data ) > 0 { merged Config . Cert File = config Auth Info . Client merged Config . Cert Data = config Auth Info . Client Certificate merged Config . Key File = config Auth Info . Client merged Config . Key Data = config Auth Info . Client Key if len ( config Auth Info . Username ) > 0 || len ( config Auth Info . Password ) > 0 { merged Config . Username = config Auth merged Config . Password = config Auth if config Auth Info . Auth Provider != nil { merged Config . Auth Provider = config Auth Info . Auth merged Config . Auth Config Persister = persist Auth if config Auth Info . Exec != nil { merged Config . Exec Provider = config Auth // if there still isn't enough information to authenticate the user, try prompting if ! can Identify User ( * merged Config ) && ( fallback Reader != nil ) { if len ( config . prompted Credentials . username ) > 0 && len ( config . prompted Credentials . password ) > 0 { merged Config . Username = config . prompted merged Config . Password = config . prompted return merged prompter := New Prompting Auth Loader ( fallback prompted Auth prompted Config := make User Identification Config ( * prompted Auth previously Merged Config := merged merged mergo . Merge With Overwrite ( merged Config , prompted mergo . Merge With Overwrite ( merged Config , previously Merged config . prompted Credentials . username = merged config . prompted Credentials . password = merged return merged } 
func make User Identification config . Cert File = info . Cert config . Key File = info . Key config . Bearer Token = info . Bearer } 
func ( config * Direct Client Config ) Namespace ( ) ( string , bool , error ) { if config . overrides != nil && config . overrides . Context . Namespace != " " { // In the event we have an empty config but we do have a namespace override, we should return // the namespace override instead of having config.Confirm if err := config . Confirm config Context , err := config . get if len ( config return config } 
func ( config * Direct Client Config ) Confirm Usable ( ) error { validation var context if len ( config . context Name ) != 0 { context Name = config . context } else { context Name = config . config . Current if len ( context Name ) > 0 { _ , exists := config . config . Contexts [ context if ! exists { validation Errors = append ( validation Errors , & err Context Not Found { context auth Info Name , _ := config . get Auth Info auth Info , _ := config . get Auth validation Errors = append ( validation Errors , validate Auth Info ( auth Info Name , auth cluster Name , _ := config . get Cluster cluster , _ := config . get validation Errors = append ( validation Errors , validate Cluster Info ( cluster // when direct client config is specified, and our only error is that no server is defined, we should // return a standard "no config" error if len ( validation Errors ) == 1 && validation Errors [ 0 ] == Err Empty Cluster { return new Err Configuration Invalid ( [ ] error { Err Empty return new Err Configuration Invalid ( validation } 
func ( config * Direct Client Config ) get Context Name ( ) ( string , bool ) { if len ( config . overrides . Current Context ) != 0 { return config . overrides . Current if len ( config . context Name ) != 0 { return config . context return config . config . Current } 
func ( config * Direct Client Config ) get Auth Info Name ( ) ( string , bool ) { if len ( config . overrides . Context . Auth Info ) != 0 { return config . overrides . Context . Auth context , _ := config . get return context . Auth } 
func ( config * Direct Client Config ) get Cluster context , _ := config . get } 
func ( config * Direct Client Config ) get context Name , required := config . get Context merged Context := clientcmdapi . New if config Context , exists := contexts [ context Name ] ; exists { mergo . Merge With Overwrite ( merged Context , config } else if required { return clientcmdapi . Context { } , fmt . Errorf ( " " , context mergo . Merge With Overwrite ( merged return * merged } 
func ( config * Direct Client Config ) get Auth Info ( ) ( clientcmdapi . Auth Info , error ) { auth Infos := config . config . Auth auth Info Name , required := config . get Auth Info merged Auth Info := clientcmdapi . New Auth if config Auth Info , exists := auth Infos [ auth Info Name ] ; exists { mergo . Merge With Overwrite ( merged Auth Info , config Auth } else if required { return clientcmdapi . Auth Info { } , fmt . Errorf ( " " , auth Info mergo . Merge With Overwrite ( merged Auth Info , config . overrides . Auth return * merged Auth } 
func ( config * Direct Client Config ) get Cluster ( ) ( clientcmdapi . Cluster , error ) { cluster cluster Info Name , required := config . get Cluster merged Cluster Info := clientcmdapi . New mergo . Merge With Overwrite ( merged Cluster Info , config . overrides . Cluster if config Cluster Info , exists := cluster Infos [ cluster Info Name ] ; exists { mergo . Merge With Overwrite ( merged Cluster Info , config Cluster } else if required { return clientcmdapi . Cluster { } , fmt . Errorf ( " " , cluster Info mergo . Merge With Overwrite ( merged Cluster Info , config . overrides . Cluster // An override of --insecure-skip-tls-verify=true and no accompanying CA/CA data should clear already-set CA/CA data // otherwise, a kubeconfig containing a CA reference would return an error that "CA and insecure-skip-tls-verify couldn't both be set" ca Len := len ( config . overrides . Cluster Info . Certificate ca Data Len := len ( config . overrides . Cluster Info . Certificate Authority if config . overrides . Cluster Info . Insecure Skip TLS Verify && ca Len == 0 && ca Data Len == 0 { merged Cluster Info . Certificate merged Cluster Info . Certificate Authority return * merged Cluster } 
func ( config * in Cluster Client return os . Getenv ( " " ) != " " && os . Getenv ( " " ) != " " && err == nil && ! fi . Is } 
func Build Config From Flags ( master Url , kubeconfig Path string ) ( * restclient . Config , error ) { if kubeconfig Path == " " && master kubeconfig , err := restclient . In Cluster return New Non Interactive Deferred Loading Client Config ( & Client Config Loading Rules { Explicit Path : kubeconfig Path } , & Config Overrides { Cluster Info : clientcmdapi . Cluster { Server : master Url } } ) . Client } 
func Build Config From Kubeconfig Getter ( master Url string , kubeconfig Getter Kubeconfig Getter ) ( * restclient . Config , error ) { // TODO: We do not need a Deferred Loader here. Refactor code and see if we can use Direct Client Config here. cc := New Non Interactive Deferred Loading Client Config ( & Client Config Getter { kubeconfig Getter : kubeconfig Getter } , & Config Overrides { Cluster Info : clientcmdapi . Cluster { Server : master return cc . Client } 
func New Kube Generic Runtime Manager ( recorder record . Event Recorder , liveness Manager proberesults . Manager , seccomp Profile Root string , container Ref Manager * kubecontainer . Ref Manager , machine Info * cadvisorapi . Machine Info , pod State Provider pod State Provider , os Interface kubecontainer . OS Interface , runtime Helper kubecontainer . Runtime Helper , http Client types . Http Getter , image Back Off * flowcontrol . Backoff , serialize Image Pulls bool , image Pull QPS float32 , image Pull Burst int , cpu CFS Quota bool , cpu CFS Quota Period metav1 . Duration , runtime Service internalapi . Runtime Service , image Service internalapi . Image Manager Service , internal Lifecycle cm . Internal Container Lifecycle , legacy Log Provider Legacy Log Provider , runtime Class Manager * runtimeclass . Manager , ) ( Kube Generic Runtime , error ) { kube Runtime Manager := & kube Generic Runtime Manager { recorder : recorder , cpu CFS Quota : cpu CFS Quota , cpu CFS Quota Period : cpu CFS Quota Period , seccomp Profile Root : seccomp Profile Root , liveness Manager : liveness Manager , container Ref Manager : container Ref Manager , machine Info : machine Info , os Interface : os Interface , runtime Helper : runtime Helper , runtime Service : new Instrumented Runtime Service ( runtime Service ) , image Service : new Instrumented Image Manager Service ( image Service ) , keyring : credentialprovider . New Docker Keyring ( ) , internal Lifecycle : internal Lifecycle , legacy Log Provider : legacy Log Provider , runtime Class Manager : runtime Class Manager , log Reduction : logreduction . New Log Reduction ( identical Error typed Version , err := kube Runtime Manager . runtime Service . Version ( kube Runtime API // Only matching kube Runtime API Version is supported now // TODO: Runtime API machinery is under discussion at https://github.com/kubernetes/kubernetes/issues/28642 if typed Version . Version != kube Runtime API Version { klog . Errorf ( " " , typed Version . Version , kube Runtime API return nil , Err Version Not kube Runtime Manager . runtime Name = typed Version . Runtime klog . Infof ( " " , typed Version . Runtime Name , typed Version . Runtime Version , typed Version . Runtime Api // If the container logs directory does not exist, create it. // TODO: create pod Logs Root Directory at kubelet.go when kubelet is refactored to // new runtime interface if _ , err := os Interface . Stat ( pod Logs Root Directory ) ; os . Is Not Exist ( err ) { if err := os Interface . Mkdir All ( pod Logs Root Directory , 0755 ) ; err != nil { klog . Errorf ( " " , pod Logs Root kube Runtime Manager . image Puller = images . New Image Manager ( kubecontainer . Filter Event Recorder ( recorder ) , kube Runtime Manager , image Back Off , serialize Image Pulls , image Pull QPS , image Pull kube Runtime Manager . runner = lifecycle . New Handler Runner ( http Client , kube Runtime Manager , kube Runtime kube Runtime Manager . container GC = new Container GC ( runtime Service , pod State Provider , kube Runtime kube Runtime Manager . version Cache = cache . New Object Cache ( func ( ) ( interface { } , error ) { return kube Runtime Manager . get Typed } , version Cache return kube Runtime } 
func ( m * kube Generic Runtime Manager ) Version ( ) ( kubecontainer . Version , error ) { typed Version , err := m . runtime Service . Version ( kube Runtime API return new Runtime Version ( typed Version . Runtime } 
func ( m * kube Generic Runtime Manager ) API Version ( ) ( kubecontainer . Version , error ) { version Object , err := m . version Cache . Get ( m . machine Info . Machine typed Version := version Object . ( * runtimeapi . Version return new Runtime Version ( typed Version . Runtime Api } 
func ( m * kube Generic Runtime Manager ) Status ( ) ( * kubecontainer . Runtime Status , error ) { status , err := m . runtime return to Kube Runtime } 
func ( m * kube Generic Runtime Manager ) Get sandboxes , err := m . get Kubelet pod if _ , ok := pods [ pod UID ] ; ! ok { pods [ pod UID ] = & kubecontainer . Pod { ID : pod p := pods [ pod converted , err := m . sandbox To Kube if err != nil { klog . V ( 4 ) . Infof ( " " , m . runtime Name , s , pod containers , err := m . get Kubelet labelled Info := get Container Info From pod , found := pods [ labelled Info . Pod if ! found { pod = & kubecontainer . Pod { ID : labelled Info . Pod UID , Name : labelled Info . Pod Name , Namespace : labelled Info . Pod pods [ labelled Info . Pod converted , err := m . to Kube if err != nil { klog . V ( 4 ) . Infof ( " " , m . runtime Name , c , labelled Info . Pod } 
func ( m * kube Generic Runtime Manager ) pod Sandbox Changed ( pod * v1 . Pod , pod Status * kubecontainer . Pod Status ) ( bool , uint32 , string ) { if len ( pod Status . Sandbox ready Sandbox for _ , s := range pod Status . Sandbox Statuses { if s . State == runtimeapi . Pod Sandbox State_SANDBOX_READY { ready Sandbox // Needs to create a new sandbox when ready Sandbox Count > 1 or the ready sandbox is not the latest one. sandbox Status := pod Status . Sandbox if ready Sandbox return true , sandbox Status . Metadata . Attempt + 1 , sandbox if sandbox Status . State != runtimeapi . Pod Sandbox return true , sandbox Status . Metadata . Attempt + 1 , sandbox // Needs to create a new sandbox when network namespace changed. if sandbox Status . Get Linux ( ) . Get Namespaces ( ) . Get Options ( ) . Get Network ( ) != network Namespace For return true , sandbox // Needs to create a new sandbox when the sandbox does not have an IP address. if ! kubecontainer . Is Host Network Pod ( pod ) && sandbox return true , sandbox Status . Metadata . Attempt + 1 , sandbox return false , sandbox Status . Metadata . Attempt , sandbox } 
func ( m * kube Generic Runtime Manager ) compute Pod Actions ( pod * v1 . Pod , pod Status * kubecontainer . Pod Status ) pod create Pod Sandbox , attempt , sandbox ID := m . pod Sandbox Changed ( pod , pod changes := pod Actions { Kill Pod : create Pod Sandbox , Create Sandbox : create Pod Sandbox , Sandbox ID : sandbox ID , Attempt : attempt , Containers To Start : [ ] int { } , Containers To Kill : make ( map [ kubecontainer . Container ID ] container To Kill // If we need to (re-)create the pod sandbox, everything will need to be // killed and recreated, and init containers should be purged. if create Pod Sandbox { if ! should Restart On Failure ( pod ) && attempt != 0 { // Should not restart the pod, just return. // we should not create a sandbox for a pod if it is already done. // if all containers are done and should not be started, there is no need to create a new sandbox. // this stops confusing logs on pods whose containers all have exit codes, but we recreate a sandbox before terminating it. changes . Create if len ( pod . Spec . Init Containers ) != 0 { // Pod has init containers, return the first one. changes . Next Init Container To Start = & pod . Spec . Init // Start all containers by default but exclude the ones that succeeded if // Restart Policy is On Failure. for idx , c := range pod . Spec . Containers { if container Succeeded ( & c , pod Status ) && pod . Spec . Restart Policy == v1 . Restart Policy On changes . Containers To Start = append ( changes . Containers To // Check initialization progress. init Last Status , next , done := find Next Init Container To Run ( pod , pod if ! done { if next != nil { init Failed := init Last Status != nil && is Init Container Failed ( init Last if init Failed && ! should Restart On Failure ( pod ) { changes . Kill } else { // Always try to stop containers in unknown state first. if init Last Status != nil && init Last Status . State == kubecontainer . Container State Unknown { changes . Containers To Kill [ init Last Status . ID ] = container To Kill Info { name : next . Name , container : next , message : fmt . Sprintf ( " " , init Last changes . Next Init Container To // Number of running containers to keep. keep // check the status of containers. for idx , container := range pod . Spec . Containers { container Status := pod Status . Find Container Status By // Call internal container post-stop lifecycle hook for any non-running container so that any // allocated cpus are released immediately. If the container is restarted, cpus will be re-allocated // to it. if container Status != nil && container Status . State != kubecontainer . Container State Running { if err := m . internal Lifecycle . Post Stop Container ( container // If container does not exist, or is not running, check whether we // need to restart it. if container Status == nil || container Status . State != kubecontainer . Container State Running { if kubecontainer . Should Container Be Restarted ( & container , pod , pod changes . Containers To Start = append ( changes . Containers To if container Status != nil && container Status . State == kubecontainer . Container State Unknown { // If container is in unknown state, we don't know whether it // is actually running or not, always try killing it before // restart to avoid having 2 running instances of the same container. changes . Containers To Kill [ container Status . ID ] = container To Kill Info { name : container Status . Name , container : & pod . Spec . Containers [ idx ] , message : fmt . Sprintf ( " " , container restart := should Restart On if _ , _ , changed := container Changed ( & container , container } else if liveness , found := m . liveness Manager . Get ( container } else { // Keep the container. keep changes . Containers To Start = append ( changes . Containers To changes . Containers To Kill [ container Status . ID ] = container To Kill Info { name : container klog . V ( 2 ) . Infof ( " " , container . Name , container if keep Count == 0 && len ( changes . Containers To Start ) == 0 { changes . Kill } 
func ( m * kube Generic Runtime Manager ) Sync Pod ( pod * v1 . Pod , pod Status * kubecontainer . Pod Status , pull Secrets [ ] v1 . Secret , back Off * flowcontrol . Backoff ) ( result kubecontainer . Pod Sync Result ) { // Step 1: Compute sandbox and container changes. pod Container Changes := m . compute Pod Actions ( pod , pod klog . V ( 3 ) . Infof ( " " , pod Container if pod Container Changes . Create Sandbox { ref , err := ref . Get if pod Container Changes . Sandbox ID != " " { m . recorder . Eventf ( ref , v1 . Event Type Normal , events . Sandbox // Step 2: Kill the pod if the sandbox has changed. if pod Container Changes . Kill Pod { if pod Container Changes . Create kill Result := m . kill Pod With Sync Result ( pod , kubecontainer . Convert Pod Status To Running Pod ( m . runtime Name , pod result . Add Pod Sync Result ( kill if kill Result . Error ( ) != nil { klog . Errorf ( " " , kill if pod Container Changes . Create Sandbox { m . purge Init Containers ( pod , pod } else { // Step 3: kill any running containers in this pod which are not to keep. for container ID , container Info := range pod Container Changes . Containers To Kill { klog . V ( 3 ) . Infof ( " " , container Info . name , container kill Container Result := kubecontainer . New Sync Result ( kubecontainer . Kill Container , container result . Add Sync Result ( kill Container if err := m . kill Container ( pod , container ID , container Info . name , container Info . message , nil ) ; err != nil { kill Container Result . Fail ( kubecontainer . Err Kill klog . Errorf ( " " , container Info . name , container // Keep terminated init containers fairly aggressively controlled // This is an optimization because container removals are typically handled // by container garbage collector. m . prune Init Containers Before Start ( pod , pod // We pass the value of the pod IP down to generate Pod Sandbox Config and // generate Container Config, which in turn passes it to various other // functions, in order to facilitate functionality that requires this // value (hosts file and downward API) and avoid races determining // the pod IP in cases where a container requires restart but the // pod IP isn't in the status manager yet. // // We default to the IP in the passed-in pod status, and overwrite it if the // sandbox needs to be (re)started. pod if pod Status != nil { pod IP = pod // Step 4: Create a sandbox for the pod if necessary. pod Sandbox ID := pod Container Changes . Sandbox if pod Container Changes . Create create Sandbox Result := kubecontainer . New Sync Result ( kubecontainer . Create Pod result . Add Sync Result ( create Sandbox pod Sandbox ID , msg , err = m . create Pod Sandbox ( pod , pod Container if err != nil { create Sandbox Result . Fail ( kubecontainer . Err Create Pod ref , referr := ref . Get m . recorder . Eventf ( ref , v1 . Event Type Warning , events . Failed Create Pod Sand klog . V ( 4 ) . Infof ( " " , pod Sandbox pod Sandbox Status , err := m . runtime Service . Pod Sandbox Status ( pod Sandbox if err != nil { ref , referr := ref . Get m . recorder . Eventf ( ref , v1 . Event Type Warning , events . Failed Status Pod Sand // If we ever allow updating a pod from non-host-network to // host-network, we may use a stale IP. if ! kubecontainer . Is Host Network Pod ( pod ) { // Overwrite the pod IP passed in the pod status, since we just started the pod sandbox. pod IP = m . determine Pod Sandbox IP ( pod . Namespace , pod . Name , pod Sandbox klog . V ( 4 ) . Infof ( " " , pod // Get pod Sandbox Config for containers to start. config Pod Sandbox Result := kubecontainer . New Sync Result ( kubecontainer . Config Pod Sandbox , pod Sandbox result . Add Sync Result ( config Pod Sandbox pod Sandbox Config , err := m . generate Pod Sandbox Config ( pod , pod Container config Pod Sandbox Result . Fail ( kubecontainer . Err Config Pod // Step 5: start the init container. if container := pod Container Changes . Next Init Container To Start ; container != nil { // Start the next init container. start Container Result := kubecontainer . New Sync Result ( kubecontainer . Start result . Add Sync Result ( start Container is In Back Off , msg , err := m . do Back Off ( pod , container , pod Status , back if is In Back Off { start Container if msg , err := m . start Container ( pod Sandbox ID , pod Sandbox Config , container , pod , pod Status , pull Secrets , pod IP ) ; err != nil { start Container utilruntime . Handle // Step 6: start containers in pod Container Changes.Containers To Start. for _ , idx := range pod Container Changes . Containers To start Container Result := kubecontainer . New Sync Result ( kubecontainer . Start result . Add Sync Result ( start Container is In Back Off , msg , err := m . do Back Off ( pod , container , pod Status , back if is In Back Off { start Container if msg , err := m . start Container ( pod Sandbox ID , pod Sandbox Config , container , pod , pod Status , pull Secrets , pod IP ) ; err != nil { start Container // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch { case err == images . Err Image Pull Back default : utilruntime . Handle } 
func ( m * kube Generic Runtime Manager ) do Back Off ( pod * v1 . Pod , container * v1 . Container , pod Status * kubecontainer . Pod Status , back Off * flowcontrol . Backoff ) ( bool , string , error ) { var c Status * kubecontainer . Container for _ , c := range pod Status . Container Statuses { if c . Name == container . Name && c . State == kubecontainer . Container State Exited { c if c // Use the finished time of the latest exited container as the start point to calculate whether to do back-off. ts := c Status . Finished // back Off requires a unique key to identify the container. key := get Stable if back Off . Is In Back Off Since ( key , ts ) { if ref , err := kubecontainer . Generate Container Ref ( pod , container ) ; err == nil { m . recorder . Eventf ( ref , v1 . Event Type Warning , events . Back Off Start err := fmt . Errorf ( " " , back return true , err . Error ( ) , kubecontainer . Err Crash Loop Back back } 
func ( m * kube Generic Runtime Manager ) Kill Pod ( pod * v1 . Pod , running Pod kubecontainer . Pod , grace Period Override * int64 ) error { err := m . kill Pod With Sync Result ( pod , running Pod , grace Period } 
func ( m * kube Generic Runtime Manager ) kill Pod With Sync Result ( pod * v1 . Pod , running Pod kubecontainer . Pod , grace Period Override * int64 ) ( result kubecontainer . Pod Sync Result ) { kill Container Results := m . kill Containers With Sync Result ( pod , running Pod , grace Period for _ , container Result := range kill Container Results { result . Add Sync Result ( container // stop sandbox, the sandbox will be removed in Garbage Collect kill Sandbox Result := kubecontainer . New Sync Result ( kubecontainer . Kill Pod Sandbox , running result . Add Sync Result ( kill Sandbox // Stop all sandboxes belongs to same pod for _ , pod Sandbox := range running Pod . Sandboxes { if err := m . runtime Service . Stop Pod Sandbox ( pod Sandbox . ID . ID ) ; err != nil { kill Sandbox Result . Fail ( kubecontainer . Err Kill Pod klog . Errorf ( " " , pod } 
func ( m * kube Generic Runtime Manager ) Get Pod Status ( uid kubetypes . UID , name , namespace string ) ( * kubecontainer . Pod Status , error ) { // Now we retain restart count of container as a container label. Each time a container // restarts, pod will read the restart count from the registered dead container, increment // it to get the new restart count, and then add a label with the new restart count on // the newly started container. // However, there are some limitations of this method: // 1. When all dead containers were garbage collected, the container status could // not get the historical value and would be *inaccurate*. Fortunately, the chance // is really slim. // 2. When working with old version containers which have no restart count label, // we can only assume their restart count is 0. // Anyhow, we only promised "best-effort" restart count reporting, we can just ignore // these limitations now. // TODO: move this comment to Sync Pod. pod Sandbox I Ds , err := m . get Sandbox ID By Pod pod Full Name := format . Pod ( & v1 . Pod { Object Meta : metav1 . Object klog . V ( 4 ) . Infof ( " " , pod Sandbox I Ds , pod Full sandbox Statuses := make ( [ ] * runtimeapi . Pod Sandbox Status , len ( pod Sandbox I pod for idx , pod Sandbox ID := range pod Sandbox I Ds { pod Sandbox Status , err := m . runtime Service . Pod Sandbox Status ( pod Sandbox if err != nil { klog . Errorf ( " " , pod Sandbox ID , pod Full sandbox Statuses [ idx ] = pod Sandbox // Only get pod IP from latest sandbox if idx == 0 && pod Sandbox Status . State == runtimeapi . Pod Sandbox State_SANDBOX_READY { pod IP = m . determine Pod Sandbox IP ( namespace , name , pod Sandbox // Get statuses of all containers visible in the pod. container Statuses , err := m . get Pod Container if err != nil { if m . log Reduction . Should Message Be Printed ( err . Error ( ) , pod Full Name ) { klog . Errorf ( " " , pod Full m . log Reduction . Clear ID ( pod Full return & kubecontainer . Pod Status { ID : uid , Name : name , Namespace : namespace , IP : pod IP , Sandbox Statuses : sandbox Statuses , Container Statuses : container } 
func ( m * kube Generic Runtime Manager ) Garbage Collect ( gc Policy kubecontainer . Container GC Policy , all Sources Ready bool , evict Non Deleted Pods bool ) error { return m . container GC . Garbage Collect ( gc Policy , all Sources Ready , evict Non Deleted } 
func ( m * kube Generic Runtime Manager ) Update Pod CIDR ( pod CIDR string ) error { // TODO(#35531): do we really want to write a method on this manager for each // field of the config? klog . Infof ( " " , pod return m . runtime Service . Update Runtime Config ( & runtimeapi . Runtime Config { Network Config : & runtimeapi . Network Config { Pod Cidr : pod } 
func scale From Stateful Set ( ss * apps . Stateful Set ) ( * autoscaling . Scale , error ) { selector , err := metav1 . Label Selector As return & autoscaling . Scale { // TODO: Create a variant of Object Meta type that only contains the fields below. Object Meta : metav1 . Object Meta { Name : ss . Name , Namespace : ss . Namespace , UID : ss . UID , Resource Version : ss . Resource Version , Creation Timestamp : ss . Creation Timestamp , } , Spec : autoscaling . Scale Spec { Replicas : ss . Spec . Replicas , } , Status : autoscaling . Scale } 
func add Conversion Funcs ( scheme * runtime . Scheme ) error { err := scheme . Add Conversion Funcs ( Convert_scheme_Scale Status_To_v1beta1_Scale Status , Convert_v1beta1_Scale Status_To_scheme_Scale } 
func Validate Event ( event * core . Event ) field . Error List { all Errs := field . Error // Because go zero // "New" Events need to have Event Time set, so it's validating old object. if event . Event Time . Time == zero Time { // Make sure event.Namespace and the involved Involved Object.Namespace agree if len ( event . Involved Object . Namespace ) == 0 { // event.Namespace must also be empty (or "default", for compatibility with old clients) if event . Namespace != metav1 . Namespace None && event . Namespace != metav1 . Namespace Default { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , event . Involved } else { // event namespace must match if event . Namespace != event . Involved Object . Namespace { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , event . Involved } else { if len ( event . Involved Object . Namespace ) == 0 && event . Namespace != metav1 . Namespace System { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , event . Involved if len ( event . Reporting Controller ) == 0 { all Errs = append ( all Errs , field . Required ( field . New for _ , msg := range validation . Is Qualified Name ( event . Reporting Controller ) { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , event . Reporting if len ( event . Reporting Instance ) == 0 { all Errs = append ( all Errs , field . Required ( field . New if len ( event . Reporting Instance ) > Reporting Instance Length Limit { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , " " , fmt . Sprintf ( " " , Reporting Instance Length if len ( event . Action ) == 0 { all Errs = append ( all Errs , field . Required ( field . New if len ( event . Action ) > Action Length Limit { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , " " , fmt . Sprintf ( " " , Action Length if len ( event . Reason ) == 0 { all Errs = append ( all Errs , field . Required ( field . New if len ( event . Reason ) > Reason Length Limit { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , " " , fmt . Sprintf ( " " , Reason Length if len ( event . Message ) > Note Length Limit { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , " " , fmt . Sprintf ( " " , Note Length for _ , msg := range validation . Is DNS1123Subdomain ( event . Namespace ) { all Errs = append ( all Errs , field . Invalid ( field . New return all } 
func Check Eviction Support ( clientset kubernetes . Interface ) ( string , error ) { discovery group List , err := discovery Client . Server found Policy var policy Group for _ , group := range group List . Groups { if group . Name == " " { found Policy policy Group Version = group . Preferred Version . Group if ! found Policy resource List , err := discovery Client . Server Resources For Group for _ , resource := range resource List . API Resources { if resource . Name == Eviction Subresource && resource . Kind == Eviction Kind { return policy Group } 
func ( d * Helper ) Delete Pod ( pod corev1 . Pod ) error { return d . Client . Core V1 ( ) . Pods ( pod . Namespace ) . Delete ( pod . Name , d . make Delete } 
func ( d * Helper ) Evict Pod ( pod corev1 . Pod , policy Group Version string ) error { eviction := & policyv1beta1 . Eviction { Type Meta : metav1 . Type Meta { API Version : policy Group Version , Kind : Eviction Kind , } , Object Meta : metav1 . Object Meta { Name : pod . Name , Namespace : pod . Namespace , } , Delete Options : d . make Delete // Remember to change change the URL manipulation func when Eviction's version change return d . Client . Policy } 
func ( d * Helper ) Get Pods For Deletion ( node Name string ) ( * pod Delete List , [ ] error ) { label Selector , err := labels . Parse ( d . Pod pod List , err := d . Client . Core V1 ( ) . Pods ( metav1 . Namespace All ) . List ( metav1 . List Options { Label Selector : label Selector . String ( ) , Field Selector : fields . Selector From Set ( fields . Set { " " : node pods := [ ] pod for _ , pod := range pod List . Items { var status pod Delete for _ , filter := range d . make pods = append ( pods , pod list := & pod Delete } 
func ( r * Eviction REST ) Group Version Kind ( containing GV schema . Group Version ) schema . Group Version Kind { return schema . Group Version } 
func propagate Dry Run ( eviction * policy . Eviction , options * metav1 . Create Options ) ( * metav1 . Delete Options , error ) { if eviction . Delete Options == nil { return & metav1 . Delete Options { Dry Run : options . Dry if len ( eviction . Delete Options . Dry Run ) == 0 { eviction . Delete Options . Dry Run = options . Dry return eviction . Delete if len ( options . Dry Run ) == 0 { return eviction . Delete if ! reflect . Deep Equal ( options . Dry Run , eviction . Delete Options . Dry Run ) { return nil , fmt . Errorf ( " " , options . Dry Run , eviction . Delete Options . Dry return eviction . Delete } 
func ( r * Eviction REST ) Create ( ctx context . Context , obj runtime . Object , create Validation rest . Validate Object Func , options * metav1 . Create deletion Options , err := propagate Dry obj , err = r . store . Get ( ctx , eviction . Name , & metav1 . Get if create Validation != nil { if err := create Validation ( eviction . Deep Copy // Evicting a terminal pod should result in direct deletion of pod as it already caused disruption by the time we are evicting. // There is no need to check for pdb. if pod . Status . Phase == api . Pod Succeeded || pod . Status . Phase == api . Pod Failed { _ , _ , err = r . store . Delete ( ctx , eviction . Name , deletion return & metav1 . Status { Status : metav1 . Status var rt var pdb err = retry . Retry On Conflict ( Evictions Retry , func ( ) error { pdbs , err := r . get Pod Disruption if len ( pdbs ) > 1 { rt Status = & metav1 . Status { Status : metav1 . Status pdb // Try to verify-and-decrement // If it was false already, or if it becomes false during the course of our retries, // raise an error marked as a 429. if err := r . check And Decrement ( pod . Namespace , pod . Name , pdb , dryrun . Is Dry Run ( deletion Options . Dry if err == wait . Err Wait Timeout { err = errors . New Timeout Error ( fmt . Sprintf ( " " , pdb if rt Status != nil { return rt // At this point there was either no PDB or we succeeded in decrementing // Try the delete _ , _ , err = r . store . Delete ( ctx , eviction . Name , deletion // Success! return & metav1 . Status { Status : metav1 . Status } 
func ( r * Eviction REST ) check And Decrement ( namespace string , pod Name string , pdb policyv1beta1 . Pod Disruption Budget , dry Run bool ) error { if pdb . Status . Observed Generation < pdb . Generation { // TODO(mml): Add a Retry-After header. Once there are time-based // budgets, we can sometimes compute a sensible suggested value. But // even without that, we can give a suggestion (10 minutes?) that // prevents well-behaved clients from hammering us. err := errors . New Too Many err . Err Status . Details . Causes = append ( err . Err Status . Details . Causes , metav1 . Status if pdb . Status . Pod Disruptions Allowed < 0 { return errors . New if len ( pdb . Status . Disrupted Pods ) > Max Disrupted Pod Size { return errors . New if pdb . Status . Pod Disruptions Allowed == 0 { err := errors . New Too Many err . Err Status . Details . Causes = append ( err . Err Status . Details . Causes , metav1 . Status Cause { Type : " " , Message : fmt . Sprintf ( " " , pdb . Name , pdb . Status . Desired Healthy , pdb . Status . Current pdb . Status . Pod Disruptions if pdb . Status . Disrupted Pods == nil { pdb . Status . Disrupted // If this is a dry-run, we don't need to go any further than that. if dry // Eviction handler needs to inform the PDB controller that it is about to delete a pod // so it should not consider it as available in calculations when updating Pod Disruptions allowed. // If the pod is not deleted within a reasonable time limit PDB controller will assume that it won't // be deleted at all and remove it from Disrupted Pod map. pdb . Status . Disrupted Pods [ pod if _ , err := r . pod Disruption Budget Client . Pod Disruption Budgets ( namespace ) . Update } 
func ( r * Eviction REST ) get Pod Disruption Budgets ( ctx context . Context , pod * api . Pod ) ( [ ] policyv1beta1 . Pod Disruption pdb List , err := r . pod Disruption Budget Client . Pod Disruption Budgets ( pod . Namespace ) . List ( metav1 . List var pdbs [ ] policyv1beta1 . Pod Disruption for _ , pdb := range pdb selector , err := metav1 . Label Selector As } 
func New Replica Set Controller ( rs Informer appsinformers . Replica Set Informer , pod Informer coreinformers . Pod Informer , kube Client clientset . Interface , burst Replicas int ) * Replica Set Controller { event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Client . Core return New Base Controller ( rs Informer , pod Informer , kube Client , burst Replicas , apps . Scheme Group Version . With Kind ( " " ) , " " , " " , controller . Real Pod Control { Kube Client : kube Client , Recorder : event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event } 
func New Base Controller ( rs Informer appsinformers . Replica Set Informer , pod Informer coreinformers . Pod Informer , kube Client clientset . Interface , burst Replicas int , gvk schema . Group Version Kind , metric Owner Name , queue Name string , pod Control controller . Pod Control Interface ) * Replica Set Controller { if kube Client != nil && kube Client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { metrics . Register Metric And Track Rate Limiter Usage ( metric Owner Name , kube Client . Core V1 ( ) . REST Client ( ) . Get Rate rsc := & Replica Set Controller { Group Version Kind : gvk , kube Client : kube Client , pod Control : pod Control , burst Replicas : burst Replicas , expectations : controller . New UID Tracking Controller Expectations ( controller . New Controller Expectations ( ) ) , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , queue rs Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : rsc . enqueue Replica Set , Update Func : rsc . update RS , // This will enter the sync loop and no-op, because the replica set has been deleted from the store. // Note that deleting a replica set immediately after scaling it to 0 will not work. The recommended // way of achieving this is by performing a `stop` operation on the replica set. Delete Func : rsc . enqueue Replica rsc . rs Lister = rs rsc . rs Lister Synced = rs Informer . Informer ( ) . Has pod Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : rsc . add Pod , // This invokes the Replica Set for every pod change, eg: host assignment. Though this might seem like // overkill the most frequent pod update is status, and the associated Replica Set will only list from // local storage, so it should be ok. Update Func : rsc . update Pod , Delete Func : rsc . delete rsc . pod Lister = pod rsc . pod Lister Synced = pod Informer . Informer ( ) . Has rsc . sync Handler = rsc . sync Replica } 
func ( rsc * Replica Set Controller ) Set Event Recorder ( recorder record . Event Recorder ) { // TODO: Hack. We can't cleanly shutdown the event recorder, so benchmarks // need to pass in a fake. rsc . pod Control = controller . Real Pod Control { Kube Client : rsc . kube } 
func ( rsc * Replica Set Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer rsc . queue . Shut controller Name := strings . To klog . Infof ( " " , controller defer klog . Infof ( " " , controller if ! controller . Wait For Cache Sync ( rsc . Kind , stop Ch , rsc . pod Lister Synced , rsc . rs Lister for i := 0 ; i < workers ; i ++ { go wait . Until ( rsc . worker , time . Second , stop <- stop } 
func ( rsc * Replica Set Controller ) get Pod Replica Sets ( pod * v1 . Pod ) [ ] * apps . Replica Set { rss , err := rsc . rs Lister . Get Pod Replica if len ( rss ) > 1 { // Controller Ref will ensure we don't do anything crazy, but more than one // item in this list nevertheless constitutes user error. utilruntime . Handle } 
func ( rsc * Replica Set Controller ) resolve Controller Ref ( namespace string , controller Ref * metav1 . Owner Reference ) * apps . Replica Set { // We can't look up by UID, so look up by Name and then verify UID. // Don't even try to look up by Name if it's the wrong Kind. if controller rs , err := rsc . rs Lister . Replica Sets ( namespace ) . Get ( controller if rs . UID != controller Ref . UID { // The controller we found with this Name is not the same one that the // Controller } 
func ( rsc * Replica Set Controller ) update RS ( old , cur interface { } ) { old RS := old . ( * apps . Replica cur RS := cur . ( * apps . Replica // You might imagine that we only really need to enqueue the // replica set when Spec changes, but it is safer to sync any // time this function is triggered. That way a full informer // resync can requeue any replica set that don't yet have pods // but whose last attempts at creating a pod have failed (since // we don't block on creation of pods) instead of those // replica sets stalling indefinitely. Enqueueing every time // does result in some spurious syncs (like when Status.Replica // is updated and the watch notification from it retriggers // this function), but in general extra resyncs shouldn't be // that bad as Replica Sets that haven't met expectations yet won't // sync, and all the listing is done using local stores. if * ( old RS . Spec . Replicas ) != * ( cur RS . Spec . Replicas ) { klog . V ( 4 ) . Infof ( " " , rsc . Kind , cur RS . Name , * ( old RS . Spec . Replicas ) , * ( cur rsc . enqueue Replica } 
func ( rsc * Replica Set Controller ) add if pod . Deletion Timestamp != nil { // on a restart of the controller manager, it's possible a new pod shows up in a state that // is already pending deletion. Prevent the pod from being a creation observation. rsc . delete // If it has a Controller Ref, that's all that matters. if controller Ref := metav1 . Get Controller Of ( pod ) ; controller Ref != nil { rs := rsc . resolve Controller Ref ( pod . Namespace , controller rs Key , err := controller . Key rsc . expectations . Creation Observed ( rs rsc . enqueue Replica // Otherwise, it's an orphan. Get a list of all matching Replica Sets and sync // them to see if anyone wants to adopt it. // DO NOT observe creation because no controller should be waiting for an // orphan. rss := rsc . get Pod Replica for _ , rs := range rss { rsc . enqueue Replica } 
func ( rsc * Replica Set Controller ) update Pod ( old , cur interface { } ) { cur old if cur Pod . Resource Version == old Pod . Resource Version { // Periodic resync will send update events for all known pods. // Two different versions of the same pod will always have different R label Changed := ! reflect . Deep Equal ( cur Pod . Labels , old if cur Pod . Deletion Timestamp != nil { // when a pod is deleted gracefully it's deletion timestamp is first modified to reflect a grace period, // and after such time has passed, the kubelet actually deletes it from the store. We receive an update // for modification of the deletion timestamp and expect an rs to create more replicas asap, not wait // until the kubelet actually deletes the pod. This is different from the Phase of a pod changing, because // an rs never initiates a phase change, and so is never asleep waiting for the same. rsc . delete Pod ( cur if label Changed { // we don't need to check the old Pod.Deletion Timestamp because Deletion Timestamp cannot be unset. rsc . delete Pod ( old cur Controller Ref := metav1 . Get Controller Of ( cur old Controller Ref := metav1 . Get Controller Of ( old controller Ref Changed := ! reflect . Deep Equal ( cur Controller Ref , old Controller if controller Ref Changed && old Controller Ref != nil { // The Controller Ref was changed. Sync the old controller, if any. if rs := rsc . resolve Controller Ref ( old Pod . Namespace , old Controller Ref ) ; rs != nil { rsc . enqueue Replica // If it has a Controller Ref, that's all that matters. if cur Controller Ref != nil { rs := rsc . resolve Controller Ref ( cur Pod . Namespace , cur Controller klog . V ( 4 ) . Infof ( " " , cur Pod . Name , old Pod . Object Meta , cur Pod . Object rsc . enqueue Replica // TODO: Min Ready Seconds in the Pod will generate an Available condition to be added in // the Pod status which in turn will trigger a requeue of the owning replica set thus // having its status updated with the newly available replica. For now, we can fake the // update by resyncing the controller Min Ready Seconds after the it is requeued because // a Pod transitioned to Ready. // Note that this still suffers from #29229, we are just moving the problem one level // "closer" to kubelet (from the deployment to the replica set controller). if ! podutil . Is Pod Ready ( old Pod ) && podutil . Is Pod Ready ( cur Pod ) && rs . Spec . Min Ready Seconds > 0 { klog . V ( 2 ) . Infof ( " " , rsc . Kind , rs . Name , rs . Spec . Min Ready // Add a second to avoid milliseconds skew in Add After. // See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info. rsc . enqueue Replica Set After ( rs , ( time . Duration ( rs . Spec . Min Ready // Otherwise, it's an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. if label Changed || controller Ref Changed { rss := rsc . get Pod Replica Sets ( cur klog . V ( 4 ) . Infof ( " " , cur Pod . Name , old Pod . Object Meta , cur Pod . Object for _ , rs := range rss { rsc . enqueue Replica } 
func ( rsc * Replica Set Controller ) delete // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the pod // changed labels the new Replica Set will not be woken up till the periodic resync. if ! ok { tombstone , ok := obj . ( cache . Deleted Final State if ! ok { utilruntime . Handle if ! ok { utilruntime . Handle controller Ref := metav1 . Get Controller if controller rs := rsc . resolve Controller Ref ( pod . Namespace , controller rs Key , err := controller . Key klog . V ( 4 ) . Infof ( " " , pod . Namespace , pod . Name , utilruntime . Get Caller ( ) , pod . Deletion rsc . expectations . Deletion Observed ( rs Key , controller . Pod rsc . enqueue Replica } 
func ( rsc * Replica Set Controller ) enqueue Replica Set ( obj interface { } ) { key , err := controller . Key if err != nil { utilruntime . Handle } 
func ( rsc * Replica Set Controller ) enqueue Replica Set After ( obj interface { } , after time . Duration ) { key , err := controller . Key if err != nil { utilruntime . Handle rsc . queue . Add } 
func ( rsc * Replica Set Controller ) manage Replicas ( filtered Pods [ ] * v1 . Pod , rs * apps . Replica Set ) error { diff := len ( filtered rs Key , err := controller . Key if err != nil { utilruntime . Handle if diff > rsc . burst Replicas { diff = rsc . burst // TODO: Track UI Ds of creates just like deletes. The problem currently // is we'd need to wait on the result of a create to record the pod's // UID, which would require locking *across* the create, which will turn // into a performance bottleneck. We should generate a UID for the pod // beforehand and store it via Expect Creations. rsc . expectations . Expect Creations ( rs // Batch the pod creates. Batch sizes start at Slow Start Initial Batch Size // and double with each successful iteration in a kind of "slow start". // This handles attempts to start large numbers of pods that would // likely all fail with the same error. For example a project with a // low quota that attempts to create a large number of pods will be // prevented from spamming the API service with the pod create requests // after one of its pods fails. Conveniently, this also prevents the // event spam that those failures would generate. successful Creations , err := slow Start Batch ( diff , controller . Slow Start Initial Batch Size , func ( ) error { err := rsc . pod Control . Create Pods With Controller Ref ( rs . Namespace , & rs . Spec . Template , rs , metav1 . New Controller Ref ( rs , rsc . Group Version if err != nil && errors . Is // Any skipped pods that we never attempted to start shouldn't be expected. // The skipped pods will be retried later. The next controller resync will // retry the slow start process. if skipped Pods := diff - successful Creations ; skipped Pods > 0 { klog . V ( 2 ) . Infof ( " " , skipped for i := 0 ; i < skipped Pods ; i ++ { // Decrement the expected number of creates because the informer won't observe this pod rsc . expectations . Creation Observed ( rs } else if diff > 0 { if diff > rsc . burst Replicas { diff = rsc . burst // Choose which Pods to delete, preferring those in earlier phases of startup. pods To Delete := get Pods To Delete ( filtered // Snapshot the UI Ds (ns/name) of the pods we're expecting to see // deleted, so we know to record their expectations exactly once either // when we see it as an update of the deletion timestamp, or as a delete. // Note that if the labels on a pod/rs change in a way that the pod gets // orphaned, the rs will only wake up after the expectations have // expired even if other pods are deleted. rsc . expectations . Expect Deletions ( rs Key , get Pod Keys ( pods To err var wg sync . Wait for _ , pod := range pods To Delete { go func ( target if err := rsc . pod Control . Delete Pod ( rs . Namespace , target Pod . Name , rs ) ; err != nil { // Decrement the expected number of deletes because the informer won't observe this deletion pod Key := controller . Pod Key ( target klog . V ( 2 ) . Infof ( " " , pod rsc . expectations . Deletion Observed ( rs Key , pod err select { case err := <- err } 
func ( rsc * Replica Set Controller ) sync Replica Set ( key string ) error { start defer func ( ) { klog . V ( 4 ) . Infof ( " " , rsc . Kind , key , time . Since ( start namespace , name , err := cache . Split Meta Namespace rs , err := rsc . rs Lister . Replica if errors . Is Not rsc . expectations . Delete rs Needs Sync := rsc . expectations . Satisfied selector , err := metav1 . Label Selector As if err != nil { utilruntime . Handle // list all pods to include the pods that don't match the rs`s selector // anymore but has the stale controller ref. // TODO: Do the List and Filter in a single pass, or use an index. all Pods , err := rsc . pod // Ignore inactive pods. filtered Pods := controller . Filter Active Pods ( all // NOTE: filtered Pods are pointing to objects from cache - if you need to // modify them, you need to copy it first. filtered Pods , err = rsc . claim Pods ( rs , selector , filtered var manage Replicas if rs Needs Sync && rs . Deletion Timestamp == nil { manage Replicas Err = rsc . manage Replicas ( filtered rs = rs . Deep new Status := calculate Status ( rs , filtered Pods , manage Replicas // Always updates status as pods come up or die. updated RS , err := update Replica Set Status ( rsc . kube Client . Apps V1 ( ) . Replica Sets ( rs . Namespace ) , rs , new // Resync the Replica Set after Min Ready Seconds as a last line of defense to guard against clock-skew. if manage Replicas Err == nil && updated RS . Spec . Min Ready Seconds > 0 && updated RS . Status . Ready Replicas == * ( updated RS . Spec . Replicas ) && updated RS . Status . Available Replicas != * ( updated RS . Spec . Replicas ) { rsc . enqueue Replica Set After ( updated RS , time . Duration ( updated RS . Spec . Min Ready return manage Replicas } 
func slow Start Batch ( count int , initial Batch for batch Size := integer . Int Min ( remaining , initial Batch Size ) ; batch Size > 0 ; batch Size = integer . Int Min ( 2 * batch Size , remaining ) { err Ch := make ( chan error , batch var wg sync . Wait wg . Add ( batch for i := 0 ; i < batch if err := fn ( ) ; err != nil { err cur Successes := batch Size - len ( err successes += cur if len ( err Ch ) > 0 { return successes , <- err remaining -= batch } 
func ( az * Cloud ) List Routes ( ctx context . Context , cluster Name string ) ( [ ] * cloudprovider . Route , error ) { klog . V ( 10 ) . Infof ( " " , cluster route Table , exists Route Table , err := az . get Route routes , err := process Routes ( route Table , exists Route // Compose routes for unmanaged routes so that node controller won't retry creating routes for them. unmanaged Nodes , err := az . Get Unmanaged az . route CID Rs defer az . route CID Rs for _ , node Name := range unmanaged Nodes . List ( ) { if cidr , ok := az . route CID Rs [ node Name ] ; ok { routes = append ( routes , & cloudprovider . Route { Name : node Name , Target Node : map Route Name To Node Name ( node Name ) , Destination } 
func process Routes ( route Table network . Route var kube if route Table . Route Table Properties Format != nil && route Table . Routes != nil { kube Routes = make ( [ ] * cloudprovider . Route , len ( * route for i , route := range * route Table . Routes { instance := map Route Name To Node cidr := * route . Address kube Routes [ i ] = & cloudprovider . Route { Name : * route . Name , Target Node : instance , Destination return kube } 
func ( az * Cloud ) Create Route ( ctx context . Context , cluster Name string , name Hint string , kube Route * cloudprovider . Route ) error { // Returns for unmanaged nodes because azure cloud provider couldn't fetch information for them. node Name := string ( kube Route . Target unmanaged , err := az . Is Node Unmanaged ( node if unmanaged { klog . V ( 2 ) . Infof ( " " , kube Route . Target az . route CID Rs defer az . route CID Rs az . route CID Rs [ node Name ] = kube Route . Destination klog . V ( 2 ) . Infof ( " " , cluster Name , kube Route . Target Node , kube Route . Destination if err := az . create Route Table If Not Exists ( cluster Name , kube target IP , _ , err := az . get IP For Machine ( kube Route . Target route Name := map Node Name To Route Name ( kube Route . Target route := network . Route { Name : to . String Ptr ( route Name ) , Route Properties Format : & network . Route Properties Format { Address Prefix : to . String Ptr ( kube Route . Destination CIDR ) , Next Hop Type : network . Route Next Hop Type Virtual Appliance , Next Hop IP Address : to . String Ptr ( target klog . V ( 3 ) . Infof ( " " , kube Route . Target Node , kube Route . Destination err = az . Create Or Update klog . V ( 2 ) . Infof ( " " , cluster Name , kube Route . Target Node , kube Route . Destination } 
func ( az * Cloud ) Delete Route ( ctx context . Context , cluster Name string , kube Route * cloudprovider . Route ) error { // Returns for unmanaged nodes because azure cloud provider couldn't fetch information for them. node Name := string ( kube Route . Target unmanaged , err := az . Is Node Unmanaged ( node if unmanaged { klog . V ( 2 ) . Infof ( " " , kube Route . Target az . route CID Rs defer az . route CID Rs delete ( az . route CID Rs , node klog . V ( 2 ) . Infof ( " " , cluster Name , kube Route . Target Node , kube Route . Destination route Name := map Node Name To Route Name ( kube Route . Target err = az . Delete Route With Name ( route klog . V ( 2 ) . Infof ( " " , cluster Name , kube Route . Target Node , kube Route . Destination } 
func map Route Name To Node Name ( route Name string ) types . Node Name { return types . Node Name ( fmt . Sprintf ( " " , route } 
func ( c * Fake Priority Classes ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( priorityclasses } 
func ( c * Fake Priority Classes ) Create ( priority Class * v1alpha1 . Priority Class ) ( result * v1alpha1 . Priority Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( priorityclasses Resource , priority Class ) , & v1alpha1 . Priority return obj . ( * v1alpha1 . Priority } 
func ( c * Fake Priority Classes ) Update ( priority Class * v1alpha1 . Priority Class ) ( result * v1alpha1 . Priority Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( priorityclasses Resource , priority Class ) , & v1alpha1 . Priority return obj . ( * v1alpha1 . Priority } 
func ( c * Fake Priority Classes ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( priorityclasses Resource , name ) , & v1alpha1 . Priority } 
func ( c * Fake Priority Classes ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( priorityclasses Resource , list _ , err := c . Fake . Invokes ( action , & v1alpha1 . Priority Class } 
func ( c * Fake Priority Classes ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1alpha1 . Priority Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( priorityclasses Resource , name , pt , data , subresources ... ) , & v1alpha1 . Priority return obj . ( * v1alpha1 . Priority } 
func New ( ttl time . Duration , stop <- chan struct { } ) Cache { cache := new Scheduler Cache ( ttl , clean Assumed } 
func ( cache * scheduler Cache ) create Image State Summary ( state * image State ) * schedulernodeinfo . Image State Summary { return & schedulernodeinfo . Image State Summary { Size : state . size , Num } 
func ( cache * scheduler Cache ) move Node Info To // if the node info list item is already at the head, we are done. if ni == cache . head if cache . head Node != nil { cache . head ni . next = cache . head cache . head } 
func ( cache * scheduler Cache ) remove Node Info From // if the removed item was at the head, we must update the head. if ni == cache . head Node { cache . head } 
func ( cache * scheduler Cache ) Snapshot ( ) * Snapshot { cache . mu . R defer cache . mu . R nodes := make ( map [ string ] * schedulernodeinfo . Node assumed Pods := make ( map [ string ] bool , len ( cache . assumed for k , v := range cache . assumed Pods { assumed return & Snapshot { Nodes : nodes , Assumed Pods : assumed } 
func ( cache * scheduler Cache ) Update Node Info Snapshot ( node Snapshot * Node Info balanced Volumes Enabled := utilfeature . Default Feature Gate . Enabled ( features . Balance Attached Node // Get the last generation of the the snapshot. snapshot Generation := node // Start from the head of the Node Info doubly linked list and update snapshot // of Node Infos updated after the last snapshot. for node := cache . head Node ; node != nil ; node = node . next { if node . info . Get Generation ( ) <= snapshot if balanced Volumes Enabled && node . info . Transient Info != nil { // Transient scheduler info is reset here. node . info . Transient Info . Reset Transient Scheduler if np := node . info . Node ( ) ; np != nil { node Snapshot . Node Info // Update the snapshot generation with the latest Node Info generation. if cache . head Node != nil { node Snapshot . Generation = cache . head Node . info . Get if len ( node Snapshot . Node Info Map ) > len ( cache . nodes ) { for name := range node Snapshot . Node Info Map { if _ , ok := cache . nodes [ name ] ; ! ok { delete ( node Snapshot . Node Info } 
func ( cache * scheduler Cache ) finish Binding ( pod * v1 . Pod , now time . Time ) error { key , err := schedulernodeinfo . Get Pod cache . mu . R defer cache . mu . R curr State , ok := cache . pod if ok && cache . assumed curr State . binding curr } 
func ( cache * scheduler Cache ) add Pod ( pod * v1 . Pod ) { n , ok := cache . nodes [ pod . Spec . Node if ! ok { n = new Node Info List Item ( schedulernodeinfo . New Node cache . nodes [ pod . Spec . Node n . info . Add cache . move Node Info To Head ( pod . Spec . Node } 
func ( cache * scheduler Cache ) update Pod ( old Pod , new Pod * v1 . Pod ) error { if err := cache . remove Pod ( old cache . add Pod ( new } 
func ( cache * scheduler Cache ) remove Pod ( pod * v1 . Pod ) error { n , ok := cache . nodes [ pod . Spec . Node if ! ok { return fmt . Errorf ( " " , pod . Spec . Node if err := n . info . Remove if len ( n . info . Pods ( ) ) == 0 && n . info . Node ( ) == nil { cache . remove Node Info From List ( pod . Spec . Node } else { cache . move Node Info To Head ( pod . Spec . Node } 
func ( cache * scheduler Cache ) add Node Image States ( node * v1 . Node , node Info * schedulernodeinfo . Node Info ) { new Sum := make ( map [ string ] * schedulernodeinfo . Image State for _ , image := range node . Status . Images { for _ , name := range image . Names { // update the entry in image States state , ok := cache . image if ! ok { state = & image State { size : image . Size Bytes , nodes : sets . New cache . image // create the image State Summary for this image if _ , ok := new Sum [ name ] ; ! ok { new Sum [ name ] = cache . create Image State node Info . Set Image States ( new } 
func ( cache * scheduler Cache ) remove Node Image for _ , image := range node . Status . Images { for _ , name := range image . Names { state , ok := cache . image if len ( state . nodes ) == 0 { // Remove the unused image to make sure the length of // image States represents the total number of different // images on all nodes delete ( cache . image } 
func ( cache * scheduler Cache ) cleanup Assumed // The size of assumed Pods should be small for key := range cache . assumed Pods { ps , ok := cache . pod if ! ps . binding if err := cache . expire } 
func New Etcd Phase ( ) workflow . Phase { phase := workflow . Phase { Name : " " , Short : " " , Long : cmdutil . Macro Command Long Description , Phases : [ ] workflow . Phase { new Etcd Local Sub } 
func Get Threshold Quantity ( value Threshold return resource . New Quantity ( int64 ( float64 ( capacity . Value ( ) ) * float64 ( value . Percentage ) ) , resource . Binary } 
func Get Name From Callsite ( ignored const max for i := 1 ; i < max if ! ok { file , line , ok = extract Stack i += max if has Package ( file , append ( ignored file = trim Package } 
func has Package ( file string , ignored Packages [ ] string ) bool { for _ , ignored Package := range ignored Packages { if strings . Contains ( file , ignored } 
func trim Package Prefix ( file string ) string { if l := strings . Last if l := strings . Last if l := strings . Last } 
func extract Stack matches := stack Creator . Find String } 
func Fs } 
func ( c * priority Classes ) Get ( name string , options v1 . Get Options ) ( result * v1alpha1 . Priority Class , err error ) { result = & v1alpha1 . Priority err = c . client . Get ( ) . Resource ( " " ) . Name ( name ) . Versioned Params ( & options , scheme . Parameter } 
func ( c * priority Classes ) Create ( priority Class * v1alpha1 . Priority Class ) ( result * v1alpha1 . Priority Class , err error ) { result = & v1alpha1 . Priority err = c . client . Post ( ) . Resource ( " " ) . Body ( priority } 
func ( c * priority Classes ) Update ( priority Class * v1alpha1 . Priority Class ) ( result * v1alpha1 . Priority Class , err error ) { result = & v1alpha1 . Priority err = c . client . Put ( ) . Resource ( " " ) . Name ( priority Class . Name ) . Body ( priority } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Cloud Provider Configuration ) ( nil ) , ( * config . Cloud Provider Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Cloud Provider Configuration_To_config_Cloud Provider Configuration ( a . ( * v1alpha1 . Cloud Provider Configuration ) , b . ( * config . Cloud Provider if err := s . Add Generated Conversion Func ( ( * config . Cloud Provider Configuration ) ( nil ) , ( * v1alpha1 . Cloud Provider Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Cloud Provider Configuration_To_v1alpha1_Cloud Provider Configuration ( a . ( * config . Cloud Provider Configuration ) , b . ( * v1alpha1 . Cloud Provider if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Deprecated Controller Configuration ) ( nil ) , ( * config . Deprecated Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Deprecated Controller Configuration_To_config_Deprecated Controller Configuration ( a . ( * v1alpha1 . Deprecated Controller Configuration ) , b . ( * config . Deprecated Controller if err := s . Add Generated Conversion Func ( ( * config . Deprecated Controller Configuration ) ( nil ) , ( * v1alpha1 . Deprecated Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Deprecated Controller Configuration_To_v1alpha1_Deprecated Controller Configuration ( a . ( * config . Deprecated Controller Configuration ) , b . ( * v1alpha1 . Deprecated Controller if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Generic Controller Manager Configuration ) ( nil ) , ( * config . Generic Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Generic Controller Manager Configuration_To_config_Generic Controller Manager Configuration ( a . ( * v1alpha1 . Generic Controller Manager Configuration ) , b . ( * config . Generic Controller Manager if err := s . Add Generated Conversion Func ( ( * config . Generic Controller Manager Configuration ) ( nil ) , ( * v1alpha1 . Generic Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Generic Controller Manager Configuration_To_v1alpha1_Generic Controller Manager Configuration ( a . ( * config . Generic Controller Manager Configuration ) , b . ( * v1alpha1 . Generic Controller Manager if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Cloud Shared Configuration ) ( nil ) , ( * config . Kube Cloud Shared Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Cloud Shared Configuration_To_config_Kube Cloud Shared Configuration ( a . ( * v1alpha1 . Kube Cloud Shared Configuration ) , b . ( * config . Kube Cloud Shared if err := s . Add Generated Conversion Func ( ( * config . Kube Cloud Shared Configuration ) ( nil ) , ( * v1alpha1 . Kube Cloud Shared Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Cloud Shared Configuration_To_v1alpha1_Kube Cloud Shared Configuration ( a . ( * config . Kube Cloud Shared Configuration ) , b . ( * v1alpha1 . Kube Cloud Shared if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Controller Manager Configuration ) ( nil ) , ( * config . Kube Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Controller Manager Configuration_To_config_Kube Controller Manager Configuration ( a . ( * v1alpha1 . Kube Controller Manager Configuration ) , b . ( * config . Kube Controller Manager if err := s . Add Generated Conversion Func ( ( * config . Kube Controller Manager Configuration ) ( nil ) , ( * v1alpha1 . Kube Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Controller Manager Configuration_To_v1alpha1_Kube Controller Manager Configuration ( a . ( * config . Kube Controller Manager Configuration ) , b . ( * v1alpha1 . Kube Controller Manager if err := s . Add Conversion Func ( ( * config . Generic Controller Manager Configuration ) ( nil ) , ( * v1alpha1 . Generic Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Generic Controller Manager Configuration_To_v1alpha1_Generic Controller Manager Configuration ( a . ( * config . Generic Controller Manager Configuration ) , b . ( * v1alpha1 . Generic Controller Manager if err := s . Add Conversion Func ( ( * config . Kube Cloud Shared Configuration ) ( nil ) , ( * v1alpha1 . Kube Cloud Shared Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Cloud Shared Configuration_To_v1alpha1_Kube Cloud Shared Configuration ( a . ( * config . Kube Cloud Shared Configuration ) , b . ( * v1alpha1 . Kube Cloud Shared if err := s . Add Conversion Func ( ( * v1alpha1 . Generic Controller Manager Configuration ) ( nil ) , ( * config . Generic Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Generic Controller Manager Configuration_To_config_Generic Controller Manager Configuration ( a . ( * v1alpha1 . Generic Controller Manager Configuration ) , b . ( * config . Generic Controller Manager if err := s . Add Conversion Func ( ( * v1alpha1 . Kube Cloud Shared Configuration ) ( nil ) , ( * config . Kube Cloud Shared Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Cloud Shared Configuration_To_config_Kube Cloud Shared Configuration ( a . ( * v1alpha1 . Kube Cloud Shared Configuration ) , b . ( * config . Kube Cloud Shared } 
func Convert_v1alpha1_Cloud Provider Configuration_To_config_Cloud Provider Configuration ( in * v1alpha1 . Cloud Provider Configuration , out * config . Cloud Provider Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Cloud Provider Configuration_To_config_Cloud Provider } 
func Convert_config_Cloud Provider Configuration_To_v1alpha1_Cloud Provider Configuration ( in * config . Cloud Provider Configuration , out * v1alpha1 . Cloud Provider Configuration , s conversion . Scope ) error { return auto Convert_config_Cloud Provider Configuration_To_v1alpha1_Cloud Provider } 
func Convert_v1alpha1_Deprecated Controller Configuration_To_config_Deprecated Controller Configuration ( in * v1alpha1 . Deprecated Controller Configuration , out * config . Deprecated Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Deprecated Controller Configuration_To_config_Deprecated Controller } 
func Convert_config_Deprecated Controller Configuration_To_v1alpha1_Deprecated Controller Configuration ( in * config . Deprecated Controller Configuration , out * v1alpha1 . Deprecated Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Deprecated Controller Configuration_To_v1alpha1_Deprecated Controller } 
func Convert_v1alpha1_Kube Controller Manager Configuration_To_config_Kube Controller Manager Configuration ( in * v1alpha1 . Kube Controller Manager Configuration , out * config . Kube Controller Manager Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Controller Manager Configuration_To_config_Kube Controller Manager } 
func Convert_config_Kube Controller Manager Configuration_To_v1alpha1_Kube Controller Manager Configuration ( in * config . Kube Controller Manager Configuration , out * v1alpha1 . Kube Controller Manager Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Controller Manager Configuration_To_v1alpha1_Kube Controller Manager } 
func Event if len ( ev . User . Groups ) > 0 { groups = audit String if ev . Impersonated User != nil { asuser = ev . Impersonated if ev . Impersonated User . Groups != nil { asgroups = audit String Slice ( ev . Impersonated if ev . Object Ref != nil && len ( ev . Object Ref . Namespace ) != 0 { namespace = ev . Object if ev . Response Status != nil { response = strconv . Itoa ( int ( ev . Response if len ( ev . Source I Ps ) > 0 { ip = ev . Source I return fmt . Sprintf ( " \" \" " , ev . Request Received Timestamp . Format ( time . RFC3339Nano ) , ev . Audit ID , ev . Stage , ip , ev . Verb , username , groups , asuser , asgroups , namespace , ev . Request } 
func ( u * storageos Util ) Create Volume ( p * storageos Provisioner ) ( * storageos Volume , error ) { if err := u . New API ( p . api opts := storageostypes . Volume Create Options { Name : p . vol Name , Size : p . size GB , Description : p . description , Pool : p . pool , FS Type : p . fs Type , Namespace : p . vol vol , err := u . api . Volume return & storageos Volume { ID : vol . ID , Name : vol . Name , Namespace : vol . Namespace , Description : vol . Description , Pool : vol . Pool , FS Type : vol . FS Type , Size } 
func ( u * storageos Util ) Attach Volume ( b * storageos Mounter ) ( string , error ) { if err := u . New API ( b . api // Get the node's device path from the API, falling back to the default if // not set on the node. if b . device Dir == " " { b . device Dir = u . Device vol , err := u . api . Volume ( b . vol Namespace , b . vol if err != nil { klog . Warningf ( " " , b . vol Name , b . vol // Clear any existing mount reference from the API. These may be leftover // from previous mounts where the unmount operation couldn't get access to // the API credentials. if vol . Mounted { opts := storageostypes . Volume Unmount if err := u . api . Volume src Path := filepath . Join ( b . device dt , err := path Device Type ( src if err != nil { klog . Warningf ( " " , src Path , b . vol switch dt { case mode Block : return src case mode File : return attach File Device ( src default : return " " , fmt . Errorf ( Err Device Not } 
func ( u * storageos Util ) Detach Volume ( b * storageos Unmounter , device Path string ) error { if ! is Loop Device ( device if _ , err := os . Stat ( device Path ) ; os . Is Not return remove Loop Device ( device } 
func ( u * storageos Util ) Attach Device ( b * storageos Mounter , device Mount Path string ) error { if err := u . New API ( b . api opts := storageostypes . Volume Mount Options { Name : b . vol Name , Namespace : b . vol Namespace , Fs Type : b . fs Type , Mountpoint : device Mount Path , Client : b . plugin . host . Get Host if err := u . api . Volume } 
func ( u * storageos Util ) Mount Volume ( b * storageos Mounter , mnt Device , device Mount Path string ) error { not Mnt , err := b . mounter . Is Likely Not Mount Point ( device Mount if err != nil { if os . Is Not Exist ( err ) { if err = os . Mkdir All ( device Mount not if err = os . Mkdir All ( device Mount Path , 0750 ) ; err != nil { klog . Errorf ( " " , device Mount if b . read if not Mnt { err = b . disk Mounter . Format And Mount ( mnt Device , device Mount Path , b . fs if err != nil { os . Remove ( device Mount } 
func ( u * storageos Util ) Unmount Volume ( b * storageos Unmounter ) error { if err := u . New API ( b . api opts := storageostypes . Volume Unmount Options { Name : b . vol Name , Namespace : b . vol Namespace , Client : b . plugin . host . Get Host return u . api . Volume } 
func ( u * storageos Util ) Delete Volume ( d * storageos Deleter ) error { if err := u . New API ( d . api // Deletes must be forced as the Storage OS API will not normally delete // volumes that it thinks are mounted. We can't be sure the unmount was // registered via the API so we trust k8s to only delete volumes it knows // are unmounted. opts := storageostypes . Delete Options { Name : d . vol Name , Namespace : d . vol return u . api . Volume } 
func ( u * storageos Util ) Device Dir ( b * storageos Mounter ) string { ctrl , err := u . api . Node ( b . plugin . host . Get Host return default Device if ctrl == nil || ctrl . Device Dir == " " { klog . Warningf ( " " , default Device return default Device return ctrl . Device } 
func path Device Type ( path string ) ( device if err != nil { return mode switch mode := fi . Mode ( ) ; { case mode & os . Mode Device != 0 : return mode case mode . Is Regular ( ) : return mode default : return mode } 
func attach File Device ( path string , exec mount . Exec ) ( string , error ) { block Device Path , err := get Loop if err != nil && err . Error ( ) != Err Device Not // If no existing loop device for the path, create one if block Device block Device Path , err = make Loop return block Device } 
func get Loop if os . Is Not Exist ( err ) { return " " , errors . New ( Err Not out , err := exec . Run ( losetup return parse Losetup Output For } 
func ( f * shared Informer Factory ) For Resource ( resource schema . Group Version Resource ) ( Generic Informer , error ) { switch resource { // Group=apiextensions.k8s.io, Version=internal Version case apiextensions . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group Resource ( ) , informer : f . Apiextensions ( ) . Internal Version ( ) . Custom Resource } 
func New Deployment Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Deployment Informer ( client , namespace , resync } 
func Storage Version // Assuming there are N kinds in the cluster, and the hash is X-byte long, // the chance of colliding hash P(N,X) approximates to 1-e^(-(N^2)/2^(8X+1)). // P(10,000, 8) ~= 2.7*10^(-12), which is low enough. // See https://en.wikipedia.org/wiki/Birthday_problem#Approximations. return base64 . Std Encoding . Encode To } 
func ( t * token Getter ) Token ( ) ( string , error ) { var options gophercloud . Auth if t . auth options , err = openstack . Auth Options From } else { options = * t . auth client , err := openstack . Authenticated return client . Token } 
func ( c * cached // no token or exceeds the TTL if c . token == " " || time . Since ( c . born ) > c . ttl { c . token , err = c . token } 
func ( t * token Round Tripper ) Round Trip ( req * http . Request ) ( * http . Response , error ) { // if the authorization header already present, use it. if req . Header . Get ( " " ) != " " { return t . Round Tripper . Round token , err := t . token return t . Round Tripper . Round } 
func new Openstack Auth Provider ( _ string , config map [ string ] string , persister restclient . Auth Provider Config Persister ) ( restclient . Auth Provider , error ) { var ttl if ! found { ttl Duration = Default TTL // persist to config config [ " " ] = ttl } else { ttl Duration , err = time . Parse auth Opt := gophercloud . Auth Options { Identity Endpoint : config [ " " ] , Username : config [ " " ] , Password : config [ " " ] , Domain Name : config [ " " ] , Tenant ID : config [ " " ] , Tenant getter := token // not empty if ( auth Opt != gophercloud . Auth Options { } ) { if len ( auth Opt . Identity getter . auth Opt = & auth return & openstack Auth Provider { ttl : ttl Duration , token } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . Cluster Role Binding { } , func ( obj interface { } ) { Set Object Defaults_Cluster Role Binding ( obj . ( * v1beta1 . Cluster Role scheme . Add Type Defaulting Func ( & v1beta1 . Cluster Role Binding List { } , func ( obj interface { } ) { Set Object Defaults_Cluster Role Binding List ( obj . ( * v1beta1 . Cluster Role Binding scheme . Add Type Defaulting Func ( & v1beta1 . Role Binding { } , func ( obj interface { } ) { Set Object Defaults_Role Binding ( obj . ( * v1beta1 . Role scheme . Add Type Defaulting Func ( & v1beta1 . Role Binding List { } , func ( obj interface { } ) { Set Object Defaults_Role Binding List ( obj . ( * v1beta1 . Role Binding } 
func Cluster Roles ( ) [ ] rbacv1 . Cluster Role { roles := [ ] rbacv1 . Cluster Role { { // a "root" role which can do absolutely anything Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " ) . Groups ( " " ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . UR Ls ( " " ) . Rule Or Die ( ) , } , } , { // a role which provides just enough power to determine if the server is ready and discover API versions for negotiation Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " ) . UR Ls ( " " , " " , " " , " " , " " , " " , " " , " " , " " , ) . Rule Or Die ( ) , } , } , { // a role which provides minimal resource access to allow a "normal" user to learn information about themselves Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { // TODO add future selfsubjectrulesreview, project request AP Is, project listing AP Is rbacv1helpers . New Rule ( " " ) . Groups ( authorization Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , } , } , { // a role which provides just enough power read insensitive cluster information Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " ) . UR Ls ( " " , " " , " " , ) . Rule Or Die ( ) , } , } , { // a role for a namespace level admin. It is `edit` plus the power to grant permissions to other users. Object Meta : metav1 . Object Meta { Name : " " } , Aggregation Rule : & rbacv1 . Aggregation Rule { Cluster Role Selectors : [ ] metav1 . Label Selector { { Match Labels : map [ string ] string { " " : " " } } , } , } , } , { // a role for a namespace level editor. It grants access to all user level actions in a namespace. // It does not grant powers for "privileged" resources which are domain of the system: `/status` // subresources or `quota`/`limits` which are used to control namespaces Object Meta : metav1 . Object Meta { Name : " " , Labels : map [ string ] string { " " : " " } } , Aggregation Rule : & rbacv1 . Aggregation Rule { Cluster Role Selectors : [ ] metav1 . Label Selector { { Match Labels : map [ string ] string { " " : " " } } , } , } , } , { // a role for namespace level viewing. It grants Read-only access to non-escalating resources in // a namespace. Object Meta : metav1 . Object Meta { Name : " " , Labels : map [ string ] string { " " : " " } } , Aggregation Rule : & rbacv1 . Aggregation Rule { Cluster Role Selectors : [ ] metav1 . Label Selector { { Match Labels : map [ string ] string { " " : " " } } , } , } , } , { // a role for a namespace level admin. It is `edit` plus the power to grant permissions to other users. Object Meta : metav1 . Object Meta { Name : " " , Labels : map [ string ] string { " " : " " } } , Rules : [ ] rbacv1 . Policy Rule { // additional admin powers rbacv1helpers . New Rule ( " " ) . Groups ( authorization Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read Write ... ) . Groups ( rbac Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , } , } , { // a role for a namespace level editor. It grants access to all user level actions in a namespace. // It does not grant powers for "privileged" resources which are domain of the system: `/status` // subresources or `quota`/`limits` which are used to control namespaces Object Meta : metav1 . Object Meta { Name : " " , Labels : map [ string ] string { " " : " " } } , Rules : [ ] rbacv1 . Policy Rule { // Allow read on escalating resources rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( apps Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( autoscaling Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( batch Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( extensions Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( policy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Write ... ) . Groups ( networking Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , } , } , { // a role for namespace level viewing. It grants Read-only access to non-escalating resources in // a namespace. Object Meta : metav1 . Object Meta { Name : " " , Labels : map [ string ] string { " " : " " } } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , // read access to namespaces at the namespace scope means you can read *this* namespace. This can be used as an // indicator of which namespaces you have access to. rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( apps Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( autoscaling Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( batch Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( extensions Group ) . Resources ( " " , " " , " " , " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( policy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( networking Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , } , } , { // a role to use for heapster's connections back to the API server Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( extensions Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role for nodes to use to have the access they need for running pods Object Meta : metav1 . Object Meta { Name : " " } , Rules : Node Rules ( ) , } , { // a role to use for node-problem-detector access. It does not get bound to default location since // deployment locations can reasonably vary. Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , events Rule ( ) , } , } , { // a role to use for setting up a proxy Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { // Used to build service Lister rbacv1helpers . New Rule ( " " , " " ) . Groups ( legacy Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , events Rule ( ) , } , } , { // a role to use for full access to the kubelet API Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { // Allow read-only access to the Node API objects rbacv1helpers . New Rule ( " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , // Allow all API calls to the nodes rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " ) . Rule Or Die ( ) , } , } , { // a role to use for bootstrapping a node's client certificates Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { // used to create a certificatesigningrequest for a node-specific client certificate, and watch for it to be signed rbacv1helpers . New Rule ( " " , " " , " " , " " ) . Groups ( certificates Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role to use for allowing authentication and authorization delegation Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { // These creates are non-mutating rbacv1helpers . New Rule ( " " ) . Groups ( authentication Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( authorization Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role to use for the API registry, summarization, and proxy handling Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { // it needs to see all services so that it knows whether the ones it points to exist or not rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , } , } , { // a role to use for bootstrapping the kube-controller-manager so it can create the shared informers // service accounts, and secrets that we need to create separate identities for other controllers Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { events Rule ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " , " " , " " , " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " , " " , " " ) . Rule Or Die ( ) , // Needed to check API access. These creates are non-mutating rbacv1helpers . New Rule ( " " ) . Groups ( authentication Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( authorization Group ) . Resources ( " " ) . Rule Or Die ( ) , // Needed for all shared informers rbacv1helpers . New Rule ( " " , " " ) . Groups ( " " ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role to use for the kube-scheduler Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { events Rule ( ) , // this is for leaderlease access // TODO: scope this to the kube-system namespace rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Names ( " " ) . Rule Or Die ( ) , // fundamental resources rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , // things that select pods rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( apps Group , extensions Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( apps Group ) . Resources ( " " ) . Rule Or Die ( ) , // things that pods use or applies to them rbacv1helpers . New Rule ( Read ... ) . Groups ( policy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( legacy Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , // Needed to check API access. These creates are non-mutating rbacv1helpers . New Rule ( " " ) . Groups ( authentication Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " ) . Groups ( authorization Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role to use for the kube-dns pod Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " , " " ) . Groups ( legacy Group ) . Resources ( " " , " " ) . Rule Or Die ( ) , } , } , { // a role for an external/out-of-tree persistent volume provisioner Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " , " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , // update is needed in addition to read access for setting lock annotations on PV Cs rbacv1helpers . New Rule ( " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( storage Group ) . Resources ( " " ) . Rule Or Die ( ) , // Needed for watching provisioning success and failure events rbacv1helpers . New Rule ( " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , events Rule ( ) , } , } , { // a role for the csi external attacher Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " , " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " , " " , " " ) . Groups ( storage Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role making the csrapprover controller approve a node client CSR Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " ) . Groups ( certificates Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { // a role making the csrapprover controller approve a node client CSR requested by the node itself Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " ) . Groups ( certificates Group ) . Resources ( " " ) . Rule Or Die ( ) , } , } , { Object Meta : metav1 . Object Meta { Name : " " } , Rules : [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( Read Update ... ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read ... ) . Groups ( storage Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( Read Update ... ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or external Provisioner Rules := [ ] rbacv1 . Policy Rule { rbacv1helpers . New Rule ( " " , " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " ) . Groups ( storage Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " , " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or Die ( ) , rbacv1helpers . New Rule ( " " , " " , " " ) . Groups ( legacy Group ) . Resources ( " " ) . Rule Or if utilfeature . Default Feature Gate . Enabled ( features . CSI Node Info ) { external Provisioner Rules = append ( external Provisioner Rules , rbacv1helpers . New Rule ( " " , " " , " " ) . Groups ( " " ) . Resources ( " " ) . Rule Or roles = append ( roles , rbacv1 . Cluster Role { // a role for the csi external provisioner Object Meta : metav1 . Object Meta { Name : " " } , Rules : external Provisioner add Cluster Role } 
func Cluster Role Bindings ( ) [ ] rbacv1 . Cluster Role Binding { rolebindings := [ ] rbacv1 . Cluster Role Binding { rbacv1helpers . New Cluster Binding ( " " ) . Groups ( user . System Privileged Group ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Groups ( user . All Authenticated ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Groups ( user . All Authenticated ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Groups ( user . All Authenticated , user . All Unauthenticated ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Users ( user . Kube Proxy ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Users ( user . Kube Controller Manager ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . S As ( " " , " " ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Users ( user . Kube Scheduler ) . Binding Or Die ( ) , rbacv1helpers . New Cluster Binding ( " " ) . Users ( user . Kube Scheduler ) . Binding Or Die ( ) , // This default binding of the system:node role to the system:nodes group is deprecated in 1.7 with the availability of the Node authorizer. // This leaves the binding, but with an empty set of subjects, so that tightening reconciliation can remove the subject. { Object Meta : metav1 . Object Meta { Name : system Node Role Name } , Role Ref : rbacv1 . Role Ref { API Group : rbacv1 . Group Name , Kind : " " , Name : system Node Role add Cluster Role Binding } 
func Cluster Role Bindings To Split ( ) map [ string ] rbacv1 . Cluster Role Binding { bindings To Split := map [ string ] rbacv1 . Cluster Role for _ , default Cluster Role Binding := range Cluster Role Bindings ( ) { switch default Cluster Role Binding . Name { case " " : bindings To Split [ " " ] = default Cluster Role return bindings To } 
func Register Metrics ( ) { register Metrics Once . Do ( func ( ) { prometheus . Must Register ( Sync Proxy Rules prometheus . Must Register ( Deprecated Sync Proxy Rules prometheus . Must Register ( Network Programming } 
func get Load Balancer Additional Tags ( annotations map [ string ] string ) map [ string ] string { additional if additional Tags List , ok := annotations [ Service Annotation Load Balancer Additional Tags ] ; ok { additional Tags List = strings . Trim Space ( additional Tags // Break up list of "Key1=Val,Key2=Val2" tag List := strings . Split ( additional Tags // Break up "Key=Val" for _ , tag Set := range tag List { tag := strings . Split ( strings . Trim Space ( tag // Accept "Key=val" or "Key=" or just "Key" if len ( tag ) >= 2 && len ( tag [ 0 ] ) != 0 { // There is a key and a value, so save it additional } else if len ( tag ) == 1 && len ( tag [ 0 ] ) != 0 { // Just "Key" additional return additional } 
func ( c * Cloud ) ensure Load Balancerv2 ( namespaced Name types . Namespaced Name , load Balancer Name string , mappings [ ] nlb Port Mapping , instance I Ds , subnet I Ds [ ] string , internal ELB bool , annotations map [ string ] string ) ( * elbv2 . Load Balancer , error ) { load Balancer , err := c . describe Load Balancerv2 ( load Balancer // Get additional tags set by the user tags := get Load Balancer Additional // Add default tags tags [ Tag Name Kubernetes Service ] = namespaced tags = c . tagging . build Tags ( Resource Lifecycle if load Balancer == nil { // Create the LB create Request := & elbv2 . Create Load Balancer Input { Type : aws . String ( elbv2 . Load Balancer Type Enum Network ) , Name : aws . String ( load Balancer if internal ELB { create // We are supposed to specify one subnet per AZ. // TODO: What happens if we have more than one subnet per AZ? create Request . Subnet Mappings = create Subnet Mappings ( subnet I for k , v := range tags { create Request . Tags = append ( create klog . Infof ( " " , namespaced Name , load Balancer create Response , err := c . elbv2 . Create Load Balancer ( create load Balancer = create Response . Load for i := range mappings { // It is easier to keep track of updates by having possibly // duplicate target groups where the backend port is the same _ , err := c . create Listener V2 ( create Response . Load Balancers [ 0 ] . Load Balancer Arn , mappings [ i ] , namespaced Name , instance I Ds , * create Response . Load Balancers [ 0 ] . Vpc } else { // TODO: Sync internal vs non-internal // sync mappings { listener Descriptions , err := c . elbv2 . Describe Listeners ( & elbv2 . Describe Listeners Input { Load Balancer Arn : load Balancer . Load Balancer // actual maps Frontend for _ , listener := range listener actual Target Groups , err := c . elbv2 . Describe Target Groups ( & elbv2 . Describe Target Groups Input { Load Balancer Arn : load Balancer . Load Balancer node Port Target Group := map [ int64 ] * elbv2 . Target for _ , target Group := range actual Target Groups . Target Groups { node Port Target Group [ * target Group . Port ] = target // Handle additions/modifications for _ , mapping := range mappings { frontend Port := mapping . Frontend node Port := mapping . Traffic // modifications if listener , ok := actual [ frontend Port ] ; ok { listener Needs if aws . String Value ( listener . Protocol ) != mapping . Frontend Protocol { listener Needs switch mapping . Frontend Protocol { case elbv2 . Protocol Enum Tls : { if aws . String Value ( listener . Ssl Policy ) != mapping . SSL Policy { listener Needs if len ( listener . Certificates ) == 0 || aws . String Value ( listener . Certificates [ 0 ] . Certificate Arn ) != mapping . SSL Certificate ARN { listener Needs case elbv2 . Protocol Enum Tcp : { if aws . String Value ( listener . Ssl Policy ) != " " { listener Needs if len ( listener . Certificates ) != 0 { listener Needs // recreate target Group if traffic Port or protocol changed target Group target Group , ok := node Port Target Group [ node if ! ok || aws . String Value ( target Group . Protocol ) != mapping . Traffic Protocol { // create new target group target Group , err = c . ensure Target Group ( nil , namespaced Name , mapping , instance I Ds , * load Balancer . Vpc target Group listener Needs if listener Needs Modification { modify Listener Input := & elbv2 . Modify Listener Input { Listener Arn : listener . Listener Arn , Port : aws . Int64 ( frontend Port ) , Protocol : aws . String ( mapping . Frontend Protocol ) , Default Actions : [ ] * elbv2 . Action { { Target Group Arn : target Group . Target Group if mapping . Frontend Protocol == elbv2 . Protocol Enum Tls { if mapping . SSL Policy != " " { modify Listener Input . Ssl Policy = aws . String ( mapping . SSL modify Listener Input . Certificates = [ ] * elbv2 . Certificate { { Certificate Arn : aws . String ( mapping . SSL Certificate if _ , err := c . elbv2 . Modify Listener ( modify Listener // Delete old target Group if needed if target Group Recreated { if _ , err := c . elbv2 . Delete Target Group ( & elbv2 . Delete Target Group Input { Target Group Arn : listener . Default Actions [ 0 ] . Target Group } else { // Run ensure Target Group to make sure instances in service are up-to-date _ , err = c . ensure Target Group ( target Group , namespaced Name , mapping , instance I Ds , * load Balancer . Vpc // Additions _ , err := c . create Listener V2 ( load Balancer . Load Balancer Arn , mapping , namespaced Name , instance I Ds , * load Balancer . Vpc front End for i := range mappings { front End Ports [ mappings [ i ] . Frontend // handle deletions for port , listener := range actual { if _ , ok := front End Ports [ port ] ; ! ok { err := c . delete Listener desired Load Balancer // Default values to ensured a remove annotation reverts back to the default desired Load Balancer // Determine if cross zone load balancing enabled/disabled has been specified cross Zone Load Balancing Enabled Annotation := annotations [ Service Annotation Load Balancer Cross Zone Load Balancing if cross Zone Load Balancing Enabled Annotation != " " { cross Zone Enabled , err := strconv . Parse Bool ( cross Zone Load Balancing Enabled if err != nil { return nil , fmt . Errorf ( " " , Service Annotation Load Balancer Cross Zone Load Balancing Enabled , cross Zone Load Balancing Enabled if cross Zone Enabled { desired Load Balancer // Whether the ELB was new or existing, sync attributes regardless. This accounts for things // that cannot be specified at the time of creation and can only be modified after the fact, // e.g. idle connection timeout. describe Attributes Request := & elbv2 . Describe Load Balancer Attributes Input { Load Balancer Arn : load Balancer . Load Balancer describe Attributes Output , err := c . elbv2 . Describe Load Balancer Attributes ( describe Attributes changed Attributes := [ ] * elbv2 . Load Balancer // Identify to be changed attributes for _ , found Attribute := range describe Attributes Output . Attributes { if target Value , ok := desired Load Balancer Attributes [ * found Attribute . Key ] ; ok { if target Value != * found Attribute . Value { changed Attributes = append ( changed Attributes , & elbv2 . Load Balancer Attribute { Key : found Attribute . Key , Value : aws . String ( target // Update attributes requiring changes if len ( changed Attributes ) > 0 { klog . V ( 2 ) . Infof ( " " , load Balancer _ , err = c . elbv2 . Modify Load Balancer Attributes ( & elbv2 . Modify Load Balancer Attributes Input { Load Balancer Arn : load Balancer . Load Balancer Arn , Attributes : changed // Subnets cannot be modified on NL Bs if dirty { load Balancers , err := c . elbv2 . Describe Load Balancers ( & elbv2 . Describe Load Balancers Input { Load Balancer Arns : [ ] * string { load Balancer . Load Balancer load Balancer = load Balancers . Load return load } 
func ( c * Cloud ) build Target Group Name ( service Name types . Namespaced Name , service Port int64 , target Protocol string , target _ , _ = hasher . Write ( [ ] byte ( c . tagging . cluster _ , _ = hasher . Write ( [ ] byte ( service _ , _ = hasher . Write ( [ ] byte ( service _ , _ = hasher . Write ( [ ] byte ( strconv . Format Int ( service _ , _ = hasher . Write ( [ ] byte ( target _ , _ = hasher . Write ( [ ] byte ( target tg UUID := hex . Encode To sanitized Namespace := invalid ELBV2Name Regex . Replace All String ( service sanitized Service Name := invalid ELBV2Name Regex . Replace All String ( service return fmt . Sprintf ( " " , sanitized Namespace , sanitized Service Name , tg } 
func ( c * Cloud ) delete Listener V2 ( listener * elbv2 . Listener ) error { _ , err := c . elbv2 . Delete Listener ( & elbv2 . Delete Listener Input { Listener Arn : listener . Listener _ , err = c . elbv2 . Delete Target Group ( & elbv2 . Delete Target Group Input { Target Group Arn : listener . Default Actions [ 0 ] . Target Group } 
func ( c * Cloud ) ensure Target Group ( target Group * elbv2 . Target Group , service Name types . Namespaced Name , mapping nlb Port Mapping , instances [ ] string , vpc ID string , tags map [ string ] string ) ( * elbv2 . Target if target Group == nil { target name := c . build Target Group Name ( service Name , mapping . Frontend Port , mapping . Traffic Protocol , target klog . Infof ( " " , service input := & elbv2 . Create Target Group Input { Vpc Id : aws . String ( vpc ID ) , Name : aws . String ( name ) , Port : aws . Int64 ( mapping . Traffic Port ) , Protocol : aws . String ( mapping . Traffic Protocol ) , Target Type : aws . String ( target Type ) , Health Check Interval Seconds : aws . Int64 ( 30 ) , Health Check Port : aws . String ( " " ) , Health Check Protocol : aws . String ( " " ) , Healthy Threshold Count : aws . Int64 ( 3 ) , Unhealthy Threshold input . Health Check Protocol = aws . String ( mapping . Health Check if mapping . Health Check Protocol != elbv2 . Protocol Enum Tcp { input . Health Check Path = aws . String ( mapping . Health Check // Account for external Traffic Policy = "Local" if mapping . Health Check Port != mapping . Traffic Port { input . Health Check Port = aws . String ( strconv . Itoa ( int ( mapping . Health Check result , err := c . elbv2 . Create Target if len ( result . Target Groups ) != 1 { return nil , fmt . Errorf ( " " , len ( result . Target if len ( tags ) != 0 { target Group for k , v := range tags { target Group Tags = append ( target Group tg Arn := aws . String Value ( result . Target Groups [ 0 ] . Target Group if _ , err := c . elbv2 . Add Tags ( & elbv2 . Add Tags Input { Resource Arns : [ ] * string { aws . String ( tg Arn ) } , Tags : target Group Tags , } ) ; err != nil { return nil , fmt . Errorf ( " " , tg register Input := & elbv2 . Register Targets Input { Target Group Arn : result . Target Groups [ 0 ] . Target Group Arn , Targets : [ ] * elbv2 . Target for _ , instance ID := range instances { register Input . Targets = append ( register Input . Targets , & elbv2 . Target Description { Id : aws . String ( string ( instance ID ) ) , Port : aws . Int64 ( mapping . Traffic _ , err = c . elbv2 . Register Targets ( register return result . Target // handle instances in service { health Response , err := c . elbv2 . Describe Target Health ( & elbv2 . Describe Target Health Input { Target Group Arn : target Group . Target Group actual I for _ , health Description := range health Response . Target Health Descriptions { if health Description . Target Health . Reason != nil { switch aws . String Value ( health Description . Target Health . Reason ) { case elbv2 . Target Health Reason Enum Target Deregistration In Progress : // We don't need to count this instance in service if it is // on its way out default : actual I Ds = append ( actual I Ds , * health actual := sets . New String ( actual I expected := sets . New if len ( additions ) > 0 { register Input := & elbv2 . Register Targets Input { Target Group Arn : target Group . Target Group Arn , Targets : [ ] * elbv2 . Target for instance ID := range additions { register Input . Targets = append ( register Input . Targets , & elbv2 . Target Description { Id : aws . String ( instance ID ) , Port : aws . Int64 ( mapping . Traffic _ , err := c . elbv2 . Register Targets ( register if len ( removals ) > 0 { deregister Input := & elbv2 . Deregister Targets Input { Target Group Arn : target Group . Target Group Arn , Targets : [ ] * elbv2 . Target for instance ID := range removals { deregister Input . Targets = append ( deregister Input . Targets , & elbv2 . Target Description { Id : aws . String ( instance ID ) , Port : aws . Int64 ( mapping . Traffic _ , err := c . elbv2 . Deregister Targets ( deregister // ensure the health check is correct { dirty Health input := & elbv2 . Modify Target Group Input { Target Group Arn : target Group . Target Group if aws . String Value ( target Group . Health Check Protocol ) != mapping . Health Check Protocol { input . Health Check Protocol = aws . String ( mapping . Health Check dirty Health if aws . String Value ( target Group . Health Check Port ) != strconv . Itoa ( int ( mapping . Health Check Port ) ) { input . Health Check Port = aws . String ( strconv . Itoa ( int ( mapping . Health Check dirty Health if mapping . Health Check Path != " " && mapping . Health Check Protocol != elbv2 . Protocol Enum Tcp { input . Health Check Path = aws . String ( mapping . Health Check dirty Health if dirty Health Check { _ , err := c . elbv2 . Modify Target if dirty { result , err := c . elbv2 . Describe Target Groups ( & elbv2 . Describe Target Groups Input { Target Group Arns : [ ] * string { target Group . Target Group target Group = result . Target return target } 
func filter For IP Range Description ( security Groups [ ] * ec2 . Security Group , lb Name string ) [ ] * ec2 . Security Group { response := [ ] * ec2 . Security client Rule := fmt . Sprintf ( " " , NLB Client Rule Description , lb health Rule := fmt . Sprintf ( " " , NLB Health Check Rule Description , lb already Added := sets . New for i := range security Groups { for j := range security Groups [ i ] . Ip Permissions { for k := range security Groups [ i ] . Ip Permissions [ j ] . Ip Ranges { description := aws . String Value ( security Groups [ i ] . Ip Permissions [ j ] . Ip if description == client Rule || description == health Rule { sg ID String := aws . String Value ( security Groups [ i ] . Group if ! already Added . Has ( sg ID String ) { response = append ( response , security already Added . Insert ( sg ID } 
func ( c * Cloud ) update Instance Security Groups For NLB Traffic ( actual Groups [ ] * ec2 . Security Group , desired Sg Ids [ ] string , ports [ ] int64 , lb Name string , client Cidrs [ ] string , client Traffic bool ) error { klog . V ( 8 ) . Infof ( " " , actual Groups , desired Sg Ids , ports , client // Map containing the groups we want to make changes on; the ports to make // changes on; and whether to add or remove it. true to add, false to remove port for _ , id := range desired Sg Ids { // consider everything an addition for now if _ , ok := port Changes [ id ] ; ! ok { port for _ , port := range ports { port // Compare to actual groups for _ , actual Group := range actual Groups { actual Group ID := aws . String Value ( actual Group . Group if actual Group ID == " " { klog . Warning ( " " , actual adding Map , ok := port Changes [ actual Group if ok { desired Set := sets . New for port := range adding Map { desired existing Set := ports For NLB ( lb Name , actual Group , client // remove from port Changes ports that are already allowed if intersection := desired Set . Intersection ( existing Set ) ; intersection . Len ( ) > 0 { for p := range intersection { delete ( port Changes [ actual Group // allowed ports that need to be removed if difference := existing Set . Difference ( desired Set ) ; difference . Len ( ) > 0 { for p := range difference { port Changes [ actual Group // Make changes we've planned on for instance Security Group ID , port Map := range port Changes { adds := [ ] * ec2 . Ip removes := [ ] * ec2 . Ip for port , add := range port Map { if add { if client Traffic { klog . V ( 2 ) . Infof ( " " , client Cidrs , instance Security Group klog . V ( 2 ) . Infof ( " " , client Cidrs , instance Security Group } else { klog . V ( 2 ) . Infof ( " " , client Cidrs , instance Security Group } else { if client Traffic { klog . V ( 2 ) . Infof ( " " , client Cidrs , instance Security Group klog . V ( 2 ) . Infof ( " " , client Cidrs , instance Security Group klog . V ( 2 ) . Infof ( " " , client Cidrs , instance Security Group if client Traffic { client Rule Annotation := fmt . Sprintf ( " " , NLB Client Rule Description , lb // Client Traffic permission := & ec2 . Ip Permission { From Port : aws . Int64 ( port ) , To Port : aws . Int64 ( port ) , Ip ranges := [ ] * ec2 . Ip for _ , cidr := range client Cidrs { ranges = append ( ranges , & ec2 . Ip Range { Cidr Ip : aws . String ( cidr ) , Description : aws . String ( client Rule permission . Ip } else { health Rule Annotation := fmt . Sprintf ( " " , NLB Health Check Rule Description , lb // NLB Health Check permission := & ec2 . Ip Permission { From Port : aws . Int64 ( port ) , To Port : aws . Int64 ( port ) , Ip ranges := [ ] * ec2 . Ip for _ , cidr := range client Cidrs { ranges = append ( ranges , & ec2 . Ip Range { Cidr Ip : aws . String ( cidr ) , Description : aws . String ( health Rule permission . Ip if len ( adds ) > 0 { changed , err := c . add Security Group Ingress ( instance Security Group if ! changed { klog . Warning ( " " , instance Security Group if len ( removes ) > 0 { changed , err := c . remove Security Group Ingress ( instance Security Group if ! changed { klog . Warning ( " " , instance Security Group if client Traffic { // MTU discovery mtu Rule Annotation := fmt . Sprintf ( " " , NLB Mtu Discovery Rule Description , lb mtu Permission := & ec2 . Ip Permission { Ip Protocol : aws . String ( " " ) , From Port : aws . Int64 ( 3 ) , To ranges := [ ] * ec2 . Ip for _ , cidr := range client Cidrs { ranges = append ( ranges , & ec2 . Ip Range { Cidr Ip : aws . String ( cidr ) , Description : aws . String ( mtu Rule mtu Permission . Ip group , err := c . find Security Group ( instance Security Group if group == nil { klog . Warning ( " " , instance Security Group icmp perm for _ , perm := range group . Ip Permissions { if * perm . Ip Protocol == " " { icmp if perm . From Port != nil { perm if ! icmp Exists && perm Count > 0 { // the icmp permission is missing changed , err := c . add Security Group Ingress ( instance Security Group ID , [ ] * ec2 . Ip Permission { mtu if ! changed { klog . Warning ( " " , instance Security Group } else if icmp Exists && perm Count == 0 { // there is no additional permissions, remove icmp changed , err := c . remove Security Group Ingress ( instance Security Group ID , [ ] * ec2 . Ip Permission { mtu if ! changed { klog . Warning ( " " , instance Security Group } 
func ( c * Cloud ) update Instance Security Groups For NLB ( mappings [ ] nlb Port Mapping , instances map [ Instance ID ] * ec2 . Instance , lb Name string , client Cidrs [ ] string ) error { if c . cfg . Global . Disable Security Group vpc Cidr Blocks , err := c . get Vpc Cidr // Unlike the classic ELB, NLB does not have a security group that we can // filter against all existing groups to see if they allow access. Instead // we use the Ip Range.Description field to annotate NLB health check and // client traffic rules // Get the actual list of groups that allow ingress for the load-balancer var actual Groups [ ] * ec2 . Security { // Server side filter describe Request := & ec2 . Describe Security Groups filters := [ ] * ec2 . Filter { new describe Request . Filters = c . tagging . add response , err := c . ec2 . Describe Security Groups ( describe for _ , sg := range response { if ! c . tagging . has Cluster actual Groups = append ( actual // client-side filter // Filter out groups that don't have IP Rules we've annotated for this service actual Groups = filter For IP Range Description ( actual Groups , lb tagged Security Groups , err := c . get Tagged Security external Traffic Policy Is traffic for i := range mappings { traffic Ports = append ( traffic Ports , mappings [ i ] . Traffic if mappings [ i ] . Traffic Port != mappings [ i ] . Health Check Port { external Traffic Policy Is health Check Ports := traffic // if external Traffic Policy is Local, all listeners use the same health // check port if external Traffic Policy Is Local && len ( mappings ) > 0 { health Check Ports = [ ] int64 { mappings [ 0 ] . Health Check desired Group // Scan instances for groups we want open for _ , instance := range instances { security Group , err := find Security Group For Instance ( instance , tagged Security if security Group == nil { klog . Warningf ( " " , aws . String Value ( instance . Instance id := aws . String Value ( security Group . Group if id == " " { klog . Warningf ( " " , security desired Group Ids = append ( desired Group // Run once for Client traffic err = c . update Instance Security Groups For NLB Traffic ( actual Groups , desired Group Ids , traffic Ports , lb Name , client // Run once for health check traffic err = c . update Instance Security Groups For NLB Traffic ( actual Groups , desired Group Ids , health Check Ports , lb Name , vpc Cidr } 
func sync Elb Listeners ( load Balancer Name string , listeners [ ] * elb . Listener , listener Descriptions [ ] * elb . Listener Description ) ( [ ] * elb . Listener , [ ] * int64 ) { found for _ , listener Description := range listener Descriptions { actual := listener if actual == nil { klog . Warning ( " " , load Balancer for i , expected := range listeners { if expected == nil { klog . Warning ( " " , load Balancer if elb Listeners Are Equal ( actual , expected ) { // The current listener on the actual // elb is in the set of desired listeners. found if ! found { removals = append ( removals , actual . Load Balancer for i := range listeners { if ! found } 
func aws Arn return strings . Equal Fold ( aws . String Value ( l ) , aws . String } 
func ( c * Cloud ) get Expected Health Check ( target string , annotations map [ string ] string ) ( * elb . Health Check , error ) { healthcheck := & elb . Health get Or Default := func ( annotation string , default Value int64 ) ( * int64 , error ) { i64 := default if s , ok := annotations [ annotation ] ; ok { i64 , err = strconv . Parse healthcheck . Healthy Threshold , err = get Or Default ( Service Annotation Load Balancer HC Healthy Threshold , default HC Healthy healthcheck . Unhealthy Threshold , err = get Or Default ( Service Annotation Load Balancer HC Unhealthy Threshold , default HC Unhealthy healthcheck . Timeout , err = get Or Default ( Service Annotation Load Balancer HC Timeout , default HC healthcheck . Interval , err = get Or Default ( Service Annotation Load Balancer HC Interval , default HC } 
func ( c * Cloud ) ensure Load Balancer Health Check ( load Balancer * elb . Load Balancer Description , protocol string , port int32 , path string , annotations map [ string ] string ) error { name := aws . String Value ( load Balancer . Load Balancer actual := load Balancer . Health expected Target := protocol + " " + strconv . Format expected , err := c . get Expected Health Check ( expected // comparing attributes 1 by 1 to avoid breakage in case a new field is // added to the HC which breaks the equality if aws . String Value ( expected . Target ) == aws . String Value ( actual . Target ) && aws . Int64Value ( expected . Healthy Threshold ) == aws . Int64Value ( actual . Healthy Threshold ) && aws . Int64Value ( expected . Unhealthy Threshold ) == aws . Int64Value ( actual . Unhealthy request := & elb . Configure Health Check request . Health request . Load Balancer Name = load Balancer . Load Balancer _ , err = c . elb . Configure Health } 
func ( c * Cloud ) ensure Load Balancer Instances ( load Balancer Name string , lb Instances [ ] * elb . Instance , instance I Ds map [ Instance ID ] * ec2 . Instance ) error { expected := sets . New for id := range instance I actual := sets . New for _ , lb Instance := range lb Instances { actual . Insert ( aws . String Value ( lb Instance . Instance add for _ , instance ID := range additions . List ( ) { add add Instance . Instance Id = aws . String ( instance add Instances = append ( add Instances , add remove for _ , instance ID := range removals . List ( ) { remove remove Instance . Instance Id = aws . String ( instance remove Instances = append ( remove Instances , remove if len ( add Instances ) > 0 { register Request := & elb . Register Instances With Load Balancer register Request . Instances = add register Request . Load Balancer Name = aws . String ( load Balancer _ , err := c . elb . Register Instances With Load Balancer ( register klog . V ( 1 ) . Infof ( " " , load Balancer if len ( remove Instances ) > 0 { deregister Request := & elb . Deregister Instances From Load Balancer deregister Request . Instances = remove deregister Request . Load Balancer Name = aws . String ( load Balancer _ , err := c . elb . Deregister Instances From Load Balancer ( deregister klog . V ( 1 ) . Infof ( " " , load Balancer } 
func ( c * Cloud ) find Instances For ELB ( nodes [ ] * v1 . Node ) ( map [ Instance ID ] * ec2 . Instance , error ) { // Map to instance ids ignoring Nodes where we cannot find the id (but logging) instance I Ds := map To AWS Instance I Ds cache Criteria := cache Criteria { // Max Age not required, because we only care about security groups, which should not change Has Instances : instance I snapshot , err := c . instance Cache . describe All Instances Cached ( cache instances := snapshot . Find Instances ( instance I } 
func ( s * daemon Set Lister ) Get Pod Daemon Sets ( pod * v1 . Pod ) ( [ ] * apps . Daemon var daemon Set * apps . Daemon list , err := s . Daemon var daemon Sets [ ] * apps . Daemon for i := range list { daemon if daemon selector , err = metav1 . Label Selector As Selector ( daemon if err != nil { // this should not happen if the Daemon // If a daemon daemon Sets = append ( daemon Sets , daemon if len ( daemon return daemon } 
func ( s * daemon Set Lister ) Get History Daemon Sets ( history * apps . Controller Revision ) ( [ ] * apps . Daemon list , err := s . Daemon var daemon Sets [ ] * apps . Daemon for _ , ds := range list { selector , err := metav1 . Label Selector As // If a Daemon daemon Sets = append ( daemon if len ( daemon return daemon } 
func New Set Service Account Options ( streams genericclioptions . IO Streams ) * Set Service Account Options { return & Set Service Account Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , Record Flags : genericclioptions . New Record Flags ( ) , Recorder : genericclioptions . Noop Recorder { } , IO } 
func New Cmd Service Account ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := New Set Service Account cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Aliases : [ ] string { " " } , Short : i18n . T ( " " ) , Long : serviceaccount Long , Example : serviceaccount Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check o . Print Flags . Add o . Record Flags . Add cmdutil . Add Filename Option Flags ( cmd , & o . file Name cmd . Flags ( ) . Bool cmd . Flags ( ) . Bool cmdutil . Add Dry Run cmdutil . Add Include Uninitialized } 
func ( o * Set Service Account o . Record o . Recorder , err = o . Record Flags . To o . short Output = cmdutil . Get Flag o . dry Run = cmdutil . Get Dry Run o . output = cmdutil . Get Flag o . update Pod Spec For Object = polymorphichelpers . Update Pod Spec For Object if o . dry Run { o . Print printer , err := o . Print Flags . To o . Print Obj = printer . Print cmd Namespace , enforce Namespace , err := f . To Raw Kube Config o . service Account builder := f . New Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Local Param ( o . local ) . Continue On Error ( ) . Namespace Param ( cmd Namespace ) . Default Namespace ( ) . Filename Param ( enforce Namespace , & o . file Name if ! o . local { builder . Resource Type Or Name } 
func ( o * Set Service Account Options ) Run ( ) error { patch patch Fn := func ( obj runtime . Object ) ( [ ] byte , error ) { _ , err := o . update Pod Spec For Object ( obj , func ( pod Spec * v1 . Pod Spec ) error { pod Spec . Service Account Name = o . service Account return runtime . Encode ( scheme . Default JSON patches := Calculate Patches ( o . infos , scheme . Default JSON Encoder ( ) , patch name := info . Object if patch . Err != nil { patch Errs = append ( patch if o . local || o . dry Run { if err := o . Print Obj ( info . Object , o . Out ) ; err != nil { patch Errs = append ( patch actual , err := resource . New Helper ( info . Client , info . Mapping ) . Patch ( info . Namespace , info . Name , types . Strategic Merge Patch if err != nil { patch Errs = append ( patch if err := o . Print Obj ( actual , o . Out ) ; err != nil { patch Errs = append ( patch return utilerrors . New Aggregate ( patch } 
func validate Common Fields ( obj , old runtime . Object , strategy REST Update Strategy ) ( field . Error List , error ) { all Errs := field . Error object old Object all Errs = append ( all Errs , genericvalidation . Validate Object Meta Accessor ( object Meta , strategy . Namespace Scoped ( ) , path . Validate Path Segment Name , field . New all Errs = append ( all Errs , genericvalidation . Validate Object Meta Accessor Update ( object Meta , old Object Meta , field . New return all } 
func Before Update ( strategy REST Update Strategy , ctx context . Context , obj , old runtime . Object ) error { object Meta , kind , kerr := object Meta And if strategy . Namespace Scoped ( ) { if ! Valid Namespace ( ctx , object Meta ) { return errors . New Bad } else if len ( object Meta . Get Namespace ( ) ) > 0 { object Meta . Set Namespace ( metav1 . Namespace // Ensure requests cannot update generation old object Meta . Set Generation ( old Meta . Get // Initializers are a deprecated alpha field and should not be saved old Meta . Set object Meta . Set // Ensure managed Fields state is removed unless Server Side Apply is enabled if ! utilfeature . Default Feature Gate . Enabled ( features . Server Side Apply ) { old Meta . Set Managed object Meta . Set Managed strategy . Prepare For // Cluster Name is ignored and should not be saved if len ( object Meta . Get Cluster Name ( ) ) > 0 { object Meta . Set Cluster // Use the existing UID if none is provided if len ( object Meta . Get UID ( ) ) == 0 { object Meta . Set UID ( old Meta . Get // ignore changes to timestamp if old Creation Time := old Meta . Get Creation Timestamp ( ) ; ! old Creation Time . Is Zero ( ) { object Meta . Set Creation Timestamp ( old Meta . Get Creation // an update can never remove/change a deletion timestamp if ! old Meta . Get Deletion Timestamp ( ) . Is Zero ( ) { object Meta . Set Deletion Timestamp ( old Meta . Get Deletion // an update can never remove/change grace period seconds if old Meta . Get Deletion Grace Period Seconds ( ) != nil && object Meta . Get Deletion Grace Period Seconds ( ) == nil { object Meta . Set Deletion Grace Period Seconds ( old Meta . Get Deletion Grace Period // Ensure some common fields, like UID, are validated for all resources. errs , err := validate Common if err != nil { return errors . New Internal errs = append ( errs , strategy . Validate if len ( errs ) > 0 { return errors . New Invalid ( kind . Group Kind ( ) , object Meta . Get } 
func Default Updated Object Info ( obj runtime . Object , transformers ... Transform Func ) Updated Object Info { return & default Updated Object } 
func ( i * default Updated Object // If empty, no preconditions needed uid := accessor . Get } 
func ( i * default Updated Object Info ) Updated Object ( ctx context . Context , old // Start with the configured object new // If the original is non-nil (might be nil if the first transformer builds the object from the old Obj), make a copy, // so we don't return the original. Before Update can mutate the returned object, doing things like clearing Resource Version. // If we're re-called, we need to be able to return the pristine version. if new Obj != nil { new Obj = new Obj . Deep Copy // Allow any configured transformers to update the new object for _ , transformer := range i . transformers { new Obj , err = transformer ( ctx , new Obj , old return new } 
func ( i * wrapped Updated Object Info ) Updated Object ( ctx context . Context , old Obj runtime . Object ) ( runtime . Object , error ) { new Obj , err := i . obj Info . Updated Object ( ctx , old if err != nil { return new // Allow any configured transformers to update the new object or error for _ , transformer := range i . transformers { new Obj , err = transformer ( ctx , new Obj , old return new } 
func Admission To Validate Object Update Func ( admit admission . Interface , static Attributes admission . Attributes , o admission . Object Interfaces ) Validate Object Update Func { validating Admission , ok := admit . ( admission . Validation return func ( obj , old runtime . Object ) error { final Attributes := admission . New Attributes Record ( obj , old , static Attributes . Get Kind ( ) , static Attributes . Get Namespace ( ) , static Attributes . Get Name ( ) , static Attributes . Get Resource ( ) , static Attributes . Get Subresource ( ) , static Attributes . Get Operation ( ) , static Attributes . Is Dry Run ( ) , static Attributes . Get User if ! validating Admission . Handles ( final Attributes . Get return validating Admission . Validate ( final } 
func New Fake Remote Runtime ( ) * Remote Runtime { fake Runtime Service := apitest . New Fake Runtime fake Image Service := apitest . New Fake Image f := & Remote Runtime { server : grpc . New Server ( ) , Runtime Service : fake Runtime Service , Image Service : fake Image kubeapi . Register Runtime Service kubeapi . Register Image Service } 
func ( f * Remote Runtime ) Start ( endpoint string ) error { l , err := util . Create } 
func ( f * Remote Runtime ) Version ( ctx context . Context , req * kubeapi . Version Request ) ( * kubeapi . Version Response , error ) { return f . Runtime } 
func ( f * Remote Runtime ) Run Pod Sandbox ( ctx context . Context , req * kubeapi . Run Pod Sandbox Request ) ( * kubeapi . Run Pod Sandbox Response , error ) { sandbox ID , err := f . Runtime Service . Run Pod Sandbox ( req . Config , req . Runtime return & kubeapi . Run Pod Sandbox Response { Pod Sandbox Id : sandbox } 
func ( f * Remote Runtime ) Stop Pod Sandbox ( ctx context . Context , req * kubeapi . Stop Pod Sandbox Request ) ( * kubeapi . Stop Pod Sandbox Response , error ) { err := f . Runtime Service . Stop Pod Sandbox ( req . Pod Sandbox return & kubeapi . Stop Pod Sandbox } 
func ( f * Remote Runtime ) Remove Pod Sandbox ( ctx context . Context , req * kubeapi . Remove Pod Sandbox Request ) ( * kubeapi . Remove Pod Sandbox Response , error ) { err := f . Runtime Service . Stop Pod Sandbox ( req . Pod Sandbox return & kubeapi . Remove Pod Sandbox } 
func ( f * Remote Runtime ) Pod Sandbox Status ( ctx context . Context , req * kubeapi . Pod Sandbox Status Request ) ( * kubeapi . Pod Sandbox Status Response , error ) { pod Status , err := f . Runtime Service . Pod Sandbox Status ( req . Pod Sandbox return & kubeapi . Pod Sandbox Status Response { Status : pod } 
func ( f * Remote Runtime ) List Pod Sandbox ( ctx context . Context , req * kubeapi . List Pod Sandbox Request ) ( * kubeapi . List Pod Sandbox Response , error ) { items , err := f . Runtime Service . List Pod return & kubeapi . List Pod Sandbox } 
func ( f * Remote Runtime ) Create Container ( ctx context . Context , req * kubeapi . Create Container Request ) ( * kubeapi . Create Container Response , error ) { container ID , err := f . Runtime Service . Create Container ( req . Pod Sandbox Id , req . Config , req . Sandbox return & kubeapi . Create Container Response { Container Id : container } 
func ( f * Remote Runtime ) Start Container ( ctx context . Context , req * kubeapi . Start Container Request ) ( * kubeapi . Start Container Response , error ) { err := f . Runtime Service . Start Container ( req . Container return & kubeapi . Start Container } 
func ( f * Remote Runtime ) Stop Container ( ctx context . Context , req * kubeapi . Stop Container Request ) ( * kubeapi . Stop Container Response , error ) { err := f . Runtime Service . Stop Container ( req . Container return & kubeapi . Stop Container } 
func ( f * Remote Runtime ) Remove Container ( ctx context . Context , req * kubeapi . Remove Container Request ) ( * kubeapi . Remove Container Response , error ) { err := f . Runtime Service . Remove Container ( req . Container return & kubeapi . Remove Container } 
func ( f * Remote Runtime ) List Containers ( ctx context . Context , req * kubeapi . List Containers Request ) ( * kubeapi . List Containers Response , error ) { items , err := f . Runtime Service . List return & kubeapi . List Containers } 
func ( f * Remote Runtime ) Container Status ( ctx context . Context , req * kubeapi . Container Status Request ) ( * kubeapi . Container Status Response , error ) { status , err := f . Runtime Service . Container Status ( req . Container return & kubeapi . Container Status } 
func ( f * Remote Runtime ) Exec Sync ( ctx context . Context , req * kubeapi . Exec Sync Request ) ( * kubeapi . Exec Sync Response , error ) { var exit stdout , stderr , err := f . Runtime Service . Exec Sync ( req . Container if err != nil { exit Error , ok := err . ( utilexec . Exit exit Code = int32 ( exit Error . Exit return & kubeapi . Exec Sync Response { Stdout : stdout , Stderr : stderr , Exit Code : exit } 
func ( f * Remote Runtime ) Exec ( ctx context . Context , req * kubeapi . Exec Request ) ( * kubeapi . Exec Response , error ) { return f . Runtime } 
func ( f * Remote Runtime ) Attach ( ctx context . Context , req * kubeapi . Attach Request ) ( * kubeapi . Attach Response , error ) { return f . Runtime } 
func ( f * Remote Runtime ) Port Forward ( ctx context . Context , req * kubeapi . Port Forward Request ) ( * kubeapi . Port Forward Response , error ) { return f . Runtime Service . Port } 
func ( f * Remote Runtime ) Container Stats ( ctx context . Context , req * kubeapi . Container Stats Request ) ( * kubeapi . Container Stats Response , error ) { stats , err := f . Runtime Service . Container Stats ( req . Container return & kubeapi . Container Stats } 
func ( f * Remote Runtime ) List Container Stats ( ctx context . Context , req * kubeapi . List Container Stats Request ) ( * kubeapi . List Container Stats Response , error ) { stats , err := f . Runtime Service . List Container return & kubeapi . List Container Stats } 
func ( f * Remote Runtime ) Update Runtime Config ( ctx context . Context , req * kubeapi . Update Runtime Config Request ) ( * kubeapi . Update Runtime Config Response , error ) { err := f . Runtime Service . Update Runtime Config ( req . Runtime return & kubeapi . Update Runtime Config } 
func ( f * Remote Runtime ) Status ( ctx context . Context , req * kubeapi . Status Request ) ( * kubeapi . Status Response , error ) { status , err := f . Runtime return & kubeapi . Status } 
func ( f * Remote Runtime ) Update Container Resources ( ctx context . Context , req * kubeapi . Update Container Resources Request ) ( * kubeapi . Update Container Resources Response , error ) { err := f . Runtime Service . Update Container Resources ( req . Container return & kubeapi . Update Container Resources } 
func ( f * Remote Runtime ) Reopen Container Log ( ctx context . Context , req * kubeapi . Reopen Container Log Request ) ( * kubeapi . Reopen Container Log Response , error ) { err := f . Runtime Service . Reopen Container Log ( req . Container return & kubeapi . Reopen Container Log } 
func ( c * Fake Horizontal Pod Autoscalers ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( horizontalpodautoscalers } 
func ( c * Fake Horizontal Pod Autoscalers ) Update ( horizontal Pod Autoscaler * v2beta1 . Horizontal Pod Autoscaler ) ( result * v2beta1 . Horizontal Pod Autoscaler , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( horizontalpodautoscalers Resource , c . ns , horizontal Pod Autoscaler ) , & v2beta1 . Horizontal Pod return obj . ( * v2beta1 . Horizontal Pod } 
func ( c * Fake Horizontal Pod Autoscalers ) Update Status ( horizontal Pod Autoscaler * v2beta1 . Horizontal Pod Autoscaler ) ( * v2beta1 . Horizontal Pod Autoscaler , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( horizontalpodautoscalers Resource , " " , c . ns , horizontal Pod Autoscaler ) , & v2beta1 . Horizontal Pod return obj . ( * v2beta1 . Horizontal Pod } 
func ( c * Fake Horizontal Pod Autoscalers ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( horizontalpodautoscalers Resource , c . ns , name ) , & v2beta1 . Horizontal Pod } 
func ( c * Fake Horizontal Pod Autoscalers ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( horizontalpodautoscalers Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v2beta1 . Horizontal Pod Autoscaler } 
func ( s Cluster Role Binding Generator V1 ) Generate ( generic Params map [ string ] interface { } ) ( runtime . Object , error ) { err := generate . Validate Params ( s . Param Names ( ) , generic delegate := & Cluster Role Binding Generator user Strings , found := generic if found { from File Array , is Array := user if ! is Array { return nil , fmt . Errorf ( " " , user delegate . Users = from File delete ( generic group Strings , found := generic if found { from Literal Array , is Array := group if ! is Array { return nil , fmt . Errorf ( " " , group delegate . Groups = from Literal delete ( generic sa Strings , found := generic if found { from Literal Array , is Array := sa if ! is Array { return nil , fmt . Errorf ( " " , sa delegate . Service Accounts = from Literal delete ( generic for key , value := range generic Params { str Val , is if ! is params [ key ] = str delegate . Cluster return delegate . Structured } 
func ( s Cluster Role Binding Generator V1 ) Structured cluster Role Binding := & rbacv1beta1 . Cluster Role cluster Role cluster Role Binding . Role Ref = rbacv1beta1 . Role Ref { API Group : rbacv1beta1 . Group Name , Kind : " " , Name : s . Cluster for _ , user := range sets . New String ( s . Users ... ) . List ( ) { cluster Role Binding . Subjects = append ( cluster Role Binding . Subjects , rbacv1beta1 . Subject { Kind : rbacv1beta1 . User Kind , API Group : rbacv1beta1 . Group for _ , group := range sets . New String ( s . Groups ... ) . List ( ) { cluster Role Binding . Subjects = append ( cluster Role Binding . Subjects , rbacv1beta1 . Subject { Kind : rbacv1beta1 . Group Kind , API Group : rbacv1beta1 . Group for _ , sa := range sets . New String ( s . Service cluster Role Binding . Subjects = append ( cluster Role Binding . Subjects , rbacv1beta1 . Subject { Kind : rbacv1beta1 . Service Account Kind , API return cluster Role } 
func ( s Cluster Role Binding Generator if len ( s . Cluster } 
func New Create Role Options ( io Streams genericclioptions . IO Streams ) * Create Role Options { return & Create Role Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , IO Streams : io } 
func New Cmd Create Role ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Create Role Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : role Long , Long : role Long , Example : role Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( o . Run Create o . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Dry Run cmd . Flags ( ) . String Slice cmd . Flags ( ) . String cmd . Flags ( ) . String Array Var ( & o . Resource Names , " " , o . Resource } 
func ( o * Create Role Options ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command for _ , v := range o . Verbs { // Verb if ! array // Support resource.group pattern. If no API Group specified, use "" as core API Group. // e.g. --resource=pods,deployments.extensions resources := cmdutil . Get Flag String for _ , r := range resources { sections := strings . Split resource := & Resource if len ( sections ) == 2 { resource . Sub parts := strings . Split if resource . Resource == " " && len ( parts ) == 1 && len ( sections ) == 1 { o . Resources = [ ] Resource // Remove duplicate resource names. resource for _ , n := range o . Resource Names { if ! array Contains ( resource Names , n ) { resource Names = append ( resource o . Resource Names = resource // Complete other options for Run. o . Mapper , err = f . To REST o . Dry Run = cmdutil . Get Dry Run o . Output Format = cmdutil . Get Flag if o . Dry Run { o . Print printer , err := o . Print Flags . To o . Print Obj = func ( obj runtime . Object ) error { return printer . Print o . Namespace , _ , err = f . To Raw Kube Config clientset , err := f . Kubernetes Client o . Client = clientset . Rbac } 
func ( o * Create Role for _ , v := range o . Verbs { if ! array Contains ( valid Resource return o . validate } 
func ( o * Create Role Options ) Run Create Role ( ) error { role := & rbacv1 . Role { // this is ok because we know exactly how we want to be serialized Type Meta : metav1 . Type Meta { API Version : rbacv1 . Scheme Group rules , err := generate Resource Policy Rules ( o . Mapper , o . Verbs , o . Resources , o . Resource // Create role. if ! o . Dry return o . Print } 
func Config Map Hash ( cm * v1 . Config Map ) ( string , error ) { encoded , err := encode Config h , err := encode } 
func Secret Hash ( sec * v1 . Secret ) ( string , error ) { encoded , err := encode h , err := encode } 
func encode Config Map ( cm * v1 . Config if len ( cm . Binary Data ) > 0 { m [ " " ] = cm . Binary } 
func encode } 
func encode } 
} 
func ( strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { _ = obj . ( * rbac . Cluster Role } 
func ( strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Cluster Role Binding := obj . ( * rbac . Cluster Role old Cluster Role Binding := old . ( * rbac . Cluster Role _ , _ = new Cluster Role Binding , old Cluster Role } 
func ( strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { cluster Role Binding := obj . ( * rbac . Cluster Role return validation . Validate Cluster Role Binding ( cluster Role } 
func ( strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new Obj := obj . ( * rbac . Cluster Role error List := validation . Validate Cluster Role Binding ( new return append ( error List , validation . Validate Cluster Role Binding Update ( new Obj , old . ( * rbac . Cluster Role } 
func ( ds * docker Service ) Container Stats ( _ context . Context , r * runtimeapi . Container Stats Request ) ( * runtimeapi . Container Stats Response , error ) { stats , err := ds . get Container Stats ( r . Container return & runtimeapi . Container Stats } 
func ( ds * docker Service ) List Container Stats ( ctx context . Context , r * runtimeapi . List Container Stats Request ) ( * runtimeapi . List Container Stats Response , error ) { container Stats Filter := r . Get filter := & runtimeapi . Container if container Stats Filter != nil { filter . Id = container Stats filter . Pod Sandbox Id = container Stats Filter . Pod Sandbox filter . Label Selector = container Stats Filter . Label list Resp , err := ds . List Containers ( ctx , & runtimeapi . List Containers var stats [ ] * runtimeapi . Container for _ , container := range list Resp . Containers { container Stats , err := ds . get Container stats = append ( stats , container return & runtimeapi . List Container Stats } 
func New Container Manager ( mount Util mount . Interface , cadvisor Interface cadvisor . Interface , node Config Node Config , fail Swap On bool , device Plugin Enabled bool , recorder record . Event Recorder ) ( Container Manager , error ) { var capacity = v1 . Resource // It is safe to invoke `Machine Info` on c Advisor before logically initializing c Advisor here because // machine info is computed and cached once as part of c Advisor object creation. // But `Root Fs Info` and `Images Fs Info` are not available at this moment so they will be called later during manager starts machine Info , err := cadvisor Interface . Machine capacity = cadvisor . Capacity From Machine Info ( machine return & container Manager Impl { capacity : capacity , node Config : node Config , cadvisor Interface : cadvisor } 
func New Defaults ( ) ( * args . Generator Args , * Custom Args ) { generic Args := args . Default ( ) . Without Default Flag custom Args := & Custom generic Args . Custom Args = ( * generators . Custom Args ) ( custom generic Args . Output File Base return generic Args , custom } 
func ( ca * Custom Args ) Add Flags ( fs * pflag . Flag Set ) { pflag . Command Line . String Slice Var ( & ca . Bounding Dirs , " " , ca . Bounding } 
func ( in * Fischer ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object if in . Disallowed Flunders != nil { in , out := & in . Disallowed Flunders , & out . Disallowed } 
func ( in * Fischer ) Deep in . Deep Copy } 
func ( in * Fischer ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Fischer List ) Deep Copy Into ( out * Fischer out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Fischer List ) Deep Copy ( ) * Fischer out := new ( Fischer in . Deep Copy } 
func ( in * Fischer List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Flunder ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object } 
func ( in * Flunder ) Deep in . Deep Copy } 
func ( in * Flunder ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Flunder List ) Deep Copy Into ( out * Flunder out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Flunder List ) Deep Copy ( ) * Flunder out := new ( Flunder in . Deep Copy } 
func ( in * Flunder List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Flunder Spec ) Deep Copy ( ) * Flunder out := new ( Flunder in . Deep Copy } 
func ( in * Flunder Status ) Deep Copy ( ) * Flunder out := new ( Flunder in . Deep Copy } 
func ( g * Cloud ) Get Target Pool ( name , region string ) ( * compute . Target Pool , error ) { ctx , cancel := cloud . Context With Call mc := new Target Pool Metric v , err := g . c . Target Pools ( ) . Get ( ctx , meta . Regional } 
func ( g * Cloud ) Create Target Pool ( tp * compute . Target Pool , region string ) error { ctx , cancel := cloud . Context With Call mc := new Target Pool Metric return mc . Observe ( g . c . Target Pools ( ) . Insert ( ctx , meta . Regional } 
func ( g * Cloud ) Delete Target Pool ( name , region string ) error { ctx , cancel := cloud . Context With Call mc := new Target Pool Metric return mc . Observe ( g . c . Target Pools ( ) . Delete ( ctx , meta . Regional } 
func ( g * Cloud ) Add Instances To Target Pool ( name , region string , instance Refs [ ] * compute . Instance Reference ) error { ctx , cancel := cloud . Context With Call req := & compute . Target Pools Add Instance Request { Instances : instance mc := new Target Pool Metric return mc . Observe ( g . c . Target Pools ( ) . Add Instance ( ctx , meta . Regional } 
func ( g * Cloud ) Remove Instances From Target Pool ( name , region string , instance Refs [ ] * compute . Instance Reference ) error { ctx , cancel := cloud . Context With Call req := & compute . Target Pools Remove Instance Request { Instances : instance mc := new Target Pool Metric return mc . Observe ( g . c . Target Pools ( ) . Remove Instance ( ctx , meta . Regional } 
func ( e List Element ) Merge ( v Strategy ) ( Result , error ) { return v . Merge } 
func ( e List Element ) Has Conflict ( ) error { for _ , item := range e . Values { if item , ok := item . ( Conflict Detector ) ; ok { if err := item . Has } 
func new Cmd Kube Config Utility ( out io . Writer ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Long : kubeconfig Long cmd . Add Command ( new Cmd User Kube } 
func new Cmd User Kube Config ( out io . Writer ) * cobra . Command { cfg := & kubeadmapiv1beta2 . Init var token , client // Creates the UX Command cmd := & cobra . Command { Use : " " , Short : " " , Long : user Kubeconfig Long Desc , Example : user Kubeconfig Example , Run : func ( cmd * cobra . Command , args [ ] string ) { if client Name == " " { kubeadmutil . Check // This call returns the ready-to-use configuration based on the default cfg populated by flags internalcfg , err := configutil . Defaulted Init kubeadmutil . Check // if the kubeconfig file for an additional user has to use a token, use it if token != " " { kubeadmutil . Check Err ( kubeconfigphase . Write Kube Config With Token ( out , internalcfg , client // Otherwise, write a kubeconfig file with a generate client cert kubeadmutil . Check Err ( kubeconfigphase . Write Kube Config With Client Cert ( out , internalcfg , client // Add flags to the command cmd . Flags ( ) . String Var ( & cfg . Certificates Dir , options . Certificates Dir , cfg . Certificates cmd . Flags ( ) . String Var ( & cfg . Local API Endpoint . Advertise Address , options . API Server Advertise Address , cfg . Local API Endpoint . Advertise cmd . Flags ( ) . Int32Var ( & cfg . Local API Endpoint . Bind Port , options . API Server Bind Port , cfg . Local API Endpoint . Bind cmd . Flags ( ) . String Var ( & token , options . Token cmd . Flags ( ) . String Var ( & client Name , " " , client cmd . Flags ( ) . String Slice } 
func ( pdev pod Devices ) container Devices ( pod UID , cont Name , resource string ) sets . String { if _ , pod Exists := pdev [ pod UID ] ; ! pod if _ , cont Exists := pdev [ pod UID ] [ cont Name ] ; ! cont devs , resource Exists := pdev [ pod UID ] [ cont if ! resource return devs . device } 
func ( pdev pod Devices ) add Container Allocated Resources ( pod UID , cont Name string , allocated Resources map [ string ] sets . String ) { containers , exists := pdev [ pod resources , exists := containers [ cont for resource , devices := range resources { allocated Resources [ resource ] = allocated Resources [ resource ] . Union ( devices . device } 
func ( pdev pod Devices ) remove Container Allocated Resources ( pod UID , cont Name string , allocated Resources map [ string ] sets . String ) { containers , exists := pdev [ pod resources , exists := containers [ cont for resource , devices := range resources { allocated Resources [ resource ] = allocated Resources [ resource ] . Difference ( devices . device } 
func ( pdev pod for _ , container Devices := range pdev { for _ , resources := range container Devices { for resource , devices := range resources { if _ , exists := ret [ resource ] ; ! exists { ret [ resource ] = sets . New if devices . alloc Resp != nil { ret [ resource ] = ret [ resource ] . Union ( devices . device } 
func ( pdev pod Devices ) to Checkpoint Data ( ) [ ] checkpoint . Pod Devices Entry { var data [ ] checkpoint . Pod Devices for pod UID , container Devices := range pdev { for con Name , resources := range container Devices { for resource , devices := range resources { dev Ids := devices . device Ids . Unsorted if devices . alloc Resp == nil { klog . Errorf ( " " , pod UID , con alloc Resp , err := devices . alloc if err != nil { klog . Errorf ( " " , pod UID , con data = append ( data , checkpoint . Pod Devices Entry { Pod UID : pod UID , Container Name : con Name , Resource Name : resource , Device I Ds : dev Ids , Alloc Resp : alloc } 
func ( pdev pod Devices ) from Checkpoint Data ( data [ ] checkpoint . Pod Devices Entry ) { for _ , entry := range data { klog . V ( 2 ) . Infof ( " \n " , entry . Pod UID , entry . Container Name , entry . Resource Name , entry . Device I Ds , entry . Alloc dev I Ds := sets . New for _ , dev ID := range entry . Device I Ds { dev I Ds . Insert ( dev alloc Resp := & pluginapi . Container Allocate err := alloc Resp . Unmarshal ( entry . Alloc if err != nil { klog . Errorf ( " " , entry . Pod UID , entry . Container Name , entry . Resource pdev . insert ( entry . Pod UID , entry . Container Name , entry . Resource Name , dev I Ds , alloc } 
func ( pdev pod Devices ) device Run Container Options ( pod UID , cont Name string ) * Device Run Container Options { containers , exists := pdev [ pod resources , exists := containers [ cont opts := & Device Run Container // Maps to detect duplicate settings. devs mounts envs annotations // Loops through Allocation Responses of all cached device resources. for _ , devices := range resources { resp := devices . alloc // Each Allocate response has the following artifacts. // Environment variables // Mount points // Device files // Container annotations // These artifacts are per resource per container. // Updates Run Container Options.Envs. for k , v := range resp . Envs { if e , ok := envs envs opts . Envs = append ( opts . Envs , kubecontainer . Env // Updates Run Container Options.Devices. for _ , dev := range resp . Devices { if d , ok := devs Map [ dev . Container Path ] ; ok { klog . V ( 4 ) . Infof ( " " , dev . Container Path , dev . Host if d != dev . Host Path { klog . Errorf ( " " , dev . Container Path , d , dev . Host klog . V ( 4 ) . Infof ( " " , dev . Container Path , dev . Host devs Map [ dev . Container Path ] = dev . Host opts . Devices = append ( opts . Devices , kubecontainer . Device Info { Path On Host : dev . Host Path , Path In Container : dev . Container // Updates Run Container Options.Mounts. for _ , mount := range resp . Mounts { if m , ok := mounts Map [ mount . Container Path ] ; ok { klog . V ( 4 ) . Infof ( " " , mount . Container Path , mount . Host if m != mount . Host Path { klog . Errorf ( " " , mount . Container Path , m , mount . Host klog . V ( 4 ) . Infof ( " " , mount . Container Path , mount . Host mounts Map [ mount . Container Path ] = mount . Host opts . Mounts = append ( opts . Mounts , kubecontainer . Mount { Name : mount . Container Path , Container Path : mount . Container Path , Host Path : mount . Host Path , Read Only : mount . Read Only , // TODO: This may need to be part of Device plugin API. SE Linux // Updates for Annotations for k , v := range resp . Annotations { if e , ok := annotations annotations } 
func ( pdev pod Devices ) get Container Devices ( pod UID , cont Name string ) [ ] * podresourcesapi . Container Devices { if _ , pod Exists := pdev [ pod UID ] ; ! pod if _ , cont Exists := pdev [ pod UID ] [ cont Name ] ; ! cont c Dev := [ ] * podresourcesapi . Container for resource , allocate Info := range pdev [ pod UID ] [ cont Name ] { c Dev = append ( c Dev , & podresourcesapi . Container Devices { Resource Name : resource , Device Ids : allocate Info . device Ids . Unsorted return c } 
func ( c * Authorization V1Client ) REST return c . rest } 
func With Authentication ( handler http . Handler , auth authenticator . Request , failed http . Handler , api return http . Handler Func ( func ( w http . Response Writer , req * http . Request ) { if len ( api Auds ) > 0 { req = req . With Context ( authenticator . With Audiences ( req . Context ( ) , api resp , ok , err := auth . Authenticate failed . Serve // TODO(mikedanese): verify the response audience matches one of api req = req . With Context ( genericapirequest . With authenticated User Counter . With Label Values ( compress Username ( resp . User . Get handler . Serve } 
func compress } 
func valid Signal ( signal evictionapi . Signal ) bool { _ , found := signal To } 
func Parse Threshold Config ( allocatable Config [ ] string , eviction Hard , eviction Soft , eviction Soft Grace Period , eviction Minimum hard Thresholds , err := parse Threshold Statements ( eviction results = append ( results , hard soft Thresholds , err := parse Threshold Statements ( eviction grace Periods , err := parse Grace Periods ( eviction Soft Grace min Reclaims , err := parse Minimum Reclaims ( eviction Minimum for i := range soft Thresholds { signal := soft period , found := grace soft Thresholds [ i ] . Grace results = append ( results , soft for i := range results { for signal , min Reclaim := range min Reclaims { if results [ i ] . Signal == signal { results [ i ] . Min Reclaim = & min for _ , key := range allocatable Config { if key == kubetypes . Node Allocatable Enforcement Key { results = add Allocatable } 
func parse Threshold for signal , val := range statements { result , err := parse Threshold } 
func parse Threshold Statement ( signal evictionapi . Signal , val string ) ( * evictionapi . Threshold , error ) { if ! valid Signal ( signal ) { return nil , fmt . Errorf ( unsupported Eviction operator := evictionapi . Op For if strings . Has percentage , err := parse return & evictionapi . Threshold { Signal : signal , Operator : operator , Value : evictionapi . Threshold quantity , err := resource . Parse if quantity . Sign ( ) < 0 || quantity . Is return & evictionapi . Threshold { Signal : signal , Operator : operator , Value : evictionapi . Threshold } 
func parse Percentage ( input string ) ( float32 , error ) { value , err := strconv . Parse Float ( strings . Trim } 
func parse Grace if ! valid Signal ( signal ) { return nil , fmt . Errorf ( unsupported Eviction grace Period , err := time . Parse if grace results [ signal ] = grace } 
func parse Minimum Reclaims ( statements map [ string ] string ) ( map [ evictionapi . Signal ] evictionapi . Threshold results := map [ evictionapi . Signal ] evictionapi . Threshold if ! valid Signal ( signal ) { return nil , fmt . Errorf ( unsupported Eviction if strings . Has Suffix ( val , " " ) { percentage , err := parse results [ signal ] = evictionapi . Threshold quantity , err := resource . Parse results [ signal ] = evictionapi . Threshold } 
func disk Usage ( fs Stats * statsapi . Fs Stats ) * resource . Quantity { if fs Stats == nil || fs Stats . Used Bytes == nil { return & resource . Quantity { Format : resource . Binary usage := int64 ( * fs Stats . Used return resource . New Quantity ( usage , resource . Binary } 
func inode Usage ( fs Stats * statsapi . Fs Stats ) * resource . Quantity { if fs Stats == nil || fs Stats . Inodes Used == nil { return & resource . Quantity { Format : resource . Decimal usage := int64 ( * fs Stats . Inodes return resource . New Quantity ( usage , resource . Decimal } 
func memory Usage ( mem Stats * statsapi . Memory Stats ) * resource . Quantity { if mem Stats == nil || mem Stats . Working Set Bytes == nil { return & resource . Quantity { Format : resource . Binary usage := int64 ( * mem Stats . Working Set return resource . New Quantity ( usage , resource . Binary } 
func container Usage ( pod Stats statsapi . Pod Stats , stats To Measure [ ] fs Stats Type ) v1 . Resource List { disk := resource . Quantity { Format : resource . Binary inodes := resource . Quantity { Format : resource . Decimal for _ , container := range pod Stats . Containers { if has Fs Stats Type ( stats To Measure , fs Stats Root ) { disk . Add ( * disk inodes . Add ( * inode if has Fs Stats Type ( stats To Measure , fs Stats Logs ) { disk . Add ( * disk inodes . Add ( * inode return v1 . Resource List { v1 . Resource Ephemeral Storage : disk , resource } 
func pod Local Volume Usage ( volume Names [ ] string , pod Stats statsapi . Pod Stats ) v1 . Resource List { disk := resource . Quantity { Format : resource . Binary inodes := resource . Quantity { Format : resource . Decimal for _ , volume Name := range volume Names { for _ , volume Stats := range pod Stats . Volume Stats { if volume Stats . Name == volume Name { disk . Add ( * disk Usage ( & volume Stats . Fs inodes . Add ( * inode Usage ( & volume Stats . Fs return v1 . Resource List { v1 . Resource Ephemeral Storage : disk , resource } 
func pod Disk Usage ( pod Stats statsapi . Pod Stats , pod * v1 . Pod , stats To Measure [ ] fs Stats Type ) ( v1 . Resource List , error ) { disk := resource . Quantity { Format : resource . Binary inodes := resource . Quantity { Format : resource . Decimal container Usage List := container Usage ( pod Stats , stats To disk . Add ( container Usage List [ v1 . Resource Ephemeral inodes . Add ( container Usage List [ resource if has Fs Stats Type ( stats To Measure , fs Stats Local Volume Source ) { volume Names := local Volume pod Local Volume Usage List := pod Local Volume Usage ( volume Names , pod disk . Add ( pod Local Volume Usage List [ v1 . Resource Ephemeral inodes . Add ( pod Local Volume Usage List [ resource return v1 . Resource List { v1 . Resource Ephemeral Storage : disk , resource } 
func local Ephemeral Volume for _ , volume := range pod . Spec . Volumes { if volume . Git Repo != nil || ( volume . Empty Dir != nil && volume . Empty Dir . Medium != v1 . Storage Medium Memory ) || volume . Config Map != nil || volume . Downward } 
func format Threshold ( threshold evictionapi . Threshold ) string { return fmt . Sprintf ( " " , threshold . Signal , threshold . Operator , evictionapi . Threshold Value ( threshold . Value ) , threshold . Grace } 
func cached Stats Func ( pod Stats [ ] statsapi . Pod Stats ) stats Func { uid2Pod Stats := map [ string ] statsapi . Pod for i := range pod Stats { uid2Pod Stats [ pod Stats [ i ] . Pod Ref . UID ] = pod return func ( pod * v1 . Pod ) ( statsapi . Pod Stats , bool ) { stats , found := uid2Pod } 
func ( ms * multi } 
func ( ms * multi } 
func ( ms * multi for k = 0 ; k < len ( ms . cmp ) - 1 ; k ++ { cmp // p1 is less than p2 if cmp // p1 is greater than p2 if cmp } 
func priority ( p1 , p2 * v1 . Pod ) int { if ! utilfeature . Default Feature Gate . Enabled ( features . Pod priority1 := schedulerutils . Get Pod priority2 := schedulerutils . Get Pod } 
func exceed Memory Requests ( stats stats Func ) cmp if ! p1Found || ! p2Found { // prioritize evicting the pod for which no stats were found return cmp p1Memory := memory p2Memory := memory p1Exceeds Requests := p1Memory . Cmp ( pod Request ( p1 , v1 . Resource p2Exceeds Requests := p2Memory . Cmp ( pod Request ( p2 , v1 . Resource // prioritize evicting the pod which exceeds its requests return cmp Bool ( p1Exceeds Requests , p2Exceeds } 
func memory ( stats stats Func ) cmp if ! p1Found || ! p2Found { // prioritize evicting the pod for which no stats were found return cmp // adjust p1, p2 usage relative to the request (if any) p1Memory := memory p1Request := pod Request ( p1 , v1 . Resource p2Memory := memory p2Request := pod Request ( p2 , v1 . Resource } 
func pod Request ( pod * v1 . Pod , resource Name v1 . Resource Name ) resource . Quantity { container Value := resource . Quantity { Format : resource . Binary if resource Name == v1 . Resource Ephemeral Storage && ! utilfeature . Default Feature Gate . Enabled ( features . Local Storage Capacity Isolation ) { // if the local storage capacity isolation feature gate is disabled, pods request 0 disk return container for i := range pod . Spec . Containers { switch resource Name { case v1 . Resource Memory : container case v1 . Resource Ephemeral Storage : container Value . Add ( * pod . Spec . Containers [ i ] . Resources . Requests . Storage init Value := resource . Quantity { Format : resource . Binary for i := range pod . Spec . Init Containers { switch resource Name { case v1 . Resource Memory : if init Value . Cmp ( * pod . Spec . Init Containers [ i ] . Resources . Requests . Memory ( ) ) < 0 { init Value = * pod . Spec . Init case v1 . Resource Ephemeral Storage : if init Value . Cmp ( * pod . Spec . Init Containers [ i ] . Resources . Requests . Storage Ephemeral ( ) ) < 0 { init Value = * pod . Spec . Init Containers [ i ] . Resources . Requests . Storage if container Value . Cmp ( init Value ) > 0 { return container return init } 
func exceed Disk Requests ( stats stats Func , fs Stats To Measure [ ] fs Stats Type , disk Resource v1 . Resource Name ) cmp if ! p1Found || ! p2Found { // prioritize evicting the pod for which no stats were found return cmp p1Usage , p1Err := pod Disk Usage ( p1Stats , p1 , fs Stats To p2Usage , p2Err := pod Disk Usage ( p2Stats , p2 , fs Stats To if p1Err != nil || p2Err != nil { // prioritize evicting the pod which had an error getting stats return cmp p1Disk := p1Usage [ disk p2Disk := p2Usage [ disk p1Exceeds Requests := p1Disk . Cmp ( pod Request ( p1 , disk p2Exceeds Requests := p2Disk . Cmp ( pod Request ( p2 , disk // prioritize evicting the pod which exceeds its requests return cmp Bool ( p1Exceeds Requests , p2Exceeds } 
func disk ( stats stats Func , fs Stats To Measure [ ] fs Stats Type , disk Resource v1 . Resource Name ) cmp if ! p1Found || ! p2Found { // prioritize evicting the pod for which no stats were found return cmp p1Usage , p1Err := pod Disk Usage ( p1Stats , p1 , fs Stats To p2Usage , p2Err := pod Disk Usage ( p2Stats , p2 , fs Stats To if p1Err != nil || p2Err != nil { // prioritize evicting the pod which had an error getting stats return cmp // adjust p1, p2 usage relative to the request (if any) p1Disk := p1Usage [ disk p2Disk := p2Usage [ disk p1Request := pod Request ( p1 , v1 . Resource Ephemeral p2Request := pod Request ( p2 , v1 . Resource Ephemeral } 
func rank Memory Pressure ( pods [ ] * v1 . Pod , stats stats Func ) { ordered By ( exceed Memory } 
func rank PID Pressure ( pods [ ] * v1 . Pod , stats stats Func ) { ordered } 
func rank Disk Pressure Func ( fs Stats To Measure [ ] fs Stats Type , disk Resource v1 . Resource Name ) rank Func { return func ( pods [ ] * v1 . Pod , stats stats Func ) { ordered By ( exceed Disk Requests ( stats , fs Stats To Measure , disk Resource ) , priority , disk ( stats , fs Stats To Measure , disk } 
func ( a by Eviction Priority ) Less ( i , j int ) bool { _ , j Signal Has Resource := signal To return a [ i ] . Signal == evictionapi . Signal Memory Available || a [ i ] . Signal == evictionapi . Signal Allocatable Memory Available || ! j Signal Has } 
func make Signal Observations ( summary * statsapi . Summary ) ( signal Observations , stats Func ) { // build the function to work against for pod stats stats Func := cached Stats // build an evaluation context for current eviction signals result := signal if memory := summary . Node . Memory ; memory != nil && memory . Available Bytes != nil && memory . Working Set Bytes != nil { result [ evictionapi . Signal Memory Available ] = signal Observation { available : resource . New Quantity ( int64 ( * memory . Available Bytes ) , resource . Binary SI ) , capacity : resource . New Quantity ( int64 ( * memory . Available Bytes + * memory . Working Set Bytes ) , resource . Binary if allocatable Container , err := get Sys Container ( summary . Node . System Containers , statsapi . System Container Pods ) ; err != nil { klog . Errorf ( " " , evictionapi . Signal Allocatable Memory } else { if memory := allocatable Container . Memory ; memory != nil && memory . Available Bytes != nil && memory . Working Set Bytes != nil { result [ evictionapi . Signal Allocatable Memory Available ] = signal Observation { available : resource . New Quantity ( int64 ( * memory . Available Bytes ) , resource . Binary SI ) , capacity : resource . New Quantity ( int64 ( * memory . Available Bytes + * memory . Working Set Bytes ) , resource . Binary if node Fs := summary . Node . Fs ; node Fs != nil { if node Fs . Available Bytes != nil && node Fs . Capacity Bytes != nil { result [ evictionapi . Signal Node Fs Available ] = signal Observation { available : resource . New Quantity ( int64 ( * node Fs . Available Bytes ) , resource . Binary SI ) , capacity : resource . New Quantity ( int64 ( * node Fs . Capacity Bytes ) , resource . Binary SI ) , time : node if node Fs . Inodes Free != nil && node Fs . Inodes != nil { result [ evictionapi . Signal Node Fs Inodes Free ] = signal Observation { available : resource . New Quantity ( int64 ( * node Fs . Inodes Free ) , resource . Decimal SI ) , capacity : resource . New Quantity ( int64 ( * node Fs . Inodes ) , resource . Decimal SI ) , time : node if summary . Node . Runtime != nil { if image Fs := summary . Node . Runtime . Image Fs ; image Fs != nil { if image Fs . Available Bytes != nil && image Fs . Capacity Bytes != nil { result [ evictionapi . Signal Image Fs Available ] = signal Observation { available : resource . New Quantity ( int64 ( * image Fs . Available Bytes ) , resource . Binary SI ) , capacity : resource . New Quantity ( int64 ( * image Fs . Capacity Bytes ) , resource . Binary SI ) , time : image if image Fs . Inodes Free != nil && image Fs . Inodes != nil { result [ evictionapi . Signal Image Fs Inodes Free ] = signal Observation { available : resource . New Quantity ( int64 ( * image Fs . Inodes Free ) , resource . Decimal SI ) , capacity : resource . New Quantity ( int64 ( * image Fs . Inodes ) , resource . Decimal SI ) , time : image if rlimit := summary . Node . Rlimit ; rlimit != nil { if rlimit . Num Of Running Processes != nil && rlimit . Max PID != nil { available := int64 ( * rlimit . Max PID ) - int64 ( * rlimit . Num Of Running result [ evictionapi . Signal PID Available ] = signal Observation { available : resource . New Quantity ( available , resource . Binary SI ) , capacity : resource . New Quantity ( int64 ( * rlimit . Max PID ) , resource . Binary return result , stats } 
func thresholds Met ( thresholds [ ] evictionapi . Threshold , observations signal Observations , enforce Min // determine if we have met the specified threshold threshold quantity := evictionapi . Get Threshold // if enforce Min Reclaim is specified, we compare relative to value - minreclaim if enforce Min Reclaim && threshold . Min Reclaim != nil { quantity . Add ( * evictionapi . Get Threshold Quantity ( * threshold . Min threshold switch threshold . Operator { case evictionapi . Op Less Than : threshold Met = threshold if threshold } 
func thresholds First Observed At ( thresholds [ ] evictionapi . Threshold , last Observed At thresholds Observed At , now time . Time ) thresholds Observed At { results := thresholds Observed for i := range thresholds { observed At , found := last Observed if ! found { observed results [ thresholds [ i ] ] = observed } 
func thresholds Met Grace Period ( observed At thresholds Observed for threshold , at := range observed if duration < threshold . Grace Period { klog . V ( 2 ) . Infof ( " " , format } 
func node Conditions ( thresholds [ ] evictionapi . Threshold ) [ ] v1 . Node Condition Type { results := [ ] v1 . Node Condition for _ , threshold := range thresholds { if node Condition , found := signal To Node Condition [ threshold . Signal ] ; found { if ! has Node Condition ( results , node Condition ) { results = append ( results , node } 
func node Conditions Last Observed At ( node Conditions [ ] v1 . Node Condition Type , last Observed At node Conditions Observed At , now time . Time ) node Conditions Observed At { results := node Conditions Observed // the input conditions were observed "now" for i := range node Conditions { results [ node // the conditions that were not observed now are merged in with their old time for key , value := range last Observed } 
func node Conditions Observed Since ( observed At node Conditions Observed At , period time . Duration , now time . Time ) [ ] v1 . Node Condition Type { results := [ ] v1 . Node Condition for node Condition , at := range observed if duration < period { results = append ( results , node } 
func has Fs Stats Type ( inputs [ ] fs Stats Type , item fs Stats } 
func has Node Condition ( inputs [ ] v1 . Node Condition Type , item v1 . Node Condition } 
func merge Thresholds ( inputs A [ ] evictionapi . Threshold , inputs B [ ] evictionapi . Threshold ) [ ] evictionapi . Threshold { results := inputs for _ , threshold := range inputs B { if ! has } 
func has Threshold ( inputs [ ] evictionapi . Threshold , item evictionapi . Threshold ) bool { for _ , input := range inputs { if input . Grace Period == item . Grace Period && input . Operator == item . Operator && input . Signal == item . Signal && compare Threshold } 
func compare Threshold Value ( a evictionapi . Threshold Value , b evictionapi . Threshold } 
func build Signal To Rank Func ( with Image Fs bool ) map [ evictionapi . Signal ] rank Func { signal To Rank Func := map [ evictionapi . Signal ] rank Func { evictionapi . Signal Memory Available : rank Memory Pressure , evictionapi . Signal Allocatable Memory Available : rank Memory Pressure , evictionapi . Signal PID Available : rank PID // usage of an imagefs is optional if with Image Fs { // with an imagefs, nodefs pod rank func for eviction only includes logs and local volumes signal To Rank Func [ evictionapi . Signal Node Fs Available ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Logs , fs Stats Local Volume Source } , v1 . Resource Ephemeral signal To Rank Func [ evictionapi . Signal Node Fs Inodes Free ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Logs , fs Stats Local Volume Source } , resource // with an imagefs, imagefs pod rank func for eviction only includes rootfs signal To Rank Func [ evictionapi . Signal Image Fs Available ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Root } , v1 . Resource Ephemeral signal To Rank Func [ evictionapi . Signal Image Fs Inodes Free ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Root } , resource } else { // without an imagefs, nodefs pod rank func for eviction looks at all fs stats. // since imagefs and nodefs share a common device, they share common ranking functions. signal To Rank Func [ evictionapi . Signal Node Fs Available ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Root , fs Stats Logs , fs Stats Local Volume Source } , v1 . Resource Ephemeral signal To Rank Func [ evictionapi . Signal Node Fs Inodes Free ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Root , fs Stats Logs , fs Stats Local Volume Source } , resource signal To Rank Func [ evictionapi . Signal Image Fs Available ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Root , fs Stats Logs , fs Stats Local Volume Source } , v1 . Resource Ephemeral signal To Rank Func [ evictionapi . Signal Image Fs Inodes Free ] = rank Disk Pressure Func ( [ ] fs Stats Type { fs Stats Root , fs Stats Logs , fs Stats Local Volume Source } , resource return signal To Rank } 
func Pod Is Evicted ( pod Status v1 . Pod Status ) bool { return pod Status . Phase == v1 . Pod Failed && pod } 
func build Signal To Node Reclaim Funcs ( image GC Image GC , container GC Container GC , with Image Fs bool ) map [ evictionapi . Signal ] node Reclaim Funcs { signal To Reclaim Func := map [ evictionapi . Signal ] node Reclaim // usage of an imagefs is optional if with Image Fs { // with an imagefs, nodefs pressure should just delete logs signal To Reclaim Func [ evictionapi . Signal Node Fs Available ] = node Reclaim signal To Reclaim Func [ evictionapi . Signal Node Fs Inodes Free ] = node Reclaim // with an imagefs, imagefs pressure should delete unused images signal To Reclaim Func [ evictionapi . Signal Image Fs Available ] = node Reclaim Funcs { container GC . Delete All Unused Containers , image GC . Delete Unused signal To Reclaim Func [ evictionapi . Signal Image Fs Inodes Free ] = node Reclaim Funcs { container GC . Delete All Unused Containers , image GC . Delete Unused } else { // without an imagefs, nodefs pressure should delete logs, and unused images // since imagefs and nodefs share a common device, they share common reclaim functions signal To Reclaim Func [ evictionapi . Signal Node Fs Available ] = node Reclaim Funcs { container GC . Delete All Unused Containers , image GC . Delete Unused signal To Reclaim Func [ evictionapi . Signal Node Fs Inodes Free ] = node Reclaim Funcs { container GC . Delete All Unused Containers , image GC . Delete Unused signal To Reclaim Func [ evictionapi . Signal Image Fs Available ] = node Reclaim Funcs { container GC . Delete All Unused Containers , image GC . Delete Unused signal To Reclaim Func [ evictionapi . Signal Image Fs Inodes Free ] = node Reclaim Funcs { container GC . Delete All Unused Containers , image GC . Delete Unused return signal To Reclaim } 
func eviction Message ( resource To Reclaim v1 . Resource Name , pod * v1 . Pod , stats stats message = fmt . Sprintf ( node Low Message Fmt , resource To container pod for _ , container Stats := range pod Stats . Containers { for _ , container := range pod . Spec . Containers { if container . Name == container Stats . Name { requests := container . Resources . Requests [ resource To switch resource To Reclaim { case v1 . Resource Ephemeral Storage : if container Stats . Rootfs != nil && container Stats . Rootfs . Used Bytes != nil && container Stats . Logs != nil && container Stats . Logs . Used Bytes != nil { usage = resource . New Quantity ( int64 ( * container Stats . Rootfs . Used Bytes + * container Stats . Logs . Used Bytes ) , resource . Binary case v1 . Resource Memory : if container Stats . Memory != nil && container Stats . Memory . Working Set Bytes != nil { usage = resource . New Quantity ( int64 ( * container Stats . Memory . Working Set Bytes ) , resource . Binary if usage != nil && usage . Cmp ( requests ) > 0 { message += fmt . Sprintf ( container Message container Usage = append ( container annotations [ Offending Containers annotations [ Offending Containers Usage Key ] = strings . Join ( container annotations [ Starved Resource Key ] = string ( resource To } 
func New REST Storage ( api Resource Config Source serverstorage . API Resource Config Source , rest Options Getter generic . REST Options Getter ) genericapiserver . API Group Info { api Group Info := genericapiserver . New Default API Group Info ( apiregistration . Group Name , aggregatorscheme . Scheme , metav1 . Parameter if api Resource Config Source . Version Enabled ( v1beta1 . Scheme Group api Service REST := apiservicestorage . New REST ( aggregatorscheme . Scheme , rest Options storage [ " " ] = api Service storage [ " " ] = apiservicestorage . New Status REST ( aggregatorscheme . Scheme , api Service api Group Info . Versioned Resources Storage if api Resource Config Source . Version Enabled ( v1 . Scheme Group api Service REST := apiservicestorage . New REST ( aggregatorscheme . Scheme , rest Options storage [ " " ] = api Service storage [ " " ] = apiservicestorage . New Status REST ( aggregatorscheme . Scheme , api Service api Group Info . Versioned Resources Storage return api Group } 
func ( c * Fake Events ) Update With Event Namespace ( event * v1 . Event ) ( * v1 . Event , error ) { action := core . New Root Update Action ( events if c . ns != " " { action = core . New Update Action ( events } 
func ( c * Fake Events ) Patch With Event Namespace ( event * v1 . Event , data [ ] byte ) ( * v1 . Event , error ) { // TODO: Should be configurable to support additional patch strategies. pt := types . Strategic Merge Patch action := core . New Root Patch Action ( events if c . ns != " " { action = core . New Patch Action ( events } 
func ( c * Fake Events ) Search ( scheme * runtime . Scheme , obj Or Ref runtime . Object ) ( * v1 . Event List , error ) { action := core . New Root List Action ( events Resource , events Kind , metav1 . List if c . ns != " " { action = core . New List Action ( events Resource , events Kind , c . ns , metav1 . List obj , err := c . Fake . Invokes ( action , & v1 . Event return obj . ( * v1 . Event } 
func ( md * cached Metrics ) Get Metrics ( ) ( * Metrics , error ) { md . once . cache ( func ( ) error { md . result Metrics , md . result Error = md . wrapped . Get return md . result return md . result Metrics , md . result } 
func ( o * cache Once ) cache ( f func ( ) error ) { if atomic . Load if err == nil { atomic . Store } 
func Subjects for _ , subject := range subjects { switch subject . Kind { case rbacv1 . Service Account case rbacv1 . User case rbacv1 . Group } 
func Compact String ( r rbacv1 . Policy Rule ) string { format String format if len ( r . API Groups ) > 0 { format String Parts = append ( format String format Args = append ( format Args , r . API if len ( r . Resources ) > 0 { format String Parts = append ( format String format Args = append ( format if len ( r . Non Resource UR Ls ) > 0 { format String Parts = append ( format String format Args = append ( format Args , r . Non Resource UR if len ( r . Resource Names ) > 0 { format String Parts = append ( format String format Args = append ( format Args , r . Resource if len ( r . Verbs ) > 0 { format String Parts = append ( format String format Args = append ( format format String := " " + strings . Join ( format String return fmt . Sprintf ( format String , format } 
func New Wait Flags ( rest Client Getter genericclioptions . REST Client Getter , streams genericclioptions . IO Streams ) * Wait Flags { return & Wait Flags { REST Client Getter : rest Client Getter , Print Flags : genericclioptions . New Print Flags ( " " ) , Resource Builder Flags : genericclioptions . New Resource Builder Flags ( ) . With Label Selector ( " " ) . With Field Selector ( " " ) . With All ( false ) . With All Namespaces ( false ) . With All ( false ) . With Latest ( ) , Timeout : 30 * time . Second , IO } 
func New Cmd Wait ( rest Client Getter genericclioptions . REST Client Getter , streams genericclioptions . IO Streams ) * cobra . Command { flags := New Wait Flags ( rest Client cmd := & cobra . Command { Use : " " , Short : " " , Long : wait Long , Example : wait Example , Disable Flags In Use Line : true , Run : func ( cmd * cobra . Command , args [ ] string ) { o , err := flags . To cmdutil . Check err = o . Run cmdutil . Check } , Suggest flags . Add } 
func ( flags * Wait Flags ) Add Flags ( cmd * cobra . Command ) { flags . Print Flags . Add flags . Resource Builder Flags . Add cmd . Flags ( ) . Duration cmd . Flags ( ) . String Var ( & flags . For Condition , " " , flags . For } 
func ( flags * Wait Flags ) To Options ( args [ ] string ) ( * Wait Options , error ) { printer , err := flags . Print Flags . To builder := flags . Resource Builder Flags . To Builder ( flags . REST Client client Config , err := flags . REST Client Getter . To REST dynamic Client , err := dynamic . New For Config ( client condition Fn , err := condition Func For ( flags . For Condition , flags . Err effective if effective Timeout < 0 { effective o := & Wait Options { Resource Finder : builder , Dynamic Client : dynamic Client , Timeout : effective Timeout , Printer : printer , Condition Fn : condition Fn , IO Streams : flags . IO } 
func ( o * Wait Options ) Run Wait ( ) error { visit err := o . Resource visit final Object , success , err := o . Condition if success { o . Printer . Print Obj ( final if err == nil { return fmt . Errorf ( " " , final if visit Count == 0 { return err No Matching } 
func Is Deleted ( info * resource . Info , o * Wait Options ) ( runtime . Object , bool , error ) { end name Selector := fields . One Term Equal // List with a name field selector to get the current resource Version to watch from (not the object's resource Version) gotten Obj List , err := o . Dynamic Client . Resource ( info . Mapping . Resource ) . Namespace ( info . Namespace ) . List ( metav1 . List Options { Field Selector : name if apierrors . Is Not if len ( gotten Obj gotten Obj := & gotten Obj resource Location := Resource Location { Group Resource : info . Mapping . Resource . Group Resource ( ) , Namespace : gotten Obj . Get Namespace ( ) , Name : gotten Obj . Get if uid , ok := o . UID Map [ resource Location ] ; ok { if gotten Obj . Get UID ( ) != uid { return gotten watch Options := metav1 . List watch Options . Field Selector = name watch Options . Resource Version = gotten Obj List . Get Resource obj Watch , err := o . Dynamic Client . Resource ( info . Mapping . Resource ) . Namespace ( info . Namespace ) . Watch ( watch if err != nil { return gotten timeout := end err Wait Timeout With Name := extend Err Wait Timeout ( wait . Err Wait if timeout < 0 { // we're out of time return gotten Obj , false , err Wait Timeout With ctx , cancel := watchtools . Context With Optional watch Event , err := watchtools . Until Without Retry ( ctx , obj Watch , Wait { err Out : o . Err Out } . Is switch { case err == nil : return watch case err == watchtools . Err Watch case err == wait . Err Wait Timeout : if watch Event != nil { return watch Event . Object , false , err Wait Timeout With return gotten Obj , false , err Wait Timeout With default : return gotten } 
func ( w Wait ) Is Deleted ( event watch . Event ) ( bool , error ) { switch event . Type { case watch . Error : // keep waiting in the event we see an error - we expect the watch to be closed by // the server if the error is unrecoverable. err := apierrors . From fmt . Fprintf ( w . err } 
func ( w Conditional Wait ) Is Condition Met ( info * resource . Info , o * Wait Options ) ( runtime . Object , bool , error ) { end name Selector := fields . One Term Equal var gotten // List with a name field selector to get the current resource Version to watch from (not the object's resource Version) gotten Obj List , err := o . Dynamic Client . Resource ( info . Mapping . Resource ) . Namespace ( info . Namespace ) . List ( metav1 . List Options { Field Selector : name resource case len ( gotten Obj List . Items ) != 1 : resource Version = gotten Obj List . Get Resource default : gotten Obj = & gotten Obj condition Met , err := w . check Condition ( gotten if condition Met { return gotten if err != nil { return gotten resource Version = gotten Obj List . Get Resource watch Options := metav1 . List watch Options . Field Selector = name watch Options . Resource Version = resource obj Watch , err := o . Dynamic Client . Resource ( info . Mapping . Resource ) . Namespace ( info . Namespace ) . Watch ( watch if err != nil { return gotten timeout := end err Wait Timeout With Name := extend Err Wait Timeout ( wait . Err Wait if timeout < 0 { // we're out of time return gotten Obj , false , err Wait Timeout With ctx , cancel := watchtools . Context With Optional watch Event , err := watchtools . Until Without Retry ( ctx , obj Watch , w . is Condition switch { case err == nil : return watch case err == watchtools . Err Watch case err == wait . Err Wait Timeout : if watch Event != nil { return watch Event . Object , false , err Wait Timeout With return gotten Obj , false , err Wait Timeout With default : return gotten } 
func New Cron Job Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Cron Job Informer ( client , namespace , resync } 
func New Filtered Cron Job Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Batch V2alpha1 ( ) . Cron } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Batch V2alpha1 ( ) . Cron } , } , & batchv2alpha1 . Cron Job { } , resync } 
} 
func Storage With Cacher ( capacity int ) generic . Storage Decorator { return func ( storage Config * storagebackend . Config , resource Prefix string , key Func func ( obj runtime . Object ) ( string , error ) , new Func func ( ) runtime . Object , new List Func func ( ) runtime . Object , get Attrs Func storage . Attr Func , trigger Func storage . Trigger Publisher Func ) ( storage . Interface , factory . Destroy Func ) { s , d := generic . New Raw Storage ( storage if capacity <= 0 { klog . V ( 5 ) . Infof ( " " , new if klog . V ( 5 ) { klog . Infof ( " " , new // TODO: we would change this later to make storage always have cacher and hide low level KV layer inside. // Currently it has two layers of same storage interface -- cacher and low level kv. cacher Config := cacherstorage . Config { Cache Capacity : capacity , Storage : s , Versioner : etcdstorage . API Object Versioner { } , Resource Prefix : resource Prefix , Key Func : key Func , New Func : new Func , New List Func : new List Func , Get Attrs Func : get Attrs Func , Trigger Publisher Func : trigger Func , Codec : storage cacher := cacherstorage . New Cacher From Config ( cacher destroy // TODO : Remove Register Storage Cleanup below when PR // https://github.com/kubernetes/kubernetes/pull/50690 // merges as that shuts down storage properly Register Storage Cleanup ( destroy return cacher , destroy } 
func Register ( ) { register Metrics . Do ( func ( ) { prometheus . Must Register ( Docker Operations prometheus . Must Register ( Docker prometheus . Must Register ( Docker Operations prometheus . Must Register ( Docker Operations prometheus . Must Register ( Deprecated Docker Operations prometheus . Must Register ( Deprecated Docker prometheus . Must Register ( Deprecated Docker Operations prometheus . Must Register ( Deprecated Docker Operations } 
func ( m * Time ) Proto } 
func ( m * Time ) Size ( ) ( n int ) { if m == nil || m . Time . Is return m . Proto } 
} 
func ( m * Time ) Marshal ( ) ( data [ ] byte , err error ) { if m == nil || m . Time . Is return m . Proto } 
func ( m * Time ) Marshal To ( data [ ] byte ) ( int , error ) { if m == nil || m . Time . Is return m . Proto Time ( ) . Marshal } 
func New ( client clientset . Interface , node Informer coreinformers . Node Informer , pod Informer coreinformers . Pod Informer , pv Informer coreinformers . Persistent Volume Informer , pvc Informer coreinformers . Persistent Volume Claim Informer , replication Controller Informer coreinformers . Replication Controller Informer , replica Set Informer appsinformers . Replica Set Informer , stateful Set Informer appsinformers . Stateful Set Informer , service Informer coreinformers . Service Informer , pdb Informer policyinformers . Pod Disruption Budget Informer , storage Class Informer storageinformers . Storage Class Informer , recorder record . Event Recorder , scheduler Algorithm Source kubeschedulerconfig . Scheduler Algorithm Source , stop Ch <- chan struct { } , registry framework . Registry , opts ... func ( o * scheduler Options ) ) ( * Scheduler , error ) { options := default Scheduler // Set up the configurator which can create schedulers from configs. configurator := factory . New Config Factory ( & factory . Config Factory Args { Scheduler Name : options . scheduler Name , Client : client , Node Informer : node Informer , Pod Informer : pod Informer , Pv Informer : pv Informer , Pvc Informer : pvc Informer , Replication Controller Informer : replication Controller Informer , Replica Set Informer : replica Set Informer , Stateful Set Informer : stateful Set Informer , Service Informer : service Informer , Pdb Informer : pdb Informer , Storage Class Informer : storage Class Informer , Hard Pod Affinity Symmetric Weight : options . hard Pod Affinity Symmetric Weight , Disable Preemption : options . disable Preemption , Percentage Of Nodes To Score : options . percentage Of Nodes To Score , Bind Timeout Seconds : options . bind Timeout source := scheduler Algorithm switch { case source . Provider != nil : // Create the config from a named algorithm provider. sc , err := configurator . Create From switch { case source . Policy . File != nil : if err := init Policy From case source . Policy . Config Map != nil : if err := init Policy From Config Map ( client , source . Policy . Config sc , err := configurator . Create From config . Disable Preemption = options . disable config . Stop Everything = stop // Create the scheduler. sched := New From Add All Event Handlers ( sched , options . scheduler Name , node Informer , pod Informer , pv Informer , pvc Informer , service Informer , storage Class } 
func init Policy From File ( policy File string , policy * schedulerapi . Policy ) error { // Use a policy serialized in a file. _ , err := os . Stat ( policy if err != nil { return fmt . Errorf ( " " , policy data , err := ioutil . Read File ( policy err = runtime . Decode } 
func init Policy From Config Map ( client clientset . Interface , policy Ref * kubeschedulerconfig . Scheduler Policy Config Map Source , policy * schedulerapi . Policy ) error { // Use a policy serialized in a config map value. policy Config Map , err := client . Core V1 ( ) . Config Maps ( policy Ref . Namespace ) . Get ( policy Ref . Name , metav1 . Get if err != nil { return fmt . Errorf ( " " , policy Ref . Namespace , policy data , found := policy Config Map . Data [ kubeschedulerconfig . Scheduler Policy Config Map if ! found { return fmt . Errorf ( " " , kubeschedulerconfig . Scheduler Policy Config Map err = runtime . Decode } 
func New From } 
func ( sched * Scheduler ) Run ( ) { if ! sched . config . Wait For Cache go wait . Until ( sched . schedule One , 0 , sched . config . Stop } 
func ( sched * Scheduler ) record Scheduling sched . config . Recorder . Event ( pod , v1 . Event Type sched . config . Pod Condition Updater . Update ( pod , & v1 . Pod Condition { Type : v1 . Pod Scheduled , Status : v1 . Condition } 
func ( sched * Scheduler ) schedule ( pod * v1 . Pod ) ( core . Schedule Result , error ) { result , err := sched . config . Algorithm . Schedule ( pod , sched . config . Node if err != nil { pod = pod . Deep sched . record Scheduling Failure ( pod , err , v1 . Pod Reason return core . Schedule } 
func ( sched * Scheduler ) preempt ( preemptor * v1 . Pod , schedule Err error ) ( string , error ) { preemptor , err := sched . config . Pod Preemptor . Get Updated node , victims , nominated Pods To Clear , err := sched . config . Algorithm . Preempt ( preemptor , sched . config . Node Lister , schedule var node if node != nil { node // Update the scheduling queue with the nominated pod information. Without // this, there would be a race condition between the next scheduling cycle // and the time the scheduler receives a Pod Update for the nominated pod. sched . config . Scheduling Queue . Update Nominated Pod For Node ( preemptor , node // Make a call to update nominated node name of the pod on the API server. err = sched . config . Pod Preemptor . Set Nominated Node Name ( preemptor , node sched . config . Scheduling Queue . Delete Nominated Pod If for _ , victim := range victims { if err := sched . config . Pod Preemptor . Delete sched . config . Recorder . Eventf ( victim , v1 . Event Type Normal , " " , " " , preemptor . Namespace , preemptor . Name , node metrics . Preemption // Clearing nominated pods should happen outside of "if node != nil". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of // the 'Nominated Pod' field. for _ , p := range nominated Pods To Clear { r Err := sched . config . Pod Preemptor . Remove Nominated Node if r Err != nil { klog . Errorf ( " " , r return node } 
func ( sched * Scheduler ) assume Volumes ( assumed * v1 . Pod , host string ) ( all Bound bool , err error ) { all Bound , err = sched . config . Volume Binder . Binder . Assume Pod if err != nil { sched . record Scheduling Failure ( assumed , err , Scheduler } 
func ( sched * Scheduler ) bind err := sched . config . Volume Binder . Binder . Bind Pod // Unassume the Pod and retry scheduling if forget Err := sched . config . Scheduler Cache . Forget Pod ( assumed ) ; forget Err != nil { klog . Errorf ( " " , forget sched . record Scheduling } 
func ( sched * Scheduler ) assume ( assumed * v1 . Pod , host string ) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed . Spec . Node if err := sched . config . Scheduler Cache . Assume // This is most probably result of a BUG in retrying logic. // We report an error here so that pod scheduling can be retried. // This relies on the fact that Error will check if the pod has been bound // to a node and if so will not add it back to the unscheduled pods queue // (otherwise this would cause an infinite loop). sched . record Scheduling Failure ( assumed , err , Scheduler // if "assumed" is a nominated pod, we should remove it from internal cache if sched . config . Scheduling Queue != nil { sched . config . Scheduling Queue . Delete Nominated Pod If } 
func ( sched * Scheduler ) bind ( assumed * v1 . Pod , b * v1 . Binding ) error { binding // If binding succeeded then Pod Scheduled condition will be updated in apiserver so that // it's atomic with setting host. err := sched . config . Get if fin Err := sched . config . Scheduler Cache . Finish Binding ( assumed ) ; fin Err != nil { klog . Errorf ( " " , fin if err := sched . config . Scheduler Cache . Forget sched . record Scheduling Failure ( assumed , err , Scheduler metrics . Binding Latency . Observe ( metrics . Since In Seconds ( binding metrics . Deprecated Binding Latency . Observe ( metrics . Since In Microseconds ( binding metrics . Scheduling Latency . With Label Values ( metrics . Binding ) . Observe ( metrics . Since In Seconds ( binding metrics . Deprecated Scheduling Latency . With Label Values ( metrics . Binding ) . Observe ( metrics . Since In Seconds ( binding sched . config . Recorder . Eventf ( assumed , v1 . Event Type } 
func ( sched * Scheduler ) schedule pod := sched . config . Next // pod could be nil when scheduler if pod . Deletion Timestamp != nil { sched . config . Recorder . Eventf ( pod , v1 . Event Type plugin Context := framework . New Plugin schedule if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fit Error , ok := err . ( * core . Fit Error ) ; ok { if ! util . Pod Priority Enabled ( ) || sched . config . Disable } else { preemption Start sched . preempt ( pod , fit metrics . Preemption metrics . Scheduling Algorithm Premption Evaluation Duration . Observe ( metrics . Since In Seconds ( preemption Start metrics . Deprecated Scheduling Algorithm Premption Evaluation Duration . Observe ( metrics . Since In Microseconds ( preemption Start metrics . Scheduling Latency . With Label Values ( metrics . Preemption Evaluation ) . Observe ( metrics . Since In Seconds ( preemption Start metrics . Deprecated Scheduling Latency . With Label Values ( metrics . Preemption Evaluation ) . Observe ( metrics . Since In Seconds ( preemption Start // Pod did not fit anywhere, so it is counted as a failure. If preemption // succeeds, the pod should get counted as a success the next time we try to // schedule it. (hopefully) metrics . Pod Schedule metrics . Pod Schedule metrics . Scheduling Algorithm Latency . Observe ( metrics . Since In metrics . Deprecated Scheduling Algorithm Latency . Observe ( metrics . Since In // Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumed Pod := pod . Deep // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then all Bound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumed Pod' if volume binding is required. all Bound , err := sched . assume Volumes ( assumed Pod , schedule Result . Suggested metrics . Pod Schedule // Run "reserve" plugins. if sts := fwk . Run Reserve Plugins ( plugin Context , assumed Pod , schedule Result . Suggested Host ) ; ! sts . Is Success ( ) { sched . record Scheduling Failure ( assumed Pod , sts . As Error ( ) , Scheduler metrics . Pod Schedule // assume modifies `assumed Pod` by setting Node Name=schedule Result.Suggested Host err = sched . assume ( assumed Pod , schedule Result . Suggested metrics . Pod Schedule // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func ( ) { // Bind volumes first before Pod if ! all Bound { err := sched . bind Volumes ( assumed metrics . Pod Schedule // Run "prebind" plugins. prebind Status := fwk . Run Prebind Plugins ( plugin Context , assumed Pod , schedule Result . Suggested if ! prebind Status . Is if prebind Status . Code ( ) == framework . Unschedulable { reason = v1 . Pod Reason } else { metrics . Pod Schedule reason = Scheduler if forget Err := sched . Cache ( ) . Forget Pod ( assumed Pod ) ; forget Err != nil { klog . Errorf ( " " , forget sched . record Scheduling Failure ( assumed Pod , prebind Status . As Error ( ) , reason , prebind err := sched . bind ( assumed Pod , & v1 . Binding { Object Meta : metav1 . Object Meta { Namespace : assumed Pod . Namespace , Name : assumed Pod . Name , UID : assumed Pod . UID } , Target : v1 . Object Reference { Kind : " " , Name : schedule Result . Suggested metrics . E2e Scheduling Latency . Observe ( metrics . Since In metrics . Deprecated E2e Scheduling Latency . Observe ( metrics . Since In metrics . Pod Schedule } else { klog . V ( 2 ) . Infof ( " " , assumed Pod . Namespace , assumed Pod . Name , schedule Result . Suggested Host , schedule Result . Evaluated Nodes , schedule Result . Feasible metrics . Pod Schedule } 
func ( m * Map String } 
func ( m * Map String arr := strings . Split k := strings . Trim v := strings . Trim bool Value , err := strconv . Parse ( * m . Map ) [ k ] = bool } 
func ( r * runtime Cache ) Get if time . Since ( r . cache Time ) > default Cache Period { if err := r . update } 
func ( r * runtime Cache ) get Pods With pods , err := r . getter . Get } 
func New API Resource Options ( io Streams genericclioptions . IO Streams ) * API Resource Options { return & API Resource Options { IO Streams : io } 
func New Cmd API Resources ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New API Resource Options ( io cmd := & cobra . Command { Use : " " , Short : " " , Long : " " , Example : apiresources Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( o . Run API cmd . Flags ( ) . Bool Var ( & o . No Headers , " " , o . No cmd . Flags ( ) . String Var cmd . Flags ( ) . String Var ( & o . API Group , " " , o . API cmd . Flags ( ) . Bool cmd . Flags ( ) . String Slice cmd . Flags ( ) . Bool } 
func ( o * API Resource Options ) Validate ( ) error { supported Output Types := sets . New if ! supported Output } 
func ( o * API Resource Options ) Complete ( cmd * cobra . Command , args [ ] string ) error { if len ( args ) != 0 { return cmdutil . Usage } 
func ( o * API Resource Options ) Run API Resources ( cmd * cobra . Command , f cmdutil . Factory ) error { w := printers . Get New Tab discoveryclient , err := f . To Discovery lists , err := discoveryclient . Server Preferred resources := [ ] group group ns for _ , list := range lists { if len ( list . API gv , err := schema . Parse Group Version ( list . Group for _ , resource := range list . API // filter api Group if group Changed && o . API // filter namespaced if ns // filter to resources that support the specified verbs if len ( o . Verbs ) > 0 && ! sets . New String ( resource . Verbs ... ) . Has resources = append ( resources , group Resource { API Group : gv . Group , API if o . No Headers == false && o . Output != " " { if err = print Context sort . Stable ( sortable Group for _ , r := range resources { switch o . Output { case " " : name := r . API if len ( r . API Group ) > 0 { name += " " + r . API case " " : if _ , err := fmt . Fprintf ( w , " \t \t \t \t \t \n " , r . API Resource . Name , strings . Join ( r . API Resource . Short Names , " " ) , r . API Group , r . API Resource . Namespaced , r . API Resource . Kind , r . API case " " : if _ , err := fmt . Fprintf ( w , " \t \t \t \t \n " , r . API Resource . Name , strings . Join ( r . API Resource . Short Names , " " ) , r . API Group , r . API Resource . Namespaced , r . API if len ( errs ) > 0 { return errors . New } 
func ( pod Strategy ) Prepare For pod . Status = api . Pod Status { Phase : api . Pod Pending , QOS Class : qos . Get Pod podutil . Drop Disabled Pod } 
func ( pod Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new old new Pod . Status = old podutil . Drop Disabled Pod Fields ( new Pod , old } 
func ( pod Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error all Errs := validation . Validate all Errs = append ( all Errs , validation . Validate Conditional Pod ( pod , nil , field . New return all } 
func ( pod Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { error List := validation . Validate error List = append ( error List , validation . Validate Pod error List = append ( error List , validation . Validate Conditional Pod ( obj . ( * api . Pod ) , old . ( * api . Pod ) , field . New return error } 
func ( pod Strategy ) Check Graceful Delete ( ctx context . Context , obj runtime . Object , options * metav1 . Delete // user has specified a value if options . Grace Period Seconds != nil { period = * options . Grace Period } else { // use the default value if set, or deletes the pod immediately (0) if pod . Spec . Termination Grace Period Seconds != nil { period = * pod . Spec . Termination Grace Period // if the pod is not scheduled, delete immediately if len ( pod . Spec . Node // if the pod is already terminated, delete immediately if pod . Status . Phase == api . Pod Failed || pod . Status . Phase == api . Pod // ensure the options and the pod are in sync options . Grace Period } 
func ( pod Strategy Without Graceful ) Check Graceful Delete ( ctx context . Context , obj runtime . Object , options * metav1 . Delete } 
func Get return labels . Set ( pod . Object Meta . Labels ) , Pod To Selectable } 
func Match Pod ( label labels . Selector , field fields . Selector ) storage . Selection Predicate { return storage . Selection Predicate { Label : label , Field : field , Get Attrs : Get Attrs , Index } 
func Pod To Selectable Fields ( pod * api . Pod ) fields . Set { // The purpose of allocation with a given number of elements is to reduce // amount of allocations needed to create the fields.Set. If you add any // field here or the number of object-meta related fields changes, this should // be adjusted. pod Specific Fields pod Specific Fields Set [ " " ] = pod . Spec . Node pod Specific Fields Set [ " " ] = string ( pod . Spec . Restart pod Specific Fields Set [ " " ] = string ( pod . Spec . Scheduler pod Specific Fields Set [ " " ] = string ( pod . Spec . Service Account pod Specific Fields pod Specific Fields Set [ " " ] = string ( pod . Status . Pod pod Specific Fields Set [ " " ] = string ( pod . Status . Nominated Node return generic . Add Object Meta Fields Set ( pod Specific Fields Set , & pod . Object } 
func Resource Location ( getter Resource Getter , rt http . Round Tripper , ctx context . Context , id string ) ( * url . URL , http . Round Tripper , error ) { // Allow ID as "podname" or "podname:port" or "scheme:podname:port". // If port is not specified, try to use the first defined port on the pod. scheme , name , port , valid := utilnet . Split Scheme Name if ! valid { return nil , nil , errors . New Bad // TODO: if port is not a number but a "(container)/(portname)", do a name lookup. pod , err := get // Try to figure out a port. if port == " " { for i := range pod . Spec . Containers { if len ( pod . Spec . Containers [ i ] . Ports ) > 0 { port = fmt . Sprintf ( " " , pod . Spec . Containers [ i ] . Ports [ 0 ] . Container if err := proxyutil . Is Proxyable IP ( pod . Status . Pod IP ) ; err != nil { return nil , nil , errors . New Bad if port == " " { loc . Host = pod . Status . Pod } else { loc . Host = net . Join Host Port ( pod . Status . Pod } 
func get Container } 
func Log Location ( getter Resource Getter , conn Info client . Connection Info Getter , ctx context . Context , name string , opts * api . Pod Log Options , ) ( * url . URL , http . Round Tripper , error ) { pod , err := get case 0 : return nil , nil , errors . New Bad default : container Names := get Container init Container Names := get Container Names ( pod . Spec . Init err := fmt . Sprintf ( " " , name , container if len ( init Container Names ) > 0 { err += fmt . Sprintf ( " " , init Container return nil , nil , errors . New Bad } else { if ! pod Has Container With Name ( pod , container ) { return nil , nil , errors . New Bad node Name := types . Node Name ( pod . Spec . Node if len ( node node Info , err := conn Info . Get Connection Info ( ctx , node if opts . Since Seconds != nil { params . Add ( " " , strconv . Format Int ( * opts . Since if opts . Since Time != nil { params . Add ( " " , opts . Since if opts . Tail Lines != nil { params . Add ( " " , strconv . Format Int ( * opts . Tail if opts . Limit Bytes != nil { params . Add ( " " , strconv . Format Int ( * opts . Limit loc := & url . URL { Scheme : node Info . Scheme , Host : net . Join Host Port ( node Info . Hostname , node Info . Port ) , Path : fmt . Sprintf ( " " , pod . Namespace , pod . Name , container ) , Raw return loc , node } 
func Attach Location ( getter Resource Getter , conn Info client . Connection Info Getter , ctx context . Context , name string , opts * api . Pod Attach Options , ) ( * url . URL , http . Round Tripper , error ) { return stream Location ( getter , conn } 
func Port Forward Location ( getter Resource Getter , conn Info client . Connection Info Getter , ctx context . Context , name string , opts * api . Pod Port Forward Options , ) ( * url . URL , http . Round Tripper , error ) { pod , err := get node Name := types . Node Name ( pod . Spec . Node if len ( node Name ) == 0 { // If pod has not been assigned a host, return an empty location return nil , nil , errors . New Bad node Info , err := conn Info . Get Connection Info ( ctx , node if err := stream loc := & url . URL { Scheme : node Info . Scheme , Host : net . Join Host Port ( node Info . Hostname , node Info . Port ) , Path : fmt . Sprintf ( " " , pod . Namespace , pod . Name ) , Raw return loc , node } 
func ( hk * Hollow Kubelet ) Run ( ) { if err := kubeletapp . Run Kubelet ( & options . Kubelet Server { Kubelet Flags : * hk . Kubelet Flags , Kubelet Configuration : * hk . Kubelet Configuration , } , hk . Kubelet } 
func Get Hollow Kubelet Config ( node Name string , kubelet Port int , kubelet Read Only Port int , max Pods int , pods Per Core int ) ( * options . Kubelet Flags , * kubeletconfig . Kubelet Configuration ) { test Root Dir := utils . Make Temp Dir Or pod File Path := utils . Make Temp Dir Or Die ( " " , test Root klog . Infof ( " " , test Root // Flags struct f := options . New Kubelet f . Enable f . Root Directory = test Root f . Hostname Override = node f . Minimum GC f . Max Container f . Max Per Pod Container f . Register f . Register // Config struct c , err := options . New Kubelet c . Static Pod c . Port = int32 ( kubelet c . Read Only Port = int32 ( kubelet Read Only c . Static Pod Path = pod File c . File Check c . HTTP Check c . Node Status Update c . Sync c . Eviction Pressure Transition c . Max Pods = int32 ( max c . Pods Per Core = int32 ( pods Per c . Cluster c . Image GC High Threshold c . Image GC Low Threshold c . Volume Stats Agg c . Cgroup c . CPUCFS c . Enable Controller Attach c . Enable Debugging c . Cgroups Per // hairpin-veth is used to allow hairpin packets. Note that this deviates from // what the "real" kubelet currently does, because there's no way to // set promiscuous mode on docker0. c . Hairpin Mode = kubeletconfig . Hairpin c . Max Open c . Registry c . Registry Pull c . Resolver Config = kubetypes . Resolv Conf c . Kubelet c . Serialize Image c . System c . Protect Kernel } 
func Media Types For Serializer ( ns runtime . Negotiated Serializer ) ( media Types , stream Media Types [ ] string ) { for _ , info := range ns . Supported Media Types ( ) { media Types = append ( media Types , info . Media if info . Stream Serializer != nil { // stream=watch is the existing mime-type parameter for watch stream Media Types = append ( stream Media Types , info . Media return media Types , stream Media } 
func Negotiate Output Media Type ( req * http . Request , ns runtime . Negotiated Serializer , restrictions Endpoint Restrictions ) ( Media Type Options , runtime . Serializer Info , error ) { media Type , ok := Negotiate Media Type Options ( req . Header . Get ( " " ) , ns . Supported Media if ! ok { supported , _ := Media Types For return media Type , runtime . Serializer Info { } , New Not Acceptable // TODO: move into resthandler info := media if ( media Type . Pretty || is Pretty Print ( req ) ) && info . Pretty Serializer != nil { info . Serializer = info . Pretty return media } 
func Negotiate Output Media Type Stream ( req * http . Request , ns runtime . Negotiated Serializer , restrictions Endpoint Restrictions ) ( runtime . Serializer Info , error ) { media Type , ok := Negotiate Media Type Options ( req . Header . Get ( " " ) , ns . Supported Media if ! ok || media Type . Accepted . Stream Serializer == nil { _ , supported := Media Types For return runtime . Serializer Info { } , New Not Acceptable return media } 
func Negotiate Input Serializer ( req * http . Request , streaming bool , ns runtime . Negotiated Serializer ) ( runtime . Serializer Info , error ) { media return Negotiate Input Serializer For Media Type ( media } 
func Negotiate Input Serializer For Media Type ( media Type string , streaming bool , ns runtime . Negotiated Serializer ) ( runtime . Serializer Info , error ) { media Types := ns . Supported Media if len ( media Type ) == 0 { media Type = media Types [ 0 ] . Media if media Type , _ , err := mime . Parse Media Type ( media Type ) ; err == nil { if info , ok := runtime . Serializer Info For Media Type ( media Types , media supported , streaming Supported := Media Types For if streaming { return runtime . Serializer Info { } , New Unsupported Media Type Error ( streaming return runtime . Serializer Info { } , New Unsupported Media Type } 
func is Pretty Print ( req * http . Request ) bool { // DEPRECATED: should be part of the content type if req . URL != nil { // avoid an allocation caused by parsing the URL query if strings . Contains ( req . URL . Raw if len ( pp ) > 0 { pretty , _ := strconv . Parse user Agent := req . User // This covers basic all browsers and cli http tools if strings . Has Prefix ( user Agent , " " ) || strings . Has Prefix ( user Agent , " " ) || strings . Has Prefix ( user } 
func accept Media Type Options ( params map [ string ] string , accepts * runtime . Serializer Info , endpoint Endpoint Restrictions ) ( Media Type Options , bool ) { var options Media Type // extract all known parameters for k , v := range params { switch k { // controls transformation of the object when returned case " " : if options . Convert == nil { options . Convert = & schema . Group Version case " " : if options . Convert == nil { options . Convert = & schema . Group Version case " " : if options . Convert == nil { options . Convert = & schema . Group Version // controls the streaming schema case " " : if len ( v ) > 0 && ( accepts . Stream Serializer == nil || ! endpoint . Allows Stream Schema ( v ) ) { return Media Type // controls the version of the server API group used // for generic output case " " : if len ( v ) > 0 && ! endpoint . Allows Server Version ( v ) { return Media Type options . Use Server if options . Convert != nil && ! endpoint . Allows Conversion ( * options . Convert , accepts . Media Type Type , accepts . Media Type Sub Type ) { return Media Type } 
func Negotiate Media Type Options ( header string , accepted [ ] runtime . Serializer Info , endpoint Endpoint Restrictions ) ( Media Type Options , bool ) { if len ( header ) == 0 && len ( accepted ) > 0 { return Media Type var candidates candidate Media Type clauses := goautoneg . Parse switch { case clause . Type == accepts . Media Type Type && clause . Sub Type == accepts . Media Type Sub Type , clause . Type == accepts . Media Type Type && clause . Sub Type == " " , clause . Type == " " && clause . Sub Type == " " : candidates = append ( candidates , candidate Media for _ , v := range candidates { if ret Val , ret := accept Media Type Options ( v . clauses . Params , v . accepted , endpoint ) ; ret { return ret return Media Type } 
func New Preflight Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Short : " " , Long : " " , Example : preflight Example , Run : run Preflight , Inherit Flags : [ ] string { options . Cfg Path , options . Ignore Preflight Errors , options . TLS Bootstrap Token , options . Token Str , options . Control Plane , options . API Server Advertise Address , options . API Server Bind Port , options . Node CRI Socket , options . Node Name , options . File Discovery , options . Token Discovery , options . Token Discovery CA Hash , options . Token Discovery Skip CA Hash , options . Certificate } 
func run Preflight ( c workflow . Run Data ) error { j , ok := c . ( Join if err := preflight . Run Join Node Checks ( utilsexec . New ( ) , j . Cfg ( ) , j . Ignore Preflight init Cfg , err := j . Init if err := preflight . Run Optional Join Node Checks ( utilsexec . New ( ) , & init Cfg . Cluster Configuration , j . Ignore Preflight if j . Cfg ( ) . Control Plane != nil { // Checks if the cluster configuration supports // joining a new control plane instance and if all the necessary certificates are provided has Certificate Key := len ( j . Certificate if err := check If Ready For Additional Control Plane ( & init Cfg . Cluster Configuration , has Certificate not Ready To Join Control Plane if err := preflight . Run Init Node Checks ( utilsexec . New ( ) , init Cfg , j . Ignore Preflight Errors ( ) , true , has Certificate if err := preflight . Run Pull Images Check ( utilsexec . New ( ) , init Cfg , j . Ignore Preflight } 
func check If Ready For Additional Control Plane ( init Configuration * kubeadmapi . Cluster Configuration , has Certificate Key bool ) error { // blocks if the cluster was created without a stable control plane endpoint if init Configuration . Control Plane if ! has Certificate Key { // checks if the certificates that must be equal across controlplane instances are provided if ret , err := certs . Shared Certificate Exists ( init } 
func New ( c * Config ) Controller { ctlr := & controller { config : * c , clock : & clock . Real } 
func ( c * controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle go func ( ) { <- stop r := New Reflector ( c . config . Lister Watcher , c . config . Object Type , c . config . Queue , c . config . Full Resync r . Should Resync = c . config . Should c . reflector c . reflector wg . Start With Channel ( stop wait . Until ( c . process Loop , time . Second , stop } 
func ( c * controller ) process Loop ( ) { for { obj , err := c . config . Queue . Pop ( Pop Process if err != nil { if err == FIFO Closed if c . config . Retry On Error { // This is the safe way to re-enqueue. c . config . Queue . Add If Not } 
func ( r Resource Event Handler Funcs ) On Add ( obj interface { } ) { if r . Add Func != nil { r . Add } 
func ( r Resource Event Handler Funcs ) On Update ( old Obj , new Obj interface { } ) { if r . Update Func != nil { r . Update Func ( old Obj , new } 
func ( r Resource Event Handler Funcs ) On Delete ( obj interface { } ) { if r . Delete Func != nil { r . Delete } 
func ( r Filtering Resource Event Handler ) On Add ( obj interface { } ) { if ! r . Filter r . Handler . On } 
func ( r Filtering Resource Event Handler ) On Update ( old Obj , new Obj interface { } ) { newer := r . Filter Func ( new older := r . Filter Func ( old switch { case newer && older : r . Handler . On Update ( old Obj , new case newer && ! older : r . Handler . On Add ( new case ! newer && older : r . Handler . On Delete ( old } 
func ( r Filtering Resource Event Handler ) On Delete ( obj interface { } ) { if ! r . Filter r . Handler . On } 
func Deletion Handling Meta Namespace Key Func ( obj interface { } ) ( string , error ) { if d , ok := obj . ( Deleted Final State return Meta Namespace Key } 
func New Informer ( lw Lister Watcher , obj Type runtime . Object , resync Period time . Duration , h Resource Event Handler , ) ( Store , Controller ) { // This will hold the client state, as we know it. client State := New Store ( Deletion Handling Meta Namespace Key return client State , new Informer ( lw , obj Type , resync Period , h , client } 
func New Indexer Informer ( lw Lister Watcher , obj Type runtime . Object , resync Period time . Duration , h Resource Event Handler , indexers Indexers , ) ( Indexer , Controller ) { // This will hold the client state, as we know it. client State := New Indexer ( Deletion Handling Meta Namespace Key return client State , new Informer ( lw , obj Type , resync Period , h , client } 
func new Informer ( lw Lister Watcher , obj Type runtime . Object , resync Period time . Duration , h Resource Event Handler , client State Store , ) Controller { // This will hold incoming changes. Note how we pass client State in as a // Key Lister, that way resync operations will result in the correct set // of update/delete deltas. fifo := New Delta FIFO ( Meta Namespace Key Func , client cfg := & Config { Queue : fifo , Lister Watcher : lw , Object Type : obj Type , Full Resync Period : resync Period , Retry On Error : false , Process : func ( obj interface { } ) error { // from oldest to newest for _ , d := range obj . ( Deltas ) { switch d . Type { case Sync , Added , Updated : if old , exists , err := client State . Get ( d . Object ) ; err == nil && exists { if err := client h . On } else { if err := client h . On case Deleted : if err := client h . On } 
func Can Read Cert And Key ( cert Path , key Path string ) ( bool , error ) { cert Readable := can Read File ( cert key Readable := can Read File ( key if cert Readable == false && key if cert Readable == false { return false , fmt . Errorf ( " " , cert if key Readable == false { return false , fmt . Errorf ( " " , key } 
func Write Cert ( cert Path string , data [ ] byte ) error { if err := os . Mkdir All ( filepath . Dir ( cert Path ) , os . File return ioutil . Write File ( cert Path , data , os . File } 
func New Pool ( filename string ) ( * x509 . Cert Pool , error ) { certs , err := Certs From pool := x509 . New Cert for _ , cert := range certs { pool . Add } 
func Certs From File ( file string ) ( [ ] * x509 . Certificate , error ) { pem Block , err := ioutil . Read certs , err := Parse Certs PEM ( pem } 
func ( c * mutating Webhook Configurations ) Create ( mutating Webhook Configuration * v1beta1 . Mutating Webhook Configuration ) ( result * v1beta1 . Mutating Webhook Configuration , err error ) { result = & v1beta1 . Mutating Webhook err = c . client . Post ( ) . Resource ( " " ) . Body ( mutating Webhook } 
func ( c * mutating Webhook Configurations ) Update ( mutating Webhook Configuration * v1beta1 . Mutating Webhook Configuration ) ( result * v1beta1 . Mutating Webhook Configuration , err error ) { result = & v1beta1 . Mutating Webhook err = c . client . Put ( ) . Resource ( " " ) . Name ( mutating Webhook Configuration . Name ) . Body ( mutating Webhook } 
func ( default Node Identifier ) Node user Name := u . Get if ! strings . Has Prefix ( user Name , node User Name is for _ , g := range u . Get Groups ( ) { if g == user . Nodes Group { is if ! is node Name := strings . Trim Prefix ( user Name , node User Name return node } 
func ( s * Secure Serving Info ) Serve ( handler http . Handler , shutdown Timeout time . Duration , stop secure Server := & http . Server { Addr : s . Listener . Addr ( ) . String ( ) , Handler : handler , Max Header Bytes : 1 << 20 , TLS Config : & tls . Config { Name To Certificate : s . SNI Certs , // Can't use SS Lv3 because of POODLE and BEAST // Can't use TL Sv1.0 because of POODLE and BEAST using CBC cipher // Can't use TL Sv1.1 because of RC4 cipher usage Min Version : tls . Version TLS12 , // enable HTTP2 for go's 1.7 HTTP Server Next if s . Min TLS Version > 0 { secure Server . TLS Config . Min Version = s . Min TLS if len ( s . Cipher Suites ) > 0 { secure Server . TLS Config . Cipher Suites = s . Cipher if s . Cert != nil { secure Server . TLS // append all named certs. Otherwise, the go tls stack will think no SNI processing // is necessary because there is only one cert anyway. // Moreover, if Server Cert.Cert File/Server Cert.Key File are not set, the first SNI // cert will become the default cert. That's what we expect anyway. for _ , c := range s . SNI Certs { secure Server . TLS Config . Certificates = append ( secure Server . TLS if s . Client CA != nil { // Populate Peer Certificates in requests, but don't reject connections without certificates // This allows certificates to be validated by authenticators, while still allowing other auth types secure Server . TLS Config . Client Auth = tls . Request Client // Specify allowed C As for client certificates secure Server . TLS Config . Client C As = s . Client // At least 99% of serialized resources in surveyed clusters were smaller than 256kb. // This should be big enough to accommodate most API POST requests in a single frame, // and small enough to allow a per connection buffer of this size multiplied by `Max Concurrent Streams`. const resource // shrink the per-stream buffer and max framesize from the 1MB default while still accommodating most API POST requests in a single frame http2Options . Max Upload Buffer Per Stream = resource http2Options . Max Read Frame Size = resource // use the overridden concurrent streams setting or make the default of 250 explicit so we can size Max Upload Buffer Per Connection appropriately if s . HTTP2Max Streams Per Connection > 0 { http2Options . Max Concurrent Streams = uint32 ( s . HTTP2Max Streams Per } else { http2Options . Max Concurrent // increase the connection buffer size from the 1MB default to handle the specified number of concurrent streams http2Options . Max Upload Buffer Per Connection = http2Options . Max Upload Buffer Per Stream * int32 ( http2Options . Max Concurrent // apply settings to the server if err := http2 . Configure Server ( secure klog . Infof ( " " , secure return Run Server ( secure Server , s . Listener , shutdown Timeout , stop } 
func Run Server ( server * http . Server , ln net . Listener , shut Down Timeout time . Duration , stop // Shutdown server gracefully. stopped go func ( ) { defer close ( stopped <- stop ctx , cancel := context . With Timeout ( context . Background ( ) , shut Down go func ( ) { defer utilruntime . Handle listener = tcp Keep Alive Listener { ln . ( * net . TCP if server . TLS Config != nil { listener = tls . New Listener ( listener , server . TLS select { case <- stop return stopped } 
func Get Named Certificate Map ( certs [ ] Named TLS Cert ) ( map [ string ] * tls . Certificate , error ) { // register certs with implicit names first, reverse order such that earlier trump over the later by cert := & certs [ i ] . TLS x509Cert , err := x509 . Parse cn := x509Cert . Subject . Common if cn == " " || len ( validation . Is DNS1123Subdomain ( strings . Trim Prefix ( cn , " " ) ) ) == 0 { by for _ , san := range x509Cert . DNS Names { by // intentionally all I Ps in the cert are ignored as SNI forbids passing I Ps // to select a cert. Before go 1.6 the tls happily passed I // register certs with explicit names last, overwriting every of the implicit ones, // again in reverse order. for i := len ( certs ) - 1 ; i >= 0 ; i -- { named for _ , name := range named Cert . Names { by Name [ name ] = & certs [ i ] . TLS return by } 
} 
func ( c * Fake CSI Drivers ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . CSI Driver , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( csidrivers Resource , name ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func ( c * Fake CSI Drivers ) List ( opts v1 . List Options ) ( result * v1beta1 . CSI Driver List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( csidrivers Resource , csidrivers Kind , opts ) , & v1beta1 . CSI Driver label , _ , _ := testing . Extract From List list := & v1beta1 . CSI Driver List { List Meta : obj . ( * v1beta1 . CSI Driver List ) . List for _ , item := range obj . ( * v1beta1 . CSI Driver } 
func ( c * Fake CSI Drivers ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( csidrivers } 
func ( c * Fake CSI Drivers ) Create ( c SI Driver * v1beta1 . CSI Driver ) ( result * v1beta1 . CSI Driver , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( csidrivers Resource , c SI Driver ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func ( c * Fake CSI Drivers ) Update ( c SI Driver * v1beta1 . CSI Driver ) ( result * v1beta1 . CSI Driver , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( csidrivers Resource , c SI Driver ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func ( c * Fake CSI Drivers ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( csidrivers Resource , name ) , & v1beta1 . CSI } 
func ( c * Fake CSI Drivers ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( csidrivers Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . CSI Driver } 
func ( c * Fake CSI Drivers ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . CSI Driver , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( csidrivers Resource , name , pt , data , subresources ... ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func With Content Type ( handler http . Handler , content Type string ) http . Handler { return http . Handler Func ( func ( w http . Response Writer , r * http . Request ) { w . Header ( ) . Set ( " " , content handler . Serve } 
func ( ds * Datastore ) Create Directory ( ctx context . Context , directory Path string , create Parents bool ) error { file Manager := object . New File err := file Manager . Make Directory ( ctx , directory Path , ds . Datacenter . Datacenter , create if err != nil { if soap . Is Soap Fault ( err ) { soap Fault := soap . To Soap if _ , ok := soap Fault . Vim Fault ( ) . ( types . File Already Exists ) ; ok { return Err File Already klog . V ( Log Level ) . Infof ( " " , directory } 
func ( ds * Datastore ) Get Type ( ctx context . Context ) ( string , error ) { var ds pc := property . Default err := pc . Retrieve One ( ctx , ds . Datastore . Reference ( ) , [ ] string { " " } , & ds return ds } 
func ( ds * Datastore ) Is Compatible With Storage Policy ( ctx context . Context , storage Policy ID string ) ( bool , string , error ) { pbm Client , err := New Pbm return pbm Client . Is Datastore Compatible ( ctx , storage Policy } 
func ( ds * Datastore ) Get Datastore Host Mounts ( ctx context . Context ) ( [ ] types . Managed Object Reference , error ) { var ds pc := property . Default err := pc . Retrieve One ( ctx , ds . Datastore . Reference ( ) , [ ] string { " " } , & ds hosts := make ( [ ] types . Managed Object Reference , len ( ds for _ , ds Host Mount := range ds Mo . Host { hosts = append ( hosts , ds Host } 
func New Cmd Join ( out io . Writer , join Options * join Options ) * cobra . Command { if join Options == nil { join Options = new Join join Runner := workflow . New cmd := & cobra . Command { Use : " " , Short : " " , Long : join Long Description , Run : func ( cmd * cobra . Command , args [ ] string ) { c , err := join Runner . Init kubeadmutil . Check data := c . ( * join err = join kubeadmutil . Check // if the node is hosting a new control plane instance if data . cfg . Control Plane != nil { // outputs the join control plane done message and exit etcd if data . init Cfg . Etcd . External == nil { etcd ctx := map [ string ] string { " " : kubeadmconstants . Get Admin Kube Config Path ( ) , " " : etcd join Contro Plane Done Temp . Execute ( data . output } else { // otherwise, if the node joined as a worker node; // outputs the join done message and exit fmt . Fprint ( data . output Writer , join Worker Node Done } , // We accept the control-plane location as an optional positional argument Args : cobra . Maximum N add Join Config Flags ( cmd . Flags ( ) , join add Join Other Flags ( cmd . Flags ( ) , join join Runner . Append Phase ( phases . New Preflight join Runner . Append Phase ( phases . New Control Plane Prepare join Runner . Append Phase ( phases . New Check Etcd join Runner . Append Phase ( phases . New Kubelet Start join Runner . Append Phase ( phases . New Control Plane Join // sets the data builder function, that will be used by the runner // both when running the entire workflow or single phases join Runner . Set Data Initializer ( func ( cmd * cobra . Command , args [ ] string ) ( workflow . Run Data , error ) { return new Join Data ( cmd , args , join // binds the Runner to kubeadm join command by altering // command help, adding --skip-phases flag and by adding phases subcommands join Runner . Bind To } 
func add Join Config Flags ( flag Set * flag . Flag Set , cfg * kubeadmapiv1beta2 . Join Configuration ) { flag Set . String Var ( & cfg . Node Registration . Name , options . Node Name , cfg . Node // add control plane endpoint flags to the specified flagset flag Set . String Var ( & cfg . Control Plane . Local API Endpoint . Advertise Address , options . API Server Advertise Address , cfg . Control Plane . Local API Endpoint . Advertise flag Set . Int32Var ( & cfg . Control Plane . Local API Endpoint . Bind Port , options . API Server Bind Port , cfg . Control Plane . Local API Endpoint . Bind // adds bootstrap token specific discovery flags to the specified flagset flag Set . String Var ( & cfg . Discovery . Bootstrap Token . Token , options . Token flag Set . String Slice Var ( & cfg . Discovery . Bootstrap Token . CA Cert Hashes , options . Token Discovery CA flag Set . Bool Var ( & cfg . Discovery . Bootstrap Token . Unsafe Skip CA Verification , options . Token Discovery Skip CA // discovery via kube config file flag flag Set . String Var ( & cfg . Discovery . File . Kube Config Path , options . File flag Set . String Var ( & cfg . Discovery . TLS Bootstrap Token , options . TLS Bootstrap Token , cfg . Discovery . TLS Bootstrap cmdutil . Add CRI Socket Flag ( flag Set , & cfg . Node Registration . CRI } 
func add Join Other Flags ( flag Set * flag . Flag Set , join Options * join Options ) { flag Set . String Var ( & join Options . cfg Path , options . Cfg Path , join Options . cfg flag Set . String Slice Var ( & join Options . ignore Preflight Errors , options . Ignore Preflight Errors , join Options . ignore Preflight flag Set . String Var ( & join Options . token , options . Token flag Set . Bool Var ( & join Options . control Plane , options . Control Plane , join Options . control flag Set . String Var ( & join Options . certificate Key , options . Certificate } 
func new Join Options ( ) * join Options { // initialize the public kubeadm config API by applying defaults externalcfg := & kubeadmapiv1beta2 . Join // Add optional config objects to host flags. // un-set objects will be cleaned up afterwards (into new Join Data func) externalcfg . Discovery . File = & kubeadmapiv1beta2 . File externalcfg . Discovery . Bootstrap Token = & kubeadmapiv1beta2 . Bootstrap Token externalcfg . Control Plane = & kubeadmapiv1beta2 . Join Control return & join } 
func new Join Data ( cmd * cobra . Command , args [ ] string , opt * join Options , out io . Writer ) ( * join // Validate standalone flags values and/or combination of flags and then assigns // validated values to the public kubeadm config API when applicable // if a token is provided, use this value for both discovery-token and tls-bootstrap-token when those values are not provided if len ( opt . token ) > 0 { if len ( opt . externalcfg . Discovery . TLS Bootstrap Token ) == 0 { opt . externalcfg . Discovery . TLS Bootstrap if len ( opt . externalcfg . Discovery . Bootstrap Token . Token ) == 0 { opt . externalcfg . Discovery . Bootstrap // if a file or URL from which to load cluster information was not provided, unset the Discovery.File object if len ( opt . externalcfg . Discovery . File . Kube Config // if an API Server Endpoint from which to retrieve cluster information was not provided, unset the Discovery.Bootstrap Token object if len ( args ) == 0 { opt . externalcfg . Discovery . Bootstrap } else { if len ( opt . cfg opt . externalcfg . Discovery . Bootstrap Token . API Server // if not joining a control plane, unset the Control Plane object if ! opt . control Plane { opt . externalcfg . Control // if the admin.conf file already exists, use it for skipping the discovery process. // NB. this case can happen when we are joining a control-plane node only (and phases are invoked atomically) var admin Kube Config Path = kubeadmconstants . Get Admin Kube Config var tls Bootstrap if _ , err := os . Stat ( admin Kube Config Path ) ; err == nil && opt . control Plane { // use the admin.conf as tls Bootstrap Cfg, that is the kubeconfig file used for reading the kubeadm-config during discovery klog . V ( 1 ) . Infof ( " " , admin Kube Config tls Bootstrap Cfg , err = clientcmd . Load From File ( admin Kube Config if err != nil { return nil , errors . Wrapf ( err , " " , admin Kube Config ignore Preflight Errors Set , err := validation . Validate Ignore Preflight Errors ( opt . ignore Preflight if err = validation . Validate Mixed // Either use the config file if specified, or convert public kubeadm API to the internal Join Configuration // and validates Join Configuration if opt . externalcfg . Node if opt . externalcfg . Control Plane != nil && opt . externalcfg . Control Plane . Local API Endpoint . Advertise // in case the command doesn't have flags for discovery, makes the join cfg validation pass checks on discovery if cmd . Flags ( ) . Lookup ( options . File Discovery ) == nil { if _ , err := os . Stat ( admin Kube Config Path ) ; os . Is Not Exist ( err ) { return nil , errors . Errorf ( " " , admin Kube Config klog . V ( 1 ) . Infof ( " " , admin Kube Config opt . externalcfg . Discovery . File = & kubeadmapiv1beta2 . File Discovery { Kube Config Path : admin Kube Config opt . externalcfg . Discovery . Bootstrap Token = nil //NB. this could be removed when we get better control on args (e.g. phases without discovery should have No cfg , err := configutil . Load Or Default Join Configuration ( opt . cfg // override node name and CRI socket from the command line opt if opt . externalcfg . Node Registration . Name != " " { cfg . Node Registration . Name = opt . externalcfg . Node if opt . externalcfg . Node Registration . CRI Socket != " " { cfg . Node Registration . CRI Socket = opt . externalcfg . Node Registration . CRI if cfg . Control Plane != nil { if err := configutil . Verify API Server Bind Address ( cfg . Control Plane . Local API Endpoint . Advertise return & join Data { cfg : cfg , tls Bootstrap Cfg : tls Bootstrap Cfg , ignore Preflight Errors : ignore Preflight Errors Set , output Writer : out , certificate Key : opt . certificate } 
func ( j * join Data ) TLS Bootstrap Cfg ( ) ( * clientcmdapi . Config , error ) { if j . tls Bootstrap Cfg != nil { return j . tls Bootstrap tls Bootstrap j . tls Bootstrap Cfg = tls Bootstrap return tls Bootstrap } 
func ( j * join Data ) Init Cfg ( ) ( * kubeadmapi . Init Configuration , error ) { if j . init Cfg != nil { return j . init if _ , err := j . TLS Bootstrap init Cfg , err := fetch Init Configuration From Join Configuration ( j . cfg , j . tls Bootstrap j . init Cfg = init return init } 
func ( j * join Data ) Client Set ( ) ( * clientset . Clientset , error ) { if j . client Set != nil { return j . client path := kubeadmconstants . Get Admin Kube Config client , err := kubeconfigutil . Client Set From j . client } 
func fetch Init Configuration From Join Configuration ( cfg * kubeadmapi . Join Configuration , tls Bootstrap Cfg * clientcmdapi . Config ) ( * kubeadmapi . Init init Configuration , err := fetch Init Configuration ( tls Bootstrap // Create the final Kube Config file with the cluster name discovered after fetching the cluster configuration clusterinfo := kubeconfigutil . Get Cluster From Kube Config ( tls Bootstrap tls Bootstrap Cfg . Clusters = map [ string ] * clientcmdapi . Cluster { init Configuration . Cluster tls Bootstrap Cfg . Contexts [ tls Bootstrap Cfg . Current Context ] . Cluster = init Configuration . Cluster // injects into the kubeadm configuration the information about the joining node init Configuration . Node Registration = cfg . Node if cfg . Control Plane != nil { init Configuration . Local API Endpoint = cfg . Control Plane . Local API return init } 
func fetch Init Configuration ( tls Bootstrap Cfg * clientcmdapi . Config ) ( * kubeadmapi . Init Configuration , error ) { // creates a client to access the cluster using the bootstrap token identity tls Client , err := kubeconfigutil . To Client Set ( tls Bootstrap // Fetches the init configuration init Configuration , err := configutil . Fetch Init Configuration From Cluster ( tls return init } 
func Validate Policy ( policy * audit . Policy ) field . Error List { var all Errs field . Error all Errs = append ( all Errs , validate Omit Stages ( policy . Omit Stages , field . New rule Path := field . New for i , rule := range policy . Rules { all Errs = append ( all Errs , validate Policy Rule ( rule , rule return all } 
func ( cfg * Config ) Complete ( ) Completed Config { c := completed Config { cfg . Generic Config . Complete ( ) , & cfg . Extra c . Generic return Completed } 
func ( c completed Config ) New ( ) ( * Wardle Server , error ) { generic Server , err := c . Generic Config . New ( " " , genericapiserver . New Empty s := & Wardle Server { Generic API Server : generic api Group Info := genericapiserver . New Default API Group Info ( wardle . Group Name , Scheme , metav1 . Parameter v1alpha1storage [ " " ] = wardleregistry . REST In Peace ( flunderstorage . New REST ( Scheme , c . Generic Config . REST Options v1alpha1storage [ " " ] = wardleregistry . REST In Peace ( fischerstorage . New REST ( Scheme , c . Generic Config . REST Options api Group Info . Versioned Resources Storage v1beta1storage [ " " ] = wardleregistry . REST In Peace ( flunderstorage . New REST ( Scheme , c . Generic Config . REST Options api Group Info . Versioned Resources Storage if err := s . Generic API Server . Install API Group ( & api Group } 
func ( ctrl * Persistent Volume Controller ) sync Claim ( claim * v1 . Persistent Volume Claim ) error { klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , get Claim Status For if ! metav1 . Has Annotation ( claim . Object Meta , ann Bind Completed ) { return ctrl . sync Unbound } else { return ctrl . sync Bound } 
func check Volume Satisfy Claim ( volume * v1 . Persistent Volume , claim * v1 . Persistent Volume Claim ) error { requested Qty := claim . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource requested Size := requested // check if PV's Deletion Time Stamp is set, if so, return error. if utilfeature . Default Feature Gate . Enabled ( features . Storage Object In Use Protection ) { if volume . Object Meta . Deletion volume Qty := volume . Spec . Capacity [ v1 . Resource volume Size := volume if volume Size < requested requested Class := v1helper . Get Persistent Volume Claim if v1helper . Get Persistent Volume Class ( volume ) != requested is Mismatch , err := check Volume Mode if is if ! check Access } 
func ( ctrl * Persistent Volume Controller ) should Delay Binding ( claim * v1 . Persistent Volume Claim ) ( bool , error ) { // If claim has already been assigned a node by scheduler for dynamic provisioning. if ctrl . is Delay Binding // If claim is in delay binding mode. return Is Delay Binding Mode ( claim , ctrl . class } 
func ( ctrl * Persistent Volume Controller ) sync Unbound Claim ( claim * v1 . Persistent Volume Claim ) error { // This is a new PVC that has not completed binding // OBSERVATION: pvc is "Pending" if claim . Spec . Volume Name == " " { // User did not care which PV they get. delay Binding , err := ctrl . should Delay // [Unit test set 1] volume , err := ctrl . volumes . find Best Match For Claim ( claim , delay if err != nil { klog . V ( 2 ) . Infof ( " " , claim To Claim return fmt . Errorf ( " " , claim To Claim if volume == nil { klog . V ( 4 ) . Infof ( " " , claim To Claim // No PV could be found // OBSERVATION: pvc is "Pending", will retry switch { case delay Binding : ctrl . event Recorder . Event ( claim , v1 . Event Type Normal , events . Wait For First case v1helper . Get Persistent Volume Claim Class ( claim ) != " " : if err = ctrl . provision default : ctrl . event Recorder . Event ( claim , v1 . Event Type Normal , events . Failed // Mark the claim as Pending and try to find a match in the next // periodic sync Claim if _ , err = ctrl . update Claim Status ( claim , v1 . Claim } else /* pv != nil */ { // Found a PV for this claim // OBSERVATION: pvc is "Pending", pv is "Available" klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , volume . Name , get Volume Status For if err = ctrl . bind ( volume , claim ) ; err != nil { // On any error saving the volume or the claim, subsequent // sync } else /* pvc.Spec.Volume Name != nil */ { // [Unit test set 2] // User asked for a specific PV. klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , claim . Spec . Volume obj , found , err := ctrl . volumes . store . Get By Key ( claim . Spec . Volume if ! found { // User asked for a PV that does not exist. // OBSERVATION: pvc is "Pending" // Retry later. klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , claim . Spec . Volume if _ , err = ctrl . update Claim Status ( claim , v1 . Claim } else { volume , ok := obj . ( * v1 . Persistent if ! ok { return fmt . Errorf ( " " , claim . Spec . Volume klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , claim . Spec . Volume Name , get Volume Status For if volume . Spec . Claim Ref == nil { // User asked for a PV that is not claimed // OBSERVATION: pvc is "Pending", pv is "Available" klog . V ( 4 ) . Infof ( " " , claim To Claim if err = check Volume Satisfy ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Volume // volume does not satisfy the requirements of the claim if _ , err = ctrl . update Claim Status ( claim , v1 . Claim } else if err = ctrl . bind ( volume , claim ) ; err != nil { // On any error saving the volume or the claim, subsequent // sync } else if Is Volume Bound To Claim ( volume , claim ) { // User asked for a PV that is claimed by this PVC // OBSERVATION: pvc is "Pending", pv is "Bound" klog . V ( 4 ) . Infof ( " " , claim To Claim } else { // User asked for a PV that is claimed by someone else // OBSERVATION: pvc is "Pending", pv is "Bound" if ! metav1 . Has Annotation ( claim . Object Meta , ann Bound By Controller ) { klog . V ( 4 ) . Infof ( " " , claim To Claim // User asked for a specific PV, retry later if _ , err = ctrl . update Claim Status ( claim , v1 . Claim } else { // This should never happen because someone had to remove // ann Bind Completed annotation on the claim. klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , claimref To Claim Key ( volume . Spec . Claim return fmt . Errorf ( " " , claim To Claim Key ( claim ) , claim . Spec . Volume Name , claimref To Claim Key ( volume . Spec . Claim } 
func ( ctrl * Persistent Volume Controller ) sync Bound Claim ( claim * v1 . Persistent Volume Claim ) error { // Has Annotation(pvc, ann Bind Completed) // This PVC has previously been bound // OBSERVATION: pvc is not "Pending" // [Unit test set 3] if claim . Spec . Volume Name == " " { // Claim was bound before but not any more. if _ , err := ctrl . update Claim Status With Event ( claim , v1 . Claim Lost , nil , v1 . Event Type obj , found , err := ctrl . volumes . store . Get By Key ( claim . Spec . Volume if ! found { // Claim is bound to a non-existing volume. if _ , err = ctrl . update Claim Status With Event ( claim , v1 . Claim Lost , nil , v1 . Event Type } else { volume , ok := obj . ( * v1 . Persistent if ! ok { return fmt . Errorf ( " " , claim . Spec . Volume klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , claim . Spec . Volume Name , get Volume Status For if volume . Spec . Claim Ref == nil { // Claim is bound but volume has come unbound. // Or, a claim was bound and the controller has not received updated // volume yet. We can't distinguish these cases. // Bind the volume again and set all states to Bound. klog . V ( 4 ) . Infof ( " " , claim To Claim if err = ctrl . bind ( volume , claim ) ; err != nil { // Objects not saved, next sync PV or sync } else if volume . Spec . Claim Ref . UID == claim . UID { // All is well // NOTE: sync PV can handle this so it can be left out. // NOTE: bind() call here will do nothing in most cases as // everything should be already set. klog . V ( 4 ) . Infof ( " " , claim To Claim if err = ctrl . bind ( volume , claim ) ; err != nil { // Objects not saved, next sync PV or sync } else { // Claim is bound but volume has a different claimant. // Set the claim phase to 'Lost', which is a terminal // phase. if _ , err = ctrl . update Claim Status With Event ( claim , v1 . Claim Lost , nil , v1 . Event Type } 
func ( ctrl * Persistent Volume Controller ) sync Volume ( volume * v1 . Persistent Volume ) error { klog . V ( 4 ) . Infof ( " " , volume . Name , get Volume Status For // [Unit test set 4] if volume . Spec . Claim if _ , err := ctrl . update Volume Phase ( volume , v1 . Volume } else /* pv.Spec.Claim Ref != nil */ { // Volume is bound to a claim. if volume . Spec . Claim Ref . UID == " " { // The PV is reserved for a PVC; that PVC has not yet been // bound to this PV; the PVC sync will handle it. klog . V ( 4 ) . Infof ( " " , volume . Name , claimref To Claim Key ( volume . Spec . Claim if _ , err := ctrl . update Volume Phase ( volume , v1 . Volume klog . V ( 4 ) . Infof ( " " , volume . Name , claimref To Claim Key ( volume . Spec . Claim // Get the PVC by _name_ var claim * v1 . Persistent Volume claim Name := claimref To Claim Key ( volume . Spec . Claim obj , found , err := ctrl . claims . Get By Key ( claim if ! found && metav1 . Has Annotation ( volume . Object Meta , ann Bound By Controller ) { // If PV is bound by external PV binder (e.g. kube-scheduler), it's // possible on heavy load that corresponding PVC is not synced to // controller local cache yet. So we need to double-check PVC in // 1) informer cache // 2) apiserver if not found in informer cache // to make sure we will not reclaim a PV wrongly. // Note that only non-released and non-failed volumes will be // updated to Released state when PVC does not exist. if volume . Status . Phase != v1 . Volume Released && volume . Status . Phase != v1 . Volume Failed { obj , err = ctrl . claim Lister . Persistent Volume Claims ( volume . Spec . Claim Ref . Namespace ) . Get ( volume . Spec . Claim if err != nil && ! apierrs . Is Not found = ! apierrs . Is Not if ! found { obj , err = ctrl . kube Client . Core V1 ( ) . Persistent Volume Claims ( volume . Spec . Claim Ref . Namespace ) . Get ( volume . Spec . Claim Ref . Name , metav1 . Get if err != nil && ! apierrs . Is Not found = ! apierrs . Is Not if ! found { klog . V ( 4 ) . Infof ( " " , volume . Name , claimref To Claim Key ( volume . Spec . Claim claim , ok = obj . ( * v1 . Persistent Volume if ! ok { return fmt . Errorf ( " " , claim . Spec . Volume klog . V ( 4 ) . Infof ( " " , volume . Name , claimref To Claim Key ( volume . Spec . Claim Ref ) , get Claim Status For if claim != nil && claim . UID != volume . Spec . Claim Ref . UID { // The claim that the PV was pointing to was deleted, and another // with the same name created. klog . V ( 4 ) . Infof ( " " , volume . Name , claimref To Claim Key ( volume . Spec . Claim if claim == nil { // If we get into this block, the claim must have been deleted; // NOTE: reclaim Volume may either release the PV back into the pool or // recycle it or do nothing (retain) // Do not overwrite previous Failed state - let the user see that // something went wrong, while we still re-try to reclaim the // volume. if volume . Status . Phase != v1 . Volume Released && volume . Status . Phase != v1 . Volume Failed { // Also, log this only once: klog . V ( 2 ) . Infof ( " " , volume . Name , volume . Spec . Persistent Volume Reclaim if volume , err = ctrl . update Volume Phase ( volume , v1 . Volume if err = ctrl . reclaim } else if claim . Spec . Volume Name == " " { if is Mismatch , err := check Volume Mode Mismatches ( & claim . Spec , & volume . Spec ) ; err != nil || is Mismatch { // Binding for the volume won't be called in sync Unbound Claim, // because find Best Match For Claim won't return the volume due to volume Mode mismatch. volume ctrl . event Recorder . Event ( volume , v1 . Event Type Warning , events . Volume Mismatch , volume claim ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Volume Mismatch , claim // Skipping sync if metav1 . Has Annotation ( volume . Object Meta , ann Bound By // In both cases, the volume is Bound and the claim is Pending. // Next sync Claim will fix it. To speed it up, we enqueue the claim // into the controller, which results in sync Claim to be called // shortly (and in the right worker goroutine). // This speeds up binding of provisioned volumes - provisioner saves // only the new PV and it expects that next sync Claim will bind the // claim to it. ctrl . claim Queue . Add ( claim To Claim } else if claim . Spec . Volume if _ , err = ctrl . update Volume Phase ( volume , v1 . Volume } else { // Volume is bound to a claim, but the claim is bound elsewhere if metav1 . Has Annotation ( volume . Object Meta , ann Dynamically Provisioned ) && volume . Spec . Persistent Volume Reclaim Policy == v1 . Persistent Volume Reclaim Delete { // This volume was dynamically provisioned for this claim. The // claim got bound elsewhere, and thus this volume is not // needed. Delete it. // Mark the volume as Released for external deleters and to let // the user know. Don't overwrite existing Failed status! if volume . Status . Phase != v1 . Volume Released && volume . Status . Phase != v1 . Volume if volume , err = ctrl . update Volume Phase ( volume , v1 . Volume if err = ctrl . reclaim } else { // Volume is bound to a claim, but the claim is bound elsewhere // and it's not dynamically provisioned. if metav1 . Has Annotation ( volume . Object Meta , ann Bound By if err = ctrl . unbind // This just updates the volume phase and clears // volume.Spec.Claim Ref.UID. It leaves the volume pre-bound // to the claim. if err = ctrl . unbind } 
func ( ctrl * Persistent Volume Controller ) update Claim Status ( claim * v1 . Persistent Volume Claim , phase v1 . Persistent Volume Claim Phase , volume * v1 . Persistent Volume ) ( * v1 . Persistent Volume Claim , error ) { klog . V ( 4 ) . Infof ( " " , claim To Claim claim Clone := claim . Deep if claim . Status . Phase != phase { claim if volume == nil { // Need to reset Access Modes and Capacity if claim . Status . Access Modes != nil { claim Clone . Status . Access if claim . Status . Capacity != nil { claim } else { // Need to update Access Modes and Capacity if ! reflect . Deep Equal ( claim . Status . Access Modes , volume . Spec . Access Modes ) { claim Clone . Status . Access Modes = volume . Spec . Access // Update Capacity if the claim is becoming Bound, not if it was already. // A discrepancy can be intentional to mean that the PVC filesystem size // doesn't match the PV block device size, so don't clobber it if claim . Status . Phase != phase { volume Cap , ok := volume . Spec . Capacity [ v1 . Resource claim Cap , ok := claim . Status . Capacity [ v1 . Resource if ! ok || volume Cap . Cmp ( claim Cap ) != 0 { claim if ! dirty { // Nothing to do. klog . V ( 4 ) . Infof ( " " , claim To Claim new Claim , err := ctrl . kube Client . Core V1 ( ) . Persistent Volume Claims ( claim Clone . Namespace ) . Update Status ( claim if err != nil { klog . V ( 4 ) . Infof ( " " , claim To Claim return new _ , err = ctrl . store Claim Update ( new if err != nil { klog . V ( 4 ) . Infof ( " " , claim To Claim return new klog . V ( 2 ) . Infof ( " " , claim To Claim return new } 
func ( ctrl * Persistent Volume Controller ) update Claim Status With Event ( claim * v1 . Persistent Volume Claim , phase v1 . Persistent Volume Claim Phase , volume * v1 . Persistent Volume , eventtype , reason , message string ) ( * v1 . Persistent Volume Claim , error ) { klog . V ( 4 ) . Infof ( " " , claim To Claim if claim . Status . Phase == phase { // Nothing to do. klog . V ( 4 ) . Infof ( " " , claim To Claim new Claim , err := ctrl . update Claim // Emit the event only when the status change happens, not every time // sync Claim is called. klog . V ( 3 ) . Infof ( " " , claim To Claim ctrl . event Recorder . Event ( new return new } 
func ( ctrl * Persistent Volume Controller ) update Volume Phase ( volume * v1 . Persistent Volume , phase v1 . Persistent Volume Phase , message string ) ( * v1 . Persistent volume Clone := volume . Deep volume volume new Vol , err := ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Update Status ( volume return new _ , err = ctrl . store Volume Update ( new return new return new } 
func ( ctrl * Persistent Volume Controller ) update Volume Phase With Event ( volume * v1 . Persistent Volume , phase v1 . Persistent Volume Phase , eventtype , reason , message string ) ( * v1 . Persistent new Vol , err := ctrl . update Volume // Emit the event only when the status change happens, not every time // sync ctrl . event Recorder . Event ( new return new } 
func ( ctrl * Persistent Volume Controller ) bind Volume To Claim ( volume * v1 . Persistent Volume , claim * v1 . Persistent Volume Claim ) ( * v1 . Persistent Volume , error ) { klog . V ( 4 ) . Infof ( " " , volume . Name , claim To Claim volume Clone , dirty , err := Get Bind Volume To // Save the volume only if something was changed if dirty { return ctrl . update Bind Volume To Claim ( volume klog . V ( 4 ) . Infof ( " " , volume . Name , claim To Claim } 
func ( ctrl * Persistent Volume Controller ) update Bind Volume To Claim ( volume Clone * v1 . Persistent Volume , claim * v1 . Persistent Volume Claim , update Cache bool ) ( * v1 . Persistent Volume , error ) { klog . V ( 2 ) . Infof ( " " , claim To Claim Key ( claim ) , volume new Vol , err := ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Update ( volume if err != nil { klog . V ( 4 ) . Infof ( " " , volume Clone . Name , claim To Claim return new if update Cache { _ , err = ctrl . store Volume Update ( new if err != nil { klog . V ( 4 ) . Infof ( " " , volume return new klog . V ( 4 ) . Infof ( " " , new Vol . Name , claim To Claim return new } 
func ( ctrl * Persistent Volume Controller ) bind Claim To Volume ( claim * v1 . Persistent Volume Claim , volume * v1 . Persistent Volume ) ( * v1 . Persistent Volume Claim , error ) { klog . V ( 4 ) . Infof ( " " , claim To Claim // Check if the claim was already bound (either by controller or by user) should if volume . Name != claim . Spec . Volume Name { should // The claim from method args can be pointing to watcher cache. We must not // modify these, therefore create a copy. claim Clone := claim . Deep if should // Bind the claim to the volume claim Clone . Spec . Volume // Set ann Bound By Controller if it is not set yet if ! metav1 . Has Annotation ( claim Clone . Object Meta , ann Bound By Controller ) { metav1 . Set Meta Data Annotation ( & claim Clone . Object Meta , ann Bound By // Set ann Bind Completed if it is not set yet if ! metav1 . Has Annotation ( claim Clone . Object Meta , ann Bind Completed ) { metav1 . Set Meta Data Annotation ( & claim Clone . Object Meta , ann Bind if dirty { klog . V ( 2 ) . Infof ( " " , volume . Name , claim To Claim new Claim , err := ctrl . kube Client . Core V1 ( ) . Persistent Volume Claims ( claim . Namespace ) . Update ( claim if err != nil { klog . V ( 4 ) . Infof ( " " , claim To Claim return new _ , err = ctrl . store Claim Update ( new if err != nil { klog . V ( 4 ) . Infof ( " " , claim To Claim return new klog . V ( 4 ) . Infof ( " " , claim To Claim return new klog . V ( 4 ) . Infof ( " " , claim To Claim } 
func ( ctrl * Persistent Volume Controller ) bind ( volume * v1 . Persistent Volume , claim * v1 . Persistent Volume // use update Claim/updated Volume to keep the original claim/volume for // logging in error cases. var updated Claim * v1 . Persistent Volume var updated Volume * v1 . Persistent klog . V ( 4 ) . Infof ( " " , volume . Name , claim To Claim if updated Volume , err = ctrl . bind Volume To Claim ( volume , claim ) ; err != nil { klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim volume = updated if updated Volume , err = ctrl . update Volume Phase ( volume , v1 . Volume Bound , " " ) ; err != nil { klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim volume = updated if updated Claim , err = ctrl . bind Claim To Volume ( claim , volume ) ; err != nil { klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim claim = updated if updated Claim , err = ctrl . update Claim Status ( claim , v1 . Claim Bound , volume ) ; err != nil { klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim claim = updated klog . V ( 4 ) . Infof ( " " , volume . Name , claim To Claim klog . V ( 4 ) . Infof ( " " , volume . Name , get Volume Status For klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , get Claim Status For } 
func ( ctrl * Persistent Volume Controller ) unbind Volume ( volume * v1 . Persistent Volume ) error { klog . V ( 4 ) . Infof ( " " , volume . Name , claimref To Claim Key ( volume . Spec . Claim // Save the PV only when any modification is necessary. volume Clone := volume . Deep if metav1 . Has Annotation ( volume . Object Meta , ann Bound By Controller ) { // The volume was bound by the controller. volume Clone . Spec . Claim delete ( volume Clone . Annotations , ann Bound By if len ( volume Clone . Annotations ) == 0 { // No annotations look better than empty annotation map (and it's easier // to test). volume } else { // The volume was pre-bound by user. Clear only the binging UID. volume Clone . Spec . Claim new Vol , err := ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Update ( volume _ , err = ctrl . store Volume Update ( new klog . V ( 4 ) . Infof ( " " , new // Update the status _ , err = ctrl . update Volume Phase ( new Vol , v1 . Volume } 
func ( ctrl * Persistent Volume Controller ) reclaim Volume ( volume * v1 . Persistent Volume ) error { switch volume . Spec . Persistent Volume Reclaim Policy { case v1 . Persistent Volume Reclaim case v1 . Persistent Volume Reclaim op ctrl . schedule Operation ( op Name , func ( ) error { ctrl . recycle Volume case v1 . Persistent Volume Reclaim op start ctrl . schedule Operation ( op Name , func ( ) error { plugin Name , err := ctrl . delete Volume time Taken := time . Since ( start metrics . Record Volume Operation Metric ( plugin Name , " " , time default : // Unknown Persistent Volume Reclaim Policy if _ , err := ctrl . update Volume Phase With Event ( volume , v1 . Volume Failed , v1 . Event Type } 
func ( ctrl * Persistent Volume Controller ) recycle Volume Operation ( volume * v1 . Persistent // This method may have been waiting for a volume lock for some time. // Previous recycle Volume Operation might just have saved an updated version, // so read current volume state now. new Volume , err := ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Get ( volume . Name , metav1 . Get needs Reclaim , err := ctrl . is Volume Released ( new if ! needs pods , used , err := ctrl . is Volume Used ( new // Verify the claim is in cache: if so, then it is a different PVC with the same name // since the volume is known to be released at this moment. Ths new (cached) PVC must use // a different PV -- we checked that the PV is unused in is Volume Released. // So the old PV is safe to be recycled. claim Name := claimref To Claim Key ( volume . Spec . Claim _ , claim Cached , err := ctrl . claims . Get By Key ( claim if err != nil { klog . V ( 3 ) . Infof ( " " , claim if used && ! claim ctrl . event Recorder . Event ( volume , v1 . Event Type Normal , events . Volume Failed // Use the newest volume copy, this will save us from version conflicts on // saving. volume = new // Find a plugin. spec := vol . New Spec From Persistent plugin , err := ctrl . volume Plugin Mgr . Find Recyclable Plugin By if err != nil { // No recycler found. Emit an event and mark the volume Failed. if _ , err = ctrl . update Volume Phase With Event ( volume , v1 . Volume Failed , v1 . Event Type Warning , events . Volume Failed // Despite the volume being Failed, the controller will retry recycling // the volume in every sync // Plugin found recorder := ctrl . new Recycler Event if _ , err = ctrl . update Volume Phase With Event ( volume , v1 . Volume Failed , v1 . Event Type Warning , events . Volume Failed // Despite the volume being Failed, the controller will retry recycling // the volume in every sync // Send an event ctrl . event Recorder . Event ( volume , v1 . Event Type Normal , events . Volume // Make the volume available again if err = ctrl . unbind } 
func ( ctrl * Persistent Volume Controller ) delete Volume Operation ( volume * v1 . Persistent // This method may have been waiting for a volume lock for some time. // Previous delete Volume Operation might just have saved an updated version, so // read current volume state now. new Volume , err := ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Get ( volume . Name , metav1 . Get needs Reclaim , err := ctrl . is Volume Released ( new if ! needs plugin Name , deleted , err := ctrl . do Delete if volerr . Is Deleted Volume In Use ( err ) { // The plugin needs more time, don't mark the volume as Failed // and send Normal event only ctrl . event Recorder . Event ( volume , v1 . Event Type Normal , events . Volume } else { // The plugin failed, mark the volume as Failed and send Warning // event if _ , err := ctrl . update Volume Phase With Event ( volume , v1 . Volume Failed , v1 . Event Type Warning , events . Volume Failed // Save failed, retry on the next deletion attempt return plugin // Despite the volume being Failed, the controller will retry deleting // the volume in every sync Volume() call. return plugin if ! deleted { // The volume waits for deletion by an external plugin. Do nothing. return plugin // Delete the volume if err = ctrl . kube Client . Core V1 ( ) . Persistent return plugin return plugin } 
func ( ctrl * Persistent Volume Controller ) is Volume Released ( volume * v1 . Persistent Volume ) ( bool , error ) { // A volume needs reclaim if it has Claim Ref and appropriate claim does not // exist. if volume . Spec . Claim if volume . Spec . Claim var claim * v1 . Persistent Volume claim Name := claimref To Claim Key ( volume . Spec . Claim obj , found , err := ctrl . claims . Get By Key ( claim claim , ok = obj . ( * v1 . Persistent Volume if claim != nil && claim . UID == volume . Spec . Claim Ref . UID { // the claim still exists and has the right UID if len ( claim . Spec . Volume Name ) > 0 && claim . Spec . Volume } 
func ( ctrl * Persistent Volume Controller ) is Volume Used ( pv * v1 . Persistent Volume ) ( [ ] string , bool , error ) { if pv . Spec . Claim claim Name := pv . Spec . Claim pod Names := sets . New pods , err := ctrl . pod Lister . Pods ( pv . Spec . Claim for _ , pod := range pods { if util . Is Pod for i := range pod . Spec . Volumes { used if used PV . Persistent Volume Claim != nil && used PV . Persistent Volume Claim . Claim Name == claim Name { pod return pod Names . List ( ) , pod } 
func ( ctrl * Persistent Volume Controller ) do Delete Volume ( volume * v1 . Persistent plugin , err := ctrl . find Deletable // Plugin found plugin Name := plugin . Get Plugin klog . V ( 5 ) . Infof ( " " , plugin spec := vol . New Spec From Persistent deleter , err := plugin . New if err != nil { // Cannot create deleter return plugin op Complete := util . Operation Complete Hook ( plugin op if err != nil { // Deleter failed return plugin return plugin } 
func ( ctrl * Persistent Volume Controller ) provision Claim ( claim * v1 . Persistent Volume Claim ) error { if ! ctrl . enable Dynamic klog . V ( 4 ) . Infof ( " " , claim To Claim op Name := fmt . Sprintf ( " " , claim To Claim start ctrl . schedule Operation ( op Name , func ( ) error { plugin Name , err := ctrl . provision Claim time Taken := time . Since ( start metrics . Record Volume Operation Metric ( plugin Name , " " , time } 
func ( ctrl * Persistent Volume Controller ) provision Claim Operation ( claim * v1 . Persistent Volume Claim ) ( string , error ) { claim Class := v1helper . Get Persistent Volume Claim klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( claim ) , claim plugin , storage Class , err := ctrl . find Provisionable if err != nil { ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning klog . V ( 2 ) . Infof ( " " , claim To Claim // The controller will retry provisioning the volume in every // sync var plugin provisioner Name := storage if plugin != nil { if plugin . Is Migrated To CSI ( ) { // plugin Name is not set here to align with existing behavior // of not setting plugin Name for external provisioners (including CSI) // Set provisioner Name to CSI plugin name for set Claim Provisioner provisioner Name , err = ctrl . get CSI Name From Intree Name ( storage if err != nil { strerr := fmt . Sprintf ( " " , storage ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning } else { plugin Name = plugin . Get Plugin // Add provisioner annotation so external provisioners know when to start new Claim , err := ctrl . set Claim Provisioner ( claim , provisioner if err != nil { // Save failed, the controller will retry in the next sync klog . V ( 2 ) . Infof ( " " , claim To Claim return plugin claim = new if plugin == nil || plugin . Is Migrated To CSI ( ) { // find Provisionable Plugin returned no error nor plugin. // This means that an unknown provisioner is requested. Report an event // and wait for the external provisioner msg := fmt . Sprintf ( " " , storage ctrl . event Recorder . Event ( claim , v1 . Event Type Normal , events . External klog . V ( 3 ) . Infof ( " " , claim To Claim return plugin // internal provisioning // A previous do Provision Claim may just have finished while we were waiting for // the locks. Check that PV (with deterministic name) hasn't been provisioned // yet. pv Name := ctrl . get Provisioned Volume Name For volume , err := ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Get ( pv Name , metav1 . Get if err == nil && volume != nil { // Volume has been already provisioned, nothing to do. klog . V ( 4 ) . Infof ( " " , claim To Claim return plugin // Prepare a claim Ref to the claim early (to fail before a volume is // provisioned) claim Ref , err := ref . Get return plugin tags [ Cloud Volume Created For Claim Namespace tags [ Cloud Volume Created For Claim Name tags [ Cloud Volume Created For Volume Name Tag ] = pv options := vol . Volume Options { Persistent Volume Reclaim Policy : * storage Class . Reclaim Policy , Mount Options : storage Class . Mount Options , Cloud Tags : & tags , Cluster Name : ctrl . cluster Name , PV Name : pv Name , PVC : claim , Parameters : storage // Refuse to provision if the plugin doesn't support mount options, creation // of PV would be rejected by validation anyway if ! plugin . Supports Mount Option ( ) && len ( options . Mount Options ) > 0 { strerr := fmt . Sprintf ( " " , storage Class . Name , options . Mount klog . V ( 2 ) . Infof ( " " , claim To Claim Key ( claim ) , storage Class . Name , options . Mount ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning return plugin Name , fmt . Errorf ( " " , plugin . Get Plugin // Provision the volume provisioner , err := plugin . New klog . V ( 2 ) . Infof ( " " , claim To Claim Key ( claim ) , storage ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning return plugin var selected if node Name , ok := claim . Annotations [ ann Selected Node ] ; ok { selected Node , err = ctrl . Node Lister . Get ( node klog . V ( 3 ) . Infof ( " " , node Name , claim To Claim ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning return plugin allowed Topologies := storage Class . Allowed op Complete := util . Operation Complete Hook ( plugin . Get Plugin volume , err = provisioner . Provision ( selected Node , allowed op if err != nil { // Other places of failure have nothing to do with Volume Scheduling, // so just let controller retry in the next sync. We'll only call func // reschedule Provisioning here when the underlying provisioning actually failed. ctrl . reschedule strerr := fmt . Sprintf ( " " , storage klog . V ( 2 ) . Infof ( " " , claim To Claim Key ( claim ) , storage ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning return plugin klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim // Create Kubernetes PV object for the volume. if volume . Name == " " { volume . Name = pv // Bind it to the claim volume . Spec . Claim Ref = claim volume . Status . Phase = v1 . Volume volume . Spec . Storage Class Name = claim // Add ann Bound By Controller (used in deleting the volume) metav1 . Set Meta Data Annotation ( & volume . Object Meta , ann Bound By metav1 . Set Meta Data Annotation ( & volume . Object Meta , ann Dynamically Provisioned , plugin . Get Plugin // Try to create the PV object several times for i := 0 ; i < ctrl . create Provisioned PV Retry Count ; i ++ { klog . V ( 4 ) . Infof ( " " , claim To Claim var new Vol * v1 . Persistent if new Vol , err = ctrl . kube Client . Core V1 ( ) . Persistent Volumes ( ) . Create ( volume ) ; err == nil || apierrs . Is Already Exists ( err ) { // Save succeeded. if err != nil { klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim } else { klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim _ , update Err := ctrl . store Volume Update ( new if update Err != nil { // We will get an "volume added" event soon, this is not a big error klog . V ( 4 ) . Infof ( " " , volume . Name , update // Save failed, try again after a while. klog . V ( 3 ) . Infof ( " " , volume . Name , claim To Claim time . Sleep ( ctrl . create Provisioned PV if err != nil { // Save failed. Now we have a storage asset outside of Kubernetes, // but we don't have appropriate PV object for it. // Emit some event here and try to delete the storage asset several // times. strerr := fmt . Sprintf ( " " , claim To Claim ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning var delete for i := 0 ; i < ctrl . create Provisioned PV Retry Count ; i ++ { _ , deleted , delete Err = ctrl . do Delete if delete Err == nil && deleted { // Delete succeeded klog . V ( 4 ) . Infof ( " " , claim To Claim if ! deleted { // This is unreachable code, the volume was provisioned by an // internal plugin and therefore there MUST be an internal // plugin that deletes it. klog . Errorf ( " " , plugin . Get Plugin // Delete failed, try again after a while. klog . V ( 3 ) . Infof ( " " , volume . Name , delete time . Sleep ( ctrl . create Provisioned PV if delete Err != nil { // Delete failed several times. There is an orphaned volume and there // is nothing we can do about it. strerr := fmt . Sprintf ( " " , claim To Claim Key ( claim ) , delete ctrl . event Recorder . Event ( claim , v1 . Event Type Warning , events . Provisioning Cleanup } else { klog . V ( 2 ) . Infof ( " " , volume . Name , claim To Claim msg := fmt . Sprintf ( " " , volume . Name , plugin . Get Plugin ctrl . event Recorder . Event ( claim , v1 . Event Type Normal , events . Provisioning return plugin } 
func ( ctrl * Persistent Volume Controller ) reschedule Provisioning ( claim * v1 . Persistent Volume Claim ) { if _ , ok := claim . Annotations [ ann Selected // The claim from method args can be pointing to watcher cache. We must not // modify these, therefore create a copy. new Claim := claim . Deep delete ( new Claim . Annotations , ann Selected // Try to update the PVC object if _ , err := ctrl . kube Client . Core V1 ( ) . Persistent Volume Claims ( new Claim . Namespace ) . Update ( new Claim ) ; err != nil { klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( new if _ , err := ctrl . store Claim Update ( new Claim ) ; err != nil { // We will get an "claim updated" event soon, this is not a big error klog . V ( 4 ) . Infof ( " " , claim To Claim Key ( new } 
func ( ctrl * Persistent Volume Controller ) get Provisioned Volume Name For Claim ( claim * v1 . Persistent Volume } 
func ( ctrl * Persistent Volume Controller ) schedule Operation ( operation Name string , operation func ( ) error ) { klog . V ( 4 ) . Infof ( " " , operation // Poke test code that an operation is just about to get started. if ctrl . pre Operation Hook != nil { ctrl . pre Operation Hook ( operation err := ctrl . running Operations . Run ( operation if err != nil { switch { case goroutinemap . Is Already Exists ( err ) : klog . V ( 4 ) . Infof ( " " , operation case exponentialbackoff . Is Exponential Backoff ( err ) : klog . V ( 4 ) . Infof ( " " , operation default : klog . Errorf ( " " , operation } 
func ( ctrl * Persistent Volume Controller ) new Recycler Event Recorder ( volume * v1 . Persistent Volume ) recyclerclient . Recycle Event Recorder { return func ( eventtype , message string ) { ctrl . event Recorder . Eventf ( volume , eventtype , events . Recycler } 
func ( ctrl * Persistent Volume Controller ) find Provisionable Plugin ( claim * v1 . Persistent Volume Claim ) ( vol . Provisionable Volume Plugin , * storage . Storage Class , error ) { // provision Claim() which leads here is never called with claim Class=="", we // can save some checks. claim Class := v1helper . Get Persistent Volume Claim class , err := ctrl . class Lister . Get ( claim // Find a plugin for the class plugin , err := ctrl . volume Plugin Mgr . Find Provisionable Plugin By if err != nil { if ! strings . Has } 
func ( ctrl * Persistent Volume Controller ) find Deletable Plugin ( volume * v1 . Persistent Volume ) ( vol . Deletable Volume Plugin , error ) { // Find a plugin. Try to find the same plugin that provisioned the volume var plugin vol . Deletable Volume if metav1 . Has Annotation ( volume . Object Meta , ann Dynamically Provisioned ) { provision Plugin Name := volume . Annotations [ ann Dynamically if provision Plugin Name != " " { plugin , err := ctrl . volume Plugin Mgr . Find Deletable Plugin By Name ( provision Plugin if err != nil { if ! strings . Has Prefix ( provision Plugin // The plugin that provisioned the volume was not found or the volume // was not dynamically provisioned. Try to find a plugin by spec. spec := vol . New Spec From Persistent plugin , err := ctrl . volume Plugin Mgr . Find Deletable Plugin By } 
func Is Not Found ( err error ) bool { _ , ok := err . ( * find . Not Found _ , ok = err . ( * find . Default Not Found } 
func format Virtual Disk UUID ( uuid string ) string { uuidwith No uuid With No Hypens := strings . Replace ( uuidwith No return strings . To Lower ( uuid With No } 
func get SCSI Controllers Of Type ( vm Devices object . Virtual Device List , scsi Type string ) [ ] * types . Virtual Controller { // get virtual scsi controllers of passed argument type var scsi Controllers [ ] * types . Virtual for _ , device := range vm Devices { dev Type := vm if dev Type == scsi Type { if c , ok := device . ( types . Base Virtual Controller ) ; ok { scsi Controllers = append ( scsi Controllers , c . Get Virtual return scsi } 
func get Available SCSI Controller ( scsi Controllers [ ] * types . Virtual Controller ) * types . Virtual Controller { // get SCSI controller which has space for adding more devices for _ , controller := range scsi Controllers { if len ( controller . Device ) < SCSI Controller Device } 
func get Next Unit Number ( devices object . Virtual Device List , c types . Base Virtual Controller ) ( int32 , error ) { var taken Unit Numbers [ SCSI Device taken Unit Numbers [ SCSI Reserved key := c . Get Virtual for _ , device := range devices { d := device . Get Virtual if d . Controller Key == key { if d . Unit Number != nil { taken Unit Numbers [ * d . Unit for unit Number , taken Unit Number := range taken Unit Numbers { if ! taken Unit Number { return int32 ( unit } 
func get SCSI Controllers ( vm Devices object . Virtual Device List ) [ ] * types . Virtual Controller { // get all virtual scsi controllers var scsi Controllers [ ] * types . Virtual for _ , device := range vm Devices { dev Type := vm switch dev Type { case SCSI Controller Type , strings . To Lower ( LSI Logic Controller Type ) , strings . To Lower ( Bus Logic Controller Type ) , PVSCSI Controller Type , strings . To Lower ( LSI Logic SAS Controller Type ) : if c , ok := device . ( types . Base Virtual Controller ) ; ok { scsi Controllers = append ( scsi Controllers , c . Get Virtual return scsi } 
func Remove Storage Cluster OR Folder Name From V Disk Path ( v Disk Path string ) string { datastore := regexp . Must Compile ( " \\ \\ " ) . Find String Submatch ( v Disk if filepath . Base ( datastore ) != datastore { v Disk Path = strings . Replace ( v Disk return v Disk } 
func Get Path From VM Disk Path ( vm Disk Path string ) string { datastore Path Obj := new ( object . Datastore is Success := datastore Path Obj . From String ( vm Disk if ! is Success { klog . Errorf ( " " , vm Disk return datastore Path } 
func Get Datastore Path Obj From VM Disk Path ( vm Disk Path string ) ( * object . Datastore Path , error ) { datastore Path Obj := new ( object . Datastore is Success := datastore Path Obj . From String ( vm Disk if ! is Success { klog . Errorf ( " " , vm Disk return nil , fmt . Errorf ( " " , vm Disk return datastore Path } 
func Is Valid UUID ( uuid string ) bool { r := regexp . Must return r . Match } 
func Is Managed Object Not Found Error ( err error ) bool { is Managed Object Not Found if soap . Is Soap Fault ( err ) { _ , is Managed Object Not Found Error = soap . To Soap Fault ( err ) . Vim Fault ( ) . ( types . Managed Object Not return is Managed Object Not Found } 
func Is Invalid Credentials Error ( err error ) bool { is Invalid Credentials if soap . Is Soap Fault ( err ) { _ , is Invalid Credentials Error = soap . To Soap Fault ( err ) . Vim Fault ( ) . ( types . Invalid return is Invalid Credentials } 
func Verify Volume Paths For VM ( vm Mo mo . Virtual Machine , vol Paths [ ] string , node Name string , node Volume Map map [ string ] map [ string ] bool ) { // Verify if the volume paths are present on the VM backing virtual disk devices vm Devices := object . Virtual Device List ( vm Verify Volume Paths For VM Devices ( vm Devices , vol Paths , node Name , node Volume } 
func Verify Volume Paths For VM Devices ( vm Devices object . Virtual Device List , vol Paths [ ] string , node Name string , node Volume Map map [ string ] map [ string ] bool ) { vol Paths for _ , vol Path := range vol Paths { vol Paths Map [ vol // Verify if the volume paths are present on the VM backing virtual disk devices for _ , device := range vm Devices { if vm Devices . Type Name ( device ) == " " { virtual Device := device . Get Virtual if backing , ok := virtual Device . Backing . ( * types . Virtual Disk Flat Ver2Backing Info ) ; ok { if vol Paths Map [ backing . File Name ] { set Node Volume Map ( node Volume Map , backing . File Name , node } 
func ( e * Encoder ) Encode ( event * watch . Event ) error { data , err := runtime . Encode ( e . embedded // FIXME: get rid of json.Raw Message. return e . encoder . Encode ( & metav1 . Watch Event { Type : string ( event . Type ) , Object : runtime . Raw Extension { Raw : json . Raw } 
func Register ( plugins * admission . Plugins ) { plugins . Register ( Deny Escalating Exec , func ( config io . Reader ) ( admission . Interface , error ) { klog . Warningf ( " " , Deny Escalating return New Deny Escalating // This is for legacy support of the Deny Exec On Privileged admission controller. Most // of the time Deny Escalating Exec should be preferred. plugins . Register ( Deny Exec On Privileged , func ( config io . Reader ) ( admission . Interface , error ) { klog . Warningf ( " " , Deny Exec On return New Deny Exec On } 
func New Deny Exec On Privileged ( ) * Deny Exec { return & Deny Exec { Handler : admission . New Handler ( admission . Connect ) , host Network : false , host IPC : false , host } 
func New Deny Escalating Exec ( ) * Deny Exec { return & Deny Exec { Handler : admission . New Handler ( admission . Connect ) , host Network : true , host IPC : true , host } 
func ( d * Deny Exec ) Validate } 
func ( d * Deny Exec ) Validate ( a admission . Attributes , o admission . Object Interfaces ) ( err error ) { path := a . Get if subresource := a . Get pod , err := d . client . Core V1 ( ) . Pods ( a . Get Namespace ( ) ) . Get ( a . Get Name ( ) , metav1 . Get if err != nil { return admission . New if d . host Network && pod . Spec . Host Network { return admission . New if d . host PID && pod . Spec . Host PID { return admission . New if d . host IPC && pod . Spec . Host IPC { return admission . New if d . privileged && is Privileged ( pod ) { return admission . New } 
func is Privileged ( pod * corev1 . Pod ) bool { for _ , c := range pod . Spec . Init Containers { if c . Security Context == nil || c . Security if * c . Security for _ , c := range pod . Spec . Containers { if c . Security Context == nil || c . Security if * c . Security } 
func ( s * eviction Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Eviction , err error ) { err = cache . List } 
func ( s * eviction Lister ) Evictions ( namespace string ) Eviction Namespace Lister { return eviction Namespace } 
func ( s eviction Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Eviction , err error ) { err = cache . List All By } 
func ( v * version ) Mutating Webhook Configurations ( ) Mutating Webhook Configuration Informer { return & mutating Webhook Configuration Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( v * version ) Validating Webhook Configurations ( ) Validating Webhook Configuration Informer { return & validating Webhook Configuration Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( as Container CPU Assignments ) Clone ( ) Container CPU Assignments { ret := make ( Container CPU } 
func New Cgroup Name ( base Cgroup Name , components ... string ) Cgroup // copy data from the base cgroup to eliminate cases where Cgroup Names share underlying slices. See #68416 base copy ( base return Cgroup Name ( append ( base } 
func ( cgroup Name Cgroup Name ) To Systemd ( ) string { if len ( cgroup Name ) == 0 || ( len ( cgroup Name ) == 1 && cgroup for _ , part := range cgroup Name { part = escape Systemd Cgroup result , err := cgroupsystemd . Expand Slice ( strings . Join ( newparts , " " ) + systemd if err != nil { // Should never happen... panic ( fmt . Errorf ( " " , cgroup } 
func ( l * libcontainer Adapter ) new Manager ( cgroups * libcontainerconfigs . Cgroup , paths map [ string ] string ) ( libcontainercgroups . Manager , error ) { switch l . cgroup Manager Type { case libcontainer case libcontainer Systemd : // this means you asked systemd to manage cgroups, but systemd was not on the host, so all you can do is panic... if ! cgroupsystemd . Use } 
func New Cgroup Manager ( cs * Cgroup Subsystems , cgroup Driver string ) Cgroup Manager { manager Type := libcontainer if cgroup Driver == string ( libcontainer Systemd ) { manager Type = libcontainer return & cgroup Manager Impl { subsystems : cs , adapter : new Libcontainer Adapter ( manager } 
func ( m * cgroup Manager Impl ) Name ( name Cgroup Name ) string { if m . adapter . cgroup Manager Type == libcontainer Systemd { return name . To return name . To } 
func ( m * cgroup Manager Impl ) Cgroup Name ( name string ) Cgroup Name { if m . adapter . cgroup Manager Type == libcontainer Systemd { return Parse Systemd To Cgroup return Parse Cgroupfs To Cgroup } 
func ( m * cgroup Manager Impl ) build Cgroup Paths ( name Cgroup Name ) map [ string ] string { cgroup Fs Adapted cgroup Paths := make ( map [ string ] string , len ( m . subsystems . Mount for key , val := range m . subsystems . Mount Points { cgroup Paths [ key ] = path . Join ( val , cgroup Fs Adapted return cgroup } 
func update Systemd Cgroup Info ( cgroup Config * libcontainerconfigs . Cgroup , cgroup Name Cgroup Name ) { dir , base := path . Split ( cgroup Name . To cgroup cgroup } 
func ( m * cgroup Manager Impl ) Exists ( name Cgroup Name ) bool { // Get map of all cgroup paths on the system for the particular cgroup cgroup Paths := m . build Cgroup // the presence of alternative control groups not known to runc confuses // the kubelet existence checks. // ideally, we would have a mechanism in runc to support Exists() logic // scoped to the set control groups it understands. this is being discussed // in https://github.com/opencontainers/runc/issues/1440 // once resolved, we can remove this code. whitelist Controllers := sets . New if utilfeature . Default Feature Gate . Enabled ( kubefeatures . Support Pod Pids Limit ) || utilfeature . Default Feature Gate . Enabled ( kubefeatures . Support Node Pids Limit ) { whitelist var missing // If even one cgroup path doesn't exist, then the cgroup doesn't exist. for controller , path := range cgroup Paths { // ignore mounts we don't care about if ! whitelist if ! libcontainercgroups . Path Exists ( path ) { missing Paths = append ( missing if len ( missing Paths ) > 0 { klog . V ( 4 ) . Infof ( " " , name , missing } 
func ( m * cgroup Manager Impl ) Destroy ( cgroup Config * Cgroup defer func ( ) { metrics . Cgroup Manager Duration . With Label Values ( " " ) . Observe ( metrics . Since In metrics . Deprecated Cgroup Manager Latency . With Label Values ( " " ) . Observe ( metrics . Since In cgroup Paths := m . build Cgroup Paths ( cgroup libcontainer Cgroup // libcontainer consumes a different field and expects a different syntax // depending on the cgroup driver in use, so we need this conditional here. if m . adapter . cgroup Manager Type == libcontainer Systemd { update Systemd Cgroup Info ( libcontainer Cgroup Config , cgroup } else { libcontainer Cgroup Config . Path = cgroup Config . Name . To manager , err := m . adapter . new Manager ( libcontainer Cgroup Config , cgroup // Delete cgroups using libcontainers Managers Destroy() method if err = manager . Destroy ( ) ; err != nil { return fmt . Errorf ( " " , cgroup } 
func get Supported Subsystems ( ) map [ subsystem ] bool { supported Subsystems := map [ subsystem ] bool { & cgroupfs . Memory Group { } : true , & cgroupfs . Cpu Group { } : true , & cgroupfs . Pids // not all hosts support hugetlb cgroup, and in the absent of hugetlb, we will fail silently by reporting no capacity. supported Subsystems [ & cgroupfs . Hugetlb if utilfeature . Default Feature Gate . Enabled ( kubefeatures . Support Pod Pids Limit ) || utilfeature . Default Feature Gate . Enabled ( kubefeatures . Support Node Pids Limit ) { supported Subsystems [ & cgroupfs . Pids return supported } 
func set Supported Subsystems ( cgroup Config * libcontainerconfigs . Cgroup ) error { for sys , required := range get Supported Subsystems ( ) { if _ , ok := cgroup if err := sys . Set ( cgroup Config . Paths [ sys . Name ( ) ] , cgroup } 
func ( m * cgroup Manager Impl ) Update ( cgroup Config * Cgroup defer func ( ) { metrics . Cgroup Manager Duration . With Label Values ( " " ) . Observe ( metrics . Since In metrics . Deprecated Cgroup Manager Latency . With Label Values ( " " ) . Observe ( metrics . Since In // Extract the cgroup resource parameters resource Config := cgroup Config . Resource resources := m . to Resources ( resource cgroup Paths := m . build Cgroup Paths ( cgroup libcontainer Cgroup Config := & libcontainerconfigs . Cgroup { Resources : resources , Paths : cgroup // libcontainer consumes a different field and expects a different syntax // depending on the cgroup driver in use, so we need this conditional here. if m . adapter . cgroup Manager Type == libcontainer Systemd { update Systemd Cgroup Info ( libcontainer Cgroup Config , cgroup } else { libcontainer Cgroup Config . Path = cgroup Config . Name . To if utilfeature . Default Feature Gate . Enabled ( kubefeatures . Support Pod Pids Limit ) && cgroup Config . Resource Parameters != nil && cgroup Config . Resource Parameters . Pids Limit != nil { libcontainer Cgroup Config . Pids Limit = * cgroup Config . Resource Parameters . Pids if err := set Supported Subsystems ( libcontainer Cgroup Config ) ; err != nil { return fmt . Errorf ( " " , cgroup } 
func ( m * cgroup Manager Impl ) Create ( cgroup Config * Cgroup defer func ( ) { metrics . Cgroup Manager Duration . With Label Values ( " " ) . Observe ( metrics . Since In metrics . Deprecated Cgroup Manager Latency . With Label Values ( " " ) . Observe ( metrics . Since In resources := m . to Resources ( cgroup Config . Resource libcontainer Cgroup // libcontainer consumes a different field and expects a different syntax // depending on the cgroup driver in use, so we need this conditional here. if m . adapter . cgroup Manager Type == libcontainer Systemd { update Systemd Cgroup Info ( libcontainer Cgroup Config , cgroup } else { libcontainer Cgroup Config . Path = cgroup Config . Name . To if utilfeature . Default Feature Gate . Enabled ( kubefeatures . Support Pod Pids Limit ) && cgroup Config . Resource Parameters != nil && cgroup Config . Resource Parameters . Pids Limit != nil { libcontainer Cgroup Config . Pids Limit = * cgroup Config . Resource Parameters . Pids // get the manager with the specified cgroup configuration manager , err := m . adapter . new Manager ( libcontainer Cgroup // it may confuse why we call set after we do apply, but the issue is that runc // follows a similar pattern. it's needed to ensure cpu quota is set properly. m . Update ( cgroup } 
func ( m * cgroup Manager Impl ) Pids ( name Cgroup Name ) [ ] int { // we need the driver specific name cgroup Fs // Get a list of processes that we need to kill pids To Kill := sets . New for _ , val := range m . subsystems . Mount Points { dir := path . Join ( val , cgroup Fs if os . Is Not // Get a list of pids that are still charged to the pod's cgroup pids , err = get Cgroup pids To // Walk Func which is called for each file and directory in the pod cgroup dir visitor := func ( path string , info os . File return filepath . Skip if ! info . Is pids , err = get Cgroup return filepath . Skip pids To // Walk through the pod cgroup directory to check if // container cgroups haven't been G return pids To } 
func ( m * cgroup Manager Impl ) Reduce CPU Limits ( cgroup Name Cgroup Name ) error { // Set lowest possible Cpu Shares value for the cgroup minimum CPU Shares := uint64 ( Min resources := & Resource Config { Cpu Shares : & minimum CPU container Config := & Cgroup Config { Name : cgroup Name , Resource return m . Update ( container } 
func ( m * cgroup Manager Impl ) Get Resource Stats ( name Cgroup Name ) ( * Resource Stats , error ) { cgroup Paths := m . build Cgroup stats , err := get Stats Supported Subsystems ( cgroup return to Resource } 
func Make Payload ( mappings [ ] v1 . Key To Path , config Map * v1 . Config Map , default Mode * int32 , optional bool ) ( map [ string ] volumeutil . File Projection , error ) { if default payload := make ( map [ string ] volumeutil . File Projection , ( len ( config Map . Data ) + len ( config Map . Binary var file Projection volumeutil . File if len ( mappings ) == 0 { for name , data := range config Map . Data { file file Projection . Mode = * default payload [ name ] = file for name , data := range config Map . Binary Data { file file Projection . Mode = * default payload [ name ] = file } else { for _ , ktp := range mappings { if string Data , ok := config Map . Data [ ktp . Key ] ; ok { file Projection . Data = [ ] byte ( string } else if binary Data , ok := config Map . Binary Data [ ktp . Key ] ; ok { file Projection . Data = binary if ktp . Mode != nil { file } else { file Projection . Mode = * default payload [ ktp . Path ] = file } 
func ( s * Drivers Store ) Get ( driver Name string ) ( Driver , bool ) { s . R defer s . R driver , ok := s . store [ driver } 
func ( s * Drivers Store ) Set ( driver s . store [ driver } 
func ( s * Drivers Store ) Delete ( driver delete ( s . store , driver } 
func ( s * Drivers } 
func ( c * Fake Custom Resource Definitions ) List ( opts v1 . List Options ) ( result * v1beta1 . Custom Resource Definition List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( customresourcedefinitions Resource , customresourcedefinitions Kind , opts ) , & v1beta1 . Custom Resource Definition label , _ , _ := testing . Extract From List list := & v1beta1 . Custom Resource Definition List { List Meta : obj . ( * v1beta1 . Custom Resource Definition List ) . List for _ , item := range obj . ( * v1beta1 . Custom Resource Definition } 
func ( c * Fake Custom Resource Definitions ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( customresourcedefinitions } 
func ( c * Fake Custom Resource Definitions ) Create ( custom Resource Definition * v1beta1 . Custom Resource Definition ) ( result * v1beta1 . Custom Resource Definition , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( customresourcedefinitions Resource , custom Resource Definition ) , & v1beta1 . Custom Resource return obj . ( * v1beta1 . Custom Resource } 
func ( c * Fake Custom Resource Definitions ) Update ( custom Resource Definition * v1beta1 . Custom Resource Definition ) ( result * v1beta1 . Custom Resource Definition , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( customresourcedefinitions Resource , custom Resource Definition ) , & v1beta1 . Custom Resource return obj . ( * v1beta1 . Custom Resource } 
func ( c * Fake Custom Resource Definitions ) Update Status ( custom Resource Definition * v1beta1 . Custom Resource Definition ) ( * v1beta1 . Custom Resource Definition , error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Subresource Action ( customresourcedefinitions Resource , " " , custom Resource Definition ) , & v1beta1 . Custom Resource return obj . ( * v1beta1 . Custom Resource } 
func File Exists ( fs utilfs . Filesystem , path string ) ( bool , error ) { if info , err := fs . Stat ( path ) ; err == nil { if info . Mode ( ) . Is } else if os . Is Not } 
func Ensure File ( fs utilfs . Filesystem , path string ) error { // if file exists, don't change it, but do report any unexpected errors if ok , err := File // create any necessary parents err := fs . Mkdir All ( filepath . Dir ( path ) , default } 
func Write Tmp File ( fs utilfs . Filesystem , path string , data [ ] byte ) ( tmp Path string , ret // create the tmp file tmp File , err := fs . Temp defer func ( ) { // close the file, return the close error only if there haven't been any other errors if err := tmp File . Close ( ) ; ret Err == nil { ret // if there was an error writing, syncing, or closing, delete the temporary file and return the error if ret Err != nil { if err := fs . Remove ( tmp Path ) ; err != nil { ret Err = fmt . Errorf ( " " , tmp Path , ret tmp // Name() will be an absolute path when using utilfs.Default FS, because ioutil.Temp File passes // an absolute path to os.Open, and we ensure similar behavior in utilfs.Fake FS for testing. tmp Path = tmp // write data if _ , err := tmp File . Write ( data ) ; err != nil { return tmp // sync file, to ensure it's written in case a hard reset happens return tmp Path , tmp } 
func Replace File ( fs utilfs . Filesystem , path string , data [ ] byte ) error { // write data to a temporary file tmp Path , err := Write Tmp // rename over existing file return fs . Rename ( tmp } 
func Ensure Dir ( fs utilfs . Filesystem , path string ) error { // if dir exists, don't change it, but do report any unexpected errors if ok , err := Dir // create the dir return fs . Mkdir All ( path , default } 
func Write Temp Dir ( fs utilfs . Filesystem , path string , files map [ string ] string ) ( tmp Path string , ret tmp Path , err = fs . Temp // be sure to clean up if there was an error defer func ( ) { if ret Err != nil { if err := fs . Remove All ( tmp Path ) ; err != nil { ret Err = fmt . Errorf ( " " , tmp Path , ret // write data for name , data := range files { // create the file file , err := fs . Create ( filepath . Join ( tmp if err != nil { return tmp // be sure to close the file when we're done defer func ( ) { // close the file when we're done, don't overwrite primary ret Err if close fails if err := file . Close ( ) ; ret Err == nil { ret // write the file if _ , err := file . Write ( [ ] byte ( data ) ) ; err != nil { return tmp // sync the file, to ensure it's written in case a hard reset happens if err := file . Sync ( ) ; err != nil { return tmp return tmp } 
func Replace Dir ( fs utilfs . Filesystem , path string , files map [ string ] string ) error { // write data to a temporary directory tmp Path , err := Write Temp // rename over target directory return fs . Rename ( tmp } 
func Convert Dynamic Policy To return & audit . Policy { Rules : [ ] audit . Policy Rule { { Level : audit . Level ( p . Level ) , } , } , Omit Stages : Invert } 
func ( d * dynamic Policy Checker ) Level And Stages ( authorizer . Attributes ) ( audit . Level , [ ] audit . Stage ) { return audit . Level Request } 
func ( c * nodes ) Patch Status ( node err := c . client . Patch ( types . Strategic Merge Patch Type ) . Resource ( " " ) . Name ( node Name ) . Sub } 
func ( mc * metric Context ) Observe ( err error ) error { api Metrics . latency . With Label if err != nil { api Metrics . errors . With Label } 
func register API Metrics ( attributes ... string ) * api Call Metrics { metrics := & api Call Metrics { latency : prometheus . New Histogram Vec ( prometheus . Histogram Opts { Name : " " , Help : " " , } , attributes , ) , errors : prometheus . New Counter Vec ( prometheus . Counter prometheus . Must prometheus . Must } 
func Parse } 
func New Pod Disruption Budget Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Pod Disruption Budget Informer ( client , namespace , resync } 
func New Filtered Pod Disruption Budget Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Policy V1beta1 ( ) . Pod Disruption } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Policy V1beta1 ( ) . Pod Disruption } , } , & policyv1beta1 . Pod Disruption Budget { } , resync } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * API Endpoint ) ( nil ) , ( * kubeadm . API Endpoint ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_API Endpoint_To_kubeadm_API Endpoint ( a . ( * API Endpoint ) , b . ( * kubeadm . API if err := s . Add Generated Conversion Func ( ( * kubeadm . API Endpoint ) ( nil ) , ( * API Endpoint ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_API Endpoint_To_v1beta1_API Endpoint ( a . ( * kubeadm . API Endpoint ) , b . ( * API if err := s . Add Generated Conversion Func ( ( * API Server ) ( nil ) , ( * kubeadm . API Server ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_API Server_To_kubeadm_API Server ( a . ( * API Server ) , b . ( * kubeadm . API if err := s . Add Generated Conversion Func ( ( * kubeadm . API Server ) ( nil ) , ( * API Server ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_API Server_To_v1beta1_API Server ( a . ( * kubeadm . API Server ) , b . ( * API if err := s . Add Generated Conversion Func ( ( * Bootstrap Token ) ( nil ) , ( * kubeadm . Bootstrap Token ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Bootstrap Token_To_kubeadm_Bootstrap Token ( a . ( * Bootstrap Token ) , b . ( * kubeadm . Bootstrap if err := s . Add Generated Conversion Func ( ( * kubeadm . Bootstrap Token ) ( nil ) , ( * Bootstrap Token ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Bootstrap Token_To_v1beta1_Bootstrap Token ( a . ( * kubeadm . Bootstrap Token ) , b . ( * Bootstrap if err := s . Add Generated Conversion Func ( ( * Bootstrap Token Discovery ) ( nil ) , ( * kubeadm . Bootstrap Token Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Bootstrap Token Discovery_To_kubeadm_Bootstrap Token Discovery ( a . ( * Bootstrap Token Discovery ) , b . ( * kubeadm . Bootstrap Token if err := s . Add Generated Conversion Func ( ( * kubeadm . Bootstrap Token Discovery ) ( nil ) , ( * Bootstrap Token Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Bootstrap Token Discovery_To_v1beta1_Bootstrap Token Discovery ( a . ( * kubeadm . Bootstrap Token Discovery ) , b . ( * Bootstrap Token if err := s . Add Generated Conversion Func ( ( * Bootstrap Token String ) ( nil ) , ( * kubeadm . Bootstrap Token String ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Bootstrap Token String_To_kubeadm_Bootstrap Token String ( a . ( * Bootstrap Token String ) , b . ( * kubeadm . Bootstrap Token if err := s . Add Generated Conversion Func ( ( * kubeadm . Bootstrap Token String ) ( nil ) , ( * Bootstrap Token String ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Bootstrap Token String_To_v1beta1_Bootstrap Token String ( a . ( * kubeadm . Bootstrap Token String ) , b . ( * Bootstrap Token if err := s . Add Generated Conversion Func ( ( * Cluster Configuration ) ( nil ) , ( * kubeadm . Cluster Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cluster Configuration_To_kubeadm_Cluster Configuration ( a . ( * Cluster Configuration ) , b . ( * kubeadm . Cluster if err := s . Add Generated Conversion Func ( ( * kubeadm . Cluster Configuration ) ( nil ) , ( * Cluster Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Cluster Configuration_To_v1beta1_Cluster Configuration ( a . ( * kubeadm . Cluster Configuration ) , b . ( * Cluster if err := s . Add Generated Conversion Func ( ( * Cluster Status ) ( nil ) , ( * kubeadm . Cluster Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cluster Status_To_kubeadm_Cluster Status ( a . ( * Cluster Status ) , b . ( * kubeadm . Cluster if err := s . Add Generated Conversion Func ( ( * kubeadm . Cluster Status ) ( nil ) , ( * Cluster Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Cluster Status_To_v1beta1_Cluster Status ( a . ( * kubeadm . Cluster Status ) , b . ( * Cluster if err := s . Add Generated Conversion Func ( ( * Control Plane Component ) ( nil ) , ( * kubeadm . Control Plane Component ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Control Plane Component_To_kubeadm_Control Plane Component ( a . ( * Control Plane Component ) , b . ( * kubeadm . Control Plane if err := s . Add Generated Conversion Func ( ( * kubeadm . Control Plane Component ) ( nil ) , ( * Control Plane Component ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Control Plane Component_To_v1beta1_Control Plane Component ( a . ( * kubeadm . Control Plane Component ) , b . ( * Control Plane if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * External Etcd ) ( nil ) , ( * kubeadm . External Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_External Etcd_To_kubeadm_External Etcd ( a . ( * External Etcd ) , b . ( * kubeadm . External if err := s . Add Generated Conversion Func ( ( * kubeadm . External Etcd ) ( nil ) , ( * External Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_External Etcd_To_v1beta1_External Etcd ( a . ( * kubeadm . External Etcd ) , b . ( * External if err := s . Add Generated Conversion Func ( ( * File Discovery ) ( nil ) , ( * kubeadm . File Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_File Discovery_To_kubeadm_File Discovery ( a . ( * File Discovery ) , b . ( * kubeadm . File if err := s . Add Generated Conversion Func ( ( * kubeadm . File Discovery ) ( nil ) , ( * File Discovery ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_File Discovery_To_v1beta1_File Discovery ( a . ( * kubeadm . File Discovery ) , b . ( * File if err := s . Add Generated Conversion Func ( ( * Host Path Mount ) ( nil ) , ( * kubeadm . Host Path Mount ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Host Path Mount_To_kubeadm_Host Path Mount ( a . ( * Host Path Mount ) , b . ( * kubeadm . Host Path if err := s . Add Generated Conversion Func ( ( * kubeadm . Host Path Mount ) ( nil ) , ( * Host Path Mount ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Host Path Mount_To_v1beta1_Host Path Mount ( a . ( * kubeadm . Host Path Mount ) , b . ( * Host Path if err := s . Add Generated Conversion Func ( ( * Image Meta ) ( nil ) , ( * kubeadm . Image Meta ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Image Meta_To_kubeadm_Image Meta ( a . ( * Image Meta ) , b . ( * kubeadm . Image if err := s . Add Generated Conversion Func ( ( * kubeadm . Image Meta ) ( nil ) , ( * Image Meta ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Image Meta_To_v1beta1_Image Meta ( a . ( * kubeadm . Image Meta ) , b . ( * Image if err := s . Add Generated Conversion Func ( ( * Init Configuration ) ( nil ) , ( * kubeadm . Init Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Init Configuration_To_kubeadm_Init Configuration ( a . ( * Init Configuration ) , b . ( * kubeadm . Init if err := s . Add Generated Conversion Func ( ( * kubeadm . Init Configuration ) ( nil ) , ( * Init Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Init Configuration_To_v1beta1_Init Configuration ( a . ( * kubeadm . Init Configuration ) , b . ( * Init if err := s . Add Generated Conversion Func ( ( * Join Configuration ) ( nil ) , ( * kubeadm . Join Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Join Configuration_To_kubeadm_Join Configuration ( a . ( * Join Configuration ) , b . ( * kubeadm . Join if err := s . Add Generated Conversion Func ( ( * kubeadm . Join Configuration ) ( nil ) , ( * Join Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Join Configuration_To_v1beta1_Join Configuration ( a . ( * kubeadm . Join Configuration ) , b . ( * Join if err := s . Add Generated Conversion Func ( ( * Join Control Plane ) ( nil ) , ( * kubeadm . Join Control Plane ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Join Control Plane_To_kubeadm_Join Control Plane ( a . ( * Join Control Plane ) , b . ( * kubeadm . Join Control if err := s . Add Generated Conversion Func ( ( * kubeadm . Join Control Plane ) ( nil ) , ( * Join Control Plane ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Join Control Plane_To_v1beta1_Join Control Plane ( a . ( * kubeadm . Join Control Plane ) , b . ( * Join Control if err := s . Add Generated Conversion Func ( ( * Local Etcd ) ( nil ) , ( * kubeadm . Local Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Local Etcd_To_kubeadm_Local Etcd ( a . ( * Local Etcd ) , b . ( * kubeadm . Local if err := s . Add Generated Conversion Func ( ( * kubeadm . Local Etcd ) ( nil ) , ( * Local Etcd ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Local Etcd_To_v1beta1_Local Etcd ( a . ( * kubeadm . Local Etcd ) , b . ( * Local if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Node Registration Options ) ( nil ) , ( * kubeadm . Node Registration Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Node Registration Options_To_kubeadm_Node Registration Options ( a . ( * Node Registration Options ) , b . ( * kubeadm . Node Registration if err := s . Add Generated Conversion Func ( ( * kubeadm . Node Registration Options ) ( nil ) , ( * Node Registration Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_kubeadm_Node Registration Options_To_v1beta1_Node Registration Options ( a . ( * kubeadm . Node Registration Options ) , b . ( * Node Registration } 
func Convert_v1beta1_API Endpoint_To_kubeadm_API Endpoint ( in * API Endpoint , out * kubeadm . API Endpoint , s conversion . Scope ) error { return auto Convert_v1beta1_API Endpoint_To_kubeadm_API } 
func Convert_kubeadm_API Endpoint_To_v1beta1_API Endpoint ( in * kubeadm . API Endpoint , out * API Endpoint , s conversion . Scope ) error { return auto Convert_kubeadm_API Endpoint_To_v1beta1_API } 
func Convert_v1beta1_API Server_To_kubeadm_API Server ( in * API Server , out * kubeadm . API Server , s conversion . Scope ) error { return auto Convert_v1beta1_API Server_To_kubeadm_API } 
func Convert_kubeadm_API Server_To_v1beta1_API Server ( in * kubeadm . API Server , out * API Server , s conversion . Scope ) error { return auto Convert_kubeadm_API Server_To_v1beta1_API } 
func Convert_v1beta1_Bootstrap Token_To_kubeadm_Bootstrap Token ( in * Bootstrap Token , out * kubeadm . Bootstrap Token , s conversion . Scope ) error { return auto Convert_v1beta1_Bootstrap Token_To_kubeadm_Bootstrap } 
func Convert_kubeadm_Bootstrap Token_To_v1beta1_Bootstrap Token ( in * kubeadm . Bootstrap Token , out * Bootstrap Token , s conversion . Scope ) error { return auto Convert_kubeadm_Bootstrap Token_To_v1beta1_Bootstrap } 
func Convert_v1beta1_Bootstrap Token Discovery_To_kubeadm_Bootstrap Token Discovery ( in * Bootstrap Token Discovery , out * kubeadm . Bootstrap Token Discovery , s conversion . Scope ) error { return auto Convert_v1beta1_Bootstrap Token Discovery_To_kubeadm_Bootstrap Token } 
func Convert_kubeadm_Bootstrap Token Discovery_To_v1beta1_Bootstrap Token Discovery ( in * kubeadm . Bootstrap Token Discovery , out * Bootstrap Token Discovery , s conversion . Scope ) error { return auto Convert_kubeadm_Bootstrap Token Discovery_To_v1beta1_Bootstrap Token } 
func Convert_v1beta1_Bootstrap Token String_To_kubeadm_Bootstrap Token String ( in * Bootstrap Token String , out * kubeadm . Bootstrap Token String , s conversion . Scope ) error { return auto Convert_v1beta1_Bootstrap Token String_To_kubeadm_Bootstrap Token } 
func Convert_kubeadm_Bootstrap Token String_To_v1beta1_Bootstrap Token String ( in * kubeadm . Bootstrap Token String , out * Bootstrap Token String , s conversion . Scope ) error { return auto Convert_kubeadm_Bootstrap Token String_To_v1beta1_Bootstrap Token } 
func Convert_v1beta1_Cluster Configuration_To_kubeadm_Cluster Configuration ( in * Cluster Configuration , out * kubeadm . Cluster Configuration , s conversion . Scope ) error { return auto Convert_v1beta1_Cluster Configuration_To_kubeadm_Cluster } 
func Convert_kubeadm_Cluster Configuration_To_v1beta1_Cluster Configuration ( in * kubeadm . Cluster Configuration , out * Cluster Configuration , s conversion . Scope ) error { return auto Convert_kubeadm_Cluster Configuration_To_v1beta1_Cluster } 
func Convert_v1beta1_Cluster Status_To_kubeadm_Cluster Status ( in * Cluster Status , out * kubeadm . Cluster Status , s conversion . Scope ) error { return auto Convert_v1beta1_Cluster Status_To_kubeadm_Cluster } 
func Convert_kubeadm_Cluster Status_To_v1beta1_Cluster Status ( in * kubeadm . Cluster Status , out * Cluster Status , s conversion . Scope ) error { return auto Convert_kubeadm_Cluster Status_To_v1beta1_Cluster } 
func Convert_v1beta1_Control Plane Component_To_kubeadm_Control Plane Component ( in * Control Plane Component , out * kubeadm . Control Plane Component , s conversion . Scope ) error { return auto Convert_v1beta1_Control Plane Component_To_kubeadm_Control Plane } 
func Convert_kubeadm_Control Plane Component_To_v1beta1_Control Plane Component ( in * kubeadm . Control Plane Component , out * Control Plane Component , s conversion . Scope ) error { return auto Convert_kubeadm_Control Plane Component_To_v1beta1_Control Plane } 
func Convert_v1beta1_DNS_To_kubeadm_DNS ( in * DNS , out * kubeadm . DNS , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_DNS_To_v1beta1_DNS ( in * kubeadm . DNS , out * DNS , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Discovery_To_kubeadm_Discovery ( in * Discovery , out * kubeadm . Discovery , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_Discovery_To_v1beta1_Discovery ( in * kubeadm . Discovery , out * Discovery , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Etcd_To_kubeadm_Etcd ( in * Etcd , out * kubeadm . Etcd , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_Etcd_To_v1beta1_Etcd ( in * kubeadm . Etcd , out * Etcd , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_External Etcd_To_kubeadm_External Etcd ( in * External Etcd , out * kubeadm . External Etcd , s conversion . Scope ) error { return auto Convert_v1beta1_External Etcd_To_kubeadm_External } 
func Convert_kubeadm_External Etcd_To_v1beta1_External Etcd ( in * kubeadm . External Etcd , out * External Etcd , s conversion . Scope ) error { return auto Convert_kubeadm_External Etcd_To_v1beta1_External } 
func Convert_v1beta1_File Discovery_To_kubeadm_File Discovery ( in * File Discovery , out * kubeadm . File Discovery , s conversion . Scope ) error { return auto Convert_v1beta1_File Discovery_To_kubeadm_File } 
func Convert_kubeadm_File Discovery_To_v1beta1_File Discovery ( in * kubeadm . File Discovery , out * File Discovery , s conversion . Scope ) error { return auto Convert_kubeadm_File Discovery_To_v1beta1_File } 
func Convert_v1beta1_Host Path Mount_To_kubeadm_Host Path Mount ( in * Host Path Mount , out * kubeadm . Host Path Mount , s conversion . Scope ) error { return auto Convert_v1beta1_Host Path Mount_To_kubeadm_Host Path } 
func Convert_kubeadm_Host Path Mount_To_v1beta1_Host Path Mount ( in * kubeadm . Host Path Mount , out * Host Path Mount , s conversion . Scope ) error { return auto Convert_kubeadm_Host Path Mount_To_v1beta1_Host Path } 
func Convert_v1beta1_Image Meta_To_kubeadm_Image Meta ( in * Image Meta , out * kubeadm . Image Meta , s conversion . Scope ) error { return auto Convert_v1beta1_Image Meta_To_kubeadm_Image } 
func Convert_kubeadm_Image Meta_To_v1beta1_Image Meta ( in * kubeadm . Image Meta , out * Image Meta , s conversion . Scope ) error { return auto Convert_kubeadm_Image Meta_To_v1beta1_Image } 
func Convert_v1beta1_Init Configuration_To_kubeadm_Init Configuration ( in * Init Configuration , out * kubeadm . Init Configuration , s conversion . Scope ) error { return auto Convert_v1beta1_Init Configuration_To_kubeadm_Init } 
func Convert_kubeadm_Init Configuration_To_v1beta1_Init Configuration ( in * kubeadm . Init Configuration , out * Init Configuration , s conversion . Scope ) error { return auto Convert_kubeadm_Init Configuration_To_v1beta1_Init } 
func Convert_v1beta1_Join Configuration_To_kubeadm_Join Configuration ( in * Join Configuration , out * kubeadm . Join Configuration , s conversion . Scope ) error { return auto Convert_v1beta1_Join Configuration_To_kubeadm_Join } 
func Convert_kubeadm_Join Configuration_To_v1beta1_Join Configuration ( in * kubeadm . Join Configuration , out * Join Configuration , s conversion . Scope ) error { return auto Convert_kubeadm_Join Configuration_To_v1beta1_Join } 
func Convert_v1beta1_Join Control Plane_To_kubeadm_Join Control Plane ( in * Join Control Plane , out * kubeadm . Join Control Plane , s conversion . Scope ) error { return auto Convert_v1beta1_Join Control Plane_To_kubeadm_Join Control } 
func Convert_kubeadm_Join Control Plane_To_v1beta1_Join Control Plane ( in * kubeadm . Join Control Plane , out * Join Control Plane , s conversion . Scope ) error { return auto Convert_kubeadm_Join Control Plane_To_v1beta1_Join Control } 
func Convert_v1beta1_Local Etcd_To_kubeadm_Local Etcd ( in * Local Etcd , out * kubeadm . Local Etcd , s conversion . Scope ) error { return auto Convert_v1beta1_Local Etcd_To_kubeadm_Local } 
func Convert_kubeadm_Local Etcd_To_v1beta1_Local Etcd ( in * kubeadm . Local Etcd , out * Local Etcd , s conversion . Scope ) error { return auto Convert_kubeadm_Local Etcd_To_v1beta1_Local } 
func Convert_v1beta1_Networking_To_kubeadm_Networking ( in * Networking , out * kubeadm . Networking , s conversion . Scope ) error { return auto } 
func Convert_kubeadm_Networking_To_v1beta1_Networking ( in * kubeadm . Networking , out * Networking , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Node Registration Options_To_kubeadm_Node Registration Options ( in * Node Registration Options , out * kubeadm . Node Registration Options , s conversion . Scope ) error { return auto Convert_v1beta1_Node Registration Options_To_kubeadm_Node Registration } 
func Convert_kubeadm_Node Registration Options_To_v1beta1_Node Registration Options ( in * kubeadm . Node Registration Options , out * Node Registration Options , s conversion . Scope ) error { return auto Convert_kubeadm_Node Registration Options_To_v1beta1_Node Registration } 
func Before Delete ( strategy REST Delete Strategy , ctx context . Context , obj runtime . Object , options * metav1 . Delete Options ) ( graceful , graceful Pending bool , err error ) { object Meta , gvk , kerr := object Meta And if errs := validation . Validate Delete Options ( options ) ; len ( errs ) > 0 { return false , false , errors . New Invalid ( schema . Group Kind { Group : metav1 . Group // Checking the Preconditions here to fail early. They'll be enforced later on when we actually do the deletion, too. if options . Preconditions != nil { if options . Preconditions . UID != nil && * options . Preconditions . UID != object Meta . Get UID ( ) { return false , false , errors . New Conflict ( schema . Group Resource { Group : gvk . Group , Resource : gvk . Kind } , object Meta . Get Name ( ) , fmt . Errorf ( " " , * options . Preconditions . UID , object Meta . Get if options . Preconditions . Resource Version != nil && * options . Preconditions . Resource Version != object Meta . Get Resource Version ( ) { return false , false , errors . New Conflict ( schema . Group Resource { Group : gvk . Group , Resource : gvk . Kind } , object Meta . Get Name ( ) , fmt . Errorf ( " " , * options . Preconditions . Resource Version , object Meta . Get Resource graceful Strategy , ok := strategy . ( REST Graceful Delete // if the object is already being deleted, no need to update generation. if object Meta . Get Deletion Timestamp ( ) != nil { // if we are already being deleted, we may only shorten the deletion grace period // this means the object was gracefully deleted previously but deletion Grace Period Seconds was not set, // so we force deletion immediately // IMPORTANT: // The deletion operation happens in two phases. // 1. Update to set Deletion Grace Period Seconds and Deletion Timestamp // 2. Delete the object from storage. // If the update succeeds, but the delete fails (network error, internal storage error, etc.), // a resource was previously left in a state that was non-recoverable. We // check if the existing stored resource has a grace period as 0 and if so // attempt to delete immediately in order to recover from this scenario. if object Meta . Get Deletion Grace Period Seconds ( ) == nil || * object Meta . Get Deletion Grace Period // only a shorter grace period may be provided by a user if options . Grace Period Seconds != nil { period := int64 ( * options . Grace Period if period >= * object Meta . Get Deletion Grace Period new Deletion Timestamp := metav1 . New Time ( object Meta . Get Deletion Timestamp ( ) . Add ( - time . Second * time . Duration ( * object Meta . Get Deletion Grace Period Seconds ( ) ) ) . Add ( time . Second * time . Duration ( * options . Grace Period object Meta . Set Deletion Timestamp ( & new Deletion object Meta . Set Deletion Grace Period // graceful deletion is pending, do nothing options . Grace Period Seconds = object Meta . Get Deletion Grace Period if ! graceful Strategy . Check Graceful now := metav1 . New Time ( metav1 . Now ( ) . Add ( time . Second * time . Duration ( * options . Grace Period object Meta . Set Deletion object Meta . Set Deletion Grace Period Seconds ( options . Grace Period // If it's the first graceful deletion we are going to set the Deletion Timestamp to non-nil. // Controllers of the object that's being deleted shouldn't take any nontrivial actions, hence its behavior changes. // Thus we need to bump object's Generation (if set). This handles generation bump during graceful deletion. // The bump for objects that don't support graceful deletion is handled in pkg/registry/generic/registry/store.go. if object Meta . Get Generation ( ) > 0 { object Meta . Set Generation ( object Meta . Get } 
func ( c * Reason c . cache . Add ( c . compose Key ( uid , name ) , reason } 
func ( c * Reason Cache ) Update ( uid types . UID , result kubecontainer . Pod Sync Result ) { for _ , r := range result . Sync Results { if r . Action != kubecontainer . Start } 
func ( c * Reason c . cache . Remove ( c . compose } 
func ( c * Reason Cache ) Get ( uid types . UID , name string ) ( * reason value , ok := c . cache . Get ( c . compose info := value . ( reason } 
func ( in * External Metric Value ) Deep Copy Into ( out * External Metric out . Type Meta = in . Type if in . Metric Labels != nil { in , out := & in . Metric Labels , & out . Metric in . Timestamp . Deep Copy if in . Window Seconds != nil { in , out := & in . Window Seconds , & out . Window out . Value = in . Value . Deep } 
func ( in * External Metric Value ) Deep Copy ( ) * External Metric out := new ( External Metric in . Deep Copy } 
func ( in * External Metric Value ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * External Metric Value List ) Deep Copy Into ( out * External Metric Value out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] External Metric for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * External Metric Value List ) Deep Copy ( ) * External Metric Value out := new ( External Metric Value in . Deep Copy } 
func ( in * External Metric Value List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func Convert_v1alpha1_Pod GC Controller Configuration_To_config_Pod GC Controller Configuration ( in * v1alpha1 . Pod GC Controller Configuration , out * config . Pod GC Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Pod GC Controller Configuration_To_config_Pod GC Controller } 
func Convert_config_Pod GC Controller Configuration_To_v1alpha1_Pod GC Controller Configuration ( in * config . Pod GC Controller Configuration , out * v1alpha1 . Pod GC Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Pod GC Controller Configuration_To_v1alpha1_Pod GC Controller } 
func ( r * gce Project Router ) Project ID ( ctx context . Context , version meta . Version , service string ) string { switch service { case " " , " " : return r . gce . Network Project default : return r . gce . project } 
func ( l * gce Rate Limiter ) Accept ( ctx context . Context , key * cloud . Rate Limit Key ) error { if key . Operation == " " && key . Service == " " { // Wait a minimum amount of time regardless of rate limiter. rl := & cloud . Minimum Rate Limiter { // Convert flowcontrol.Rate Limiter into cloud.Rate Limiter Rate Limiter : & cloud . Accept Rate Limiter { Acceptor : l . gce . operation Poll Rate Limiter , } , Minimum : operation Poll } 
func Create GCE Cloud With Cloud ( config * Cloud Config , c cloud . Cloud ) ( * Cloud , error ) { gce Cloud , err := Create GCE if err == nil { gce return gce } 
func To Proto Models ( open API Spec * spec . Swagger ) ( proto . Models , error ) { spec Bytes , err := json . Marshal Indent ( open API var info yaml . Map err = yaml . Unmarshal ( spec doc , err := openapi_v2 . New Document ( info , compiler . New models , err := proto . New Open API } 
func New Operation Executor ( operation Generator Operation Generator ) Operation Executor { return & operation Executor { pending Operations : nestedpendingoperations . New Nested Pending Operations ( true /* exponential Back Off On Error */ ) , operation Generator : operation } 
func err Suffix ( err error ) string { err if err != nil { err return err } 
func generate Volume Msg Detailed ( prefix Msg , suffix Msg , volume Name , details string ) ( detailed Msg string ) { return fmt . Sprintf ( " " , prefix Msg , volume Name , details , suffix } 
func generate Volume Msg ( prefix Msg , suffix Msg , volume Name , details string ) ( simple Msg , detailed Msg string ) { simple Msg = fmt . Sprintf ( " " , prefix Msg , volume Name , suffix return simple Msg , generate Volume Msg Detailed ( prefix Msg , suffix Msg , volume } 
func ( volume * Volume To Attach ) Generate Msg ( prefix Msg , suffix Msg string ) ( simple Msg , detailed Msg string ) { detailed Str := fmt . Sprintf ( " " , volume . Volume Name , volume . Node volume Spec if volume . Volume Spec != nil { volume Spec Name = volume . Volume return generate Volume Msg ( prefix Msg , suffix Msg , volume Spec Name , detailed } 
func ( volume * Volume To Attach ) Generate Error ( prefix Msg string , err error ) ( simple Err , detailed Err error ) { simple Msg , detailed Msg := volume . Generate Msg ( prefix Msg , err return fmt . Errorf ( simple Msg ) , fmt . Errorf ( detailed } 
func ( volume * Volume To Mount ) Generate Msg Detailed ( prefix Msg , suffix Msg string ) ( detailed Msg string ) { detailed Str := fmt . Sprintf ( " " , volume . Volume volume Spec if volume . Volume Spec != nil { volume Spec Name = volume . Volume return generate Volume Msg Detailed ( prefix Msg , suffix Msg , volume Spec Name , detailed } 
func ( volume * Volume To Mount ) Generate Msg ( prefix Msg , suffix Msg string ) ( simple Msg , detailed Msg string ) { detailed Str := fmt . Sprintf ( " " , volume . Volume volume Spec if volume . Volume Spec != nil { volume Spec Name = volume . Volume return generate Volume Msg ( prefix Msg , suffix Msg , volume Spec Name , detailed } 
func ( volume * Attached Volume ) Generate Msg Detailed ( prefix Msg , suffix Msg string ) ( detailed Msg string ) { detailed Str := fmt . Sprintf ( " " , volume . Volume Name , volume . Node volume Spec if volume . Volume Spec != nil { volume Spec Name = volume . Volume return generate Volume Msg Detailed ( prefix Msg , suffix Msg , volume Spec Name , detailed } 
func ( volume * Attached Volume ) Generate Error Detailed ( prefix Msg string , err error ) ( detailed Err error ) { return fmt . Errorf ( volume . Generate Msg Detailed ( prefix Msg , err } 
func ( volume * Mounted Volume ) Generate Msg Detailed ( prefix Msg , suffix Msg string ) ( detailed Msg string ) { detailed Str := fmt . Sprintf ( " " , volume . Volume Name , volume . Pod Name , volume . Pod return generate Volume Msg Detailed ( prefix Msg , suffix Msg , volume . Outer Volume Spec Name , detailed } 
func ( volume * Mounted Volume ) Generate Msg ( prefix Msg , suffix Msg string ) ( simple Msg , detailed Msg string ) { detailed Str := fmt . Sprintf ( " " , volume . Volume Name , volume . Pod Name , volume . Pod return generate Volume Msg ( prefix Msg , suffix Msg , volume . Outer Volume Spec Name , detailed } 
func ( oe * operation Executor ) Reconstruct Volume Operation ( volume Mode v1 . Persistent Volume Mode , plugin volume . Volume Plugin , mapper Plugin volume . Block Volume Plugin , uid types . UID , pod Name volumetypes . Unique Pod Name , volume Spec Name string , volume Path string , plugin Name string ) ( * volume . Spec , error ) { // Filesystem Volume case if volume Mode == v1 . Persistent Volume Filesystem { // Create volume volume Spec , err := plugin . Construct Volume Spec ( volume Spec Name , volume return volume // Block Volume case // Create volume if mapper Plugin == nil { return nil , fmt . Errorf ( " " , plugin Name , volume Spec Name , pod // volume Path contains volume Name on the path. In the case of block volume, {volume Name} is symbolic link // corresponding to raw block device. // ex. volume Path: pods/{pod Uid}}/{Default Kubelet Volume Devices Dir Name}/{escape Qualified Plugin Name}/{volume Name} volume Spec , err := mapper Plugin . Construct Block Volume Spec ( uid , volume Spec Name , volume return volume } 
func ( oe * operation Executor ) Check Volume Existence Operation ( volume Spec * volume . Spec , mount Path , volume Name string , mounter mount . Interface , unique Volume Name v1 . Unique Volume Name , pod Name volumetypes . Unique Pod Name , pod UID types . UID , attachable volume . Attachable Volume Plugin ) ( bool , error ) { fs Volume , err := util . Check Volume Mode Filesystem ( volume // Filesystem Volume case // For attachable volume case, check mount path directory if volume is still existing and mounted. // Return true if volume is mounted. if fs Volume { if attachable != nil { var is Not var mount Check if is Not Mount , mount Check Err = mounter . Is Likely Not Mount Point ( mount Path ) ; mount Check Err != nil { return false , fmt . Errorf ( " " , unique Volume Name , volume Name , pod Name , pod UID , mount Check return ! is Not // Block Volume case // Check mount path directory if volume still exists, then return true if volume // is there. Either plugin is attachable or non-attachable, the plugin should // have symbolic link associated to raw block device under pod device map // if volume exists. blkutil := volumepathhandler . New Block Volume Path var islink var check if islink Exist , check Err = blkutil . Is Symlink Exist ( mount Path ) ; check Err != nil { return false , fmt . Errorf ( " " , unique Volume Name , volume Name , pod Name , pod UID , check return islink } 
func ( s * cluster Role Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Cluster Role , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1alpha1 . Cluster } 
func ( s * cluster Role Lister ) Get ( name string ) ( * v1alpha1 . Cluster Role , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1alpha1 . Cluster } 
func ( v * version ) Pod Presets ( ) Pod Preset Informer { return & pod Preset Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( c * Clientset ) Samplecontroller V1alpha1 ( ) samplecontrollerv1alpha1 . Samplecontroller V1alpha1Interface { return & fakesamplecontrollerv1alpha1 . Fake Samplecontroller } 
func Confirm No Escalation ( ctx context . Context , rule Resolver Authorization Rule Resolver , rules [ ] rbacv1 . Policy Rule ) error { rule Resolution user , ok := genericapirequest . User namespace , _ := genericapirequest . Namespace owner Rules , err := rule Resolver . Rules if err != nil { // As per Authorization Rule rule Resolution Errors = append ( rule Resolution owner Rights Cover , missing Rights := Covers ( owner if ! owner Rights Cover { compact Missing Rights := missing if compact , err := Compact Rules ( missing Rights ) ; err == nil { compact Missing missing Descriptions := sets . New for _ , missing := range compact Missing Rights { missing Descriptions . Insert ( rbacv1helpers . Compact msg := fmt . Sprintf ( " \n " , user . Get Name ( ) , user . Get Groups ( ) , strings . Join ( missing if len ( rule Resolution Errors ) > 0 { msg = msg + fmt . Sprintf ( " " , rule Resolution } 
func ( r * Default Rule Resolver ) Get Role Reference Rules ( role Ref rbacv1 . Role Ref , binding Namespace string ) ( [ ] rbacv1 . Policy Rule , error ) { switch role Ref . Kind { case " " : role , err := r . role Getter . Get Role ( binding Namespace , role case " " : cluster Role , err := r . cluster Role Getter . Get Cluster Role ( role return cluster default : return nil , fmt . Errorf ( " " , role } 
func applies To ( user user . Info , binding Subjects [ ] rbacv1 . Subject , namespace string ) ( int , bool ) { for i , binding Subject := range binding Subjects { if applies To User ( user , binding } 
func ( c * cluster Role Bindings ) Create ( cluster Role Binding * v1 . Cluster Role Binding ) ( result * v1 . Cluster Role Binding , err error ) { result = & v1 . Cluster Role err = c . client . Post ( ) . Resource ( " " ) . Body ( cluster Role } 
func ( c * cluster Role Bindings ) Update ( cluster Role Binding * v1 . Cluster Role Binding ) ( result * v1 . Cluster Role Binding , err error ) { result = & v1 . Cluster Role err = c . client . Put ( ) . Resource ( " " ) . Name ( cluster Role Binding . Name ) . Body ( cluster Role } 
func New Desired State Of World ( volume Plugin Mgr * volume . Volume Plugin Mgr ) Desired State Of World { return & desired State Of World { volumes To Mount : make ( map [ v1 . Unique Volume Name ] volume To Mount ) , volume Plugin Mgr : volume Plugin } 
func ( e * Combined Etcd Client ) Set Etcd Version Key Value ( version * Etcd } 
func ( e * Combined Etcd Client ) Put ( version * Etcd Version , key , value string ) error { if version . Major == 2 { v2client , err := e . client v3client , err := e . client } 
func ( e * Combined Etcd Client ) Get ( version * Etcd Version , key string ) ( string , error ) { if version . Major == 2 { v2client , err := e . client v3client , err := e . client } 
func ( e * Combined Etcd Client ) Backup ( version * Etcd Version , backup return e . run Etcdctl Command ( version , " " , " " , " " , e . cfg . data Directory , " " , backup } 
func ( e * Combined Etcd Client ) Snapshot ( version * Etcd Version , snapshot return e . run Etcdctl Command ( version , " " , e . endpoint ( ) , " " , " " , snapshot } 
func ( e * Combined Etcd Client ) Restore ( version * Etcd Version , snapshot return e . run Etcdctl Command ( version , " " , " " , snapshot File , " " , e . cfg . data Directory , " " , e . cfg . name , " " , e . cfg . peer Advertise Urls , " " , e . cfg . initial } 
func ( e * Combined Etcd Client ) Migrate ( version * Etcd return e . run Etcdctl Command ( version , " " , " " , e . cfg . data } 
func ( e * Combined Etcd Client ) Attach Lease ( lease Duration time . Duration ) error { ttl Keys Prefix := e . cfg . ttl Keys // Make sure that ttl Keys Prefix is ended with "/" so that we only get children "directories". if ! strings . Has Suffix ( ttl Keys Prefix , " " ) { ttl Keys v3client , err := e . client objects Resp , err := v3client . KV . Get ( ctx , ttl Keys Prefix , clientv3 . With lease , err := v3client . Lease . Grant ( ctx , int64 ( lease klog . Infof ( " " , len ( objects for _ , kv := range objects Resp . Kvs { put Resp , err := v3client . KV . Put ( ctx , string ( kv . Key ) , string ( kv . Value ) , clientv3 . With Lease ( lease . ID ) , clientv3 . With Prev if bytes . Compare ( put Resp . Prev Kv . Value , kv . Value ) != 0 { return fmt . Errorf ( " " , kv . Key , kv . Value , put Resp . Prev } 
func legacy Log Symlink ( container ID string , container Name , pod Name , pod Namespace string ) string { return log Symlink ( legacy Container Logs Dir , kubecontainer . Build Pod Full Name ( pod Name , pod Namespace ) , container Name , container } 
func Create Node Name To Info Map ( pods [ ] * v1 . Pod , nodes [ ] * v1 . Node ) map [ string ] * Node Info { node Name To Info := make ( map [ string ] * Node for _ , pod := range pods { node Name := pod . Spec . Node if _ , ok := node Name To Info [ node Name ] ; ! ok { node Name To Info [ node Name ] = New Node node Name To Info [ node Name ] . Add image Existence Map := create Image Existence for _ , node := range nodes { if _ , ok := node Name To Info [ node . Name ] ; ! ok { node Name To Info [ node . Name ] = New Node node Info := node Name To node Info . Set node Info . image States = get Node Image States ( node , image Existence return node Name To } 
func get Node Image States ( node * v1 . Node , image Existence Map map [ string ] sets . String ) map [ string ] * Image State Summary { image States := make ( map [ string ] * Image State for _ , image := range node . Status . Images { for _ , name := range image . Names { image States [ name ] = & Image State Summary { Size : image . Size Bytes , Num Nodes : len ( image Existence return image } 
func create Image Existence Map ( nodes [ ] * v1 . Node ) map [ string ] sets . String { image Existence for _ , node := range nodes { for _ , image := range node . Status . Images { for _ , name := range image . Names { if _ , ok := image Existence Map [ name ] ; ! ok { image Existence Map [ name ] = sets . New } else { image Existence return image Existence } 
func ( cron Job Strategy ) Default Garbage Collection Policy ( ctx context . Context ) rest . Garbage Collection Policy { var group Version schema . Group if request Info , found := genericapirequest . Request Info From ( ctx ) ; found { group Version = schema . Group Version { Group : request Info . API Group , Version : request Info . API switch group Version { case batchv1beta1 . Scheme Group Version , batchv2alpha1 . Scheme Group Version : // for back compatibility return rest . Orphan default : return rest . Delete } 
func ( cron Job Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { cron Job := obj . ( * batch . Cron cron Job . Status = batch . Cron Job pod . Drop Disabled Template Fields ( & cron Job . Spec . Job } 
func ( cron Job Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Cron Job := obj . ( * batch . Cron old Cron Job := old . ( * batch . Cron new Cron Job . Status = old Cron pod . Drop Disabled Template Fields ( & new Cron Job . Spec . Job Template . Spec . Template , & old Cron Job . Spec . Job } 
func ( cron Job Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { cron Job := obj . ( * batch . Cron all Errs := validation . Validate Cron Job ( cron all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & cron Job . Spec . Job Template . Spec . Template , nil , field . New return all } 
func ( cron Job Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new Cron Job := obj . ( * batch . Cron old Cron Job := old . ( * batch . Cron all Errs := validation . Validate Cron Job Update ( new Cron Job , old Cron all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & new Cron Job . Spec . Job Template . Spec . Template , & old Cron Job . Spec . Job Template . Spec . Template , field . New return all } 
func Create Basic ( server URL , cluster Name , user Name string , ca Cert [ ] byte ) * clientcmdapi . Config { // Use the cluster and the username as the context name context Name := fmt . Sprintf ( " " , user Name , cluster return & clientcmdapi . Config { Clusters : map [ string ] * clientcmdapi . Cluster { cluster Name : { Server : server URL , Certificate Authority Data : ca Cert , } , } , Contexts : map [ string ] * clientcmdapi . Context { context Name : { Cluster : cluster Name , Auth Info : user Name , } , } , Auth Infos : map [ string ] * clientcmdapi . Auth Info { } , Current Context : context } 
func Create With Certs ( server URL , cluster Name , user Name string , ca Cert [ ] byte , client Key [ ] byte , client Cert [ ] byte ) * clientcmdapi . Config { config := Create Basic ( server URL , cluster Name , user Name , ca config . Auth Infos [ user Name ] = & clientcmdapi . Auth Info { Client Key Data : client Key , Client Certificate Data : client } 
func Create With Token ( server URL , cluster Name , user Name string , ca Cert [ ] byte , token string ) * clientcmdapi . Config { config := Create Basic ( server URL , cluster Name , user Name , ca config . Auth Infos [ user Name ] = & clientcmdapi . Auth } 
func Client Set From File ( path string ) ( * clientset . Clientset , error ) { config , err := clientcmd . Load From return To Client } 
func To Client Set ( config * clientcmdapi . Config ) ( * clientset . Clientset , error ) { client Config , err := clientcmd . New Default Client Config ( * config , & clientcmd . Config Overrides { } ) . Client client , err := clientset . New For Config ( client } 
func Write To Disk ( filename string , kubeconfig * clientcmdapi . Config ) error { err := clientcmd . Write To } 
func Get Cluster From Kube if config . Contexts [ config . Current Context ] != nil { return config . Clusters [ config . Contexts [ config . Current } 
func Validate Configuration ( config * eventratelimitapi . Configuration ) field . Error List { all Errs := field . Error limits Path := field . New if len ( config . Limits ) == 0 { all Errs = append ( all Errs , field . Invalid ( limits for i , limit := range config . Limits { idx Path := limits if ! limit Types [ limit . Type ] { allowed Values := make ( [ ] string , len ( limit for limit Type := range limit Types { allowed Values [ i ] = string ( limit all Errs = append ( all Errs , field . Not Supported ( idx Path . Child ( " " ) , limit . Type , allowed if limit . Burst <= 0 { all Errs = append ( all Errs , field . Invalid ( idx if limit . QPS <= 0 { all Errs = append ( all Errs , field . Invalid ( idx if limit . Type != eventratelimitapi . Server Limit Type { if limit . Cache Size < 0 { all Errs = append ( all Errs , field . Invalid ( idx Path . Child ( " " ) , limit . Cache return all } 
func ( c * stateful Sets ) Create ( stateful Set * v1 . Stateful Set ) ( result * v1 . Stateful Set , err error ) { result = & v1 . Stateful err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( stateful } 
func ( c * stateful Sets ) Get Scale ( stateful Set Name string , options metav1 . Get err = c . client . Get ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( stateful Set Name ) . Sub Resource ( " " ) . Versioned Params ( & options , scheme . Parameter } 
func ( c * stateful Sets ) Update Scale ( stateful Set err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( stateful Set Name ) . Sub } 
func New Command Start Wardle Server ( defaults * Wardle Server Options , stop cmd := & cobra . Command { Short : " " , Long : " " , Run if err := o . Run Wardle Server ( stop o . Recommended Options . Add utilfeature . Default Mutable Feature Gate . Add } 
func can Be Exposed ( kind schema . Group Kind ) error { switch kind { case corev1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , corev1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , corev1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , appsv1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , appsv1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , extensionsv1beta1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , extensionsv1beta1 . Scheme Group Version . With Kind ( " " ) . Group } 
func Lookup Container Port Number By Service if svc . Spec . Cluster IP == v1 . Cluster IP if svcportspec . Target Port . Type == intstr . Int { if svcportspec . Target Port . Int Value ( ) == 0 { // target Port is omitted, and the Int return int32 ( svcportspec . Target Port . Int return Lookup Container Port Number By Name ( pod , svcportspec . Target } 
func Lookup Service Port Number By } 
func ( c * Fake API Services ) List ( opts v1 . List Options ) ( result * apiregistrationv1 . API Service List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( apiservices Resource , apiservices Kind , opts ) , & apiregistrationv1 . API Service label , _ , _ := testing . Extract From List list := & apiregistrationv1 . API Service List { List Meta : obj . ( * apiregistrationv1 . API Service List ) . List for _ , item := range obj . ( * apiregistrationv1 . API Service } 
func ( c * Fake API Services ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( apiservices } 
func ( c * Fake API Services ) Create ( a PI Service * apiregistrationv1 . API Service ) ( result * apiregistrationv1 . API Service , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( apiservices Resource , a PI Service ) , & apiregistrationv1 . API return obj . ( * apiregistrationv1 . API } 
func ( h Client CA Registration Hook ) try To Write Client C As ( client corev1client . Core V1Interface ) ( bool , error ) { if err := create Namespace If Needed ( client , metav1 . Namespace System ) ; err != nil { utilruntime . Handle if len ( h . Client CA ) > 0 { data [ " " ] = string ( h . Client if len ( h . Request Header // encoding errors aren't going to get better, so just fail on them. data [ " " ] , err = json Serialize String Slice ( h . Request Header Username data [ " " ] , err = json Serialize String Slice ( h . Request Header Group data [ " " ] , err = json Serialize String Slice ( h . Request Header Extra Header data [ " " ] = string ( h . Request Header data [ " " ] , err = json Serialize String Slice ( h . Request Header Allowed // write errors may work next time if we retry, so queue for retry if err := write Config Map ( client , " " , data ) ; err != nil { utilruntime . Handle } 
func Funcs ( codecs runtimeserializer . Codec Factory ) [ ] interface { } { return [ ] interface { } { func ( e * audit . Event , c fuzz . Continue ) { c . Fuzz No switch c . Rand Bool ( ) { case true : e . Request case false : e . Request Object = & runtime . Unknown { Type Meta : runtime . Type Meta { API Version : " " , Kind : " " } , Raw : [ ] byte ( `{"api Version":"","kind":"Pod","some Key":"some Value"}` ) , Content Type : runtime . Content Type switch c . Rand Bool ( ) { case true : e . Response case false : e . Response Object = & runtime . Unknown { Type Meta : runtime . Type Meta { API Version : " " , Kind : " " } , Raw : [ ] byte ( `{"api Version":"","kind":"Pod","some Key":"some Value"}` ) , Content Type : runtime . Content Type } , func ( o * audit . Object Reference , c fuzz . Continue ) { c . Fuzz No switch c . Intn ( 3 ) { case 0 : // core api group o . API o . API case 1 : // other group o . API o . API default : // use random value, but without / as it is used as separator o . API Group = strings . Replace ( o . API o . API Version = strings . Replace ( o . API } 
func New Node Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Node Informer ( client , resync } 
func New Group Adder ( auth authenticator . Request , groups [ ] string ) authenticator . Request { return & Group } 
func ( g * Cloud ) Get HTTP Health Check ( name string ) ( * compute . Http Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric v , err := g . c . Http Health Checks ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Create HTTP Health Check ( hc * compute . Http Health Check ) error { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric return mc . Observe ( g . c . Http Health Checks ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) List HTTP Health Checks ( ) ( [ ] * compute . Http Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric v , err := g . c . Http Health } 
func ( g * Cloud ) Get HTTPS Health Check ( name string ) ( * compute . Https Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric v , err := g . c . Https Health Checks ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Update HTTPS Health Check ( hc * compute . Https Health Check ) error { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric return mc . Observe ( g . c . Https Health Checks ( ) . Update ( ctx , meta . Global } 
func ( g * Cloud ) Delete HTTPS Health Check ( name string ) error { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric return mc . Observe ( g . c . Https Health Checks ( ) . Delete ( ctx , meta . Global } 
func ( g * Cloud ) List HTTPS Health Checks ( ) ( [ ] * compute . Https Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric v , err := g . c . Https Health } 
func ( g * Cloud ) Get Health Check ( name string ) ( * compute . Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric v , err := g . c . Health Checks ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Get Alpha Health Check ( name string ) ( * computealpha . Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric Context With Version ( " " , compute Alpha v , err := g . c . Alpha Health Checks ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Get Beta Health Check ( name string ) ( * computebeta . Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric Context With Version ( " " , compute Beta v , err := g . c . Beta Health Checks ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Update Health Check ( hc * compute . Health Check ) error { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric return mc . Observe ( g . c . Health Checks ( ) . Update ( ctx , meta . Global } 
func ( g * Cloud ) Update Alpha Health Check ( hc * computealpha . Health Check ) error { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric Context With Version ( " " , compute Alpha return mc . Observe ( g . c . Alpha Health Checks ( ) . Update ( ctx , meta . Global } 
func ( g * Cloud ) Create Beta Health Check ( hc * computebeta . Health Check ) error { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric Context With Version ( " " , compute Beta return mc . Observe ( g . c . Beta Health Checks ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) List Health Checks ( ) ( [ ] * compute . Health Check , error ) { ctx , cancel := cloud . Context With Call mc := new Healthcheck Metric v , err := g . c . Health } 
func is At Least Min Nodes Health Check Version ( vstring string ) bool { version , err := utilversion . Parse return version . At Least ( min Nodes Health Check } 
func supports Nodes Health Check ( nodes [ ] * v1 . Node ) bool { for _ , node := range nodes { if ! is At Least Min Nodes Health Check Version ( node . Status . Node Info . Kube Proxy } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Allowed CSI Driver ) ( nil ) , ( * policy . Allowed CSI Driver ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Allowed CSI Driver_To_policy_Allowed CSI Driver ( a . ( * v1beta1 . Allowed CSI Driver ) , b . ( * policy . Allowed CSI if err := s . Add Generated Conversion Func ( ( * policy . Allowed CSI Driver ) ( nil ) , ( * v1beta1 . Allowed CSI Driver ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Allowed CSI Driver_To_v1beta1_Allowed CSI Driver ( a . ( * policy . Allowed CSI Driver ) , b . ( * v1beta1 . Allowed CSI if err := s . Add Generated Conversion Func ( ( * v1beta1 . Allowed Flex Volume ) ( nil ) , ( * policy . Allowed Flex Volume ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Allowed Flex Volume_To_policy_Allowed Flex Volume ( a . ( * v1beta1 . Allowed Flex Volume ) , b . ( * policy . Allowed Flex if err := s . Add Generated Conversion Func ( ( * policy . Allowed Flex Volume ) ( nil ) , ( * v1beta1 . Allowed Flex Volume ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Allowed Flex Volume_To_v1beta1_Allowed Flex Volume ( a . ( * policy . Allowed Flex Volume ) , b . ( * v1beta1 . Allowed Flex if err := s . Add Generated Conversion Func ( ( * v1beta1 . Allowed Host Path ) ( nil ) , ( * policy . Allowed Host Path ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Allowed Host Path_To_policy_Allowed Host Path ( a . ( * v1beta1 . Allowed Host Path ) , b . ( * policy . Allowed Host if err := s . Add Generated Conversion Func ( ( * policy . Allowed Host Path ) ( nil ) , ( * v1beta1 . Allowed Host Path ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Allowed Host Path_To_v1beta1_Allowed Host Path ( a . ( * policy . Allowed Host Path ) , b . ( * v1beta1 . Allowed Host if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . FS Group Strategy Options ) ( nil ) , ( * policy . FS Group Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_FS Group Strategy Options_To_policy_FS Group Strategy Options ( a . ( * v1beta1 . FS Group Strategy Options ) , b . ( * policy . FS Group Strategy if err := s . Add Generated Conversion Func ( ( * policy . FS Group Strategy Options ) ( nil ) , ( * v1beta1 . FS Group Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_FS Group Strategy Options_To_v1beta1_FS Group Strategy Options ( a . ( * policy . FS Group Strategy Options ) , b . ( * v1beta1 . FS Group Strategy if err := s . Add Generated Conversion Func ( ( * v1beta1 . Host Port Range ) ( nil ) , ( * policy . Host Port Range ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Host Port Range_To_policy_Host Port Range ( a . ( * v1beta1 . Host Port Range ) , b . ( * policy . Host Port if err := s . Add Generated Conversion Func ( ( * policy . Host Port Range ) ( nil ) , ( * v1beta1 . Host Port Range ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Host Port Range_To_v1beta1_Host Port Range ( a . ( * policy . Host Port Range ) , b . ( * v1beta1 . Host Port if err := s . Add Generated Conversion Func ( ( * v1beta1 . ID Range ) ( nil ) , ( * policy . ID Range ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_ID Range_To_policy_ID Range ( a . ( * v1beta1 . ID Range ) , b . ( * policy . ID if err := s . Add Generated Conversion Func ( ( * policy . ID Range ) ( nil ) , ( * v1beta1 . ID Range ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_ID Range_To_v1beta1_ID Range ( a . ( * policy . ID Range ) , b . ( * v1beta1 . ID if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Disruption Budget ) ( nil ) , ( * policy . Pod Disruption Budget ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Disruption Budget_To_policy_Pod Disruption Budget ( a . ( * v1beta1 . Pod Disruption Budget ) , b . ( * policy . Pod Disruption if err := s . Add Generated Conversion Func ( ( * policy . Pod Disruption Budget ) ( nil ) , ( * v1beta1 . Pod Disruption Budget ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Disruption Budget_To_v1beta1_Pod Disruption Budget ( a . ( * policy . Pod Disruption Budget ) , b . ( * v1beta1 . Pod Disruption if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Disruption Budget List ) ( nil ) , ( * policy . Pod Disruption Budget List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Disruption Budget List_To_policy_Pod Disruption Budget List ( a . ( * v1beta1 . Pod Disruption Budget List ) , b . ( * policy . Pod Disruption Budget if err := s . Add Generated Conversion Func ( ( * policy . Pod Disruption Budget List ) ( nil ) , ( * v1beta1 . Pod Disruption Budget List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Disruption Budget List_To_v1beta1_Pod Disruption Budget List ( a . ( * policy . Pod Disruption Budget List ) , b . ( * v1beta1 . Pod Disruption Budget if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Disruption Budget Spec ) ( nil ) , ( * policy . Pod Disruption Budget Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Disruption Budget Spec_To_policy_Pod Disruption Budget Spec ( a . ( * v1beta1 . Pod Disruption Budget Spec ) , b . ( * policy . Pod Disruption Budget if err := s . Add Generated Conversion Func ( ( * policy . Pod Disruption Budget Spec ) ( nil ) , ( * v1beta1 . Pod Disruption Budget Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Disruption Budget Spec_To_v1beta1_Pod Disruption Budget Spec ( a . ( * policy . Pod Disruption Budget Spec ) , b . ( * v1beta1 . Pod Disruption Budget if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Disruption Budget Status ) ( nil ) , ( * policy . Pod Disruption Budget Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Disruption Budget Status_To_policy_Pod Disruption Budget Status ( a . ( * v1beta1 . Pod Disruption Budget Status ) , b . ( * policy . Pod Disruption Budget if err := s . Add Generated Conversion Func ( ( * policy . Pod Disruption Budget Status ) ( nil ) , ( * v1beta1 . Pod Disruption Budget Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Disruption Budget Status_To_v1beta1_Pod Disruption Budget Status ( a . ( * policy . Pod Disruption Budget Status ) , b . ( * v1beta1 . Pod Disruption Budget if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Security Policy ) ( nil ) , ( * policy . Pod Security Policy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Security Policy_To_policy_Pod Security Policy ( a . ( * v1beta1 . Pod Security Policy ) , b . ( * policy . Pod Security if err := s . Add Generated Conversion Func ( ( * policy . Pod Security Policy ) ( nil ) , ( * v1beta1 . Pod Security Policy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Security Policy_To_v1beta1_Pod Security Policy ( a . ( * policy . Pod Security Policy ) , b . ( * v1beta1 . Pod Security if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Security Policy List ) ( nil ) , ( * policy . Pod Security Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Security Policy List_To_policy_Pod Security Policy List ( a . ( * v1beta1 . Pod Security Policy List ) , b . ( * policy . Pod Security Policy if err := s . Add Generated Conversion Func ( ( * policy . Pod Security Policy List ) ( nil ) , ( * v1beta1 . Pod Security Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Security Policy List_To_v1beta1_Pod Security Policy List ( a . ( * policy . Pod Security Policy List ) , b . ( * v1beta1 . Pod Security Policy if err := s . Add Generated Conversion Func ( ( * v1beta1 . Pod Security Policy Spec ) ( nil ) , ( * policy . Pod Security Policy Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Pod Security Policy Spec_To_policy_Pod Security Policy Spec ( a . ( * v1beta1 . Pod Security Policy Spec ) , b . ( * policy . Pod Security Policy if err := s . Add Generated Conversion Func ( ( * policy . Pod Security Policy Spec ) ( nil ) , ( * v1beta1 . Pod Security Policy Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Pod Security Policy Spec_To_v1beta1_Pod Security Policy Spec ( a . ( * policy . Pod Security Policy Spec ) , b . ( * v1beta1 . Pod Security Policy if err := s . Add Generated Conversion Func ( ( * v1beta1 . Run As Group Strategy Options ) ( nil ) , ( * policy . Run As Group Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Run As Group Strategy Options_To_policy_Run As Group Strategy Options ( a . ( * v1beta1 . Run As Group Strategy Options ) , b . ( * policy . Run As Group Strategy if err := s . Add Generated Conversion Func ( ( * policy . Run As Group Strategy Options ) ( nil ) , ( * v1beta1 . Run As Group Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Run As Group Strategy Options_To_v1beta1_Run As Group Strategy Options ( a . ( * policy . Run As Group Strategy Options ) , b . ( * v1beta1 . Run As Group Strategy if err := s . Add Generated Conversion Func ( ( * v1beta1 . Run As User Strategy Options ) ( nil ) , ( * policy . Run As User Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Run As User Strategy Options_To_policy_Run As User Strategy Options ( a . ( * v1beta1 . Run As User Strategy Options ) , b . ( * policy . Run As User Strategy if err := s . Add Generated Conversion Func ( ( * policy . Run As User Strategy Options ) ( nil ) , ( * v1beta1 . Run As User Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Run As User Strategy Options_To_v1beta1_Run As User Strategy Options ( a . ( * policy . Run As User Strategy Options ) , b . ( * v1beta1 . Run As User Strategy if err := s . Add Generated Conversion Func ( ( * v1beta1 . Runtime Class Strategy Options ) ( nil ) , ( * policy . Runtime Class Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Runtime Class Strategy Options_To_policy_Runtime Class Strategy Options ( a . ( * v1beta1 . Runtime Class Strategy Options ) , b . ( * policy . Runtime Class Strategy if err := s . Add Generated Conversion Func ( ( * policy . Runtime Class Strategy Options ) ( nil ) , ( * v1beta1 . Runtime Class Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Runtime Class Strategy Options_To_v1beta1_Runtime Class Strategy Options ( a . ( * policy . Runtime Class Strategy Options ) , b . ( * v1beta1 . Runtime Class Strategy if err := s . Add Generated Conversion Func ( ( * v1beta1 . SE Linux Strategy Options ) ( nil ) , ( * policy . SE Linux Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_SE Linux Strategy Options_To_policy_SE Linux Strategy Options ( a . ( * v1beta1 . SE Linux Strategy Options ) , b . ( * policy . SE Linux Strategy if err := s . Add Generated Conversion Func ( ( * policy . SE Linux Strategy Options ) ( nil ) , ( * v1beta1 . SE Linux Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_SE Linux Strategy Options_To_v1beta1_SE Linux Strategy Options ( a . ( * policy . SE Linux Strategy Options ) , b . ( * v1beta1 . SE Linux Strategy if err := s . Add Generated Conversion Func ( ( * v1beta1 . Supplemental Groups Strategy Options ) ( nil ) , ( * policy . Supplemental Groups Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Supplemental Groups Strategy Options_To_policy_Supplemental Groups Strategy Options ( a . ( * v1beta1 . Supplemental Groups Strategy Options ) , b . ( * policy . Supplemental Groups Strategy if err := s . Add Generated Conversion Func ( ( * policy . Supplemental Groups Strategy Options ) ( nil ) , ( * v1beta1 . Supplemental Groups Strategy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_policy_Supplemental Groups Strategy Options_To_v1beta1_Supplemental Groups Strategy Options ( a . ( * policy . Supplemental Groups Strategy Options ) , b . ( * v1beta1 . Supplemental Groups Strategy } 
func Convert_v1beta1_Allowed CSI Driver_To_policy_Allowed CSI Driver ( in * v1beta1 . Allowed CSI Driver , out * policy . Allowed CSI Driver , s conversion . Scope ) error { return auto Convert_v1beta1_Allowed CSI Driver_To_policy_Allowed CSI } 
func Convert_policy_Allowed CSI Driver_To_v1beta1_Allowed CSI Driver ( in * policy . Allowed CSI Driver , out * v1beta1 . Allowed CSI Driver , s conversion . Scope ) error { return auto Convert_policy_Allowed CSI Driver_To_v1beta1_Allowed CSI } 
func Convert_v1beta1_Allowed Flex Volume_To_policy_Allowed Flex Volume ( in * v1beta1 . Allowed Flex Volume , out * policy . Allowed Flex Volume , s conversion . Scope ) error { return auto Convert_v1beta1_Allowed Flex Volume_To_policy_Allowed Flex } 
func Convert_policy_Allowed Flex Volume_To_v1beta1_Allowed Flex Volume ( in * policy . Allowed Flex Volume , out * v1beta1 . Allowed Flex Volume , s conversion . Scope ) error { return auto Convert_policy_Allowed Flex Volume_To_v1beta1_Allowed Flex } 
func Convert_v1beta1_Allowed Host Path_To_policy_Allowed Host Path ( in * v1beta1 . Allowed Host Path , out * policy . Allowed Host Path , s conversion . Scope ) error { return auto Convert_v1beta1_Allowed Host Path_To_policy_Allowed Host } 
func Convert_policy_Allowed Host Path_To_v1beta1_Allowed Host Path ( in * policy . Allowed Host Path , out * v1beta1 . Allowed Host Path , s conversion . Scope ) error { return auto Convert_policy_Allowed Host Path_To_v1beta1_Allowed Host } 
func Convert_v1beta1_Eviction_To_policy_Eviction ( in * v1beta1 . Eviction , out * policy . Eviction , s conversion . Scope ) error { return auto } 
func Convert_policy_Eviction_To_v1beta1_Eviction ( in * policy . Eviction , out * v1beta1 . Eviction , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_FS Group Strategy Options_To_policy_FS Group Strategy Options ( in * v1beta1 . FS Group Strategy Options , out * policy . FS Group Strategy Options , s conversion . Scope ) error { return auto Convert_v1beta1_FS Group Strategy Options_To_policy_FS Group Strategy } 
func Convert_policy_FS Group Strategy Options_To_v1beta1_FS Group Strategy Options ( in * policy . FS Group Strategy Options , out * v1beta1 . FS Group Strategy Options , s conversion . Scope ) error { return auto Convert_policy_FS Group Strategy Options_To_v1beta1_FS Group Strategy } 
func Convert_v1beta1_Host Port Range_To_policy_Host Port Range ( in * v1beta1 . Host Port Range , out * policy . Host Port Range , s conversion . Scope ) error { return auto Convert_v1beta1_Host Port Range_To_policy_Host Port } 
func Convert_policy_Host Port Range_To_v1beta1_Host Port Range ( in * policy . Host Port Range , out * v1beta1 . Host Port Range , s conversion . Scope ) error { return auto Convert_policy_Host Port Range_To_v1beta1_Host Port } 
func Convert_v1beta1_ID Range_To_policy_ID Range ( in * v1beta1 . ID Range , out * policy . ID Range , s conversion . Scope ) error { return auto Convert_v1beta1_ID Range_To_policy_ID } 
func Convert_policy_ID Range_To_v1beta1_ID Range ( in * policy . ID Range , out * v1beta1 . ID Range , s conversion . Scope ) error { return auto Convert_policy_ID Range_To_v1beta1_ID } 
func Convert_v1beta1_Pod Disruption Budget_To_policy_Pod Disruption Budget ( in * v1beta1 . Pod Disruption Budget , out * policy . Pod Disruption Budget , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Disruption Budget_To_policy_Pod Disruption } 
func Convert_policy_Pod Disruption Budget_To_v1beta1_Pod Disruption Budget ( in * policy . Pod Disruption Budget , out * v1beta1 . Pod Disruption Budget , s conversion . Scope ) error { return auto Convert_policy_Pod Disruption Budget_To_v1beta1_Pod Disruption } 
func Convert_v1beta1_Pod Disruption Budget List_To_policy_Pod Disruption Budget List ( in * v1beta1 . Pod Disruption Budget List , out * policy . Pod Disruption Budget List , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Disruption Budget List_To_policy_Pod Disruption Budget } 
func Convert_policy_Pod Disruption Budget List_To_v1beta1_Pod Disruption Budget List ( in * policy . Pod Disruption Budget List , out * v1beta1 . Pod Disruption Budget List , s conversion . Scope ) error { return auto Convert_policy_Pod Disruption Budget List_To_v1beta1_Pod Disruption Budget } 
func Convert_v1beta1_Pod Disruption Budget Spec_To_policy_Pod Disruption Budget Spec ( in * v1beta1 . Pod Disruption Budget Spec , out * policy . Pod Disruption Budget Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Disruption Budget Spec_To_policy_Pod Disruption Budget } 
func Convert_policy_Pod Disruption Budget Spec_To_v1beta1_Pod Disruption Budget Spec ( in * policy . Pod Disruption Budget Spec , out * v1beta1 . Pod Disruption Budget Spec , s conversion . Scope ) error { return auto Convert_policy_Pod Disruption Budget Spec_To_v1beta1_Pod Disruption Budget } 
func Convert_v1beta1_Pod Disruption Budget Status_To_policy_Pod Disruption Budget Status ( in * v1beta1 . Pod Disruption Budget Status , out * policy . Pod Disruption Budget Status , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Disruption Budget Status_To_policy_Pod Disruption Budget } 
func Convert_policy_Pod Disruption Budget Status_To_v1beta1_Pod Disruption Budget Status ( in * policy . Pod Disruption Budget Status , out * v1beta1 . Pod Disruption Budget Status , s conversion . Scope ) error { return auto Convert_policy_Pod Disruption Budget Status_To_v1beta1_Pod Disruption Budget } 
func Convert_v1beta1_Pod Security Policy_To_policy_Pod Security Policy ( in * v1beta1 . Pod Security Policy , out * policy . Pod Security Policy , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Security Policy_To_policy_Pod Security } 
func Convert_policy_Pod Security Policy_To_v1beta1_Pod Security Policy ( in * policy . Pod Security Policy , out * v1beta1 . Pod Security Policy , s conversion . Scope ) error { return auto Convert_policy_Pod Security Policy_To_v1beta1_Pod Security } 
func Convert_v1beta1_Pod Security Policy List_To_policy_Pod Security Policy List ( in * v1beta1 . Pod Security Policy List , out * policy . Pod Security Policy List , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Security Policy List_To_policy_Pod Security Policy } 
func Convert_policy_Pod Security Policy List_To_v1beta1_Pod Security Policy List ( in * policy . Pod Security Policy List , out * v1beta1 . Pod Security Policy List , s conversion . Scope ) error { return auto Convert_policy_Pod Security Policy List_To_v1beta1_Pod Security Policy } 
func Convert_v1beta1_Pod Security Policy Spec_To_policy_Pod Security Policy Spec ( in * v1beta1 . Pod Security Policy Spec , out * policy . Pod Security Policy Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Pod Security Policy Spec_To_policy_Pod Security Policy } 
func Convert_policy_Pod Security Policy Spec_To_v1beta1_Pod Security Policy Spec ( in * policy . Pod Security Policy Spec , out * v1beta1 . Pod Security Policy Spec , s conversion . Scope ) error { return auto Convert_policy_Pod Security Policy Spec_To_v1beta1_Pod Security Policy } 
func Convert_v1beta1_Run As Group Strategy Options_To_policy_Run As Group Strategy Options ( in * v1beta1 . Run As Group Strategy Options , out * policy . Run As Group Strategy Options , s conversion . Scope ) error { return auto Convert_v1beta1_Run As Group Strategy Options_To_policy_Run As Group Strategy } 
func Convert_policy_Run As Group Strategy Options_To_v1beta1_Run As Group Strategy Options ( in * policy . Run As Group Strategy Options , out * v1beta1 . Run As Group Strategy Options , s conversion . Scope ) error { return auto Convert_policy_Run As Group Strategy Options_To_v1beta1_Run As Group Strategy } 
func Convert_v1beta1_Run As User Strategy Options_To_policy_Run As User Strategy Options ( in * v1beta1 . Run As User Strategy Options , out * policy . Run As User Strategy Options , s conversion . Scope ) error { return auto Convert_v1beta1_Run As User Strategy Options_To_policy_Run As User Strategy } 
func Convert_policy_Run As User Strategy Options_To_v1beta1_Run As User Strategy Options ( in * policy . Run As User Strategy Options , out * v1beta1 . Run As User Strategy Options , s conversion . Scope ) error { return auto Convert_policy_Run As User Strategy Options_To_v1beta1_Run As User Strategy } 
func Convert_v1beta1_Runtime Class Strategy Options_To_policy_Runtime Class Strategy Options ( in * v1beta1 . Runtime Class Strategy Options , out * policy . Runtime Class Strategy Options , s conversion . Scope ) error { return auto Convert_v1beta1_Runtime Class Strategy Options_To_policy_Runtime Class Strategy } 
func Convert_policy_Runtime Class Strategy Options_To_v1beta1_Runtime Class Strategy Options ( in * policy . Runtime Class Strategy Options , out * v1beta1 . Runtime Class Strategy Options , s conversion . Scope ) error { return auto Convert_policy_Runtime Class Strategy Options_To_v1beta1_Runtime Class Strategy } 
func Convert_v1beta1_SE Linux Strategy Options_To_policy_SE Linux Strategy Options ( in * v1beta1 . SE Linux Strategy Options , out * policy . SE Linux Strategy Options , s conversion . Scope ) error { return auto Convert_v1beta1_SE Linux Strategy Options_To_policy_SE Linux Strategy } 
func Convert_policy_SE Linux Strategy Options_To_v1beta1_SE Linux Strategy Options ( in * policy . SE Linux Strategy Options , out * v1beta1 . SE Linux Strategy Options , s conversion . Scope ) error { return auto Convert_policy_SE Linux Strategy Options_To_v1beta1_SE Linux Strategy } 
func Convert_v1beta1_Supplemental Groups Strategy Options_To_policy_Supplemental Groups Strategy Options ( in * v1beta1 . Supplemental Groups Strategy Options , out * policy . Supplemental Groups Strategy Options , s conversion . Scope ) error { return auto Convert_v1beta1_Supplemental Groups Strategy Options_To_policy_Supplemental Groups Strategy } 
func Convert_policy_Supplemental Groups Strategy Options_To_v1beta1_Supplemental Groups Strategy Options ( in * policy . Supplemental Groups Strategy Options , out * v1beta1 . Supplemental Groups Strategy Options , s conversion . Scope ) error { return auto Convert_policy_Supplemental Groups Strategy Options_To_v1beta1_Supplemental Groups Strategy } 
func New Encodable ( e Encoder , obj Object , versions ... schema . Group } 
func ( e encodable ) Marshal } 
func ( e Unknown ) Marshal JSON ( ) ( [ ] byte , error ) { // If Content Type is unset, we assume this is JSON. if e . Content Type != " " && e . Content Type != Content Type } 
func new Cmd Certs cmd . Add Command ( new Cmd Certs } 
func new Cmd Certs Renewal ( ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Long : cmdutil . Macro Command Long Description , Run E : cmdutil . Sub Cmd Run cmd . Add Command ( get Renew Sub } 
func ( c * Fake Horizontal Pod Autoscalers ) Get ( name string , options v1 . Get Options ) ( result * v2beta2 . Horizontal Pod Autoscaler , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( horizontalpodautoscalers Resource , c . ns , name ) , & v2beta2 . Horizontal Pod return obj . ( * v2beta2 . Horizontal Pod } 
func ( c * Fake Horizontal Pod Autoscalers ) List ( opts v1 . List Options ) ( result * v2beta2 . Horizontal Pod Autoscaler List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( horizontalpodautoscalers Resource , horizontalpodautoscalers Kind , c . ns , opts ) , & v2beta2 . Horizontal Pod Autoscaler label , _ , _ := testing . Extract From List list := & v2beta2 . Horizontal Pod Autoscaler List { List Meta : obj . ( * v2beta2 . Horizontal Pod Autoscaler List ) . List for _ , item := range obj . ( * v2beta2 . Horizontal Pod Autoscaler } 
func ( c * Fake Horizontal Pod Autoscalers ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v2beta2 . Horizontal Pod Autoscaler , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( horizontalpodautoscalers Resource , c . ns , name , pt , data , subresources ... ) , & v2beta2 . Horizontal Pod return obj . ( * v2beta2 . Horizontal Pod } 
func New Kubelet Flags ( ) * Kubelet Flags { remote Runtime if runtime . GOOS == " " { remote Runtime } else if runtime . GOOS == " " { remote Runtime return & Kubelet Flags { Enable Server : true , Container Runtime Options : * New Container Runtime Options ( ) , Cert Directory : " " , Root Directory : default Root Dir , Master Service Namespace : metav1 . Namespace Default , Max Container Count : - 1 , Max Per Pod Container Count : 1 , Minimum GC Age : metav1 . Duration { Duration : 0 } , Non Masquerade CIDR : " " , Register Schedulable : true , Experimental Kernel Memcg Notification : false , Remote Runtime Endpoint : remote Runtime Endpoint , Node Labels : make ( map [ string ] string ) , Volume Plugin Dir : " " , Register Node : true , Seccomp Profile Root : filepath . Join ( default Root Dir , " " ) , Host Network Sources : [ ] string { kubetypes . All Source } , Host PID Sources : [ ] string { kubetypes . All Source } , Host IPC Sources : [ ] string { kubetypes . All Source } , // TODO(#58010:v1.13.0): Remove --allow-privileged, it is deprecated Allow Privileged : true , // prior to the introduction of this flag, there was a hardcoded cap of 50 images Node Status Max } 
func Validate Kubelet Flags ( f * Kubelet Flags ) error { // ensure that nobody sets Dynamic Config Dir if the dynamic config feature gate is turned off if f . Dynamic Config Dir . Provided ( ) && ! utilfeature . Default Feature Gate . Enabled ( features . Dynamic Kubelet if f . Node Status Max unknown Labels := sets . New for k := range f . Node Labels { if is Kubernetes Label ( k ) && ! kubeletapis . Is Kubelet Label ( k ) { unknown if len ( unknown Labels ) > 0 { // TODO(liggitt): in 1.16, return an error klog . Warningf ( " " , unknown klog . Warningf ( " " , strings . Join ( kubeletapis . Kubelet Label Namespaces ( ) , " " ) , strings . Join ( kubeletapis . Kubelet } 
func New Kubelet Configuration ( ) ( * kubeletconfig . Kubelet Configuration , error ) { scheme , _ , err := kubeletscheme . New Scheme And versioned := & v1beta1 . Kubelet config := & kubeletconfig . Kubelet apply Legacy } 
func apply Legacy Defaults ( kc * kubeletconfig . Kubelet // --authorization-mode kc . Authorization . Mode = kubeletconfig . Kubelet Authorization Mode Always // --read-only-port kc . Read Only Port = ports . Kubelet Read Only } 
func New Kubelet Server ( ) ( * Kubelet Server , error ) { config , err := New Kubelet return & Kubelet Server { Kubelet Flags : * New Kubelet Flags ( ) , Kubelet } 
func Validate Kubelet Server ( s * Kubelet Server ) error { // please add any Kubelet Configuration validation to the kubeletconfigvalidation.Validate Kubelet Configuration function if err := kubeletconfigvalidation . Validate Kubelet Configuration ( & s . Kubelet if err := Validate Kubelet Flags ( & s . Kubelet } 
func ( s * Kubelet Server ) Add Flags ( fs * pflag . Flag Set ) { s . Kubelet Flags . Add Add Kubelet Config Flags ( fs , & s . Kubelet } 
func ( f * Kubelet Flags ) Add Flags ( mainfs * pflag . Flag Set ) { fs := pflag . New Flag Set ( " " , pflag . Exit On defer func ( ) { // Unhide deprecated flags. We want deprecated flags to show in Kubelet help. // We have some hidden flags, but we might as well unhide these when they are deprecated, // as silently deprecating and removing (even hidden) things is unkind to people who use them. fs . Visit mainfs . Add Flag f . Container Runtime Options . Add f . add OS fs . String Var ( & f . Kubelet Config File , " " , f . Kubelet Config fs . String Var ( & f . Kube Config , " " , f . Kube fs . String Var ( & f . Bootstrap Kubeconfig , " " , f . Bootstrap fs . Bool Var ( & f . Really Crash For Testing , " " , f . Really Crash For fs . Float64Var ( & f . Chaos Chance , " " , f . Chaos fs . Bool Var ( & f . Run Once , " " , f . Run fs . Bool Var ( & f . Enable Server , " " , f . Enable fs . String Var ( & f . Hostname Override , " " , f . Hostname fs . String Var ( & f . Node IP , " " , f . Node fs . String Var ( & f . Provider ID , " " , f . Provider fs . String Var ( & f . Cert Directory , " " , f . Cert fs . String Var ( & f . Cloud Provider , " " , f . Cloud fs . String Var ( & f . Cloud Config File , " " , f . Cloud Config fs . String Var ( & f . Root Directory , " " , f . Root fs . Var ( & f . Dynamic Config fs . Bool Var ( & f . Register Node , " " , f . Register fs . Var ( utiltaints . New Taints Var ( & f . Register With // EXPERIMENTAL FLAGS fs . String Var ( & f . Experimental Mounter Path , " " , f . Experimental Mounter fs . String Slice Var ( & f . Allowed Unsafe Sysctls , " " , f . Allowed Unsafe fs . Bool Var ( & f . Experimental Kernel Memcg Notification , " " , f . Experimental Kernel Memcg fs . String Var ( & f . Remote Runtime Endpoint , " " , f . Remote Runtime fs . String Var ( & f . Remote Image Endpoint , " " , f . Remote Image fs . Bool Var ( & f . Experimental Check Node Capabilities Before Mount , " " , f . Experimental Check Node Capabilities Before fs . Bool Var ( & f . Experimental Node Allocatable Ignore Eviction Threshold , " " , f . Experimental Node Allocatable Ignore Eviction bindable Node Labels := cliflag . Configuration Map ( f . Node fs . Var ( & bindable Node Labels , " " , fmt . Sprintf ( " " , strings . Join ( kubeletapis . Kubelet Label Namespaces ( ) , " " ) , strings . Join ( kubeletapis . Kubelet fs . String Var ( & f . Volume Plugin Dir , " " , f . Volume Plugin fs . String Var ( & f . Lock File Path , " " , f . Lock File fs . Bool Var ( & f . Exit On Lock Contention , " " , f . Exit On Lock fs . String Var ( & f . Seccomp Profile Root , " " , f . Seccomp Profile fs . String Var ( & f . Bootstrap Checkpoint Path , " " , f . Bootstrap Checkpoint fs . Int32Var ( & f . Node Status Max Images , " " , f . Node Status Max // DEPRECATED FLAGS fs . Bool fs . Mark fs . String Var ( & f . Bootstrap Kubeconfig , " " , f . Bootstrap fs . Mark fs . Duration Var ( & f . Minimum GC Age . Duration , " " , f . Minimum GC fs . Mark fs . Int32Var ( & f . Max Per Pod Container Count , " " , f . Max Per Pod Container fs . Mark fs . Int32Var ( & f . Max Container Count , " " , f . Max Container fs . Mark fs . String Var ( & f . Master Service Namespace , " " , f . Master Service fs . Mark fs . Bool Var ( & f . Register Schedulable , " " , f . Register fs . Mark fs . String Var ( & f . Non Masquerade CIDR , " " , f . Non Masquerade fs . Mark fs . Bool Var ( & f . Keep Terminated Pod Volumes , " " , f . Keep Terminated Pod fs . Mark // TODO(#58010:v1.13.0): Remove --allow-privileged, it is deprecated fs . Bool Var ( & f . Allow Privileged , " " , f . Allow fs . Mark // TODO(#58010:v1.12.0): Remove --host-network-sources, it is deprecated fs . String Slice Var ( & f . Host Network Sources , " " , f . Host Network fs . Mark // TODO(#58010:v1.12.0): Remove --host-pid-sources, it is deprecated fs . String Slice Var ( & f . Host PID Sources , " " , f . Host PID fs . Mark // TODO(#58010:v1.12.0): Remove --host-ipc-sources, it is deprecated fs . String Slice Var ( & f . Host IPC Sources , " " , f . Host IPC fs . Mark } 
func Add Kubelet Config Flags ( mainfs * pflag . Flag Set , c * kubeletconfig . Kubelet Configuration ) { fs := pflag . New Flag Set ( " " , pflag . Exit On defer func ( ) { // All Kubelet Configuration flags are now deprecated, and any new flags that point to // Kubelet Configuration fields are deprecated-on-creation. When removing flags at the end // of their deprecation period, be careful to check that they have *actually* been deprecated // members of the Kubelet fs . Visit mainfs . Add Flag fs . Bool Var ( & c . Fail Swap On , " " , c . Fail Swap fs . String Var ( & c . Static Pod Path , " " , c . Static Pod fs . Duration Var ( & c . Sync Frequency . Duration , " " , c . Sync fs . Duration Var ( & c . File Check Frequency . Duration , " " , c . File Check fs . Duration Var ( & c . HTTP Check Frequency . Duration , " " , c . HTTP Check fs . String Var ( & c . Static Pod URL , " " , c . Static Pod fs . Var ( cliflag . New Colon Separated Multimap String String ( & c . Static Pod URL fs . Var ( utilflag . IP fs . Int32Var ( & c . Read Only Port , " " , c . Read Only // Authentication fs . Bool fs . Bool fs . Duration Var ( & c . Authentication . Webhook . Cache TTL . Duration , " " , c . Authentication . Webhook . Cache fs . String Var ( & c . Authentication . X509 . Client CA File , " " , c . Authentication . X509 . Client CA // Authorization fs . String fs . Duration Var ( & c . Authorization . Webhook . Cache Authorized TTL . Duration , " " , c . Authorization . Webhook . Cache Authorized fs . Duration Var ( & c . Authorization . Webhook . Cache Unauthorized TTL . Duration , " " , c . Authorization . Webhook . Cache Unauthorized fs . String Var ( & c . TLS Cert File , " " , c . TLS Cert fs . String Var ( & c . TLS Private Key File , " " , c . TLS Private Key fs . Bool Var ( & c . Server TLS Bootstrap , " " , c . Server TLS tls Cipher Possible Values := cliflag . TLS Cipher Possible fs . String Slice Var ( & c . TLS Cipher Suites , " " , c . TLS Cipher Suites , " " + " " + " " + strings . Join ( tls Cipher Possible tls Possible Versions := cliflag . TLS Possible fs . String Var ( & c . TLS Min Version , " " , c . TLS Min Version , " " + " " + strings . Join ( tls Possible fs . Bool Var ( & c . Rotate Certificates , " " , c . Rotate fs . Int32Var ( & c . Registry Pull QPS , " " , c . Registry Pull fs . Int32Var ( & c . Registry Burst , " " , c . Registry fs . Int32Var ( & c . Event Record QPS , " " , c . Event Record fs . Int32Var ( & c . Event Burst , " " , c . Event fs . Bool Var ( & c . Enable Debugging Handlers , " " , c . Enable Debugging fs . Bool Var ( & c . Enable Contention Profiling , " " , c . Enable Contention fs . Int32Var ( & c . Healthz Port , " " , c . Healthz fs . Var ( utilflag . IP Var { Val : & c . Healthz Bind fs . Int32Var ( & c . OOM Score Adj , " " , c . OOM Score fs . String Var ( & c . Cluster Domain , " " , c . Cluster fs . String Slice Var ( & c . Cluster DNS , " " , c . Cluster fs . Duration Var ( & c . Streaming Connection Idle Timeout . Duration , " " , c . Streaming Connection Idle fs . Duration Var ( & c . Node Status Update Frequency . Duration , " " , c . Node Status Update fs . Duration Var ( & c . Image Minimum GC Age . Duration , " " , c . Image Minimum GC fs . Int32Var ( & c . Image GC High Threshold Percent , " " , c . Image GC High Threshold fs . Int32Var ( & c . Image GC Low Threshold Percent , " " , c . Image GC Low Threshold fs . Duration Var ( & c . Volume Stats Agg Period . Duration , " " , c . Volume Stats Agg fs . Var ( cliflag . New Map String Bool ( & c . Feature Gates ) , " " , " " + " \n " + strings . Join ( utilfeature . Default Feature Gate . Known fs . String Var ( & c . Kubelet Cgroups , " " , c . Kubelet fs . String Var ( & c . System Cgroups , " " , c . System fs . Bool Var ( & c . Cgroups Per QOS , " " , c . Cgroups Per fs . String Var ( & c . Cgroup Driver , " " , c . Cgroup fs . String Var ( & c . Cgroup Root , " " , c . Cgroup fs . String Var ( & c . CPU Manager Policy , " " , c . CPU Manager fs . Duration Var ( & c . CPU Manager Reconcile Period . Duration , " " , c . CPU Manager Reconcile fs . Var ( cliflag . New Map String String ( & c . QOS fs . Duration Var ( & c . Runtime Request Timeout . Duration , " " , c . Runtime Request fs . String Var ( & c . Hairpin Mode , " " , c . Hairpin fs . Int32Var ( & c . Max Pods , " " , c . Max fs . String Var ( & c . Pod CIDR , " " , c . Pod fs . Int64Var ( & c . Pod Pids Limit , " " , c . Pod Pids fs . String Var ( & c . Resolver Config , " " , c . Resolver fs . Bool Var ( & c . CPUCFS Quota , " " , c . CPUCFS fs . Duration Var ( & c . CPUCFS Quota Period . Duration , " " , c . CPUCFS Quota fs . Bool Var ( & c . Enable Controller Attach Detach , " " , c . Enable Controller Attach fs . Bool Var ( & c . Make IP Tables Util Chains , " " , c . Make IP Tables Util fs . Int32Var ( & c . IP Tables Masquerade Bit , " " , c . IP Tables Masquerade fs . Int32Var ( & c . IP Tables Drop Bit , " " , c . IP Tables Drop fs . String Var ( & c . Container Log Max Size , " " , c . Container Log Max fs . Int32Var ( & c . Container Log Max Files , " " , c . Container Log Max // Flags intended for testing, not recommended used in production environments. fs . Int64Var ( & c . Max Open Files , " " , c . Max Open fs . String Var ( & c . Content Type , " " , c . Content fs . Int32Var ( & c . Kube APIQPS , " " , c . Kube fs . Int32Var ( & c . Kube API Burst , " " , c . Kube API fs . Bool Var ( & c . Serialize Image Pulls , " " , c . Serialize Image fs . Var ( cliflag . New Langle Separated Map String String ( & c . Eviction fs . Var ( cliflag . New Langle Separated Map String String ( & c . Eviction fs . Var ( cliflag . New Map String String ( & c . Eviction Soft Grace fs . Duration Var ( & c . Eviction Pressure Transition Period . Duration , " " , c . Eviction Pressure Transition fs . Int32Var ( & c . Eviction Max Pod Grace Period , " " , c . Eviction Max Pod Grace fs . Var ( cliflag . New Map String String ( & c . Eviction Minimum fs . Int32Var ( & c . Pods Per Core , " " , c . Pods Per fs . Bool Var ( & c . Protect Kernel Defaults , " " , c . Protect Kernel // Node Allocatable Flags fs . Var ( cliflag . New Map String String ( & c . System fs . Var ( cliflag . New Map String String ( & c . Kube fs . String Slice Var ( & c . Enforce Node Allocatable , " " , c . Enforce Node fs . String Var ( & c . System Reserved Cgroup , " " , c . System Reserved fs . String Var ( & c . Kube Reserved Cgroup , " " , c . Kube Reserved } 
func Set Kubernetes Version ( cfg * kubeadmapiv1beta2 . Cluster Configuration ) { if cfg . Kubernetes Version != kubeadmapiv1beta2 . Default Kubernetes Version && cfg . Kubernetes cfg . Kubernetes } 
klog . Error Depth ( 1 , fmt . Sprintf ( log } 
klog . Info Depth ( 1 , fmt . Sprintf ( log } 
func New CSV ( path string ) ( * Password record users := make ( map [ string ] * user Password reader := csv . New reader . Fields Per obj := & user Password Info { info : & user . Default record if _ , exist := users [ obj . info . Name ] ; exist { klog . Warningf ( " " , obj . info . Name , path , record return & Password } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( resourcequotaapi . Add To utilruntime . Must ( resourcequotav1beta1 . Add To utilruntime . Must ( resourcequotav1alpha1 . Add To utilruntime . Must ( scheme . Set Version Priority ( resourcequotav1beta1 . Scheme Group Version , resourcequotav1alpha1 . Scheme Group } 
func Require Key Unchanged ( key string ) Precondition Func { return func ( patch interface { } ) bool { patch // The presence of key means that its value has been changed, so the test fails. _ , ok = patch } 
func Require Metadata Key Unchanged ( key string ) Precondition Func { return func ( patch interface { } ) bool { patch patch Map1 , ok := patch patch Map2 , ok := patch _ , ok = patch } 
func Convert_v1alpha1_CSR Signing Controller Configuration_To_config_CSR Signing Controller Configuration ( in * v1alpha1 . CSR Signing Controller Configuration , out * csrsigningconfig . CSR Signing Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_CSR Signing Controller Configuration_To_config_CSR Signing Controller } 
func Convert_config_CSR Signing Controller Configuration_To_v1alpha1_CSR Signing Controller Configuration ( in * csrsigningconfig . CSR Signing Controller Configuration , out * v1alpha1 . CSR Signing Controller Configuration , s conversion . Scope ) error { return auto Convert_config_CSR Signing Controller Configuration_To_v1alpha1_CSR Signing Controller } 
func ( w * length Delimited Frame Writer ) Write ( data [ ] byte ) ( int , error ) { binary . Big Endian . Put if n != len ( w . h ) { return 0 , io . Err Short } 
func ( r * length Delimited Frame n , err := io . Read At if n != 4 { return 0 , io . Err Unexpected frame Length := int ( binary . Big r . remaining = frame n , err := io . Read At if err == io . Err Short Buffer || r . remaining > 0 { return n , io . Err Short if n != expect { return n , io . Err Unexpected } 
func New JSON Framed Reader ( r io . Read Closer ) io . Read Closer { return & json Frame Reader { r : r , decoder : json . New } 
func ( r * json Frame return n , io . Err Short // Raw m := json . Raw return n , io . Err Short } 
func New Metric Converter ( ) * Metric Converter { return & Metric Converter { scheme : scheme . Scheme , codecs : serializer . New Codec Factory ( scheme . Scheme ) , internal Versioner : runtime . New Multi Group Versioner ( scheme . Scheme Group Version , schema . Group Kind { Group : cmint . Group Name , Kind : " " } , schema . Group Kind { Group : cmv1beta1 . Group Name , Kind : " " } , schema . Group Kind { Group : cmv1beta2 . Group } 
func ( c * Metric Converter ) Convert List Options To Version ( opts * cmint . Metric List Options , version schema . Group Version ) ( runtime . Object , error ) { param Obj , err := c . Unsafe Convert To Version return param } 
func ( c * Metric Converter ) Convert Result To Version ( res rest . Result , gv schema . Group metric decoder := c . codecs . Universal Decoder ( Metric raw Metric Obj , err := runtime . Decode ( decoder , metric metric Obj , err := c . Unsafe Convert To Version Via ( raw Metric return metric } 
func ( c * Metric Converter ) Unsafe Convert To Version Via ( obj runtime . Object , external Version schema . Group Version ) ( runtime . Object , error ) { obj Int , err := c . scheme . Unsafe Convert To Version ( obj , schema . Group Version { Group : external Version . Group , Version : runtime . API Version obj Ext , err := c . scheme . Unsafe Convert To Version ( obj Int , external return obj } 
func key Func ( obj object With hashutil . Deep Hash Object ( hash , & equivalence Label Obj { namespace : obj . Get Namespace ( ) , labels : obj . Get } 
func ( c * Matching Cache ) Add ( label Obj object With Meta , selector Obj object With Meta ) { key := key Func ( label c . cache . Add ( key , selector } 
func ( c * Matching Cache ) Get Matching Object ( label Obj object With Meta ) ( controller interface { } , exists bool ) { key := key Func ( label // NOTE: we use Lock() instead of R } 
func ( c * Matching Cache ) Update ( label Obj object With Meta , selector Obj object With Meta ) { c . Add ( label Obj , selector } 
func ( c * Matching Cache ) Invalidate c . cache = lru . New ( c . cache . Max } 
func ( c * request // If the cache is full, reject the request. if c . ll . Len ( ) == max In Flight { return " " , New Error Too Many In token , err = c . unique ele := c . ll . Push Front ( & cache Entry { token , req , c . clock . Now ( ) . Add ( cache } 
func ( c * request entry := ele . Value . ( * cache if c . clock . Now ( ) . After ( entry . expire } 
func ( c * request Cache ) unique Token ( ) ( string , error ) { const max // Number of bytes to be token Len when base64 encoded. token Size := math . Ceil ( float64 ( token raw Token := make ( [ ] byte , int ( token for i := 0 ; i < max Tries ; i ++ { if _ , err := rand . Read ( raw encoded := base64 . Raw URL Encoding . Encode To String ( raw token := encoded [ : token } 
func ( c * request entry := oldest . Value . ( * cache if ! now . After ( entry . expire } 
func New Controller ( config * Config , kube Client clientset . Interface , cloud cloudprovider . Interface , cluster CIDR , service CIDR * net . IP Net , node CIDR Mask Size int ) ( * Controller , error ) { if ! nodesync . Is Valid gce if ! ok { return nil , fmt . Errorf ( " " , cloud . Provider set , err := cidrset . New CIDR Set ( cluster CIDR , node CIDR Mask c := & Controller { config : config , adapter : new Adapter ( kube Client , gce Cloud ) , syncers : make ( map [ string ] * nodesync . Node if err := occupy Service CIDR ( c . set , cluster CIDR , service } 
func ( c * Controller ) Start ( node Informer informers . Node nodes , err := list for _ , node := range nodes . Items { if node . Spec . Pod CIDR != " " { _ , cidr Range , err := net . Parse CIDR ( node . Spec . Pod if err == nil { c . set . Occupy ( cidr klog . V ( 3 ) . Infof ( " " , node . Name , node . Spec . Pod } else { klog . Errorf ( " " , node . Name , node . Spec . Pod // XXX/bowei -- stagger the start of each sync cycle. syncer := c . new node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : nodeutil . Create Add Node Handler ( c . on Add ) , Update Func : nodeutil . Create Update Node Handler ( c . on Update ) , Delete Func : nodeutil . Create Delete Node Handler ( c . on } 
func occupy Service CIDR ( set * cidrset . Cidr Set , cluster CIDR , service CIDR * net . IP Net ) error { if cluster CIDR . Contains ( service CIDR . IP ) || service CIDR . Contains ( cluster CIDR . IP ) { if err := set . Occupy ( service } 
func New File Renewal ( ca Cert * x509 . Certificate , ca Key crypto . Signer ) Interface { return & File Renewal { ca Cert : ca Cert , ca Key : ca } 
func ( r * File Renewal ) Renew ( cfg * certutil . Config ) ( * x509 . Certificate , crypto . Signer , error ) { return pkiutil . New Cert And Key ( r . ca Cert , r . ca } 
func ( g * API Group Version ) Install REST ( container * restful . Container ) error { prefix := path . Join ( g . Root , g . Group Version . Group , g . Group installer := & API Installer { group : g , prefix : prefix , min Request Timeout : g . Min Request Timeout , enable API Response Compression : g . Enable API Response api Resources , ws , registration version Discovery Handler := discovery . New API Version Handler ( g . Serializer , g . Group Version , static Lister { api version Discovery Handler . Add To Web return utilerrors . New Aggregate ( registration } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Aggregation Rule ) ( nil ) , ( * rbac . Aggregation Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Aggregation Rule_To_rbac_Aggregation Rule ( a . ( * v1 . Aggregation Rule ) , b . ( * rbac . Aggregation if err := s . Add Generated Conversion Func ( ( * rbac . Aggregation Rule ) ( nil ) , ( * v1 . Aggregation Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Aggregation Rule_To_v1_Aggregation Rule ( a . ( * rbac . Aggregation Rule ) , b . ( * v1 . Aggregation if err := s . Add Generated Conversion Func ( ( * v1 . Cluster Role ) ( nil ) , ( * rbac . Cluster Role ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cluster Role_To_rbac_Cluster Role ( a . ( * v1 . Cluster Role ) , b . ( * rbac . Cluster if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role ) ( nil ) , ( * v1 . Cluster Role ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role_To_v1_Cluster Role ( a . ( * rbac . Cluster Role ) , b . ( * v1 . Cluster if err := s . Add Generated Conversion Func ( ( * v1 . Cluster Role Binding ) ( nil ) , ( * rbac . Cluster Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cluster Role Binding_To_rbac_Cluster Role Binding ( a . ( * v1 . Cluster Role Binding ) , b . ( * rbac . Cluster Role if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role Binding ) ( nil ) , ( * v1 . Cluster Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role Binding_To_v1_Cluster Role Binding ( a . ( * rbac . Cluster Role Binding ) , b . ( * v1 . Cluster Role if err := s . Add Generated Conversion Func ( ( * v1 . Cluster Role Binding List ) ( nil ) , ( * rbac . Cluster Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cluster Role Binding List_To_rbac_Cluster Role Binding List ( a . ( * v1 . Cluster Role Binding List ) , b . ( * rbac . Cluster Role Binding if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role Binding List ) ( nil ) , ( * v1 . Cluster Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role Binding List_To_v1_Cluster Role Binding List ( a . ( * rbac . Cluster Role Binding List ) , b . ( * v1 . Cluster Role Binding if err := s . Add Generated Conversion Func ( ( * v1 . Cluster Role List ) ( nil ) , ( * rbac . Cluster Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cluster Role List_To_rbac_Cluster Role List ( a . ( * v1 . Cluster Role List ) , b . ( * rbac . Cluster Role if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role List ) ( nil ) , ( * v1 . Cluster Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role List_To_v1_Cluster Role List ( a . ( * rbac . Cluster Role List ) , b . ( * v1 . Cluster Role if err := s . Add Generated Conversion Func ( ( * v1 . Policy Rule ) ( nil ) , ( * rbac . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Policy Rule_To_rbac_Policy Rule ( a . ( * v1 . Policy Rule ) , b . ( * rbac . Policy if err := s . Add Generated Conversion Func ( ( * rbac . Policy Rule ) ( nil ) , ( * v1 . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Policy Rule_To_v1_Policy Rule ( a . ( * rbac . Policy Rule ) , b . ( * v1 . Policy if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Role Binding ) ( nil ) , ( * rbac . Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Role Binding_To_rbac_Role Binding ( a . ( * v1 . Role Binding ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role Binding ) ( nil ) , ( * v1 . Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Binding_To_v1_Role Binding ( a . ( * rbac . Role Binding ) , b . ( * v1 . Role if err := s . Add Generated Conversion Func ( ( * v1 . Role Binding List ) ( nil ) , ( * rbac . Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Role Binding List_To_rbac_Role Binding List ( a . ( * v1 . Role Binding List ) , b . ( * rbac . Role Binding if err := s . Add Generated Conversion Func ( ( * rbac . Role Binding List ) ( nil ) , ( * v1 . Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Binding List_To_v1_Role Binding List ( a . ( * rbac . Role Binding List ) , b . ( * v1 . Role Binding if err := s . Add Generated Conversion Func ( ( * v1 . Role List ) ( nil ) , ( * rbac . Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Role List_To_rbac_Role List ( a . ( * v1 . Role List ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role List ) ( nil ) , ( * v1 . Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role List_To_v1_Role List ( a . ( * rbac . Role List ) , b . ( * v1 . Role if err := s . Add Generated Conversion Func ( ( * v1 . Role Ref ) ( nil ) , ( * rbac . Role Ref ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Role Ref_To_rbac_Role Ref ( a . ( * v1 . Role Ref ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role Ref ) ( nil ) , ( * v1 . Role Ref ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Ref_To_v1_Role Ref ( a . ( * rbac . Role Ref ) , b . ( * v1 . Role if err := s . Add Generated Conversion if err := s . Add Generated Conversion } 
func Convert_v1_Aggregation Rule_To_rbac_Aggregation Rule ( in * v1 . Aggregation Rule , out * rbac . Aggregation Rule , s conversion . Scope ) error { return auto Convert_v1_Aggregation Rule_To_rbac_Aggregation } 
func Convert_rbac_Aggregation Rule_To_v1_Aggregation Rule ( in * rbac . Aggregation Rule , out * v1 . Aggregation Rule , s conversion . Scope ) error { return auto Convert_rbac_Aggregation Rule_To_v1_Aggregation } 
func Convert_v1_Cluster Role_To_rbac_Cluster Role ( in * v1 . Cluster Role , out * rbac . Cluster Role , s conversion . Scope ) error { return auto Convert_v1_Cluster Role_To_rbac_Cluster } 
func Convert_rbac_Cluster Role_To_v1_Cluster Role ( in * rbac . Cluster Role , out * v1 . Cluster Role , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role_To_v1_Cluster } 
func Convert_v1_Cluster Role Binding_To_rbac_Cluster Role Binding ( in * v1 . Cluster Role Binding , out * rbac . Cluster Role Binding , s conversion . Scope ) error { return auto Convert_v1_Cluster Role Binding_To_rbac_Cluster Role } 
func Convert_rbac_Cluster Role Binding_To_v1_Cluster Role Binding ( in * rbac . Cluster Role Binding , out * v1 . Cluster Role Binding , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role Binding_To_v1_Cluster Role } 
func Convert_v1_Cluster Role Binding List_To_rbac_Cluster Role Binding List ( in * v1 . Cluster Role Binding List , out * rbac . Cluster Role Binding List , s conversion . Scope ) error { return auto Convert_v1_Cluster Role Binding List_To_rbac_Cluster Role Binding } 
func Convert_rbac_Cluster Role Binding List_To_v1_Cluster Role Binding List ( in * rbac . Cluster Role Binding List , out * v1 . Cluster Role Binding List , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role Binding List_To_v1_Cluster Role Binding } 
func Convert_v1_Cluster Role List_To_rbac_Cluster Role List ( in * v1 . Cluster Role List , out * rbac . Cluster Role List , s conversion . Scope ) error { return auto Convert_v1_Cluster Role List_To_rbac_Cluster Role } 
func Convert_rbac_Cluster Role List_To_v1_Cluster Role List ( in * rbac . Cluster Role List , out * v1 . Cluster Role List , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role List_To_v1_Cluster Role } 
func Convert_v1_Policy Rule_To_rbac_Policy Rule ( in * v1 . Policy Rule , out * rbac . Policy Rule , s conversion . Scope ) error { return auto Convert_v1_Policy Rule_To_rbac_Policy } 
func Convert_rbac_Policy Rule_To_v1_Policy Rule ( in * rbac . Policy Rule , out * v1 . Policy Rule , s conversion . Scope ) error { return auto Convert_rbac_Policy Rule_To_v1_Policy } 
func Convert_v1_Role_To_rbac_Role ( in * v1 . Role , out * rbac . Role , s conversion . Scope ) error { return auto } 
func Convert_rbac_Role_To_v1_Role ( in * rbac . Role , out * v1 . Role , s conversion . Scope ) error { return auto } 
func Convert_v1_Role Binding_To_rbac_Role Binding ( in * v1 . Role Binding , out * rbac . Role Binding , s conversion . Scope ) error { return auto Convert_v1_Role Binding_To_rbac_Role } 
func Convert_rbac_Role Binding_To_v1_Role Binding ( in * rbac . Role Binding , out * v1 . Role Binding , s conversion . Scope ) error { return auto Convert_rbac_Role Binding_To_v1_Role } 
func Convert_v1_Role Binding List_To_rbac_Role Binding List ( in * v1 . Role Binding List , out * rbac . Role Binding List , s conversion . Scope ) error { return auto Convert_v1_Role Binding List_To_rbac_Role Binding } 
func Convert_rbac_Role Binding List_To_v1_Role Binding List ( in * rbac . Role Binding List , out * v1 . Role Binding List , s conversion . Scope ) error { return auto Convert_rbac_Role Binding List_To_v1_Role Binding } 
func Convert_v1_Role List_To_rbac_Role List ( in * v1 . Role List , out * rbac . Role List , s conversion . Scope ) error { return auto Convert_v1_Role List_To_rbac_Role } 
func Convert_rbac_Role List_To_v1_Role List ( in * rbac . Role List , out * v1 . Role List , s conversion . Scope ) error { return auto Convert_rbac_Role List_To_v1_Role } 
func Convert_v1_Role Ref_To_rbac_Role Ref ( in * v1 . Role Ref , out * rbac . Role Ref , s conversion . Scope ) error { return auto Convert_v1_Role Ref_To_rbac_Role } 
func Convert_rbac_Role Ref_To_v1_Role Ref ( in * rbac . Role Ref , out * v1 . Role Ref , s conversion . Scope ) error { return auto Convert_rbac_Role Ref_To_v1_Role } 
func Convert_v1_Subject_To_rbac_Subject ( in * v1 . Subject , out * rbac . Subject , s conversion . Scope ) error { return auto } 
func Convert_rbac_Subject_To_v1_Subject ( in * rbac . Subject , out * v1 . Subject , s conversion . Scope ) error { return auto } 
func ( config Config ) New ( ) ( authenticator . Request , * spec . Security var token security Definitions := spec . Security // front-proxy, Basic Auth methods, local first, then remote // Add the front proxy authenticator if requested if config . Request Header Config != nil { request Header Authenticator , err := headerrequest . New Secure ( config . Request Header Config . Client CA , config . Request Header Config . Allowed Client Names , config . Request Header Config . Username Headers , config . Request Header Config . Group Headers , config . Request Header Config . Extra Header authenticators = append ( authenticators , authenticator . Wrap Audience Agnostic Request ( config . API Audiences , request Header // basic auth if len ( config . Basic Auth File ) > 0 { basic Auth , err := new Authenticator From Basic Auth File ( config . Basic Auth authenticators = append ( authenticators , authenticator . Wrap Audience Agnostic Request ( config . API Audiences , basic security Definitions [ " " ] = & spec . Security Scheme { Security Scheme Props : spec . Security Scheme // X509 methods if len ( config . Client CA File ) > 0 { cert Auth , err := new Authenticator From Client CA File ( config . Client CA authenticators = append ( authenticators , cert // Bearer token methods, local first, then remote if len ( config . Token Auth File ) > 0 { token Auth , err := new Authenticator From Token File ( config . Token Auth token Authenticators = append ( token Authenticators , authenticator . Wrap Audience Agnostic Token ( config . API Audiences , token if len ( config . Service Account Key Files ) > 0 { service Account Auth , err := new Legacy Service Account Authenticator ( config . Service Account Key Files , config . Service Account Lookup , config . API Audiences , config . Service Account Token token Authenticators = append ( token Authenticators , service Account if utilfeature . Default Feature Gate . Enabled ( features . Token Request ) && config . Service Account Issuer != " " { service Account Auth , err := new Service Account Authenticator ( config . Service Account Issuer , config . Service Account Key Files , config . API Audiences , config . Service Account Token token Authenticators = append ( token Authenticators , service Account if config . Bootstrap Token { if config . Bootstrap Token Authenticator != nil { // TODO: This can sometimes be nil because of token Authenticators = append ( token Authenticators , authenticator . Wrap Audience Agnostic Token ( config . API Audiences , config . Bootstrap Token // NOTE(ericchiang): Keep the Open ID Connect after Service Accounts. // // Because both plugins verify JW Ts whichever comes first in the union experiences // cache misses for all requests using the other. While the service account plugin // simply returns an error, the Open ID Connect plugin may query the provider to // update the keys, causing performance hits. if len ( config . OIDC Issuer URL ) > 0 && len ( config . OIDC Client ID ) > 0 { oidc Auth , err := new Authenticator From OIDC Issuer URL ( oidc . Options { Issuer URL : config . OIDC Issuer URL , Client ID : config . OIDC Client ID , API Audiences : config . API Audiences , CA File : config . OIDCCA File , Username Claim : config . OIDC Username Claim , Username Prefix : config . OIDC Username Prefix , Groups Claim : config . OIDC Groups Claim , Groups Prefix : config . OIDC Groups Prefix , Supported Signing Algs : config . OIDC Signing Algs , Required Claims : config . OIDC Required token Authenticators = append ( token Authenticators , oidc if len ( config . Webhook Token Authn Config File ) > 0 { webhook Token Auth , err := new Webhook Token Authenticator ( config . Webhook Token Authn Config File , config . Webhook Token Authn Cache TTL , config . API token Authenticators = append ( token Authenticators , webhook Token if len ( token Authenticators ) > 0 { // Union the token authenticators token Auth := tokenunion . New ( token // Optionally cache authentication results if config . Token Success Cache TTL > 0 || config . Token Failure Cache TTL > 0 { token Auth = tokencache . New ( token Auth , true , config . Token Success Cache TTL , config . Token Failure Cache authenticators = append ( authenticators , bearertoken . New ( token Auth ) , websocket . New Protocol Authenticator ( token security Definitions [ " " ] = & spec . Security Scheme { Security Scheme Props : spec . Security Scheme if len ( authenticators ) == 0 { if config . Anonymous { return anonymous . New Authenticator ( ) , & security return nil , & security authenticator = group . New Authenticated Group if config . Anonymous { // If the authenticator chain returns an error, return an error (don't consider a bad bearer token // or invalid username/password combination anonymous). authenticator = union . New Fail On Error ( authenticator , anonymous . New return authenticator , & security } 
func Is Valid Service Account Key File ( file string ) bool { _ , err := keyutil . Public Keys From } 
func new Authenticator From Basic Auth File ( basic Auth File string ) ( authenticator . Request , error ) { basic Authenticator , err := passwordfile . New CSV ( basic Auth return basicauth . New ( basic } 
func new Authenticator From Token File ( token Auth File string ) ( authenticator . Token , error ) { token Authenticator , err := tokenfile . New CSV ( token Auth return token } 
func new Authenticator From OIDC Issuer URL ( opts oidc . Options ) ( authenticator . Token , error ) { const no Username if opts . Username Prefix == " " && opts . Username Claim != " " { // Old behavior. If a username Prefix isn't provided, prefix all claims other than "email" // with the issuer URL. // // See https://github.com/kubernetes/kubernetes/issues/31380 opts . Username Prefix = opts . Issuer if opts . Username Prefix == no Username Prefix { // Special value indicating usernames shouldn't be prefixed. opts . Username token return token } 
func new Legacy Service Account Authenticator ( keyfiles [ ] string , lookup bool , api Audiences authenticator . Audiences , service Account Getter serviceaccount . Service Account Token Getter ) ( authenticator . Token , error ) { all Public for _ , keyfile := range keyfiles { public Keys , err := keyutil . Public Keys From all Public Keys = append ( all Public Keys , public token Authenticator := serviceaccount . JWT Token Authenticator ( serviceaccount . Legacy Issuer , all Public Keys , api Audiences , serviceaccount . New Legacy Validator ( lookup , service Account return token } 
func new Service Account Authenticator ( iss string , keyfiles [ ] string , api Audiences authenticator . Audiences , service Account Getter serviceaccount . Service Account Token Getter ) ( authenticator . Token , error ) { all Public for _ , keyfile := range keyfiles { public Keys , err := keyutil . Public Keys From all Public Keys = append ( all Public Keys , public token Authenticator := serviceaccount . JWT Token Authenticator ( iss , all Public Keys , api Audiences , serviceaccount . New Validator ( service Account return token } 
func new Authenticator From Client CA File ( client CA File string ) ( authenticator . Request , error ) { roots , err := certutil . New Pool ( client CA opts := x509 . Default Verify return x509 . New ( opts , x509 . Common Name User } 
func ( c * thread Safe Map ) List Keys ( ) [ ] string { c . lock . R defer c . lock . R } 
func ( c * thread Safe Map ) Index Keys ( index Name , index Key string ) ( [ ] string , error ) { c . lock . R defer c . lock . R index Func := c . indexers [ index if index Func == nil { return nil , fmt . Errorf ( " " , index index := c . indices [ index set := index [ index } 
func ( c * thread Safe Map ) update Indices ( old Obj interface { } , new Obj interface { } , key string ) { // if we got an old object, we need to remove it before we add it again if old Obj != nil { c . delete From Indices ( old for name , index Func := range c . indexers { index Values , err := index Func ( new for _ , index Value := range index Values { set := index [ index index [ index } 
func ( c * thread Safe Map ) delete From Indices ( obj interface { } , key string ) { for name , index Func := range c . indexers { index Values , err := index for _ , index Value := range index Values { set := index [ index } 
func Add Global Flags ( fs * pflag . Flag Set ) { add Klog add Cadvisor add Credential Provider verflag . Add logs . Add } 
func pflag Register ( global , local * pflag . Flag Set , global Name string ) { if f := global . Lookup ( global local . Add } else { panic ( fmt . Sprintf ( " " , global } 
func register Deprecated ( global * flag . Flag Set , local * pflag . Flag Set , global Name , deprecated string ) { register ( global , local , global local . Lookup ( normalize ( global } 
func add Credential Provider Flags ( fs * pflag . Flag Set ) { // lookup flags in global flag set and re-register the values with our flagset global := pflag . Command local := pflag . New Flag Set ( os . Args [ 0 ] , pflag . Exit On // TODO(#58034): This is not a static file, so it's not quite as straightforward as --google-json-key. // We need to figure out how ACR users can dynamically provide pull credentials before we can deprecate this. pflag fs . Add Flag } 
func add Klog Flags ( fs * pflag . Flag Set ) { local := flag . New Flag Set ( os . Args [ 0 ] , flag . Exit On klog . Init fs . Add Go Flag } 
func Fields To return set , fields } 
func Set To f := new str , err = Path Element } else { tf . Map [ str ] = new tf . Map [ " " ] = new f = remove Useless } 
func New Horizontal Pod Autoscaler Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Horizontal Pod Autoscaler Informer ( client , namespace , resync } 
func new Cache Round Tripper ( cache Dir string , rt http . Round Tripper ) http . Round Tripper { d := diskv . New ( diskv . Options { Base Path : cache Dir , Temp Dir : filepath . Join ( cache t := httpcache . New Transport ( diskcache . New With return & cache Round } 
func ( c * Autoscaling V2beta2Client ) REST return c . rest } 
func ( c * Controller ) sync To Stdout ( key string ) error { obj , exists , err := c . indexer . Get By } else { // Note that you also have to check the uid if you have a local controlled resource, which // is dependent on the actual instance, to detect that a Pod was recreated with the same name fmt . Printf ( " \n " , obj . ( * v1 . Pod ) . Get } 
func ( c * Controller ) handle Err ( err error , key interface { } ) { if err == nil { // Forget about the #Add Rate // This controller retries 5 times if something goes wrong. After that, it stops trying. if c . queue . Num // Re-enqueue the key rate limited. Based on the rate limiter on the // queue and the re-enqueue history, the key will be processed later again. c . queue . Add Rate // Report to an external entity that, even after several retries, we could not successfully process this key runtime . Handle } 
func ( in * AWS Elastic Block Store Volume Source ) Deep Copy ( ) * AWS Elastic Block Store Volume out := new ( AWS Elastic Block Store Volume in . Deep Copy } 
func ( in * Affinity ) Deep Copy if in . Node Affinity != nil { in , out := & in . Node Affinity , & out . Node * out = new ( Node ( * in ) . Deep Copy if in . Pod Affinity != nil { in , out := & in . Pod Affinity , & out . Pod * out = new ( Pod ( * in ) . Deep Copy if in . Pod Anti Affinity != nil { in , out := & in . Pod Anti Affinity , & out . Pod Anti * out = new ( Pod Anti ( * in ) . Deep Copy } 
func ( in * Affinity ) Deep in . Deep Copy } 
func ( in * Attached Volume ) Deep Copy ( ) * Attached out := new ( Attached in . Deep Copy } 
func ( in * Avoid Pods ) Deep Copy Into ( out * Avoid if in . Prefer Avoid Pods != nil { in , out := & in . Prefer Avoid Pods , & out . Prefer Avoid * out = make ( [ ] Prefer Avoid Pods for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Avoid Pods ) Deep Copy ( ) * Avoid out := new ( Avoid in . Deep Copy } 
func ( in * Azure Disk Volume Source ) Deep Copy Into ( out * Azure Disk Volume if in . Caching Mode != nil { in , out := & in . Caching Mode , & out . Caching * out = new ( Azure Data Disk Caching if in . FS Type != nil { in , out := & in . FS Type , & out . FS if in . Read Only != nil { in , out := & in . Read Only , & out . Read * out = new ( Azure Data Disk } 
func ( in * Azure Disk Volume Source ) Deep Copy ( ) * Azure Disk Volume out := new ( Azure Disk Volume in . Deep Copy } 
func ( in * Azure File Persistent Volume Source ) Deep Copy Into ( out * Azure File Persistent Volume if in . Secret Namespace != nil { in , out := & in . Secret Namespace , & out . Secret } 
func ( in * Azure File Persistent Volume Source ) Deep Copy ( ) * Azure File Persistent Volume out := new ( Azure File Persistent Volume in . Deep Copy } 
func ( in * Azure File Volume Source ) Deep Copy ( ) * Azure File Volume out := new ( Azure File Volume in . Deep Copy } 
func ( in * Binding ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object } 
func ( in * Binding ) Deep in . Deep Copy } 
func ( in * Binding ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * CSI Persistent Volume Source ) Deep Copy Into ( out * CSI Persistent Volume if in . Volume Attributes != nil { in , out := & in . Volume Attributes , & out . Volume if in . Controller Publish Secret Ref != nil { in , out := & in . Controller Publish Secret Ref , & out . Controller Publish Secret * out = new ( Secret if in . Node Stage Secret Ref != nil { in , out := & in . Node Stage Secret Ref , & out . Node Stage Secret * out = new ( Secret if in . Node Publish Secret Ref != nil { in , out := & in . Node Publish Secret Ref , & out . Node Publish Secret * out = new ( Secret } 
func ( in * CSI Persistent Volume Source ) Deep Copy ( ) * CSI Persistent Volume out := new ( CSI Persistent Volume in . Deep Copy } 
func ( in * CSI Volume Source ) Deep Copy Into ( out * CSI Volume if in . Read Only != nil { in , out := & in . Read Only , & out . Read if in . FS Type != nil { in , out := & in . FS Type , & out . FS if in . Volume Attributes != nil { in , out := & in . Volume Attributes , & out . Volume if in . Node Publish Secret Ref != nil { in , out := & in . Node Publish Secret Ref , & out . Node Publish Secret * out = new ( Local Object } 
func ( in * CSI Volume Source ) Deep Copy ( ) * CSI Volume out := new ( CSI Volume in . Deep Copy } 
func ( in * Capabilities ) Deep Copy } 
func ( in * Capabilities ) Deep in . Deep Copy } 
func ( in * Ceph FS Persistent Volume Source ) Deep Copy Into ( out * Ceph FS Persistent Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret } 
func ( in * Ceph FS Persistent Volume Source ) Deep Copy ( ) * Ceph FS Persistent Volume out := new ( Ceph FS Persistent Volume in . Deep Copy } 
func ( in * Ceph FS Volume Source ) Deep Copy Into ( out * Ceph FS Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object } 
func ( in * Ceph FS Volume Source ) Deep Copy ( ) * Ceph FS Volume out := new ( Ceph FS Volume in . Deep Copy } 
func ( in * Cinder Persistent Volume Source ) Deep Copy Into ( out * Cinder Persistent Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret } 
func ( in * Cinder Persistent Volume Source ) Deep Copy ( ) * Cinder Persistent Volume out := new ( Cinder Persistent Volume in . Deep Copy } 
func ( in * Cinder Volume Source ) Deep Copy Into ( out * Cinder Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object } 
func ( in * Cinder Volume Source ) Deep Copy ( ) * Cinder Volume out := new ( Cinder Volume in . Deep Copy } 
func ( in * Client IP Config ) Deep Copy Into ( out * Client IP if in . Timeout Seconds != nil { in , out := & in . Timeout Seconds , & out . Timeout } 
func ( in * Client IP Config ) Deep Copy ( ) * Client IP out := new ( Client IP in . Deep Copy } 
func ( in * Component Condition ) Deep Copy ( ) * Component out := new ( Component in . Deep Copy } 
func ( in * Component Status ) Deep Copy Into ( out * Component out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object * out = make ( [ ] Component } 
func ( in * Component Status ) Deep Copy ( ) * Component out := new ( Component in . Deep Copy } 
func ( in * Component Status ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Component Status List ) Deep Copy Into ( out * Component Status out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Component for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Component Status List ) Deep Copy ( ) * Component Status out := new ( Component Status in . Deep Copy } 
func ( in * Component Status List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Config Map ) Deep Copy Into ( out * Config out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object if in . Binary Data != nil { in , out := & in . Binary Data , & out . Binary for key , val := range * in { var out } else { in , out := & val , & out ( * out ) [ key ] = out } 
func ( in * Config Map ) Deep Copy ( ) * Config out := new ( Config in . Deep Copy } 
func ( in * Config Map ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Config Map Env Source ) Deep Copy ( ) * Config Map Env out := new ( Config Map Env in . Deep Copy } 
func ( in * Config Map Key Selector ) Deep Copy ( ) * Config Map Key out := new ( Config Map Key in . Deep Copy } 
func ( in * Config Map List ) Deep Copy Into ( out * Config Map out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Config for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Config Map List ) Deep Copy ( ) * Config Map out := new ( Config Map in . Deep Copy } 
func ( in * Config Map List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Config Map Node Config Source ) Deep Copy ( ) * Config Map Node Config out := new ( Config Map Node Config in . Deep Copy } 
func ( in * Config Map Projection ) Deep Copy ( ) * Config Map out := new ( Config Map in . Deep Copy } 
func ( in * Config Map Volume Source ) Deep Copy ( ) * Config Map Volume out := new ( Config Map Volume in . Deep Copy } 
func ( in * Container ) Deep Copy * out = make ( [ ] Container if in . Env From != nil { in , out := & in . Env From , & out . Env * out = make ( [ ] Env From for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Env for i := range * in { ( * in ) [ i ] . Deep Copy in . Resources . Deep Copy if in . Volume Mounts != nil { in , out := & in . Volume Mounts , & out . Volume * out = make ( [ ] Volume for i := range * in { ( * in ) [ i ] . Deep Copy if in . Volume Devices != nil { in , out := & in . Volume Devices , & out . Volume * out = make ( [ ] Volume if in . Liveness Probe != nil { in , out := & in . Liveness Probe , & out . Liveness ( * in ) . Deep Copy if in . Readiness Probe != nil { in , out := & in . Readiness Probe , & out . Readiness ( * in ) . Deep Copy ( * in ) . Deep Copy if in . Security Context != nil { in , out := & in . Security Context , & out . Security * out = new ( Security ( * in ) . Deep Copy } 
func ( in * Container ) Deep in . Deep Copy } 
func ( in * Container Image ) Deep Copy Into ( out * Container } 
func ( in * Container Image ) Deep Copy ( ) * Container out := new ( Container in . Deep Copy } 
func ( in * Container Port ) Deep Copy ( ) * Container out := new ( Container in . Deep Copy } 
func ( in * Container State ) Deep Copy Into ( out * Container * out = new ( Container State * out = new ( Container State ( * in ) . Deep Copy * out = new ( Container State ( * in ) . Deep Copy } 
func ( in * Container State ) Deep Copy ( ) * Container out := new ( Container in . Deep Copy } 
func ( in * Container State Running ) Deep Copy Into ( out * Container State in . Started At . Deep Copy Into ( & out . Started } 
func ( in * Container State Running ) Deep Copy ( ) * Container State out := new ( Container State in . Deep Copy } 
func ( in * Container State Terminated ) Deep Copy Into ( out * Container State in . Started At . Deep Copy Into ( & out . Started in . Finished At . Deep Copy Into ( & out . Finished } 
func ( in * Container State Terminated ) Deep Copy ( ) * Container State out := new ( Container State in . Deep Copy } 
func ( in * Container State Waiting ) Deep Copy ( ) * Container State out := new ( Container State in . Deep Copy } 
func ( in * Container Status ) Deep Copy Into ( out * Container in . State . Deep Copy in . Last Termination State . Deep Copy Into ( & out . Last Termination } 
func ( in * Container Status ) Deep Copy ( ) * Container out := new ( Container in . Deep Copy } 
func ( in * Daemon Endpoint ) Deep Copy ( ) * Daemon out := new ( Daemon in . Deep Copy } 
func ( in * Downward API Projection ) Deep Copy Into ( out * Downward API * out = make ( [ ] Downward API Volume for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Downward API Projection ) Deep Copy ( ) * Downward API out := new ( Downward API in . Deep Copy } 
func ( in * Downward API Volume File ) Deep Copy Into ( out * Downward API Volume if in . Field Ref != nil { in , out := & in . Field Ref , & out . Field * out = new ( Object Field if in . Resource Field Ref != nil { in , out := & in . Resource Field Ref , & out . Resource Field * out = new ( Resource Field ( * in ) . Deep Copy } 
func ( in * Downward API Volume File ) Deep Copy ( ) * Downward API Volume out := new ( Downward API Volume in . Deep Copy } 
func ( in * Downward API Volume Source ) Deep Copy Into ( out * Downward API Volume * out = make ( [ ] Downward API Volume for i := range * in { ( * in ) [ i ] . Deep Copy if in . Default Mode != nil { in , out := & in . Default Mode , & out . Default } 
func ( in * Downward API Volume Source ) Deep Copy ( ) * Downward API Volume out := new ( Downward API Volume in . Deep Copy } 
func ( in * Empty Dir Volume Source ) Deep Copy Into ( out * Empty Dir Volume if in . Size Limit != nil { in , out := & in . Size Limit , & out . Size x := ( * in ) . Deep } 
func ( in * Empty Dir Volume Source ) Deep Copy ( ) * Empty Dir Volume out := new ( Empty Dir Volume in . Deep Copy } 
func ( in * Endpoint Address ) Deep Copy Into ( out * Endpoint if in . Node Name != nil { in , out := & in . Node Name , & out . Node if in . Target Ref != nil { in , out := & in . Target Ref , & out . Target * out = new ( Object } 
func ( in * Endpoint Address ) Deep Copy ( ) * Endpoint out := new ( Endpoint in . Deep Copy } 
func ( in * Endpoint Port ) Deep Copy ( ) * Endpoint out := new ( Endpoint in . Deep Copy } 
func ( in * Endpoint Subset ) Deep Copy Into ( out * Endpoint * out = make ( [ ] Endpoint for i := range * in { ( * in ) [ i ] . Deep Copy if in . Not Ready Addresses != nil { in , out := & in . Not Ready Addresses , & out . Not Ready * out = make ( [ ] Endpoint for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Endpoint } 
func ( in * Endpoint Subset ) Deep Copy ( ) * Endpoint out := new ( Endpoint in . Deep Copy } 
func ( in * Endpoints ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object * out = make ( [ ] Endpoint for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Endpoints ) Deep in . Deep Copy } 
func ( in * Endpoints ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Endpoints List ) Deep Copy Into ( out * Endpoints out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Endpoints List ) Deep Copy ( ) * Endpoints out := new ( Endpoints in . Deep Copy } 
func ( in * Endpoints List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Env From Source ) Deep Copy Into ( out * Env From if in . Config Map Ref != nil { in , out := & in . Config Map Ref , & out . Config Map * out = new ( Config Map Env ( * in ) . Deep Copy if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret Env ( * in ) . Deep Copy } 
func ( in * Env From Source ) Deep Copy ( ) * Env From out := new ( Env From in . Deep Copy } 
func ( in * Env Var ) Deep Copy Into ( out * Env if in . Value From != nil { in , out := & in . Value From , & out . Value * out = new ( Env Var ( * in ) . Deep Copy } 
func ( in * Env Var ) Deep Copy ( ) * Env out := new ( Env in . Deep Copy } 
func ( in * Env Var Source ) Deep Copy Into ( out * Env Var if in . Field Ref != nil { in , out := & in . Field Ref , & out . Field * out = new ( Object Field if in . Resource Field Ref != nil { in , out := & in . Resource Field Ref , & out . Resource Field * out = new ( Resource Field ( * in ) . Deep Copy if in . Config Map Key Ref != nil { in , out := & in . Config Map Key Ref , & out . Config Map Key * out = new ( Config Map Key ( * in ) . Deep Copy if in . Secret Key Ref != nil { in , out := & in . Secret Key Ref , & out . Secret Key * out = new ( Secret Key ( * in ) . Deep Copy } 
func ( in * Env Var Source ) Deep Copy ( ) * Env Var out := new ( Env Var in . Deep Copy } 
func ( in * Event ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object out . Involved Object = in . Involved in . First Timestamp . Deep Copy Into ( & out . First in . Last Timestamp . Deep Copy Into ( & out . Last in . Event Time . Deep Copy Into ( & out . Event * out = new ( Event ( * in ) . Deep Copy * out = new ( Object } 
func ( in * Event ) Deep in . Deep Copy } 
func ( in * Event ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Event List ) Deep Copy Into ( out * Event out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Event List ) Deep Copy ( ) * Event out := new ( Event in . Deep Copy } 
func ( in * Event List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Event Series ) Deep Copy Into ( out * Event in . Last Observed Time . Deep Copy Into ( & out . Last Observed } 
func ( in * Event Series ) Deep Copy ( ) * Event out := new ( Event in . Deep Copy } 
func ( in * Event Source ) Deep Copy ( ) * Event out := new ( Event in . Deep Copy } 
func ( in * Exec Action ) Deep Copy Into ( out * Exec } 
func ( in * Exec Action ) Deep Copy ( ) * Exec out := new ( Exec in . Deep Copy } 
func ( in * FC Volume Source ) Deep Copy Into ( out * FC Volume if in . Target WW Ns != nil { in , out := & in . Target WW Ns , & out . Target WW if in . WWI Ds != nil { in , out := & in . WWI Ds , & out . WWI } 
func ( in * FC Volume Source ) Deep Copy ( ) * FC Volume out := new ( FC Volume in . Deep Copy } 
func ( in * Flex Persistent Volume Source ) Deep Copy Into ( out * Flex Persistent Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret } 
func ( in * Flex Persistent Volume Source ) Deep Copy ( ) * Flex Persistent Volume out := new ( Flex Persistent Volume in . Deep Copy } 
func ( in * Flex Volume Source ) Deep Copy Into ( out * Flex Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object } 
func ( in * Flex Volume Source ) Deep Copy ( ) * Flex Volume out := new ( Flex Volume in . Deep Copy } 
func ( in * Flocker Volume Source ) Deep Copy ( ) * Flocker Volume out := new ( Flocker Volume in . Deep Copy } 
func ( in * GCE Persistent Disk Volume Source ) Deep Copy ( ) * GCE Persistent Disk Volume out := new ( GCE Persistent Disk Volume in . Deep Copy } 
func ( in * Git Repo Volume Source ) Deep Copy ( ) * Git Repo Volume out := new ( Git Repo Volume in . Deep Copy } 
func ( in * Glusterfs Persistent Volume Source ) Deep Copy Into ( out * Glusterfs Persistent Volume if in . Endpoints Namespace != nil { in , out := & in . Endpoints Namespace , & out . Endpoints } 
func ( in * Glusterfs Persistent Volume Source ) Deep Copy ( ) * Glusterfs Persistent Volume out := new ( Glusterfs Persistent Volume in . Deep Copy } 
func ( in * Glusterfs Volume Source ) Deep Copy ( ) * Glusterfs Volume out := new ( Glusterfs Volume in . Deep Copy } 
func ( in * HTTP Get Action ) Deep Copy Into ( out * HTTP Get if in . HTTP Headers != nil { in , out := & in . HTTP Headers , & out . HTTP * out = make ( [ ] HTTP } 
func ( in * HTTP Get Action ) Deep Copy ( ) * HTTP Get out := new ( HTTP Get in . Deep Copy } 
func ( in * HTTP Header ) Deep Copy ( ) * HTTP out := new ( HTTP in . Deep Copy } 
func ( in * Handler ) Deep Copy * out = new ( Exec ( * in ) . Deep Copy if in . HTTP Get != nil { in , out := & in . HTTP Get , & out . HTTP * out = new ( HTTP Get ( * in ) . Deep Copy if in . TCP Socket != nil { in , out := & in . TCP Socket , & out . TCP * out = new ( TCP Socket } 
func ( in * Handler ) Deep in . Deep Copy } 
func ( in * Host Alias ) Deep Copy Into ( out * Host } 
func ( in * Host Alias ) Deep Copy ( ) * Host out := new ( Host in . Deep Copy } 
func ( in * Host Path Volume Source ) Deep Copy Into ( out * Host Path Volume * out = new ( Host Path } 
func ( in * Host Path Volume Source ) Deep Copy ( ) * Host Path Volume out := new ( Host Path Volume in . Deep Copy } 
func ( in * ISCSI Persistent Volume Source ) Deep Copy Into ( out * ISCSI Persistent Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret if in . Initiator Name != nil { in , out := & in . Initiator Name , & out . Initiator } 
func ( in * ISCSI Persistent Volume Source ) Deep Copy ( ) * ISCSI Persistent Volume out := new ( ISCSI Persistent Volume in . Deep Copy } 
func ( in * ISCSI Volume Source ) Deep Copy Into ( out * ISCSI Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object if in . Initiator Name != nil { in , out := & in . Initiator Name , & out . Initiator } 
func ( in * ISCSI Volume Source ) Deep Copy ( ) * ISCSI Volume out := new ( ISCSI Volume in . Deep Copy } 
func ( in * Key To Path ) Deep Copy Into ( out * Key To } 
func ( in * Key To Path ) Deep Copy ( ) * Key To out := new ( Key To in . Deep Copy } 
func ( in * Lifecycle ) Deep Copy if in . Post Start != nil { in , out := & in . Post Start , & out . Post ( * in ) . Deep Copy if in . Pre Stop != nil { in , out := & in . Pre Stop , & out . Pre ( * in ) . Deep Copy } 
func ( in * Lifecycle ) Deep in . Deep Copy } 
func ( in * Limit Range ) Deep Copy Into ( out * Limit out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Limit Range ) Deep Copy ( ) * Limit out := new ( Limit in . Deep Copy } 
func ( in * Limit Range ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Limit Range Item ) Deep Copy Into ( out * Limit Range * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep if in . Default Request != nil { in , out := & in . Default Request , & out . Default * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep if in . Max Limit Request Ratio != nil { in , out := & in . Max Limit Request Ratio , & out . Max Limit Request * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep } 
func ( in * Limit Range Item ) Deep Copy ( ) * Limit Range out := new ( Limit Range in . Deep Copy } 
func ( in * Limit Range List ) Deep Copy Into ( out * Limit Range out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Limit for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Limit Range List ) Deep Copy ( ) * Limit Range out := new ( Limit Range in . Deep Copy } 
func ( in * Limit Range List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Limit Range Spec ) Deep Copy Into ( out * Limit Range * out = make ( [ ] Limit Range for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Limit Range Spec ) Deep Copy ( ) * Limit Range out := new ( Limit Range in . Deep Copy } 
func ( in * List ) Deep in . Deep Copy } 
func ( in * List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Load Balancer Ingress ) Deep Copy ( ) * Load Balancer out := new ( Load Balancer in . Deep Copy } 
func ( in * Load Balancer Status ) Deep Copy Into ( out * Load Balancer * out = make ( [ ] Load Balancer } 
func ( in * Load Balancer Status ) Deep Copy ( ) * Load Balancer out := new ( Load Balancer in . Deep Copy } 
func ( in * Local Object Reference ) Deep Copy ( ) * Local Object out := new ( Local Object in . Deep Copy } 
func ( in * Local Volume Source ) Deep Copy Into ( out * Local Volume if in . FS Type != nil { in , out := & in . FS Type , & out . FS } 
func ( in * Local Volume Source ) Deep Copy ( ) * Local Volume out := new ( Local Volume in . Deep Copy } 
func ( in * NFS Volume Source ) Deep Copy ( ) * NFS Volume out := new ( NFS Volume in . Deep Copy } 
func ( in * Namespace ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Namespace ) Deep in . Deep Copy } 
func ( in * Namespace ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Namespace List ) Deep Copy Into ( out * Namespace out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Namespace List ) Deep Copy ( ) * Namespace out := new ( Namespace in . Deep Copy } 
func ( in * Namespace List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Namespace Spec ) Deep Copy Into ( out * Namespace * out = make ( [ ] Finalizer } 
func ( in * Namespace Spec ) Deep Copy ( ) * Namespace out := new ( Namespace in . Deep Copy } 
func ( in * Namespace Status ) Deep Copy ( ) * Namespace out := new ( Namespace in . Deep Copy } 
func ( in * Node ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Node ) Deep in . Deep Copy } 
func ( in * Node ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Node Address ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node Affinity ) Deep Copy Into ( out * Node if in . Required During Scheduling Ignored During Execution != nil { in , out := & in . Required During Scheduling Ignored During Execution , & out . Required During Scheduling Ignored During * out = new ( Node ( * in ) . Deep Copy if in . Preferred During Scheduling Ignored During Execution != nil { in , out := & in . Preferred During Scheduling Ignored During Execution , & out . Preferred During Scheduling Ignored During * out = make ( [ ] Preferred Scheduling for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Node Affinity ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node Condition ) Deep Copy Into ( out * Node in . Last Heartbeat Time . Deep Copy Into ( & out . Last Heartbeat in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Node Condition ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node Config Source ) Deep Copy Into ( out * Node Config if in . Config Map != nil { in , out := & in . Config Map , & out . Config * out = new ( Config Map Node Config } 
func ( in * Node Config Source ) Deep Copy ( ) * Node Config out := new ( Node Config in . Deep Copy } 
func ( in * Node Config Status ) Deep Copy Into ( out * Node Config * out = new ( Node Config ( * in ) . Deep Copy * out = new ( Node Config ( * in ) . Deep Copy if in . Last Known Good != nil { in , out := & in . Last Known Good , & out . Last Known * out = new ( Node Config ( * in ) . Deep Copy } 
func ( in * Node Config Status ) Deep Copy ( ) * Node Config out := new ( Node Config in . Deep Copy } 
func ( in * Node Daemon Endpoints ) Deep Copy Into ( out * Node Daemon out . Kubelet Endpoint = in . Kubelet } 
func ( in * Node Daemon Endpoints ) Deep Copy ( ) * Node Daemon out := new ( Node Daemon in . Deep Copy } 
func ( in * Node List ) Deep Copy Into ( out * Node out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Node List ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Node Proxy Options ) Deep Copy Into ( out * Node Proxy out . Type Meta = in . Type } 
func ( in * Node Proxy Options ) Deep Copy ( ) * Node Proxy out := new ( Node Proxy in . Deep Copy } 
func ( in * Node Proxy Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Node Resources ) Deep Copy Into ( out * Node * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep } 
func ( in * Node Resources ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node Selector ) Deep Copy Into ( out * Node if in . Node Selector Terms != nil { in , out := & in . Node Selector Terms , & out . Node Selector * out = make ( [ ] Node Selector for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Node Selector ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node Selector Requirement ) Deep Copy ( ) * Node Selector out := new ( Node Selector in . Deep Copy } 
func ( in * Node Selector Term ) Deep Copy Into ( out * Node Selector if in . Match Expressions != nil { in , out := & in . Match Expressions , & out . Match * out = make ( [ ] Node Selector for i := range * in { ( * in ) [ i ] . Deep Copy if in . Match Fields != nil { in , out := & in . Match Fields , & out . Match * out = make ( [ ] Node Selector for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Node Selector Term ) Deep Copy ( ) * Node Selector out := new ( Node Selector in . Deep Copy } 
func ( in * Node Spec ) Deep Copy Into ( out * Node for i := range * in { ( * in ) [ i ] . Deep Copy if in . Config Source != nil { in , out := & in . Config Source , & out . Config * out = new ( Node Config ( * in ) . Deep Copy } 
func ( in * Node Spec ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node Status ) Deep Copy Into ( out * Node * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( [ ] Node for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Node out . Daemon Endpoints = in . Daemon out . Node Info = in . Node * out = make ( [ ] Container for i := range * in { ( * in ) [ i ] . Deep Copy if in . Volumes In Use != nil { in , out := & in . Volumes In Use , & out . Volumes In * out = make ( [ ] Unique Volume if in . Volumes Attached != nil { in , out := & in . Volumes Attached , & out . Volumes * out = make ( [ ] Attached * out = new ( Node Config ( * in ) . Deep Copy } 
func ( in * Node Status ) Deep Copy ( ) * Node out := new ( Node in . Deep Copy } 
func ( in * Node System Info ) Deep Copy ( ) * Node System out := new ( Node System in . Deep Copy } 
func ( in * Object Field Selector ) Deep Copy ( ) * Object Field out := new ( Object Field in . Deep Copy } 
func ( in * Object Reference ) Deep Copy ( ) * Object out := new ( Object in . Deep Copy } 
func ( in * Object Reference ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Persistent Volume ) Deep Copy Into ( out * Persistent out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Persistent Volume ) Deep Copy ( ) * Persistent out := new ( Persistent in . Deep Copy } 
func ( in * Persistent Volume ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Persistent Volume Claim ) Deep Copy Into ( out * Persistent Volume out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Persistent Volume Claim ) Deep Copy ( ) * Persistent Volume out := new ( Persistent Volume in . Deep Copy } 
func ( in * Persistent Volume Claim ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Persistent Volume Claim Condition ) Deep Copy Into ( out * Persistent Volume Claim in . Last Probe Time . Deep Copy Into ( & out . Last Probe in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Persistent Volume Claim Condition ) Deep Copy ( ) * Persistent Volume Claim out := new ( Persistent Volume Claim in . Deep Copy } 
func ( in * Persistent Volume Claim List ) Deep Copy Into ( out * Persistent Volume Claim out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Persistent Volume for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Persistent Volume Claim List ) Deep Copy ( ) * Persistent Volume Claim out := new ( Persistent Volume Claim in . Deep Copy } 
func ( in * Persistent Volume Claim List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Persistent Volume Claim Spec ) Deep Copy Into ( out * Persistent Volume Claim if in . Access Modes != nil { in , out := & in . Access Modes , & out . Access * out = make ( [ ] Persistent Volume Access * out = new ( v1 . Label ( * in ) . Deep Copy in . Resources . Deep Copy if in . Storage Class Name != nil { in , out := & in . Storage Class Name , & out . Storage Class if in . Volume Mode != nil { in , out := & in . Volume Mode , & out . Volume * out = new ( Persistent Volume if in . Data Source != nil { in , out := & in . Data Source , & out . Data * out = new ( Typed Local Object ( * in ) . Deep Copy } 
func ( in * Persistent Volume Claim Spec ) Deep Copy ( ) * Persistent Volume Claim out := new ( Persistent Volume Claim in . Deep Copy } 
func ( in * Persistent Volume Claim Status ) Deep Copy Into ( out * Persistent Volume Claim if in . Access Modes != nil { in , out := & in . Access Modes , & out . Access * out = make ( [ ] Persistent Volume Access * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( [ ] Persistent Volume Claim for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Persistent Volume Claim Status ) Deep Copy ( ) * Persistent Volume Claim out := new ( Persistent Volume Claim in . Deep Copy } 
func ( in * Persistent Volume Claim Volume Source ) Deep Copy ( ) * Persistent Volume Claim Volume out := new ( Persistent Volume Claim Volume in . Deep Copy } 
func ( in * Persistent Volume List ) Deep Copy Into ( out * Persistent Volume out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Persistent for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Persistent Volume List ) Deep Copy ( ) * Persistent Volume out := new ( Persistent Volume in . Deep Copy } 
func ( in * Persistent Volume List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Persistent Volume Source ) Deep Copy Into ( out * Persistent Volume if in . GCE Persistent Disk != nil { in , out := & in . GCE Persistent Disk , & out . GCE Persistent * out = new ( GCE Persistent Disk Volume if in . AWS Elastic Block Store != nil { in , out := & in . AWS Elastic Block Store , & out . AWS Elastic Block * out = new ( AWS Elastic Block Store Volume if in . Host Path != nil { in , out := & in . Host Path , & out . Host * out = new ( Host Path Volume ( * in ) . Deep Copy * out = new ( Glusterfs Persistent Volume ( * in ) . Deep Copy * out = new ( NFS Volume * out = new ( RBD Persistent Volume ( * in ) . Deep Copy * out = new ( Quobyte Volume * out = new ( ISCSI Persistent Volume ( * in ) . Deep Copy if in . Flex Volume != nil { in , out := & in . Flex Volume , & out . Flex * out = new ( Flex Persistent Volume ( * in ) . Deep Copy * out = new ( Cinder Persistent Volume ( * in ) . Deep Copy if in . Ceph FS != nil { in , out := & in . Ceph FS , & out . Ceph * out = new ( Ceph FS Persistent Volume ( * in ) . Deep Copy * out = new ( FC Volume ( * in ) . Deep Copy * out = new ( Flocker Volume if in . Azure File != nil { in , out := & in . Azure File , & out . Azure * out = new ( Azure File Persistent Volume ( * in ) . Deep Copy if in . Vsphere Volume != nil { in , out := & in . Vsphere Volume , & out . Vsphere * out = new ( Vsphere Virtual Disk Volume if in . Azure Disk != nil { in , out := & in . Azure Disk , & out . Azure * out = new ( Azure Disk Volume ( * in ) . Deep Copy if in . Photon Persistent Disk != nil { in , out := & in . Photon Persistent Disk , & out . Photon Persistent * out = new ( Photon Persistent Disk Volume if in . Portworx Volume != nil { in , out := & in . Portworx Volume , & out . Portworx * out = new ( Portworx Volume if in . Scale IO != nil { in , out := & in . Scale IO , & out . Scale * out = new ( Scale IO Persistent Volume ( * in ) . Deep Copy * out = new ( Local Volume ( * in ) . Deep Copy if in . Storage OS != nil { in , out := & in . Storage OS , & out . Storage * out = new ( Storage OS Persistent Volume ( * in ) . Deep Copy * out = new ( CSI Persistent Volume ( * in ) . Deep Copy } 
func ( in * Persistent Volume Source ) Deep Copy ( ) * Persistent Volume out := new ( Persistent Volume in . Deep Copy } 
func ( in * Persistent Volume Spec ) Deep Copy Into ( out * Persistent Volume * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep in . Persistent Volume Source . Deep Copy Into ( & out . Persistent Volume if in . Access Modes != nil { in , out := & in . Access Modes , & out . Access * out = make ( [ ] Persistent Volume Access if in . Claim Ref != nil { in , out := & in . Claim Ref , & out . Claim * out = new ( Object if in . Mount Options != nil { in , out := & in . Mount Options , & out . Mount if in . Volume Mode != nil { in , out := & in . Volume Mode , & out . Volume * out = new ( Persistent Volume if in . Node Affinity != nil { in , out := & in . Node Affinity , & out . Node * out = new ( Volume Node ( * in ) . Deep Copy } 
func ( in * Persistent Volume Spec ) Deep Copy ( ) * Persistent Volume out := new ( Persistent Volume in . Deep Copy } 
func ( in * Persistent Volume Status ) Deep Copy ( ) * Persistent Volume out := new ( Persistent Volume in . Deep Copy } 
func ( in * Photon Persistent Disk Volume Source ) Deep Copy ( ) * Photon Persistent Disk Volume out := new ( Photon Persistent Disk Volume in . Deep Copy } 
func ( in * Pod ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Pod ) Deep in . Deep Copy } 
func ( in * Pod ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Affinity ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod Affinity Term ) Deep Copy ( ) * Pod Affinity out := new ( Pod Affinity in . Deep Copy } 
func ( in * Pod Anti Affinity ) Deep Copy Into ( out * Pod Anti if in . Required During Scheduling Ignored During Execution != nil { in , out := & in . Required During Scheduling Ignored During Execution , & out . Required During Scheduling Ignored During * out = make ( [ ] Pod Affinity for i := range * in { ( * in ) [ i ] . Deep Copy if in . Preferred During Scheduling Ignored During Execution != nil { in , out := & in . Preferred During Scheduling Ignored During Execution , & out . Preferred During Scheduling Ignored During * out = make ( [ ] Weighted Pod Affinity for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Anti Affinity ) Deep Copy ( ) * Pod Anti out := new ( Pod Anti in . Deep Copy } 
func ( in * Pod Attach Options ) Deep Copy Into ( out * Pod Attach out . Type Meta = in . Type } 
func ( in * Pod Attach Options ) Deep Copy ( ) * Pod Attach out := new ( Pod Attach in . Deep Copy } 
func ( in * Pod Attach Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Condition ) Deep Copy Into ( out * Pod in . Last Probe Time . Deep Copy Into ( & out . Last Probe in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Pod Condition ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod DNS Config ) Deep Copy Into ( out * Pod DNS * out = make ( [ ] Pod DNS Config for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod DNS Config ) Deep Copy ( ) * Pod DNS out := new ( Pod DNS in . Deep Copy } 
func ( in * Pod DNS Config Option ) Deep Copy Into ( out * Pod DNS Config } 
func ( in * Pod DNS Config Option ) Deep Copy ( ) * Pod DNS Config out := new ( Pod DNS Config in . Deep Copy } 
func ( in * Pod Exec Options ) Deep Copy Into ( out * Pod Exec out . Type Meta = in . Type } 
func ( in * Pod Exec Options ) Deep Copy ( ) * Pod Exec out := new ( Pod Exec in . Deep Copy } 
func ( in * Pod Exec Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod List ) Deep Copy Into ( out * Pod out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod List ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Log Options ) Deep Copy ( ) * Pod Log out := new ( Pod Log in . Deep Copy } 
func ( in * Pod Log Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Port Forward Options ) Deep Copy Into ( out * Pod Port Forward out . Type Meta = in . Type } 
func ( in * Pod Port Forward Options ) Deep Copy ( ) * Pod Port Forward out := new ( Pod Port Forward in . Deep Copy } 
func ( in * Pod Port Forward Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Proxy Options ) Deep Copy Into ( out * Pod Proxy out . Type Meta = in . Type } 
func ( in * Pod Proxy Options ) Deep Copy ( ) * Pod Proxy out := new ( Pod Proxy in . Deep Copy } 
func ( in * Pod Proxy Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Readiness Gate ) Deep Copy ( ) * Pod Readiness out := new ( Pod Readiness in . Deep Copy } 
func ( in * Pod Security Context ) Deep Copy Into ( out * Pod Security if in . Share Process Namespace != nil { in , out := & in . Share Process Namespace , & out . Share Process if in . SE Linux Options != nil { in , out := & in . SE Linux Options , & out . SE Linux * out = new ( SE Linux if in . Windows Options != nil { in , out := & in . Windows Options , & out . Windows * out = new ( Windows Security Context if in . Run As User != nil { in , out := & in . Run As User , & out . Run As if in . Run As Group != nil { in , out := & in . Run As Group , & out . Run As if in . Run As Non Root != nil { in , out := & in . Run As Non Root , & out . Run As Non if in . Supplemental Groups != nil { in , out := & in . Supplemental Groups , & out . Supplemental if in . FS Group != nil { in , out := & in . FS Group , & out . FS } 
func ( in * Pod Security Context ) Deep Copy ( ) * Pod Security out := new ( Pod Security in . Deep Copy } 
func ( in * Pod Signature ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod Spec ) Deep Copy Into ( out * Pod for i := range * in { ( * in ) [ i ] . Deep Copy if in . Init Containers != nil { in , out := & in . Init Containers , & out . Init for i := range * in { ( * in ) [ i ] . Deep Copy for i := range * in { ( * in ) [ i ] . Deep Copy if in . Termination Grace Period Seconds != nil { in , out := & in . Termination Grace Period Seconds , & out . Termination Grace Period if in . Active Deadline Seconds != nil { in , out := & in . Active Deadline Seconds , & out . Active Deadline if in . Node Selector != nil { in , out := & in . Node Selector , & out . Node if in . Automount Service Account Token != nil { in , out := & in . Automount Service Account Token , & out . Automount Service Account if in . Security Context != nil { in , out := & in . Security Context , & out . Security * out = new ( Pod Security ( * in ) . Deep Copy if in . Image Pull Secrets != nil { in , out := & in . Image Pull Secrets , & out . Image Pull * out = make ( [ ] Local Object ( * in ) . Deep Copy for i := range * in { ( * in ) [ i ] . Deep Copy if in . Host Aliases != nil { in , out := & in . Host Aliases , & out . Host * out = make ( [ ] Host for i := range * in { ( * in ) [ i ] . Deep Copy if in . DNS Config != nil { in , out := & in . DNS Config , & out . DNS * out = new ( Pod DNS ( * in ) . Deep Copy if in . Readiness Gates != nil { in , out := & in . Readiness Gates , & out . Readiness * out = make ( [ ] Pod Readiness if in . Runtime Class Name != nil { in , out := & in . Runtime Class Name , & out . Runtime Class if in . Enable Service Links != nil { in , out := & in . Enable Service Links , & out . Enable Service } 
func ( in * Pod Spec ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod Status ) Deep Copy Into ( out * Pod * out = make ( [ ] Pod for i := range * in { ( * in ) [ i ] . Deep Copy if in . Start Time != nil { in , out := & in . Start Time , & out . Start * out = ( * in ) . Deep if in . Init Container Statuses != nil { in , out := & in . Init Container Statuses , & out . Init Container * out = make ( [ ] Container for i := range * in { ( * in ) [ i ] . Deep Copy if in . Container Statuses != nil { in , out := & in . Container Statuses , & out . Container * out = make ( [ ] Container for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Status ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod Status Result ) Deep Copy Into ( out * Pod Status out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Status . Deep Copy } 
func ( in * Pod Status Result ) Deep Copy ( ) * Pod Status out := new ( Pod Status in . Deep Copy } 
func ( in * Pod Status Result ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Template ) Deep Copy Into ( out * Pod out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Template . Deep Copy } 
func ( in * Pod Template ) Deep Copy ( ) * Pod out := new ( Pod in . Deep Copy } 
func ( in * Pod Template ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Template List ) Deep Copy Into ( out * Pod Template out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Pod for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Template List ) Deep Copy ( ) * Pod Template out := new ( Pod Template in . Deep Copy } 
func ( in * Pod Template List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Pod Template Spec ) Deep Copy Into ( out * Pod Template in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Pod Template Spec ) Deep Copy ( ) * Pod Template out := new ( Pod Template in . Deep Copy } 
func ( in * Portworx Volume Source ) Deep Copy ( ) * Portworx Volume out := new ( Portworx Volume in . Deep Copy } 
func ( in * Preconditions ) Deep Copy } 
func ( in * Preconditions ) Deep in . Deep Copy } 
func ( in * Prefer Avoid Pods Entry ) Deep Copy Into ( out * Prefer Avoid Pods in . Pod Signature . Deep Copy Into ( & out . Pod in . Eviction Time . Deep Copy Into ( & out . Eviction } 
func ( in * Prefer Avoid Pods Entry ) Deep Copy ( ) * Prefer Avoid Pods out := new ( Prefer Avoid Pods in . Deep Copy } 
func ( in * Preferred Scheduling Term ) Deep Copy Into ( out * Preferred Scheduling in . Preference . Deep Copy } 
func ( in * Preferred Scheduling Term ) Deep Copy ( ) * Preferred Scheduling out := new ( Preferred Scheduling in . Deep Copy } 
func ( in * Probe ) Deep Copy in . Handler . Deep Copy } 
func ( in * Probe ) Deep in . Deep Copy } 
func ( in * Projected Volume Source ) Deep Copy Into ( out * Projected Volume * out = make ( [ ] Volume for i := range * in { ( * in ) [ i ] . Deep Copy if in . Default Mode != nil { in , out := & in . Default Mode , & out . Default } 
func ( in * Projected Volume Source ) Deep Copy ( ) * Projected Volume out := new ( Projected Volume in . Deep Copy } 
func ( in * Quobyte Volume Source ) Deep Copy ( ) * Quobyte Volume out := new ( Quobyte Volume in . Deep Copy } 
func ( in * RBD Persistent Volume Source ) Deep Copy Into ( out * RBD Persistent Volume if in . Ceph Monitors != nil { in , out := & in . Ceph Monitors , & out . Ceph if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret } 
func ( in * RBD Persistent Volume Source ) Deep Copy ( ) * RBD Persistent Volume out := new ( RBD Persistent Volume in . Deep Copy } 
func ( in * RBD Volume Source ) Deep Copy Into ( out * RBD Volume if in . Ceph Monitors != nil { in , out := & in . Ceph Monitors , & out . Ceph if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object } 
func ( in * RBD Volume Source ) Deep Copy ( ) * RBD Volume out := new ( RBD Volume in . Deep Copy } 
func ( in * Range Allocation ) Deep Copy Into ( out * Range out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object } 
func ( in * Range Allocation ) Deep Copy ( ) * Range out := new ( Range in . Deep Copy } 
func ( in * Range Allocation ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Replication Controller ) Deep Copy Into ( out * Replication out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Replication Controller ) Deep Copy ( ) * Replication out := new ( Replication in . Deep Copy } 
func ( in * Replication Controller ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Replication Controller Condition ) Deep Copy Into ( out * Replication Controller in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Replication Controller Condition ) Deep Copy ( ) * Replication Controller out := new ( Replication Controller in . Deep Copy } 
func ( in * Replication Controller List ) Deep Copy Into ( out * Replication Controller out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Replication for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Replication Controller List ) Deep Copy ( ) * Replication Controller out := new ( Replication Controller in . Deep Copy } 
func ( in * Replication Controller List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Replication Controller Spec ) Deep Copy ( ) * Replication Controller out := new ( Replication Controller in . Deep Copy } 
func ( in * Replication Controller Status ) Deep Copy Into ( out * Replication Controller * out = make ( [ ] Replication Controller for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Replication Controller Status ) Deep Copy ( ) * Replication Controller out := new ( Replication Controller in . Deep Copy } 
func ( in * Resource Field Selector ) Deep Copy Into ( out * Resource Field out . Divisor = in . Divisor . Deep } 
func ( in * Resource Field Selector ) Deep Copy ( ) * Resource Field out := new ( Resource Field in . Deep Copy } 
func ( in Resource List ) Deep Copy Into ( out * Resource * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep } 
func ( in Resource List ) Deep Copy ( ) Resource out := new ( Resource in . Deep Copy } 
func ( in * Resource Quota ) Deep Copy Into ( out * Resource out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Resource Quota ) Deep Copy ( ) * Resource out := new ( Resource in . Deep Copy } 
func ( in * Resource Quota ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Resource Quota List ) Deep Copy Into ( out * Resource Quota out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Resource for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Resource Quota List ) Deep Copy ( ) * Resource Quota out := new ( Resource Quota in . Deep Copy } 
func ( in * Resource Quota List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Resource Quota Spec ) Deep Copy Into ( out * Resource Quota * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( [ ] Resource Quota if in . Scope Selector != nil { in , out := & in . Scope Selector , & out . Scope * out = new ( Scope ( * in ) . Deep Copy } 
func ( in * Resource Quota Spec ) Deep Copy ( ) * Resource Quota out := new ( Resource Quota in . Deep Copy } 
func ( in * Resource Quota Status ) Deep Copy Into ( out * Resource Quota * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep } 
func ( in * Resource Quota Status ) Deep Copy ( ) * Resource Quota out := new ( Resource Quota in . Deep Copy } 
func ( in * Resource Requirements ) Deep Copy Into ( out * Resource * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep * out = make ( Resource for key , val := range * in { ( * out ) [ key ] = val . Deep } 
func ( in * Resource Requirements ) Deep Copy ( ) * Resource out := new ( Resource in . Deep Copy } 
func ( in * SE Linux Options ) Deep Copy ( ) * SE Linux out := new ( SE Linux in . Deep Copy } 
func ( in * Scale IO Persistent Volume Source ) Deep Copy Into ( out * Scale IO Persistent Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Secret } 
func ( in * Scale IO Persistent Volume Source ) Deep Copy ( ) * Scale IO Persistent Volume out := new ( Scale IO Persistent Volume in . Deep Copy } 
func ( in * Scale IO Volume Source ) Deep Copy Into ( out * Scale IO Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object } 
func ( in * Scale IO Volume Source ) Deep Copy ( ) * Scale IO Volume out := new ( Scale IO Volume in . Deep Copy } 
func ( in * Scope Selector ) Deep Copy Into ( out * Scope if in . Match Expressions != nil { in , out := & in . Match Expressions , & out . Match * out = make ( [ ] Scoped Resource Selector for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Scope Selector ) Deep Copy ( ) * Scope out := new ( Scope in . Deep Copy } 
func ( in * Scoped Resource Selector Requirement ) Deep Copy Into ( out * Scoped Resource Selector } 
func ( in * Scoped Resource Selector Requirement ) Deep Copy ( ) * Scoped Resource Selector out := new ( Scoped Resource Selector in . Deep Copy } 
func ( in * Secret ) Deep in . Deep Copy } 
func ( in * Secret ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Secret Env Source ) Deep Copy ( ) * Secret Env out := new ( Secret Env in . Deep Copy } 
func ( in * Secret Key Selector ) Deep Copy Into ( out * Secret Key out . Local Object Reference = in . Local Object } 
func ( in * Secret Key Selector ) Deep Copy ( ) * Secret Key out := new ( Secret Key in . Deep Copy } 
func ( in * Secret List ) Deep Copy Into ( out * Secret out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Secret List ) Deep Copy ( ) * Secret out := new ( Secret in . Deep Copy } 
func ( in * Secret List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Secret Projection ) Deep Copy Into ( out * Secret out . Local Object Reference = in . Local Object * out = make ( [ ] Key To for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Secret Projection ) Deep Copy ( ) * Secret out := new ( Secret in . Deep Copy } 
func ( in * Secret Reference ) Deep Copy ( ) * Secret out := new ( Secret in . Deep Copy } 
func ( in * Secret Volume Source ) Deep Copy Into ( out * Secret Volume * out = make ( [ ] Key To for i := range * in { ( * in ) [ i ] . Deep Copy if in . Default Mode != nil { in , out := & in . Default Mode , & out . Default } 
func ( in * Secret Volume Source ) Deep Copy ( ) * Secret Volume out := new ( Secret Volume in . Deep Copy } 
func ( in * Security Context ) Deep Copy Into ( out * Security ( * in ) . Deep Copy if in . SE Linux Options != nil { in , out := & in . SE Linux Options , & out . SE Linux * out = new ( SE Linux if in . Windows Options != nil { in , out := & in . Windows Options , & out . Windows * out = new ( Windows Security Context if in . Run As User != nil { in , out := & in . Run As User , & out . Run As if in . Run As Group != nil { in , out := & in . Run As Group , & out . Run As if in . Run As Non Root != nil { in , out := & in . Run As Non Root , & out . Run As Non if in . Read Only Root Filesystem != nil { in , out := & in . Read Only Root Filesystem , & out . Read Only Root if in . Allow Privilege Escalation != nil { in , out := & in . Allow Privilege Escalation , & out . Allow Privilege if in . Proc Mount != nil { in , out := & in . Proc Mount , & out . Proc * out = new ( Proc Mount } 
func ( in * Security Context ) Deep Copy ( ) * Security out := new ( Security in . Deep Copy } 
func ( in * Serialized Reference ) Deep Copy Into ( out * Serialized out . Type Meta = in . Type } 
func ( in * Serialized Reference ) Deep Copy ( ) * Serialized out := new ( Serialized in . Deep Copy } 
func ( in * Serialized Reference ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Service ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Service ) Deep in . Deep Copy } 
func ( in * Service ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Service Account ) Deep Copy Into ( out * Service out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object * out = make ( [ ] Object if in . Image Pull Secrets != nil { in , out := & in . Image Pull Secrets , & out . Image Pull * out = make ( [ ] Local Object if in . Automount Service Account Token != nil { in , out := & in . Automount Service Account Token , & out . Automount Service Account } 
func ( in * Service Account ) Deep Copy ( ) * Service out := new ( Service in . Deep Copy } 
func ( in * Service Account ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Service Account List ) Deep Copy Into ( out * Service Account out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Service for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Service Account List ) Deep Copy ( ) * Service Account out := new ( Service Account in . Deep Copy } 
func ( in * Service Account List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Service Account Token Projection ) Deep Copy ( ) * Service Account Token out := new ( Service Account Token in . Deep Copy } 
func ( in * Service List ) Deep Copy Into ( out * Service out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Service List ) Deep Copy ( ) * Service out := new ( Service in . Deep Copy } 
func ( in * Service List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Service Port ) Deep Copy Into ( out * Service out . Target Port = in . Target } 
func ( in * Service Port ) Deep Copy ( ) * Service out := new ( Service in . Deep Copy } 
func ( in * Service Proxy Options ) Deep Copy Into ( out * Service Proxy out . Type Meta = in . Type } 
func ( in * Service Proxy Options ) Deep Copy ( ) * Service Proxy out := new ( Service Proxy in . Deep Copy } 
func ( in * Service Proxy Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Service Spec ) Deep Copy Into ( out * Service * out = make ( [ ] Service if in . External I Ps != nil { in , out := & in . External I Ps , & out . External I if in . Session Affinity Config != nil { in , out := & in . Session Affinity Config , & out . Session Affinity * out = new ( Session Affinity ( * in ) . Deep Copy if in . Load Balancer Source Ranges != nil { in , out := & in . Load Balancer Source Ranges , & out . Load Balancer Source } 
func ( in * Service Spec ) Deep Copy ( ) * Service out := new ( Service in . Deep Copy } 
func ( in * Service Status ) Deep Copy Into ( out * Service in . Load Balancer . Deep Copy Into ( & out . Load } 
func ( in * Service Status ) Deep Copy ( ) * Service out := new ( Service in . Deep Copy } 
func ( in * Session Affinity Config ) Deep Copy Into ( out * Session Affinity if in . Client IP != nil { in , out := & in . Client IP , & out . Client * out = new ( Client IP ( * in ) . Deep Copy } 
func ( in * Session Affinity Config ) Deep Copy ( ) * Session Affinity out := new ( Session Affinity in . Deep Copy } 
func ( in * Storage OS Persistent Volume Source ) Deep Copy Into ( out * Storage OS Persistent Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Object } 
func ( in * Storage OS Persistent Volume Source ) Deep Copy ( ) * Storage OS Persistent Volume out := new ( Storage OS Persistent Volume in . Deep Copy } 
func ( in * Storage OS Volume Source ) Deep Copy Into ( out * Storage OS Volume if in . Secret Ref != nil { in , out := & in . Secret Ref , & out . Secret * out = new ( Local Object } 
func ( in * Storage OS Volume Source ) Deep Copy ( ) * Storage OS Volume out := new ( Storage OS Volume in . Deep Copy } 
func ( in * Sysctl ) Deep in . Deep Copy } 
func ( in * TCP Socket Action ) Deep Copy Into ( out * TCP Socket } 
func ( in * TCP Socket Action ) Deep Copy ( ) * TCP Socket out := new ( TCP Socket in . Deep Copy } 
func ( in * Taint ) Deep Copy if in . Time Added != nil { in , out := & in . Time Added , & out . Time * out = ( * in ) . Deep } 
func ( in * Taint ) Deep in . Deep Copy } 
func ( in * Toleration ) Deep Copy if in . Toleration Seconds != nil { in , out := & in . Toleration Seconds , & out . Toleration } 
func ( in * Toleration ) Deep in . Deep Copy } 
func ( in * Topology Selector Label Requirement ) Deep Copy ( ) * Topology Selector Label out := new ( Topology Selector Label in . Deep Copy } 
func ( in * Topology Selector Term ) Deep Copy Into ( out * Topology Selector if in . Match Label Expressions != nil { in , out := & in . Match Label Expressions , & out . Match Label * out = make ( [ ] Topology Selector Label for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Topology Selector Term ) Deep Copy ( ) * Topology Selector out := new ( Topology Selector in . Deep Copy } 
func ( in * Typed Local Object Reference ) Deep Copy Into ( out * Typed Local Object if in . API Group != nil { in , out := & in . API Group , & out . API } 
func ( in * Typed Local Object Reference ) Deep Copy ( ) * Typed Local Object out := new ( Typed Local Object in . Deep Copy } 
func ( in * Volume ) Deep Copy in . Volume Source . Deep Copy Into ( & out . Volume } 
func ( in * Volume ) Deep in . Deep Copy } 
func ( in * Volume Device ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func ( in * Volume Mount ) Deep Copy Into ( out * Volume if in . Mount Propagation != nil { in , out := & in . Mount Propagation , & out . Mount * out = new ( Mount Propagation } 
func ( in * Volume Mount ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func ( in * Volume Node Affinity ) Deep Copy Into ( out * Volume Node * out = new ( Node ( * in ) . Deep Copy } 
func ( in * Volume Node Affinity ) Deep Copy ( ) * Volume Node out := new ( Volume Node in . Deep Copy } 
func ( in * Volume Projection ) Deep Copy Into ( out * Volume * out = new ( Secret ( * in ) . Deep Copy if in . Downward API != nil { in , out := & in . Downward API , & out . Downward * out = new ( Downward API ( * in ) . Deep Copy if in . Config Map != nil { in , out := & in . Config Map , & out . Config * out = new ( Config Map ( * in ) . Deep Copy if in . Service Account Token != nil { in , out := & in . Service Account Token , & out . Service Account * out = new ( Service Account Token } 
func ( in * Volume Projection ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func ( in * Volume Source ) Deep Copy Into ( out * Volume if in . Host Path != nil { in , out := & in . Host Path , & out . Host * out = new ( Host Path Volume ( * in ) . Deep Copy if in . Empty Dir != nil { in , out := & in . Empty Dir , & out . Empty * out = new ( Empty Dir Volume ( * in ) . Deep Copy if in . GCE Persistent Disk != nil { in , out := & in . GCE Persistent Disk , & out . GCE Persistent * out = new ( GCE Persistent Disk Volume if in . AWS Elastic Block Store != nil { in , out := & in . AWS Elastic Block Store , & out . AWS Elastic Block * out = new ( AWS Elastic Block Store Volume if in . Git Repo != nil { in , out := & in . Git Repo , & out . Git * out = new ( Git Repo Volume * out = new ( Secret Volume ( * in ) . Deep Copy * out = new ( NFS Volume * out = new ( ISCSI Volume ( * in ) . Deep Copy * out = new ( Glusterfs Volume if in . Persistent Volume Claim != nil { in , out := & in . Persistent Volume Claim , & out . Persistent Volume * out = new ( Persistent Volume Claim Volume * out = new ( RBD Volume ( * in ) . Deep Copy * out = new ( Quobyte Volume if in . Flex Volume != nil { in , out := & in . Flex Volume , & out . Flex * out = new ( Flex Volume ( * in ) . Deep Copy * out = new ( Cinder Volume ( * in ) . Deep Copy if in . Ceph FS != nil { in , out := & in . Ceph FS , & out . Ceph * out = new ( Ceph FS Volume ( * in ) . Deep Copy * out = new ( Flocker Volume if in . Downward API != nil { in , out := & in . Downward API , & out . Downward * out = new ( Downward API Volume ( * in ) . Deep Copy * out = new ( FC Volume ( * in ) . Deep Copy if in . Azure File != nil { in , out := & in . Azure File , & out . Azure * out = new ( Azure File Volume if in . Config Map != nil { in , out := & in . Config Map , & out . Config * out = new ( Config Map Volume ( * in ) . Deep Copy if in . Vsphere Volume != nil { in , out := & in . Vsphere Volume , & out . Vsphere * out = new ( Vsphere Virtual Disk Volume if in . Azure Disk != nil { in , out := & in . Azure Disk , & out . Azure * out = new ( Azure Disk Volume ( * in ) . Deep Copy if in . Photon Persistent Disk != nil { in , out := & in . Photon Persistent Disk , & out . Photon Persistent * out = new ( Photon Persistent Disk Volume * out = new ( Projected Volume ( * in ) . Deep Copy if in . Portworx Volume != nil { in , out := & in . Portworx Volume , & out . Portworx * out = new ( Portworx Volume if in . Scale IO != nil { in , out := & in . Scale IO , & out . Scale * out = new ( Scale IO Volume ( * in ) . Deep Copy if in . Storage OS != nil { in , out := & in . Storage OS , & out . Storage * out = new ( Storage OS Volume ( * in ) . Deep Copy * out = new ( CSI Volume ( * in ) . Deep Copy } 
func ( in * Volume Source ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func ( in * Vsphere Virtual Disk Volume Source ) Deep Copy ( ) * Vsphere Virtual Disk Volume out := new ( Vsphere Virtual Disk Volume in . Deep Copy } 
func ( in * Weighted Pod Affinity Term ) Deep Copy Into ( out * Weighted Pod Affinity in . Pod Affinity Term . Deep Copy Into ( & out . Pod Affinity } 
func ( in * Weighted Pod Affinity Term ) Deep Copy ( ) * Weighted Pod Affinity out := new ( Weighted Pod Affinity in . Deep Copy } 
func ( in * Windows Security Context Options ) Deep Copy ( ) * Windows Security Context out := new ( Windows Security Context in . Deep Copy } 
func get Seccomp Security Opts ( seccomp Profile string , separator rune ) ( [ ] string , error ) { seccomp Opts , err := get Seccomp Docker Opts ( seccomp return fmt Docker Opts ( seccomp } 
func To Status Err ( webhook Name string , result * metav1 . Status ) * apierrors . Status Error { denied By := fmt . Sprintf ( " " , webhook const no if result == nil { result = & metav1 . Status { Status : metav1 . Status switch { case len ( result . Message ) > 0 : result . Message = fmt . Sprintf ( " " , denied case len ( result . Reason ) > 0 : result . Message = fmt . Sprintf ( " " , denied default : result . Message = fmt . Sprintf ( " " , denied By , no return & apierrors . Status Error { Err } 
func New Dry Run Unsupported Err ( webhook Name string ) * apierrors . Status Error { reason := fmt . Sprintf ( " " , webhook return apierrors . New Bad } 
func ( authz Handler union Authz for _ , curr Authz Handler := range authz Handler { decision , reason , err := curr Authz switch decision { case authorizer . Decision Allow , authorizer . Decision case authorizer . Decision No return authorizer . Decision No Opinion , strings . Join ( reasonlist , " \n " ) , utilerrors . New } 
func ( authz Handler union Authz Rules Handler ) Rules For ( user user . Info , namespace string ) ( [ ] authorizer . Resource Rule Info , [ ] authorizer . Non Resource Rule Info , bool , error ) { var ( err resource Rules List [ ] authorizer . Resource Rule non Resource Rules List [ ] authorizer . Non Resource Rule incomplete for _ , curr Authz Handler := range authz Handler { resource Rules , non Resource Rules , incomplete , err := curr Authz Handler . Rules if incomplete == true { incomplete if err != nil { err List = append ( err if len ( resource Rules ) > 0 { resource Rules List = append ( resource Rules List , resource if len ( non Resource Rules ) > 0 { non Resource Rules List = append ( non Resource Rules List , non Resource return resource Rules List , non Resource Rules List , incomplete Status , utilerrors . New Aggregate ( err } 
func New Cmd Selfhosting ( in io . Reader ) * cobra . Command { cmd := & cobra . Command { Use : " " , Aliases : [ ] string { " " , " " } , Short : " " , Long : cmdutil . Macro Command Long cmd . Add Command ( get Selfhosting Sub } 
func get Selfhosting Sub Command ( in io . Reader ) * cobra . Command { cfg := & kubeadmapiv1beta2 . Init var cfg Path , feature Gates String , kube Config force Pivot , certs In // Creates the UX Command cmd := & cobra . Command { Use : " " , Aliases : [ ] string { " " } , Short : " " , Long : selfhosting Long Desc , Example : selfhosting if ! force s := bufio . New kubeadmutil . Check if strings . To Lower ( s . Text ( ) ) != " " { kubeadmutil . Check if cfg . Feature Gates , err = features . New Feature Gate ( & features . Init Feature Gates , feature Gates String ) ; err != nil { kubeadmutil . Check if err := validation . Validate Mixed Arguments ( cmd . Flags ( ) ) ; err != nil { kubeadmutil . Check // Gets the Kubernetes client kube Config File = cmdutil . Get Kube Config Path ( kube Config client , err := kubeconfigutil . Client Set From File ( kube Config kubeadmutil . Check // Kubernetes Version is not used, but we set it explicitly to avoid the lookup // of the version from the internet when executing Load Or Default Init Configuration phases . Set Kubernetes Version ( & cfg . Cluster // This call returns the ready-to-use configuration based on the configuration file that might or might not exist and the default cfg populated by flags internalcfg , err := configutil . Load Or Default Init Configuration ( cfg kubeadmutil . Check // Converts the Static Pod-hosted control plane into a self-hosted one waiter := apiclient . New Kube err = selfhosting . Create Self Hosted Control Plane ( constants . Get Static Pod Directory ( ) , constants . Kubernetes Dir , internalcfg , client , waiter , false , certs In kubeadmutil . Check // Add flags to the command // flags bound to the configuration object cmd . Flags ( ) . String Var ( & cfg . Certificates Dir , " " , cfg . Certificates options . Add Config Flag ( cmd . Flags ( ) , & cfg cmd . Flags ( ) . Bool Var P ( & certs In cmd . Flags ( ) . Bool Var P ( & force // flags that are not bound to the configuration object // Note: All flags that are not bound to the cfg object should be whitelisted in cmd/kubeadm/app/apis/kubeadm/validation/validation.go options . Add Kube Config Flag ( cmd . Flags ( ) , & kube Config } 
func Pod Requests And Limits ( pod * corev1 . Pod ) ( reqs , limits corev1 . Resource List ) { reqs , limits = corev1 . Resource List { } , corev1 . Resource for _ , container := range pod . Spec . Containers { add Resource add Resource // init containers define the minimum of any resource for _ , container := range pod . Spec . Init Containers { max Resource max Resource } 
func convert Resource CPU To String ( cpu * resource . Quantity , divisor resource . Quantity ) ( string , error ) { c := int64 ( math . Ceil ( float64 ( cpu . Milli Value ( ) ) / float64 ( divisor . Milli return strconv . Format } 
func convert Resource Memory To return strconv . Format } 
func convert Resource Ephemeral Storage To String ( ephemeral Storage * resource . Quantity , divisor resource . Quantity ) ( string , error ) { m := int64 ( math . Ceil ( float64 ( ephemeral return strconv . Format } 
func Is Standard Container Resource Name ( str string ) bool { return standard Container Resources . Has ( str ) || Is Huge Page Resource Name ( corev1 . Resource } 
func Is Huge Page Resource Name ( name corev1 . Resource Name ) bool { return strings . Has Prefix ( string ( name ) , corev1 . Resource Huge Pages } 
func ( c * Discovery Controller ) process Next Work err := c . sync Fn ( key . ( schema . Group utilruntime . Handle c . queue . Add Rate } 
func Build Argument List From Map ( base Arguments map [ string ] string , override args for k , v := range base Arguments { args for k , v := range override Arguments { args for k := range args for _ , k := range keys { command = append ( command , fmt . Sprintf ( " " , k , args } 
func Parse Argument List To Map ( arguments [ ] string ) map [ string ] string { resulting for i , arg := range arguments { key , val , err := parse resulting return resulting } 
func Replace Argument ( command [ ] string , arg Mutate Func func ( map [ string ] string ) map [ string ] string ) [ ] string { arg Map := Parse Argument List To // Save the first command (the executable) if we're sure it's not an argument (i.e. no --) var new if len ( command ) > 0 && ! strings . Has Prefix ( command [ 0 ] , " " ) { new Command = append ( new new Arg Map := arg Mutate Func ( arg new Command = append ( new Command , Build Argument List From Map ( new Arg return new } 
func parse Argument ( arg string ) ( string , string , error ) { if ! strings . Has // Remove the starting -- arg = strings . Trim // Split the string on =. Return only two substrings, since we want only key/value, but the value can include '=' as well keyval Slice := strings . Split // Make sure both a key and value is present if len ( keyval if len ( keyval return keyval Slice [ 0 ] , keyval } 
func New Hyper Kube Command ( stop Ch <- chan struct { } ) ( * cobra . Command , [ ] func ( ) * cobra . Command ) { // these have to be functions since the command is polymorphic. Cobra wants you to be top level // command to get executed apiserver := func ( ) * cobra . Command { ret := kubeapiserver . New API Server Command ( stop controller := func ( ) * cobra . Command { ret := kubecontrollermanager . New Controller Manager proxy := func ( ) * cobra . Command { ret := kubeproxy . New Proxy scheduler := func ( ) * cobra . Command { ret := kubescheduler . New Scheduler kubectl Cmd := func ( ) * cobra . Command { return kubectl . New Default Kubectl kubelet := func ( ) * cobra . Command { return kubelet . New Kubelet Command ( stop cloud Controller := func ( ) * cobra . Command { return cloudcontrollermanager . New Cloud Controller Manager command Fns := [ ] func ( ) * cobra . Command { apiserver , controller , proxy , scheduler , kubectl Cmd , kubelet , cloud make Symlinks cmd := & cobra . Command { Use : " " , Short : " " , Run : func ( cmd * cobra . Command , args [ ] string ) { if len ( args ) != 0 || ! make Symlinks if err := make Symlinks ( os . Args [ 0 ] , command cmd . Flags ( ) . Bool Var ( & make Symlinks Flag , " " , make Symlinks cmd . Flags ( ) . Mark cmd . Flags ( ) . Mark for i := range command Fns { cmd . Add Command ( command return cmd , command } 
func make Symlinks ( target Name string , command for _ , command Fn := range command Fns { command := command err := os . Symlink ( target } 
func New Defaulting Codec For Scheme ( // TODO: I should be a scheme interface? scheme * runtime . Scheme , encoder runtime . Encoder , decoder runtime . Decoder , encode Version runtime . Group Versioner , decode Version runtime . Group Versioner , ) runtime . Codec { return New Codec ( encoder , decoder , runtime . Unsafe Object Convertor ( scheme ) , scheme , scheme , scheme , encode Version , decode } 
func New Codec ( encoder runtime . Encoder , decoder runtime . Decoder , convertor runtime . Object Convertor , creater runtime . Object Creater , typer runtime . Object Typer , defaulter runtime . Object Defaulter , encode Version runtime . Group Versioner , decode Version runtime . Group Versioner , original Scheme Name string , ) runtime . Codec { internal := & codec { encoder : encoder , decoder : decoder , convertor : convertor , creater : creater , typer : typer , defaulter : defaulter , encode Version : encode Version , decode Version : decode Version , original Scheme Name : original Scheme } 
func ( c * codec ) Decode ( data [ ] byte , default GVK * schema . Group Version Kind , into runtime . Object ) ( runtime . Object , * schema . Group Version Kind , error ) { versioned , is Versioned := into . ( * runtime . Versioned if is // If the into object is unstructured and expresses an opinion about its group/version, // create a new instance of the type so we always exercise the conversion path (skips short-circuiting on `into == obj`) decode if into != nil { if _ , ok := into . ( runtime . Unstructured ) ; ok && ! into . Get Object Kind ( ) . Group Version Kind ( ) . Group Version ( ) . Empty ( ) { decode Into = reflect . New ( reflect . Type obj , gvk , err := c . decoder . Decode ( data , default GVK , decode if d , ok := obj . ( runtime . Nested Object Decoder ) ; ok { if err := d . Decode Nested Objects ( runtime . Without Version // if we specify a target, use generic conversion. if into != nil { if into == obj { if is // perform defaulting if requested if c . defaulter != nil { // create a copy to ensure defaulting is not applied to the original versioned objects if is Versioned { versioned . Objects = [ ] runtime . Object { obj . Deep Copy } else { if is if err := c . convertor . Convert ( obj , into , c . decode if is // Convert if needed. if is Versioned { // create a copy, because Convert To Version does not guarantee non-mutation of objects versioned . Objects = [ ] runtime . Object { obj . Deep Copy out , err := c . convertor . Convert To Version ( obj , c . decode if is } 
case runtime . Unstructured : // An unstructured list can contain objects of multiple group version kinds. don't short-circuit just // because the top-level type matches our desired destination type. actually send the object to the converter // to give it a chance to convert the list items if needed. if _ , ok := obj . ( * unstructured . Unstructured List ) ; ! ok { // avoid conversion roundtrip if GVK is the right one already or is empty (yes, this is a hack, but the old behaviour we rely on in kubectl) obj GVK := obj . Get Object Kind ( ) . Group Version if len ( obj target GVK , ok := c . encode Version . Kind For Group Version Kinds ( [ ] schema . Group Version Kind { obj if ! ok { return runtime . New Not Registered GVK Err For Target ( c . original Scheme Name , obj GVK , c . encode if target GVK == obj gvks , is Unversioned , err := c . typer . Object object Kind := obj . Get Object old := object Kind . Group Version // restore the old GVK after encoding defer object Kind . Set Group Version if c . encode Version == nil || is Unversioned { if e , ok := obj . ( runtime . Nested Object Encoder ) ; ok { if err := e . Encode Nested Objects ( runtime . With Version Encoder { Encoder : c . encoder , Object object Kind . Set Group Version // Perform a conversion if necessary out , err := c . convertor . Convert To Version ( obj , c . encode if e , ok := out . ( runtime . Nested Object Encoder ) ; ok { if err := e . Encode Nested Objects ( runtime . With Version Encoder { Version : c . encode Version , Encoder : c . encoder , Object } 
func ( secret Credential Manager * Secret Credential Manager ) Get Credential ( server string ) ( * Credential , error ) { err := secret Credential Manager . update Credentials if err != nil { status Err , ok := err . ( * apierrors . Status if ( ok && status Err . Err Status . Code != http . Status Not // Handle secrets deletion by finding credentials from cache klog . Warningf ( " " , secret Credential Manager . Secret Name , secret Credential Manager . Secret credential , found := secret Credential Manager . Cache . Get return nil , Err Credentials Not } 
func parse Config ( data map [ string ] [ ] byte , config map [ string ] * Credential ) error { if len ( data ) == 0 { return Err Credential for credential Key , credential Value := range data { credential Key = strings . To Lower ( credential vc if strings . Has Suffix ( credential Key , " " ) { vc Server = strings . Split ( credential if _ , ok := config [ vc Server ] ; ! ok { config [ vc config [ vc Server ] . Password = string ( credential } else if strings . Has Suffix ( credential Key , " " ) { vc Server = strings . Split ( credential if _ , ok := config [ vc Server ] ; ! ok { config [ vc config [ vc Server ] . User = string ( credential } else { klog . Errorf ( " " , credential return Err Unknown Secret for vc Server , credential := range config { if credential . User == " " || credential . Password == " " { klog . Errorf ( " " , vc return Err Credential } 
func ( v * version ) Ingresses ( ) Ingress Informer { return & ingress Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( s * storage Class Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Storage Class , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Storage } 
func ( s * storage Class Lister ) Get ( name string ) ( * v1 . Storage Class , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1 . Storage } 
func New Leader Elector ( lec Leader Election Config ) ( * Leader Elector , error ) { if lec . Lease Duration <= lec . Renew if lec . Renew Deadline <= time . Duration ( Jitter Factor * float64 ( lec . Retry if lec . Lease if lec . Renew if lec . Retry le := Leader Elector { config : lec , clock : clock . Real Clock { } , metrics : global Metrics Factory . new Leader le . metrics . leader } 
func ( le * Leader Elector ) Run ( ctx context . Context ) { defer func ( ) { runtime . Handle le . config . Callbacks . On Stopped ctx , cancel := context . With go le . config . Callbacks . On Started } 
func Run Or Die ( ctx context . Context , lec Leader Election Config ) { le , err := New Leader if lec . Watch Dog != nil { lec . Watch Dog . Set Leader } 
func ( le * Leader Elector ) Is Leader ( ) bool { return le . observed Record . Holder } 
func ( le * Leader Elector ) acquire ( ctx context . Context ) bool { ctx , cancel := context . With wait . Jitter Until ( func ( ) { succeeded = le . try Acquire Or le . maybe Report le . config . Lock . Record le . metrics . leader } , le . config . Retry Period , Jitter } 
func ( le * Leader Elector ) renew ( ctx context . Context ) { ctx , cancel := context . With wait . Until ( func ( ) { timeout Ctx , timeout Cancel := context . With Timeout ( ctx , le . config . Renew defer timeout err := wait . Poll Immediate Until ( le . config . Retry done <- le . try Acquire Or select { case <- timeout Ctx . Done ( ) : return false , fmt . Errorf ( " " , timeout } , timeout le . maybe Report le . config . Lock . Record le . metrics . leader } , le . config . Retry // if we hold the lease, give it up if le . config . Release On } 
func ( le * Leader Elector ) release ( ) bool { if ! le . Is leader Election Record := rl . Leader Election Record { Leader Transitions : le . observed Record . Leader if err := le . config . Lock . Update ( leader Election le . observed Record = leader Election le . observed } 
func ( le * Leader Elector ) try Acquire Or leader Election Record := rl . Leader Election Record { Holder Identity : le . config . Lock . Identity ( ) , Lease Duration Seconds : int ( le . config . Lease Duration / time . Second ) , Renew Time : now , Acquire // 1. obtain or create the Election Record old Leader Election if err != nil { if ! errors . Is Not if err = le . config . Lock . Create ( leader Election le . observed Record = leader Election le . observed // 2. Record obtained, check the Identity & Time if ! reflect . Deep Equal ( le . observed Record , * old Leader Election Record ) { le . observed Record = * old Leader Election le . observed if len ( old Leader Election Record . Holder Identity ) > 0 && le . observed Time . Add ( le . config . Lease Duration ) . After ( now . Time ) && ! le . Is Leader ( ) { klog . V ( 4 ) . Infof ( " " , old Leader Election Record . Holder // 3. We're going to try to update. The leader Election Record is set to it's default // here. Let's correct it before updating. if le . Is Leader ( ) { leader Election Record . Acquire Time = old Leader Election Record . Acquire leader Election Record . Leader Transitions = old Leader Election Record . Leader } else { leader Election Record . Leader Transitions = old Leader Election Record . Leader // update the lock itself if err = le . config . Lock . Update ( leader Election le . observed Record = leader Election le . observed } 
func ( le * Leader Elector ) Check ( max Tolerable Expired Lease time . Duration ) error { if ! le . Is // If we are more than timeout seconds after the lease duration that is past the timeout // on the lease renew. Time to start reporting ourselves as unhealthy. We should have // died but conditions like deadlock can prevent this. (See #70819) if le . clock . Since ( le . observed Time ) > le . config . Lease Duration + max Tolerable Expired } 
func ( f * apply Flags ) session Is Interactive ( ) bool { return ! ( f . non Interactive Mode || f . dry } 
func New Cmd Apply ( apf * apply Plan Flags ) * cobra . Command { flags := & apply Flags { apply Plan Flags : apf , image Pull Timeout : default Image Pull Timeout , etcd Upgrade : true , renew Certs : true , // Don't set cri Socket to a default value here, as this will override the setting in the stored config in Run cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : " " , Run : func ( cmd * cobra . Command , args [ ] string ) { user Version , err := get K8s Version From User Input ( flags . apply Plan kubeadmutil . Check err = run Apply ( flags , user kubeadmutil . Check // Register the common flags for apply and plan add Apply Plan Flags ( cmd . Flags ( ) , flags . apply Plan // Specify the valid flags specific for apply cmd . Flags ( ) . Bool Var P ( & flags . non Interactive Mode , " " , " " , flags . non Interactive cmd . Flags ( ) . Bool Var cmd . Flags ( ) . Bool Var ( & flags . dry Run , options . Dry Run , flags . dry cmd . Flags ( ) . Bool Var ( & flags . etcd Upgrade , " " , flags . etcd cmd . Flags ( ) . Bool Var ( & flags . renew Certs , " " , flags . renew cmd . Flags ( ) . Duration Var ( & flags . image Pull Timeout , " " , flags . image Pull // The CRI socket flag is deprecated here, since it should be taken from the Node Registration Options for the current // node instead of the command line. This prevents errors by the users (such as attempts to use wrong CRI during upgrade). cmdutil . Add CRI Socket Flag ( cmd . Flags ( ) , & flags . cri cmd . Flags ( ) . Mark Deprecated ( options . Node CRI } 
func run Apply ( flags * apply Flags , user Version string ) error { // Start with the basics, verify that the cluster is healthy and get the configuration from the cluster (using the Config client , version Getter , cfg , err := enforce Requirements ( flags . apply Plan Flags , flags . dry Run , user if len ( flags . cri cfg . Node Registration . CRI Socket = flags . cri if err := configutil . Normalize Kubernetes Version ( & cfg . Cluster // Use normalized version string in all following code. new K8s Version , err := version . Parse Semantic ( cfg . Kubernetes if err != nil { return errors . Errorf ( " " , cfg . Kubernetes if err := features . Validate Version ( features . Init Feature Gates , cfg . Feature Gates , cfg . Kubernetes if err := Enforce Version Policies ( cfg . Kubernetes Version , new K8s Version , flags , version // If the current session is interactive, ask the user whether they really want to upgrade. if flags . session Is Interactive ( ) { if err := Interactively Confirm waiter := get Waiter ( flags . dry // Use a prepuller implementation based on creating Daemon Sets // and block until all Daemon prepuller := upgrade . New Daemon Set Prepuller ( client , waiter , & cfg . Cluster components To Prepull := constants . Control Plane if cfg . Etcd . External == nil && flags . etcd Upgrade { components To Prepull = append ( components To if err := upgrade . Prepull Images In Parallel ( prepuller , flags . image Pull Timeout , components To if err := Perform Control Plane if err := upgrade . Perform Post Upgrade Tasks ( client , cfg , new K8s Version , flags . dry if flags . dry fmt . Printf ( " \n " , cfg . Kubernetes } 
func Enforce Version Policies ( new K8s Version Str string , new K8s Version * version . Version , flags * apply Flags , version Getter upgrade . Version Getter ) error { fmt . Printf ( " \n " , new K8s Version version Skew Errs := upgrade . Enforce Version Policies ( version Getter , new K8s Version Str , new K8s Version , flags . allow Experimental Upgrades , flags . allow RC if version Skew Errs != nil { if len ( version Skew Errs . Mandatory ) > 0 { return errors . Errorf ( " \n \n \n " , kubeadmutil . Format Err Msg ( version Skew if len ( version Skew Errs . Skippable ) > 0 { // Return the error if the user hasn't specified the --force flag if ! flags . force { return errors . Errorf ( " \n \n \n " , kubeadmutil . Format Err Msg ( version Skew // Soft errors found, but --force was specified fmt . Printf ( " \n \n " , len ( version Skew Errs . Skippable ) , kubeadmutil . Format Err Msg ( version Skew } 
func Perform Control Plane Upgrade ( flags * apply Flags , client clientset . Interface , waiter apiclient . Waiter , internalcfg * kubeadmapi . Init Configuration ) error { // OK, the cluster is hosted using static pods. Upgrade a static-pod hosted cluster fmt . Printf ( " \n " , internalcfg . Kubernetes if flags . dry Run { return Dry Run Static Pod // Don't save etcd backup directory if etcd is HA, as this could cause corruption return Perform Static Pod Upgrade ( client , waiter , internalcfg , flags . etcd Upgrade , flags . renew } 
func Get Path Manager For Upgrade ( internalcfg * kubeadmapi . Init Configuration , etcd Upgrade bool ) ( upgrade . Static Pod Path Manager , error ) { is HA Etcd := etcdutil . Check Configuration Is return upgrade . New Kube Static Pod Path Manager Using Temp Dirs ( constants . Get Static Pod Directory ( ) , true , etcd Upgrade && ! is HA } 
func Perform Static Pod Upgrade ( client clientset . Interface , waiter apiclient . Waiter , internalcfg * kubeadmapi . Init Configuration , etcd Upgrade , renew Certs bool ) error { path Manager , err := Get Path Manager For Upgrade ( internalcfg , etcd // The arguments old Etcd Client and new Etd Client, are uninitialized because passing in the clients allow for mocking the client during testing return upgrade . Static Pod Control Plane ( client , waiter , path Manager , internalcfg , etcd Upgrade , renew } 
func Dry Run Static Pod Upgrade ( internalcfg * kubeadmapi . Init Configuration ) error { dry Run Manifest Dir , err := constants . Create Temp Dir For defer os . Remove All ( dry Run Manifest if err := controlplane . Create Init Static Pod Manifest Files ( dry Run Manifest // Print the contents of the upgraded manifests and pretend like they were in /etc/kubernetes/manifests files := [ ] dryrunutil . File To for _ , component := range constants . Control Plane Components { real Path := constants . Get Static Pod Filepath ( component , dry Run Manifest output Path := constants . Get Static Pod Filepath ( component , constants . Get Static Pod files = append ( files , dryrunutil . New File To Print ( real Path , output return dryrunutil . Print Dry Run } 
func ( o * Debugging Options ) Add Flags ( fs * pflag . Flag fs . Bool Var ( & o . Enable Profiling , " " , o . Enable fs . Bool Var ( & o . Enable Contention Profiling , " " , o . Enable Contention } 
func ( o * Debugging Options ) Apply To ( cfg * componentbaseconfig . Debugging cfg . Enable Profiling = o . Enable cfg . Enable Contention Profiling = o . Enable Contention } 
func ( o * Debugging } 
func ( cml * Config Map Lock ) Get ( ) ( * Leader Election Record , error ) { var record Leader Election cml . cm , err = cml . Client . Config Maps ( cml . Config Map Meta . Namespace ) . Get ( cml . Config Map Meta . Name , metav1 . Get if record Bytes , found := cml . cm . Annotations [ Leader Election Record Annotation Key ] ; found { if err := json . Unmarshal ( [ ] byte ( record } 
func ( cml * Config Map Lock ) Create ( ler Leader Election Record ) error { record cml . cm , err = cml . Client . Config Maps ( cml . Config Map Meta . Namespace ) . Create ( & v1 . Config Map { Object Meta : metav1 . Object Meta { Name : cml . Config Map Meta . Name , Namespace : cml . Config Map Meta . Namespace , Annotations : map [ string ] string { Leader Election Record Annotation Key : string ( record } 
func ( cml * Config Map Lock ) Update ( ler Leader Election record cml . cm . Annotations [ Leader Election Record Annotation Key ] = string ( record cml . cm , err = cml . Client . Config Maps ( cml . Config Map } 
func ( cml * Config Map Lock ) Record Event ( s string ) { if cml . Lock Config . Event events := fmt . Sprintf ( " " , cml . Lock cml . Lock Config . Event Recorder . Eventf ( & v1 . Config Map { Object Meta : cml . cm . Object Meta } , v1 . Event Type } 
func ( cml * Config Map Lock ) Describe ( ) string { return fmt . Sprintf ( " " , cml . Config Map Meta . Namespace , cml . Config Map } 
func ( * Mounter ) Get File Type ( _ string ) ( mount . File Type , error ) { return mount . File } 
func New Retry Watcher ( initial Resource Version string , watcher Client cache . Watcher ) ( * Retry Watcher , error ) { return new Retry Watcher ( initial Resource Version , watcher } 
func ( rw * Retry Watcher ) do Receive ( ) ( bool , time . Duration ) { watcher , err := rw . watcher Client . Watch ( metav1 . List Options { Resource Version : rw . last Resource case io . Err Unexpected if net . Is Probable ch := watcher . Result for { select { case <- rw . stop case event , ok := <- ch : if ! ok { klog . V ( 4 ) . Infof ( " " , rw . last Resource // We need to inspect the event and get Resource Version out of it switch event . Type { case watch . Added , watch . Modified , watch . Deleted , watch . Bookmark : meta Object , ok := event . Object . ( resource Version if ! ok { _ = rw . send ( watch . Event { Type : watch . Error , Object : & apierrors . New Internal Error ( errors . New ( " " ) ) . Err // We have to abort here because this might cause last Resource resource Version := meta Object . Get Resource if resource Version == " " { _ = rw . send ( watch . Event { Type : watch . Error , Object : & apierrors . New Internal Error ( fmt . Errorf ( " " , event . Object ) ) . Err // We have to abort here because this might cause last Resource // All is fine; send the event and update last Resource rw . last Resource Version = resource case watch . Error : // This round trip allows us to handle unstructured status err Object := apierrors . From status Err , ok := err Object . ( * apierrors . Status status := status Err . Err status if status . Details != nil { status Delay = time . Duration ( status . Details . Retry After switch status . Code { case http . Status case http . Status Gateway Timeout , http . Status Internal Server Error : // Retry return false , status default : // We retry by default. Retry // Retry return false , status _ = rw . send ( watch . Event { Type : watch . Error , Object : & apierrors . New Internal Error ( fmt . Errorf ( " " , event . Type ) ) . Err // We are unable to restart the watch and have to stop the loop or this might cause last Resource } 
func ( rw * Retry Watcher ) receive ( ) { defer close ( rw . done defer close ( rw . result ctx , cancel := context . With go func ( ) { select { case <- rw . stop // We use non sliding until so we don't introduce delays on happy path when WATCH call // timeouts or gets closed and we need to reestablish it while also avoiding hot loops. wait . Non Sliding Until With Context ( ctx , func ( ctx context . Context ) { done , retry After := rw . do time . Sleep ( retry klog . V ( 4 ) . Infof ( " " , rw . last Resource } , rw . min Restart } 
func Using Legacy Cadvisor Stats ( runtime , runtime Endpoint string ) bool { return ( runtime == kubetypes . Docker Container Runtime && goruntime . GOOS == " " ) || runtime Endpoint == Crio Socket || runtime Endpoint == " " + Crio } 
func Default Meta V1Field Selector } 
func JSON Key Mapper ( key string , source Tag , dest Tag reflect . Struct Tag ) ( string , string ) { if s := dest Tag . Get ( " " ) ; len ( s ) > 0 { return strings . Split if s := source Tag . Get ( " " ) ; len ( s ) > 0 { return key , strings . Split } 
switch strings . To } 
func throttle Image Pulling ( image Service kubecontainer . Image Service , qps float32 , burst int ) kubecontainer . Image Service { if qps == 0.0 { return image return & throttled Image Service { Image Service : image Service , limiter : flowcontrol . New Token Bucket Rate } 
func Pod ( pod * v1 . Pod ) string { return Pod } 
func Pod Desc ( pod Name , pod Namespace string , pod UID types . UID ) string { // Use underscore as the delimiter because it is not allowed in pod name // (DNS subdomain format), while allowed in the container name format. return fmt . Sprintf ( " " , pod Name , pod Namespace , pod } 
func Pod With Deletion Timestamp ( pod * v1 . Pod ) string { var deletion if pod . Deletion Timestamp != nil { deletion Timestamp = " " + pod . Deletion return Pod ( pod ) + deletion } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Test Type { } , & Test Type scheme . Add Known Types ( Scheme Group } 
func get Device Prefix Ref Count ( mounter mount . Interface , device Name // Find the number of references to the device. ref for i := range mps { if strings . Has Prefix ( mps [ i ] . Path , device Name Prefix ) { ref return ref } 
func make PD Name Internal ( host volume . Volume Host , portal string , iqn string , lun string , iface string ) string { return filepath . Join ( host . Get Plugin Dir ( iscsi Plugin } 
func make VDPD Name Internal ( host volume . Volume Host , portal string , iqn string , lun string , iface string ) string { return filepath . Join ( host . Get Volume Device Plugin Dir ( iscsi Plugin } 
func ( util * ISCSI Util ) Make Global PD Name ( iscsi iscsi Disk ) string { return make PD Name } 
func ( util * ISCSI Util ) Make Global VDPD Name ( iscsi iscsi Disk ) string { return make VDPD Name } 
func scan One Lun ( host Number int , lun Number int ) error { filename := fmt . Sprintf ( " " , host fd , err := os . Open // Channel/Target are always 0 for i SCSI scan Cmd := fmt . Sprintf ( " " , lun if written , err := fd . Write String ( scan klog . V ( 3 ) . Infof ( " " , host Number , lun } 
func ( util * ISCSI Util ) Attach Disk ( b iscsi Disk Mounter ) ( string , error ) { var device device var iscsi var last iscsi Transport = extract bkp // create new iface and copy parameters from pre-configured iface to the created iface if b . Initiator Name != " " { // new iface name is <target portal>:<volume name> new Iface := bkp Portal [ 0 ] + " " + b . Vol err = clone Iface ( b , new // update iface name b . Iface = new // Lock the target while we login to avoid races between 2 volumes that share the same // target both logging in or one logging out while another logs in. b . plugin . target Locks . Lock defer b . plugin . target Locks . Unlock // Build a map of SCSI hosts for each target portal. We will need this to // issue the bus rescans. portal Host Map , err := b . device Util . Get ISCSI Portal Host Map For klog . V ( 4 ) . Infof ( " " , b . Iqn , portal Host for i := 1 ; i <= max Attach Attempts ; i ++ { for _ , tp := range bkp Portal { if _ , found := device host Number , logged In := portal Host if ! logged // update discoverydb with CHAP secret err = update ISCSI if err != nil { last last err = update ISCSI if err != nil { // failure to update node db is rare. But deleting record will likely impact those who already start using it. last last // Rebuild the host map after logging in portal Host Map , err := b . device Util . Get ISCSI Portal Host Map For klog . V ( 6 ) . Infof ( " " , b . Iqn , portal Host host Number , logged In = portal Host if ! logged klog . V ( 5 ) . Infof ( " " , host lun // Scan the i SCSI bus for the LUN err = scan One Lun ( host Number , lun if iscsi if iscsi Transport == " " { device } else { device if exist := wait For Path To Exist ( & device Path , multipath Device Timeout , iscsi // update last error last } else { device Paths [ tp ] = device klog . V ( 4 ) . Infof ( " " , b . Iqn , i , len ( device if len ( device klog . Errorf ( " \n " , last return " " , fmt . Errorf ( " \n " , last if len ( device Paths ) == len ( bkp if len ( device Paths ) >= min Multipath Count && i >= min Attach Attempts { // We have at least two paths for multipath and we tried the other paths long enough klog . V ( 4 ) . Infof ( " " , len ( device if last Err != nil { klog . Errorf ( " \n " , last device Path for _ , path := range device Paths { device Path List = append ( device Path // Try to find a multipath device for the volume if len ( bkp Portal ) > 1 { // Multipath volume was requested. Wait up to 10 seconds for the multipath device to appear. device Path = wait For Multi Path To Exist ( device Path List , 10 , b . device } else { // For P Vs with 1 portal, just try one time to find the multipath device. This // avoids a long pause when the multipath device will never get created, and // matches legacy behavior. device Path = wait For Multi Path To Exist ( device Path List , 1 , b . device // When no multipath device is found, just use the first (and presumably only) device if device Path == " " { device Path = device Path klog . V ( 5 ) . Infof ( " " , device // run global mount path related operations based on volume Mode return global PD Path Operation ( b ) ( b , device } 
func global PD Path Operation ( b iscsi Disk Mounter ) func ( iscsi Disk Mounter , string , * ISCSI Util ) ( string , error ) { // TODO: remove feature gate check after no longer needed if utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) { klog . V ( 5 ) . Infof ( " " , b . volume if b . volume Mode == v1 . Persistent Volume Block { // If the volume Mode is 'Block', plugin don't need to format the volume. return func ( b iscsi Disk Mounter , device Path string , util * ISCSI Util ) ( string , error ) { global PD Path := b . manager . Make Global VDPD Name ( * b . iscsi // Create dir like /var/lib/kubelet/plugins/kubernetes.io/iscsi/volume Devices/{iface Name}/{portal-some_iqn-lun-lun_id} if err := os . Mkdir All ( global PD Path , 0750 ) ; err != nil { klog . Errorf ( " " , global PD // Persist iscsi disk config to json file for Detach Disk path util . persist ISCSI ( * ( b . iscsi Disk ) , global PD return device // If the volume Mode is 'Filesystem', plugin needs to format the volume // and mount it to global PD Path. return func ( b iscsi Disk Mounter , device Path string , util * ISCSI Util ) ( string , error ) { global PD Path := b . manager . Make Global PD Name ( * b . iscsi not Mnt , err := b . mounter . Is Likely Not Mount Point ( global PD if err != nil && ! os . Is Not // Return confirmed device Path to caller if ! not Mnt { klog . Infof ( " " , global PD return device // Create dir like /var/lib/kubelet/plugins/kubernetes.io/iscsi/{iface Name}/{portal-some_iqn-lun-lun_id} if err := os . Mkdir All ( global PD Path , 0750 ) ; err != nil { klog . Errorf ( " " , global PD // Persist iscsi disk config to json file for Detach Disk path util . persist ISCSI ( * ( b . iscsi Disk ) , global PD err = b . mounter . Format And Mount ( device Path , global PD Path , b . fs if err != nil { klog . Errorf ( " " , device Path , b . fs Type , global PD return device } 
func delete Device ( device Name string ) error { filename := fmt . Sprintf ( " " , device fd , err := os . Open if written , err := fd . Write klog . V ( 4 ) . Infof ( " " , device } 
func delete Devices ( c iscsi Disk Unmounter ) error { lun Number , err := strconv . Atoi ( c . iscsi if err != nil { klog . Errorf ( " \n " , c . iscsi // Enumerate the devices so we can delete them device Names , err := c . device Util . Find Devices For ISCSI Lun ( c . iscsi Disk . Iqn , lun if err != nil { klog . Errorf ( " \n " , lun Number , c . iscsi // Find the multipath device path(s) mpath for _ , device Name := range device Names { path := " " + device // check if the dev is using mpio and if so mount it via the dm-XX device if mapped Device Path := c . device Util . Find Multipath Device For Device ( path ) ; mapped Device Path != " " { mpath Devices [ mapped Device // Flush any multipath device maps for mpath Device := range mpath Devices { _ , err = c . exec . Run ( " " , " " , mpath if err != nil { klog . Warningf ( " \n " , mpath klog . V ( 4 ) . Infof ( " " , mpath for _ , device Name := range device Names { err = delete Device ( device if err != nil { klog . Warningf ( " \n " , device } 
func ( util * ISCSI Util ) Detach Disk ( c iscsi Disk Unmounter , mnt Path string ) error { if path Exists , path Err := mount . Path Exists ( mnt Path ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path Exists { klog . Warningf ( " " , mnt not Mnt , err := c . mounter . Is Likely Not Mount Point ( mnt if ! not Mnt { if err := c . mounter . Unmount ( mnt Path ) ; err != nil { klog . Errorf ( " \n " , mnt // if device is no longer used, see if need to logout the target device , prefix , err := extract Device And Prefix ( mnt var bkp var vol Name , iqn , iface , initiator // load iscsi disk config from json file if err := util . load ISCSI ( c . iscsi Disk , mnt Path ) ; err == nil { bkp Portal , iqn , iface , vol Name = c . iscsi Disk . Portals , c . iscsi Disk . Iqn , c . iscsi Disk . Iface , c . iscsi Disk . Vol initiator Name = c . iscsi Disk . Initiator } else { // If the iscsi disk config is not found, fall back to the original behavior. // This portal/iqn/iface is no longer referenced, log out. // Extract the portal and iqn from device path. bkp bkp Portal [ 0 ] , iqn , err = extract Portal And // Extract the iface from the mount Path and use it to log out. If the iface // is not found, maintain the previous behavior to facilitate kubelet upgrade. // Logout may fail as no session may exist for the portal/IQN on the specified interface. iface , found = extract Iface ( mnt // Delete all the scsi devices and any multipath devices after unmounting if err = delete // Lock the target while we determine if we can safely log out or not c . plugin . target Locks . Lock defer c . plugin . target Locks . Unlock // if device is no longer used, see if need to logout the target ref Count , err := get Device Prefix Ref if err != nil || ref portals := remove Duplicate ( bkp err = util . detach ISCSI Disk ( c . exec , portals , iqn , iface , vol Name , initiator } 
func ( util * ISCSI Util ) Detach Block ISCSI Disk ( c iscsi Disk Unmapper , map Path string ) error { if path Exists , path Err := mount . Path Exists ( map Path ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path Exists { klog . Warningf ( " " , map // If we arrive here, device is no longer used, see if need to logout the target // device: 192.168.0.10:3260-iqn.2017-05.com.example:test-lun-0 device , _ , err := extract Device And Prefix ( map var bkp var vol Name , iqn , lun , iface , initiator // load iscsi disk config from json file if err := util . load ISCSI ( c . iscsi Disk , map Path ) ; err == nil { bkp Portal , iqn , lun , iface , vol Name = c . iscsi Disk . Portals , c . iscsi Disk . Iqn , c . iscsi Disk . Lun , c . iscsi Disk . Iface , c . iscsi Disk . Vol initiator Name = c . iscsi Disk . Initiator } else { // If the iscsi disk config is not found, fall back to the original behavior. // This portal/iqn/iface is no longer referenced, log out. // Extract the portal and iqn from device path. bkp bkp Portal [ 0 ] , iqn , err = extract Portal And if len ( arr ) < 2 { return fmt . Errorf ( " " , map // Extract the iface from the mount Path and use it to log out. If the iface // is not found, maintain the previous behavior to facilitate kubelet upgrade. // Logout may fail as no session may exist for the portal/IQN on the specified interface. iface , found = extract Iface ( map portals := remove Duplicate ( bkp device Path := get Dev By klog . V ( 5 ) . Infof ( " " , device if _ , err = os . Stat ( device Path ) ; err != nil { return fmt . Errorf ( " " , device // check if the dev is using mpio and if so mount it via the dm-XX device if mapped Device Path := c . device Util . Find Multipath Device For Device ( device Path ) ; mapped Device Path != " " { device Path = mapped Device // Detach a volume from kubelet node err = util . detach ISCSI Disk ( c . exec , portals , iqn , iface , vol Name , initiator } 
func remove } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( autoscaling . Add To utilruntime . Must ( v2beta2 . Add To utilruntime . Must ( v2beta1 . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1 . Scheme Group Version , v2beta1 . Scheme Group Version , v2beta2 . Scheme Group } 
func Is Controlled By ( obj Object , owner Object ) bool { ref := Get Controller return ref . UID == owner . Get } 
func Get Controller Of ( controllee Object ) * Owner Reference { for _ , ref := range controllee . Get Owner } 
func New Controller Ref ( owner Object , gvk schema . Group Version Kind ) * Owner Reference { block Owner is return & Owner Reference { API Version : gvk . Group Version ( ) . String ( ) , Kind : gvk . Kind , Name : owner . Get Name ( ) , UID : owner . Get UID ( ) , Block Owner Deletion : & block Owner Deletion , Controller : & is } 
func ( plugin * azure Data Disk Plugin ) New Attacher ( ) ( volume . Attacher , error ) { azure , err := get if err != nil { klog . Errorf ( " " , plugin . host . Get Host return & azure Disk } 
func ( f * mounter Defaults ) Set Up At ( dir string , fs Group * int64 ) error { klog . Warning ( log src , err := f . plugin . get Device Mount if err := do } 
func ( f * mounter Defaults ) Get Attributes ( ) volume . Attributes { klog . V ( 5 ) . Infof ( log return volume . Attributes { Read Only : f . read Only , Managed : ! f . read Only , Supports SE Linux : f . flex Volume . plugin . capabilities . SE Linux } 
func ( attacher * gce Persistent Disk Attacher ) Attach ( spec * volume . Spec , node Name types . Node Name ) ( string , error ) { volume Source , read Only , err := get Volume pd Name := volume Source . PD attached , err := attacher . gce Disks . Disk Is Attached ( pd Name , node if err != nil { // Log error and continue with attach klog . Errorf ( " " , pd Name , node if err == nil && attached { // Volume is already attached to node. klog . Infof ( " " , pd Name , node } else { if err := attacher . gce Disks . Attach Disk ( pd Name , node Name , read Only , is Regional PD ( spec ) ) ; err != nil { klog . Errorf ( " " , pd Name , node return filepath . Join ( disk By ID Path , disk Google Prefix + pd } 
func get Disk ID ( pd Name string , exec mount . Exec ) ( string , error ) { // TODO: replace Get-Gce Pd Name with native windows support of Get-Disk, see issue #74674 cmd := `Get-Gce Pd Name | select Name, Device Id | Convert for _ , pd := range data { if json Name , ok := pd [ " " ] ; ok { if name , ok := json Name . ( string ) ; ok { if name == pd if disk Num , ok := pd [ " " ] ; ok { switch v := disk default : // disk Num isn't one of the types above klog . Warningf ( " " , name , disk return " " , fmt . Errorf ( " " , pd } 
func ( detacher * gce Persistent Disk Detacher ) Detach ( volume Name string , node Name types . Node Name ) error { pd Name := path . Base ( volume attached , err := detacher . gce Disks . Disk Is Attached ( pd Name , node if err != nil { // Log error and continue with detach klog . Errorf ( " " , pd Name , node if err == nil && ! attached { // Volume is not attached to node. Success! klog . Infof ( " " , pd Name , node if err = detacher . gce Disks . Detach Disk ( pd Name , node Name ) ; err != nil { klog . Errorf ( " " , pd Name , node } 
func read Gluster Log ( path string , pod klog . Infof ( " " , pod // Check and make sure path exists if len ( path ) == 0 { return fmt . Errorf ( " " , pod if err != nil { return fmt . Errorf ( " " , pod // read in and scan the file using scanner // from stdlib fscan := bufio . New } 
} 
func Intn } 
func Int63n } 
rng . rand = rand . New ( rand . New } 
} 
random remaining := max Alphanums Per for i := 0 ; i < n ; { if remaining == 0 { random Int63 , remaining = rng . rand . Int63 ( ) , max Alphanums Per if idx := int ( random Int63 & alphanums Idx random Int63 >>= alphanums Idx } 
func Safe Encode } 
func ( u * Unstructured ) Marshal err := Unstructured JSON } 
func ( u * Unstructured ) Unmarshal JSON ( b [ ] byte ) error { _ , _ , err := Unstructured JSON } 
func ( audit Sink Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { ic := obj . ( * audit . Audit } 
func ( audit Sink Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new IC := obj . ( * audit . Audit old IC := old . ( * audit . Audit // Any changes to the policy or backend increment the generation number // See metav1.Object Meta description for more information on Generation. if ! reflect . Deep Equal ( old IC . Spec , new IC . Spec ) { new IC . Generation = old } 
func ( audit Sink Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { ic := obj . ( * audit . Audit return validation . Validate Audit } 
func ( audit Sink Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { validation Error List := validation . Validate Audit Sink ( obj . ( * audit . Audit update Error List := validation . Validate Audit Sink Update ( obj . ( * audit . Audit Sink ) , old . ( * audit . Audit return append ( validation Error List , update Error } 
func New Defaults ( ) ( * args . Generator Args , * Custom Args ) { generic Args := args . Default ( ) . Without Default Flag custom Args := & Custom generic Args . Custom Args = custom if pkg := codegenutil . Current Package ( ) ; len ( pkg ) != 0 { generic Args . Output Package return generic Args , custom } 
func ( r Registration ) Marshal ( obj runtime . Object ) ( [ ] byte , error ) { return kubeadmutil . Marshal To Yaml For Codecs ( obj , r . Marshal Group } 
func ( r Registration ) Unmarshal ( file Content [ ] byte ) ( runtime . Object , error ) { // Do a deepcopy of the empty value so we don't mutate it, which could lead to strange errors obj := r . Empty Value . Deep Copy // Decode the file content into obj which is a pointer to an empty struct of the internal Component Config if err := unmarshal Object ( obj , file } 
func ( rs * Registrations ) Add To Scheme ( scheme * runtime . Scheme ) error { for _ , registration := range * rs { for _ , add To Scheme Func := range registration . Add To Scheme Funcs { if err := add To Scheme } 
func ( rs * Registrations ) Default ( internalcfg * kubeadmapi . Cluster Configuration ) { for _ , registration := range * rs { registration . Defaulter } 
func ( rs * Registrations ) Validate ( internalcfg * kubeadmapi . Cluster Configuration ) field . Error List { all Errs := field . Error for kind , registration := range * rs { all Errs = append ( all Errs , registration . Validate Func ( internalcfg , field . New return all } 
func Recommended Default Node Lifecycle Controller Configuration ( obj * kubectrlmgrconfigv1alpha1 . Node Lifecycle Controller if obj . Pod Eviction Timeout == zero { obj . Pod Eviction if obj . Node Monitor Grace Period == zero { obj . Node Monitor Grace if obj . Node Startup Grace Period == zero { obj . Node Startup Grace if obj . Enable Taint Manager == nil { obj . Enable Taint Manager = utilpointer . Bool } 
func ( g * Cloud ) Create Global Forwarding Rule ( rule * compute . Forwarding Rule ) error { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric return mc . Observe ( g . c . Global Forwarding Rules ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) Set Proxy For Global Forwarding Rule ( forwarding Rule Name , target Proxy Link string ) error { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric target := & compute . Target Reference { Target : target Proxy return mc . Observe ( g . c . Global Forwarding Rules ( ) . Set Target ( ctx , meta . Global Key ( forwarding Rule } 
func ( g * Cloud ) Delete Global Forwarding Rule ( name string ) error { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric return mc . Observe ( g . c . Global Forwarding Rules ( ) . Delete ( ctx , meta . Global } 
func ( g * Cloud ) Get Global Forwarding Rule ( name string ) ( * compute . Forwarding Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric v , err := g . c . Global Forwarding Rules ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) List Global Forwarding Rules ( ) ( [ ] * compute . Forwarding Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric v , err := g . c . Global Forwarding } 
func ( g * Cloud ) Get Region Forwarding Rule ( name , region string ) ( * compute . Forwarding Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric v , err := g . c . Forwarding Rules ( ) . Get ( ctx , meta . Regional } 
func ( g * Cloud ) Get Alpha Region Forwarding Rule ( name , region string ) ( * computealpha . Forwarding Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric Context With Version ( " " , region , compute Alpha v , err := g . c . Alpha Forwarding Rules ( ) . Get ( ctx , meta . Regional } 
func ( g * Cloud ) List Region Forwarding Rules ( region string ) ( [ ] * compute . Forwarding Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric v , err := g . c . Forwarding } 
func ( g * Cloud ) List Alpha Region Forwarding Rules ( region string ) ( [ ] * computealpha . Forwarding Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric Context With Version ( " " , region , compute Alpha v , err := g . c . Alpha Forwarding } 
func ( g * Cloud ) Create Region Forwarding Rule ( rule * compute . Forwarding Rule , region string ) error { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric return mc . Observe ( g . c . Forwarding Rules ( ) . Insert ( ctx , meta . Regional } 
func ( g * Cloud ) Create Alpha Region Forwarding Rule ( rule * computealpha . Forwarding Rule , region string ) error { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric Context With Version ( " " , region , compute Alpha return mc . Observe ( g . c . Alpha Forwarding Rules ( ) . Insert ( ctx , meta . Regional } 
func ( g * Cloud ) Delete Region Forwarding Rule ( name , region string ) error { ctx , cancel := cloud . Context With Call mc := new Forwarding Rule Metric return mc . Observe ( g . c . Forwarding Rules ( ) . Delete ( ctx , meta . Regional } 
func ( g * Cloud ) get Network Tier From Forwarding Rule ( name , region string ) ( string , error ) { if ! g . Alpha Feature Gate . Enabled ( Alpha Feature Network Tiers ) { return cloud . Network Tier Default . To GCE fwd Rule , err := g . Get Alpha Region Forwarding if err != nil { return handle Alpha Network Tier Get return fwd Rule . Network } 
func Escalation Allowed ( ctx context . Context ) bool { u , ok := genericapirequest . User // system:masters is special because the API server uses it for privileged loopback connections // therefore we know that a member of system:masters can always do anything for _ , group := range u . Get Groups ( ) { if group == user . System Privileged } 
func Role Escalation user , ok := genericapirequest . User request Info , ok := genericapirequest . Request Info if ! request Info . Is Resource request Resource := schema . Group Resource { Group : request Info . API Group , Resource : request if ! role Resources [ request attrs := authorizer . Attributes Record { User : user , Verb : " " , API Group : request Info . API Group , Resource : request Info . Resource , Name : request Info . Name , Namespace : request Info . Namespace , Resource if err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , user , request Resource , request Info . Name , request return decision == authorizer . Decision } 
func Binding Authorized ( ctx context . Context , role Ref rbac . Role Ref , binding user , ok := genericapirequest . User attrs := authorizer . Attributes Record { User : user , Verb : " " , // check against the namespace where the binding is being created (or the empty namespace for clusterrolebindings). // this allows delegation to bind particular clusterroles in rolebindings within particular namespaces, // and to authorize binding a clusterrole across all namespaces in a clusterrolebinding. Namespace : binding Namespace , Resource // This occurs after defaulting and conversion, so values pulled from the role Ref won't change // Invalid API Group or Name values will fail validation switch role Ref . Kind { case " " : attrs . API Group = role Ref . API attrs . Name = role case " " : attrs . API Group = role Ref . API attrs . Name = role if err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , user , role Ref , binding return decision == authorizer . Decision } 
func ( a * Alt Token get Token if err != nil { get Token Fail } 
func New Alt Token Source ( token URL , token Body string ) oauth2 . Token Source { client := oauth2 . New Client ( oauth2 . No Context , google . Compute Token a := & Alt Token Source { oauth Client : client , token URL : token URL , token Body : token Body , throttle : flowcontrol . New Token Bucket Rate Limiter ( token URLQPS , token URL return oauth2 . Reuse Token } 
func New Factory ( ) * factory . Kust Factory { return factory . New Kust Factory ( kunstruct . New Kunstructured Factory Impl ( ) , validator . New Kust Validator ( ) , transformer . New Factory } 
func Convert_v1alpha1_Daemon Set Controller Configuration_To_config_Daemon Set Controller Configuration ( in * v1alpha1 . Daemon Set Controller Configuration , out * daemonconfig . Daemon Set Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Daemon Set Controller Configuration_To_config_Daemon Set Controller } 
func Convert_config_Daemon Set Controller Configuration_To_v1alpha1_Daemon Set Controller Configuration ( in * daemonconfig . Daemon Set Controller Configuration , out * v1alpha1 . Daemon Set Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Daemon Set Controller Configuration_To_v1alpha1_Daemon Set Controller } 
func ( c * Node V1alpha1Client ) REST return c . rest } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1 . Job { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Job List { } , func ( obj interface { } ) { Set Object Defaults_Job List ( obj . ( * v1 . Job } 
func new Worker ( m * manager , probe Type probe Type , pod * v1 . Pod , container v1 . Container ) * worker { w := & worker { stop Ch : make ( chan struct { } , 1 ) , // Buffer so stop() can be non-blocking. pod : pod , container : container , probe Type : probe Type , probe switch probe Type { case readiness : w . spec = container . Readiness w . results Manager = m . readiness w . initial case liveness : w . spec = container . Liveness w . results Manager = m . liveness w . initial basic Metric Labels := prometheus . Labels { " " : w . probe w . prober Results Successful Metric Labels = deep Copy Prometheus Labels ( basic Metric w . prober Results Successful Metric Labels [ " " ] = probe Result w . prober Results Failed Metric Labels = deep Copy Prometheus Labels ( basic Metric w . prober Results Failed Metric Labels [ " " ] = probe Result w . prober Results Unknown Metric Labels = deep Copy Prometheus Labels ( basic Metric w . prober Results Unknown Metric Labels [ " " ] = probe Result } 
func ( w * worker ) run ( ) { probe Ticker Period := time . Duration ( w . spec . Period // If kubelet restarted the probes could be started in rapid succession. // Let the worker wait for a random portion of ticker Period before probing. time . Sleep ( time . Duration ( rand . Float64 ( ) * float64 ( probe Ticker probe Ticker := time . New Ticker ( probe Ticker defer func ( ) { // Clean up. probe if ! w . container ID . Is Empty ( ) { w . results Manager . Remove ( w . container w . probe Manager . remove Worker ( w . pod . UID , w . container . Name , w . probe Prober Results . Delete ( w . prober Results Successful Metric Prober Results . Delete ( w . prober Results Failed Metric Prober Results . Delete ( w . prober Results Unknown Metric probe Loop : for w . do Probe ( ) { // Wait for next probe tick. select { case <- w . stop Ch : break probe case <- probe } 
func ( w * worker ) do Probe ( ) ( keep Going bool ) { defer func ( ) { recover ( ) } ( ) // Actually eat panics (Handle defer runtime . Handle Crash ( func ( _ interface { } ) { keep status , ok := w . probe Manager . status Manager . Get Pod // Worker should terminate if pod is terminated. if status . Phase == v1 . Pod Failed || status . Phase == v1 . Pod c , ok := podutil . Get Container Status ( status . Container if ! ok || len ( c . Container if w . container ID . String ( ) != c . Container ID { if ! w . container ID . Is Empty ( ) { w . results Manager . Remove ( w . container w . container ID = kubecontainer . Parse Container ID ( c . Container w . results Manager . Set ( w . container ID , w . initial // We've got a new container; resume probing. w . on if w . on if ! w . container ID . Is Empty ( ) { w . results Manager . Set ( w . container // Abort if the container will not be restarted. return c . State . Terminated == nil || w . pod . Spec . Restart Policy != v1 . Restart Policy if int32 ( time . Since ( c . State . Running . Started At . Time ) . Seconds ( ) ) < w . spec . Initial Delay // TODO: in order for exec probes to correctly handle downward API env, we must be able to reconstruct // the full container environment here, OR we must make a call to the CRI in order to get those environment // values from the running container. result , err := w . probe Manager . prober . probe ( w . probe Type , w . pod , status , w . container , w . container switch result { case results . Success : Prober Results . With ( w . prober Results Successful Metric case results . Failure : Prober Results . With ( w . prober Results Failed Metric default : Prober Results . With ( w . prober Results Unknown Metric if w . last Result == result { w . result } else { w . last w . result if ( result == results . Failure && w . result Run < int ( w . spec . Failure Threshold ) ) || ( result == results . Success && w . result Run < int ( w . spec . Success w . results Manager . Set ( w . container if w . probe Type == liveness && result == results . Failure { // The container fails a liveness check, it will need to be restarted. // Stop probing until we see a new container ID. This is to reduce the // chance of hitting #21751, where running `docker exec` when a // container is being stopped may lead to corrupted container state. w . on w . result } 
func ( c * Fake Stateful Sets ) List ( opts v1 . List Options ) ( result * v1beta2 . Stateful Set List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( statefulsets Resource , statefulsets Kind , c . ns , opts ) , & v1beta2 . Stateful Set label , _ , _ := testing . Extract From List list := & v1beta2 . Stateful Set List { List Meta : obj . ( * v1beta2 . Stateful Set List ) . List for _ , item := range obj . ( * v1beta2 . Stateful Set } 
func ( c * Fake Stateful Sets ) Update Status ( stateful Set * v1beta2 . Stateful Set ) ( * v1beta2 . Stateful Set , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( statefulsets Resource , " " , c . ns , stateful Set ) , & v1beta2 . Stateful return obj . ( * v1beta2 . Stateful } 
func ( c * Fake Stateful Sets ) Get Scale ( stateful Set Name string , options v1 . Get Options ) ( result * v1beta2 . Scale , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Subresource Action ( statefulsets Resource , c . ns , " " , stateful Set } 
func ( c * Fake Stateful Sets ) Update Scale ( stateful Set Name string , scale * v1beta2 . Scale ) ( result * v1beta2 . Scale , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( statefulsets } 
func New Log Reduction ( identical Error Delay time . Duration ) * Log Reduction { l := new ( Log l . last l . error l . identical Error Delay = identical Error } 
func ( l * Log Reduction ) Should Message Be Printed ( message string , parent ID string ) bool { l . error Map defer l . error Map l . cleanup Error last Msg , ok := l . last Error [ parent last Printed , ok1 := l . error Printed [ parent if ! ok || ! ok1 || message != last Msg || nowfunc ( ) . Sub ( last Printed ) >= l . identical Error Delay { l . error Printed [ parent l . last Error [ parent } 
func ( l * Log Reduction ) Clear ID ( parent ID string ) { l . error Map defer l . error Map delete ( l . last Error , parent delete ( l . error Printed , parent } 
func ( i plugin Initializer ) Initialize ( plugin admission . Interface ) { if wants , ok := plugin . ( Wants Internal Wardle Informer Factory ) ; ok { wants . Set Internal Wardle Informer } 
func ( g * Cloud ) Get URL Map ( name string ) ( * compute . Url Map , error ) { ctx , cancel := cloud . Context With Call mc := new URL Map Metric v , err := g . c . Url Maps ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Create URL Map ( url Map * compute . Url Map ) error { ctx , cancel := cloud . Context With Call mc := new URL Map Metric return mc . Observe ( g . c . Url Maps ( ) . Insert ( ctx , meta . Global Key ( url Map . Name ) , url } 
func ( g * Cloud ) Delete URL Map ( name string ) error { ctx , cancel := cloud . Context With Call mc := new URL Map Metric return mc . Observe ( g . c . Url Maps ( ) . Delete ( ctx , meta . Global } 
func ( g * Cloud ) List URL Maps ( ) ( [ ] * compute . Url Map , error ) { ctx , cancel := cloud . Context With Call mc := new URL Map Metric v , err := g . c . Url } 
func New Kubemark Controller ( external Client kubeclient . Interface , external Informer Factory informers . Shared Informer Factory , kubemark Client kubeclient . Interface , kubemark Node Informer informersv1 . Node Informer ) ( * Kubemark Controller , error ) { rc Informer := external Informer Factory . Informer For ( & apiv1 . Replication Controller { } , new Replication Controller pod Informer := external Informer Factory . Informer For ( & apiv1 . Pod { } , new Pod controller := & Kubemark Controller { external Cluster : external Cluster { rc Lister : listersv1 . New Replication Controller Lister ( rc Informer . Get Indexer ( ) ) , rc Synced : rc Informer . Has Synced , pod Lister : listersv1 . New Pod Lister ( pod Informer . Get Indexer ( ) ) , pod Synced : pod Informer . Has Synced , client : external Client , } , kubemark Cluster : kubemark Cluster { node Lister : kubemark Node Informer . Lister ( ) , node Synced : kubemark Node Informer . Informer ( ) . Has Synced , client : kubemark Client , nodes To Delete : make ( map [ string ] bool ) , nodes To Delete Lock : sync . Mutex { } , } , rand : rand . New ( rand . New Source ( time . Now ( ) . Unix Nano ( ) ) ) , create Node Queue : make ( chan string , 1000 ) , node Group Queue Size : make ( map [ string ] int ) , node Group Queue Size kubemark Node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Update Func : controller . kubemark Cluster . remove Unneeded } 
func ( kubemark Controller * Kubemark Controller ) Wait For Cache Sync ( stop Ch chan struct { } ) bool { return controller . Wait For Cache Sync ( " " , stop Ch , kubemark Controller . external Cluster . rc Synced , kubemark Controller . external Cluster . pod Synced , kubemark Controller . kubemark Cluster . node } 
func ( kubemark Controller * Kubemark Controller ) Run ( stop Ch chan struct { } ) { node Template , err := kubemark Controller . get Node kubemark Controller . node Template = node go kubemark Controller . run Node Creation ( stop <- stop } 
func ( kubemark Controller * Kubemark Controller ) Get Node Names For Node Group ( node Group string ) ( [ ] string , error ) { selector := labels . Selector From Set ( labels . Set { node Group Label : node pods , err := kubemark Controller . external Cluster . pod for _ , pod := range pods { result = append ( result , pod . Object } 
func ( kubemark Controller * Kubemark Controller ) Get Node Group Size ( node Group string ) ( int , error ) { selector := labels . Selector From Set ( labels . Set ( map [ string ] string { node Group Label : node nodes , err := kubemark Controller . external Cluster . rc } 
func ( kubemark Controller * Kubemark Controller ) Get Node Group Target Size ( node Group string ) ( int , error ) { kubemark Controller . node Group Queue Size defer kubemark Controller . node Group Queue Size real Size , err := kubemark Controller . Get Node Group Size ( node if err != nil { return real return real Size + kubemark Controller . node Group Queue Size [ node } 
func ( kubemark Controller * Kubemark Controller ) Set Node Group Size ( node Group string , size int ) error { curr Size , err := kubemark Controller . Get Node Group Target Size ( node switch delta := size - curr Size ; { case delta < 0 : abs nodes , err := kubemark Controller . Get Node Names For Node Group ( node if len ( nodes ) < abs Delta { return fmt . Errorf ( " " , abs Delta , node for i , node := range nodes { if i == abs if err := kubemark Controller . Remove Node From Node Group ( node case delta > 0 : kubemark Controller . node Group Queue Size for i := 0 ; i < delta ; i ++ { kubemark Controller . node Group Queue Size [ node kubemark Controller . create Node Queue <- node kubemark Controller . node Group Queue Size } 
func ( kubemark Controller * Kubemark Controller ) Get Node Group For Node ( node string ) ( string , error ) { pod := kubemark Controller . get Pod By node Group , ok := pod . Object Meta . Labels [ node Group if ok { return node return " " , fmt . Errorf ( " " , node , node Group } 
func ( kubemark Controller * Kubemark Controller ) get Node Template ( ) ( * apiv1 . Replication Controller , error ) { pod Name , err := kubemark Controller . kubemark Cluster . get Hollow Node hollow Node Name , err := kubemark Controller . get Node Name For Pod ( pod if hollow Node := kubemark Controller . get Replication Controller By Name ( hollow Node Name ) ; hollow Node != nil { node Template := & apiv1 . Replication Controller { Spec : apiv1 . Replication Controller Spec { Template : hollow node node Template . Namespace = namespace node return node } 
func ( az * Cloud ) get Standard Machine ID ( resource Group , machine Name string ) string { return fmt . Sprintf ( machine ID Template , az . Subscription ID , strings . To Lower ( resource Group ) , machine } 
func ( az * Cloud ) get Availability Set ID ( resource Group , availability Set Name string ) string { return fmt . Sprintf ( availability Set ID Template , az . Subscription ID , resource Group , availability Set } 
func ( az * Cloud ) get Frontend IP Config ID ( lb Name , fip Config Name string ) string { return fmt . Sprintf ( frontend IP Config ID Template , az . Subscription ID , az . Resource Group , lb Name , fip Config } 
func ( az * Cloud ) get Backend Pool ID ( lb Name , backend Pool Name string ) string { return fmt . Sprintf ( backend Pool ID Template , az . Subscription ID , az . Resource Group , lb Name , backend Pool } 
func ( az * Cloud ) get Load Balancer Probe ID ( lb Name , lb Rule Name string ) string { return fmt . Sprintf ( load Balancer Probe ID Template , az . Subscription ID , az . Resource Group , lb Name , lb Rule } 
func ( az * Cloud ) get Azure Load Balancer Name ( cluster Name string , vm Set Name string , is Internal bool ) string { lb Name Prefix := vm Set if strings . Equal Fold ( vm Set Name , az . vm Set . Get Primary VM Set Name ( ) ) || az . use Standard Load Balancer ( ) { lb Name Prefix = cluster if is Internal { return fmt . Sprintf ( " " , lb Name Prefix , Internal Load Balancer Name return lb Name } 
func is Master Node ( node * v1 . Node ) bool { if val , ok := node . Labels [ node Label } 
func get Last } 
func get Protocols From Kubernetes Protocol ( protocol v1 . Protocol ) ( * network . Transport Protocol , * network . Security Rule Protocol , * network . Probe Protocol , error ) { var transport Proto network . Transport var security Proto network . Security Rule var probe Proto network . Probe switch protocol { case v1 . Protocol TCP : transport Proto = network . Transport Protocol security Proto = network . Security Rule Protocol probe Proto = network . Probe Protocol return & transport Proto , & security Proto , & probe case v1 . Protocol UDP : transport Proto = network . Transport Protocol security Proto = network . Security Rule Protocol return & transport Proto , & security default : return & transport Proto , & security Proto , & probe } 
func get Primary Interface ID ( machine compute . Virtual Machine ) ( string , error ) { if len ( * machine . Network Profile . Network Interfaces ) == 1 { return * ( * machine . Network Profile . Network for _ , ref := range * machine . Network Profile . Network } 
func get Service } 
func ( az * Cloud ) get Rule Prefix ( service * v1 . Service ) string { return az . Get Load Balancer } 
func get Next Available Priority ( rules [ ] network . Security Rule ) ( int32 , error ) { var smallest int32 = load Balancer Minimum outer : for smallest < load Balancer Maximum } 
func Make CRC32 ( str string ) string { crc := crc32 . New ( poly return strconv . Format } 
func ( as * availability Set ) Get Instance ID By Node Name ( name string ) ( string , error ) { var machine compute . Virtual machine , err = as . get Virtual Machine ( types . Node if err == cloudprovider . Instance Not Found { return " " , cloudprovider . Instance Not if err != nil { if as . Cloud Provider machine , err = as . Get Virtual Machine With Retry ( types . Node resource converted Resource ID , err := convert Resource Group Name To Lower ( resource return converted Resource } 
func ( as * availability Set ) Get Power Status By Node Name ( name string ) ( power State string , err error ) { vm , err := as . get Virtual Machine ( types . Node if err != nil { return power if vm . Instance View != nil && vm . Instance View . Statuses != nil { statuses := * vm . Instance if strings . Has Prefix ( state , vm Power State Prefix ) { return strings . Trim Prefix ( state , vm Power State } 
func ( as * availability Set ) Get Node Name By Provider ID ( provider ID string ) ( types . Node Name , error ) { // Node Name is part of provider ID for standard instances. matches := provider IDRE . Find String Submatch ( provider return types . Node } 
func ( as * availability Set ) Get Instance Type By Node Name ( name string ) ( string , error ) { machine , err := as . get Virtual Machine ( types . Node return string ( machine . Hardware Profile . VM } 
func ( as * availability Set ) Get Zone By Node Name ( name string ) ( cloudprovider . Zone , error ) { vm , err := as . get Virtual Machine ( types . Node var failure zone failure Domain = as . make Zone ( zone } else { // Availability zone is not used for the node, falling back to fault domain. failure Domain = strconv . Itoa ( int ( * vm . Virtual Machine Properties . Instance View . Platform Fault zone := cloudprovider . Zone { Failure Domain : failure } 
func ( as * availability Set ) Get IP By Node Name ( name string ) ( string , string , error ) { nic , err := as . Get Primary ip Config , err := get Primary IP private IP := * ip Config . Private IP public if ip Config . Public IP Address != nil && ip Config . Public IP Address . ID != nil { pip ID := * ip Config . Public IP pip Name , err := get Last Segment ( pip if err != nil { return " " , " " , fmt . Errorf ( " " , name , pip pip , exists Pip , err := as . get Public IP Address ( as . Resource Group , pip if exists Pip { public IP = * pip . IP return private IP , public } 
func ( as * availability Set ) get Agent Pool Availabiliy Sets ( nodes [ ] * v1 . Node ) ( agent Pool Availability Sets * [ ] string , err error ) { vms , err := as . List Virtual Machines ( as . Resource vm Name To Availability Set if vm . Availability Set != nil { vm Name To Availability Set ID [ * vm . Name ] = * vm . Availability availability Set I Ds := sets . New agent Pool Availability for nx := range nodes { node if is Master as ID , ok := vm Name To Availability Set ID [ node if ! ok { klog . Errorf ( " " , node return nil , fmt . Errorf ( " " , node if availability Set I Ds . Has ( as as Name , err := get Last Segment ( as if err != nil { klog . Errorf ( " " , node Name , as // Availability Set ID is currently upper cased in a indeterministic way // We want to keep it lower case, before the ID get fixed as Name = strings . To Lower ( as * agent Pool Availability Sets = append ( * agent Pool Availability Sets , as return agent Pool Availability } 
func ( as * availability Set ) Get VM Set Names ( service * v1 . Service , nodes [ ] * v1 . Node ) ( availability Set Names * [ ] string , err error ) { has Mode , is Auto , service Availability Set Names := get Service Load Balancer if ! has Mode { // no mode specified in service annotation default to Primary Availability Set Name availability Set Names = & [ ] string { as . Config . Primary Availability Set return availability Set availability Set Names , err = as . get Agent Pool Availabiliy if len ( * availability Set // sort the list to have deterministic selection sort . Strings ( * availability Set if ! is Auto { if service Availability Set Names == nil || len ( service Availability Set for sasx := range service Availability Set Names { for asx := range * availability Set Names { if strings . Equal Fold ( ( * availability Set Names ) [ asx ] , service Availability Set service Availability Set Names [ sasx ] = ( * availability Set if ! found { klog . Errorf ( " " , service Availability Set return nil , fmt . Errorf ( " " , service Availability Set availability Set Names = & service Availability Set return availability Set } 
func ( as * availability Set ) Get Primary Interface ( node Name string ) ( network . Interface , error ) { return as . get Primary Interface With VM Set ( node } 
func extract Resource Group By Nic ID ( nic ID string ) ( string , error ) { matches := nic Resource Group RE . Find String Submatch ( nic if len ( matches ) != 2 { return " " , fmt . Errorf ( " " , nic } 
func ( as * availability Set ) get Primary Interface With VM Set ( node Name , vm Set Name string ) ( network . Interface , error ) { var machine compute . Virtual machine , err := as . Get Virtual Machine With Retry ( types . Node Name ( node if err != nil { klog . V ( 2 ) . Infof ( " " , node Name , vm Set primary Nic ID , err := get Primary Interface nic Name , err := get Last Segment ( primary Nic node Resource Group , err := as . Get Node Resource Group ( node // Check availability set name. Note that vm Set Name is empty string when getting // the Node's IP address. While vm Set Name is not empty, it should be checked with // Node's real availability set name: // - For basic SKU load balancer, err Not In VM Set should be returned if the node's // availability set is mismatched with vm Set Name. // - For standard SKU load balancer, backend could belong to multiple VMAS, so we // don't check vm Set for it. if vm Set Name != " " && ! as . use Standard Load Balancer ( ) { expected Availability Set Name := as . get Availability Set ID ( node Resource Group , vm Set if machine . Availability Set == nil || ! strings . Equal Fold ( * machine . Availability Set . ID , expected Availability Set Name ) { klog . V ( 3 ) . Infof ( " " , nic Name , vm Set return network . Interface { } , err Not In VM nic Resource Group , err := extract Resource Group By Nic ID ( primary Nic ctx , cancel := get Context With nic , err := as . Interfaces Client . Get ( ctx , nic Resource Group , nic } 
func ( as * availability Set ) Ensure Host In Pool ( service * v1 . Service , node Name types . Node Name , backend Pool ID string , vm Set Name string , is Internal bool ) error { vm Name := map Node Name To VM Name ( node service Name := get Service nic , err := as . get Primary Interface With VM Set ( vm Name , vm Set if err != nil { if err == err Not In VM Set { klog . V ( 3 ) . Infof ( " " , node Name , vm Set klog . Errorf ( " " , node Name , vm Name , vm Set if nic . Provisioning State != nil && * nic . Provisioning State == nic Failed State { klog . Warningf ( " " , node var primary IP Config * network . Interface IP primary IP Config , err = get Primary IP found new Backend Pools := [ ] network . Backend Address if primary IP Config . Load Balancer Backend Address Pools != nil { new Backend Pools = * primary IP Config . Load Balancer Backend Address for _ , existing Pool := range new Backend Pools { if strings . Equal Fold ( backend Pool ID , * existing Pool . ID ) { found if ! found Pool { if as . use Standard Load Balancer ( ) && len ( new Backend Pools ) > 0 { // Although standard load balancer supports backends from multiple availability // sets, the same network interface couldn't be added to more than one load balancer of // the same type. Omit those nodes (e.g. masters) so Azure ARM won't complain // about this. new Backend Pools I Ds := make ( [ ] string , 0 , len ( new Backend for _ , pool := range new Backend Pools { if pool . ID != nil { new Backend Pools I Ds = append ( new Backend Pools I is Same LB , old LB Name , err := is Backend Pool On Same LB ( backend Pool ID , new Backend Pools I if ! is Same LB { klog . V ( 4 ) . Infof ( " " , node Name , old LB new Backend Pools = append ( new Backend Pools , network . Backend Address Pool { ID : to . String Ptr ( backend Pool primary IP Config . Load Balancer Backend Address Pools = & new Backend nic klog . V ( 3 ) . Infof ( " " , service Name , nic err := as . Create Or Update } 
func ( as * availability Set ) Ensure Hosts In Pool ( service * v1 . Service , nodes [ ] * v1 . Node , backend Pool ID string , vm Set Name string , is Internal bool ) error { host for _ , node := range nodes { local Node if as . use Standard Load Balancer ( ) && as . exclude Master Nodes From Standard LB ( ) && is Master Node ( node ) { klog . V ( 4 ) . Infof ( " " , local Node Name , backend Pool if as . Should Node Excluded From Load Balancer ( node ) { klog . V ( 4 ) . Infof ( " " , local Node f := func ( ) error { err := as . Ensure Host In Pool ( service , types . Node Name ( local Node Name ) , backend Pool ID , vm Set Name , is if err != nil { return fmt . Errorf ( " " , get Service Name ( service ) , backend Pool host Updates = append ( host errs := utilerrors . Aggregate Goroutines ( host } 
func ( as * availability Set ) Ensure Backend Pool Deleted ( service * v1 . Service , backend Pool ID , vm Set Name string , backend Address Pools * [ ] network . Backend Address } 
func generate Storage Account Name ( account Name Prefix string ) string { unique ID := strings . Replace ( string ( uuid . New account Name := strings . To Lower ( account Name Prefix + unique if len ( account Name ) > storage Account Name Max Length { return account Name [ : storage Account Name Max return account } 
func ( c * storage Classes ) Create ( storage Class * v1 . Storage Class ) ( result * v1 . Storage Class , err error ) { result = & v1 . Storage err = c . client . Post ( ) . Resource ( " " ) . Body ( storage } 
func ( c * storage Classes ) Update ( storage Class * v1 . Storage Class ) ( result * v1 . Storage Class , err error ) { result = & v1 . Storage err = c . client . Put ( ) . Resource ( " " ) . Name ( storage Class . Name ) . Body ( storage } 
func ( f * Fake Cloud ) Get Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service ) ( * v1 . Load Balancer Status , bool , error ) { status := & v1 . Load Balancer status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : f . External } 
func ( f * Fake Cloud ) Get Load Balancer Name ( ctx context . Context , cluster Name string , service * v1 . Service ) string { // TODO: replace Default Load Balancer Name to generate more meaningful loadbalancer names. return cloudprovider . Default Load Balancer } 
func ( f * Fake Cloud ) Ensure Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node ) ( * v1 . Load Balancer Status , error ) { f . add if f . Balancers == nil { f . Balancers = make ( map [ string ] Fake name := f . Get Load Balancer Name ( ctx , cluster zone , err := f . Get f . Balancers [ name ] = Fake Balancer { name , region , spec . Load Balancer status := & v1 . Load Balancer status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : f . External } 
func ( f * Fake Cloud ) Update Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node ) error { f . add f . Update Calls = append ( f . Update Calls , Fake Update Balancer } 
func ( f * Fake Cloud ) Ensure Load Balancer Deleted ( ctx context . Context , cluster Name string , service * v1 . Service ) error { f . add } 
func ( f * Fake Cloud ) Current Node Name ( ctx context . Context , hostname string ) ( types . Node Name , error ) { return types . Node } 
func ( f * Fake Cloud ) Node Addresses ( ctx context . Context , instance types . Node Name ) ( [ ] v1 . Node Address , error ) { f . add f . addresses defer f . addresses } 
func ( f * Fake Cloud ) Node Addresses By Provider ID ( ctx context . Context , provider ID string ) ( [ ] v1 . Node Address , error ) { f . add f . addresses defer f . addresses } 
func ( f * Fake Cloud ) Instance ID ( ctx context . Context , node Name types . Node Name ) ( string , error ) { f . add return f . Ext ID [ node } 
func ( f * Fake Cloud ) Instance Type ( ctx context . Context , instance types . Node Name ) ( string , error ) { f . add return f . Instance } 
func ( f * Fake Cloud ) Instance Type By Provider ID ( ctx context . Context , provider ID string ) ( string , error ) { f . add return f . Instance Types [ types . Node Name ( provider } 
func ( f * Fake Cloud ) Instance Exists By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { f . add return f . Exists By Provider ID , f . Err By Provider } 
func ( f * Fake Cloud ) Instance Shutdown By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { f . add return f . Node Shutdown , f . Err Shutdown By Provider } 
func ( f * Fake Cloud ) List ( filter string ) ( [ ] types . Node Name , error ) { f . add result := [ ] types . Node for _ , machine := range f . Machines { if match , _ := regexp . Match } 
func ( f * Fake Cloud ) Get Zone By Provider ID ( ctx context . Context , provider ID string ) ( cloudprovider . Zone , error ) { f . add } 
func ( f * Fake Cloud ) Get Zone By Node Name ( ctx context . Context , node Name types . Node Name ) ( cloudprovider . Zone , error ) { f . add } 
func ( c * Apps V1beta1Client ) REST return c . rest } 
func ( p * Provision ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { // Don't create a namespace if the request is for a dry-run. if a . Is Dry // if we're here, then we've already passed authentication, so we're allowed to do what we're trying to do // if we're here, then the API server has found a route, which means that if we have a non-empty namespace // its a namespaced resource. if len ( a . Get Namespace ( ) ) == 0 || a . Get Kind ( ) . Group // we need to wait for our caches to warm if ! p . Wait For Ready ( ) { return admission . New _ , err := p . namespace Lister . Get ( a . Get if ! errors . Is Not Found ( err ) { return admission . New namespace := & corev1 . Namespace { Object Meta : metav1 . Object Meta { Name : a . Get Namespace ( ) , Namespace : " " , } , Status : corev1 . Namespace _ , err = p . client . Core if err != nil && ! errors . Is Already Exists ( err ) { return admission . New } 
func ( p * Provision ) Validate Initialization ( ) error { if p . namespace } 
func ( dc * Deployment Controller ) sync Rollout Status ( all R Ss [ ] * apps . Replica Set , new RS * apps . Replica Set , d * apps . Deployment ) error { new Status := calculate Status ( all R Ss , new // If there is no progress Deadline Seconds set, remove any Progressing condition. if ! util . Has Progress Deadline ( d ) { util . Remove Deployment Condition ( & new Status , apps . Deployment // If there is only one replica set that is active then that means we are not running // a new rollout and this is a resync where we don't need to estimate any progress. // In such a case, we should simply not estimate any progress for this deployment. current Cond := util . Get Deployment Condition ( d . Status , apps . Deployment is Complete Deployment := new Status . Replicas == new Status . Updated Replicas && current Cond != nil && current Cond . Reason == util . New RS Available // Check for progress only if there is a progress deadline set and the latest rollout // hasn't completed yet. if util . Has Progress Deadline ( d ) && ! is Complete Deployment { switch { case util . Deployment Complete ( d , & new if new RS != nil { msg = fmt . Sprintf ( " " , new condition := util . New Deployment Condition ( apps . Deployment Progressing , v1 . Condition True , util . New RS Available util . Set Deployment Condition ( & new case util . Deployment Progressing ( d , & new Status ) : // If there is any progress made, continue by not checking if the deployment failed. This // behavior emulates the rolling updater progress if new RS != nil { msg = fmt . Sprintf ( " " , new condition := util . New Deployment Condition ( apps . Deployment Progressing , v1 . Condition True , util . Replica Set Updated // Update the current Progressing condition or add a new one if it doesn't exist. // If a Progressing condition with status=true already exists, we should update // everything but last Transition Time. Set Deployment Condition already does that but // it also is not updating conditions when the reason of the new condition is the // same as the old. The Progressing condition is a special case because we want to // update with the same reason and change just last Update Time iff we notice any // progress. That's why we handle it here. if current Cond != nil { if current Cond . Status == v1 . Condition True { condition . Last Transition Time = current Cond . Last Transition util . Remove Deployment Condition ( & new Status , apps . Deployment util . Set Deployment Condition ( & new case util . Deployment Timed Out ( d , & new if new RS != nil { msg = fmt . Sprintf ( " " , new condition := util . New Deployment Condition ( apps . Deployment Progressing , v1 . Condition False , util . Timed Out util . Set Deployment Condition ( & new // Move failure conditions of all replica sets in deployment conditions. For now, // only one failure condition is returned from get Replica Failures. if replica Failure Cond := dc . get Replica Failures ( all R Ss , new RS ) ; len ( replica Failure Cond ) > 0 { // There will be only one Replica Failure condition on the replica set. util . Set Deployment Condition ( & new Status , replica Failure } else { util . Remove Deployment Condition ( & new Status , apps . Deployment Replica // Do not update if there is nothing new to add. if reflect . Deep Equal ( d . Status , new Status ) { // Requeue the deployment if required. dc . requeue Stuck Deployment ( d , new new new Deployment . Status = new _ , err := dc . client . Apps V1 ( ) . Deployments ( new Deployment . Namespace ) . Update Status ( new } 
func ( dc * Deployment Controller ) get Replica Failures ( all R Ss [ ] * apps . Replica Set , new RS * apps . Replica Set ) [ ] apps . Deployment Condition { var conditions [ ] apps . Deployment if new RS != nil { for _ , c := range new RS . Status . Conditions { if c . Type != apps . Replica Set Replica conditions = append ( conditions , util . Replica Set To Deployment for i := range all R Ss { rs := all R for _ , c := range rs . Status . Conditions { if c . Type != apps . Replica Set Replica conditions = append ( conditions , util . Replica Set To Deployment } 
func ( dc * Deployment Controller ) requeue Stuck Deployment ( d * apps . Deployment , new Status apps . Deployment Status ) time . Duration { current Cond := util . Get Deployment Condition ( d . Status , apps . Deployment // Can't estimate progress if there is no deadline in the spec or progressing condition in the current status. if ! util . Has Progress Deadline ( d ) || current // No need to estimate progress if the rollout is complete or already timed out. if util . Deployment Complete ( d , & new Status ) || current Cond . Reason == util . Timed Out // If there is no sign of progress at this point then there is a high chance that the // deployment is stuck. We should resync this deployment at some point in the future[1] // and check whether it has timed out. We definitely need this, otherwise we depend on the // controller resync interval. See https://github.com/kubernetes/kubernetes/issues/34458. // // [1] Progressing Condition.Last Updated Time + progress Deadline Seconds - time.Now() // // For example, if a Deployment updated its Progressing condition 3 minutes ago and has a // deadline of 10 minutes, it would need to be resynced for a progress check after 7 minutes. // // last Updated: 00:00:00 // now: 00:03:00 // progress Deadline Seconds: 600 (10 minutes) // // last Updated + progress Deadline Seconds - now => 00:00:00 + 00:10:00 - 00:03:00 => 07:00 after := current Cond . Last Update Time . Time . Add ( time . Duration ( * d . Spec . Progress Deadline Seconds ) * time . Second ) . Sub ( now // If the remaining time is less than a second, then requeue the deployment immediately. // Make it ratelimited so we stay on the safe side, eventually the Deployment should // transition either to a Complete or to a Timed dc . enqueue Rate // Add a second to avoid milliseconds skew in Add After. // See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info. dc . enqueue } 
func New Cmd API Versions ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New API Versions Options ( io cmd := & cobra . Command { Use : " " , Short : " \" \" " , Long : " \" \" " , Example : apiversions Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check Err ( o . Run API } 
func ( o * API Versions Options ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { if len ( args ) != 0 { return cmdutil . Usage o . discovery Client , err = f . To Discovery } 
func ( o * API Versions Options ) Run API Versions ( ) error { // Always request fresh data from the server o . discovery group List , err := o . discovery Client . Server api Versions := metav1 . Extract Group Versions ( group sort . Strings ( api for _ , v := range api } 
func New Resource Store ( ) * Resource Store { return & Resource Store { Secret Store : make ( map [ string ] * corev1 . Secret ) , Config Map Store : make ( map [ string ] * corev1 . Config } 
func get Secret Ref Value ( client kubernetes . Interface , namespace string , store * Resource Store , secret Selector * corev1 . Secret Key Selector ) ( string , error ) { secret , ok := store . Secret Store [ secret secret , err = client . Core V1 ( ) . Secrets ( namespace ) . Get ( secret Selector . Name , metav1 . Get store . Secret Store [ secret if data , ok := secret . Data [ secret return " " , fmt . Errorf ( " " , secret Selector . Key , secret } 
func get Config Map Ref Value ( client kubernetes . Interface , namespace string , store * Resource Store , config Map Selector * corev1 . Config Map Key Selector ) ( string , error ) { config Map , ok := store . Config Map Store [ config Map config Map , err = client . Core V1 ( ) . Config Maps ( namespace ) . Get ( config Map Selector . Name , metav1 . Get store . Config Map Store [ config Map Selector . Name ] = config if data , ok := config Map . Data [ config Map return " " , fmt . Errorf ( " " , config Map Selector . Key , config Map } 
func get Field Ref ( obj runtime . Object , from * corev1 . Env Var Source ) ( string , error ) { return extract Field Path As String ( obj , from . Field Ref . Field } 
func get Resource Field Ref ( from * corev1 . Env Var Source , container * corev1 . Container ) ( string , error ) { return extract Container Resource Value ( from . Resource Field } 
func extract Container Resource Value ( fs * corev1 . Resource Field if divisor . Cmp ( fs . Divisor ) == 0 { divisor = resource . Must switch fs . Resource { case " " : return convert Resource CPU To case " " : return convert Resource Memory To case " " : return convert Resource Ephemeral Storage To String ( container . Resources . Limits . Storage case " " : return convert Resource CPU To case " " : return convert Resource Memory To case " " : return convert Resource Ephemeral Storage To String ( container . Resources . Requests . Storage } 
func Get Env Var Ref Value ( kc kubernetes . Interface , ns string , store * Resource Store , from * corev1 . Env Var Source , obj runtime . Object , c * corev1 . Container ) ( string , error ) { if from . Secret Key Ref != nil { return get Secret Ref Value ( kc , ns , store , from . Secret Key if from . Config Map Key Ref != nil { return get Config Map Ref Value ( kc , ns , store , from . Config Map Key if from . Field Ref != nil { return get Field if from . Resource Field Ref != nil { return get Resource Field } 
func Get Env Var Ref String ( from * corev1 . Env Var Source ) string { if from . Config Map Key Ref != nil { return fmt . Sprintf ( " " , from . Config Map Key Ref . Name , from . Config Map Key if from . Secret Key Ref != nil { return fmt . Sprintf ( " " , from . Secret Key Ref . Name , from . Secret Key if from . Field Ref != nil { return fmt . Sprintf ( " " , from . Field Ref . Field if from . Resource Field Ref != nil { container if from . Resource Field Ref . Container Name != " " { container Prefix = fmt . Sprintf ( " " , from . Resource Field Ref . Container return fmt . Sprintf ( " " , container Prefix , from . Resource Field } 
func Get CSI Attach Limit Key ( driver Name string ) string { csi Prefix Length := len ( CSI Attach Limit totalkey Length := csi Prefix Length + len ( driver if totalkey Length >= Resource Name Length Limit { chars From Driver Name := driver hash . Write ( [ ] byte ( driver hashed := hex . Encode To return CSI Attach Limit Prefix + chars From Driver return CSI Attach Limit Prefix + driver } 
func Is Web Socket Request ( req * http . Request ) bool { if ! strings . Equal return connection Upgrade Regex . Match String ( strings . To } 
func Ignore Receives ( ws * websocket . Conn , timeout time . Duration ) { defer runtime . Handle for { reset } 
} 
func New Default Channel Protocols ( channels [ ] Channel Type ) map [ string ] Channel Protocol Config { return map [ string ] Channel Protocol Config { " " : { Binary : true , Channels : channels } , Channel Web Socket Protocol : { Binary : true , Channels : channels } , Base64Channel Web Socket } 
func New Conn ( protocols map [ string ] Channel Protocol } 
func ( conn * Conn ) Open ( w http . Response Writer , req * http . Request ) ( string , [ ] io . Read Write Closer , error ) { go func ( ) { defer runtime . Handle websocket . Server { Handshake : conn . handshake , Handler : conn . handle } . Serve rwc := make ( [ ] io . Read Write return conn . selected } 
} 
func ( conn * Conn ) write ( num byte , data [ ] byte ) ( int , error ) { conn . reset switch conn . codec { case raw case base64Codec : frame := string ( '0' + num ) + base64 . Std Encoding . Encode To } 
func new Websocket Channel ( conn * Conn , num byte , read , write bool ) * websocket return & websocket } 
func ( p * websocket Channel ) Data From switch p . conn . codec { case raw n , err := base64 . Std } 
func Register Volume Scheduling Metrics ( ) { prometheus . Must Register ( Volume Binding Request Scheduler Binder prometheus . Must Register ( Volume Scheduling Stage prometheus . Must Register ( Volume Scheduling Stage } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Validating Webhook Configuration { } , & Validating Webhook Configuration List { } , & Mutating Webhook Configuration { } , & Mutating Webhook Configuration } 
func Load Config ( config File io . Reader ) ( string , error ) { var kubeconfig if config File != nil { // we have a config so parse it. data , err := ioutil . Read All ( config decoder := codecs . Universal decoded config , ok := decoded Obj . ( * webhookadmission . Webhook if ! ok { return " " , fmt . Errorf ( " " , decoded if ! path . Is Abs ( config . Kube Config File ) { return " " , field . Invalid ( field . New Path ( " " ) , config . Kube Config kubeconfig File = config . Kube Config return kubeconfig } 
func Create Listener ( endpoint string ) ( net . Listener , error ) { protocol , addr , err := parse switch protocol { case tcp Protocol : return net . Listen ( tcp case npipe Protocol : return winio . Listen } 
func Get Address And Dialer ( endpoint string ) ( string , func ( addr string , timeout time . Duration ) ( net . Conn , error ) , error ) { protocol , addr , err := parse if protocol == tcp Protocol { return addr , tcp if protocol == npipe Protocol { return addr , npipe } 
func Local Endpoint ( path , file string ) string { u := url . URL { Scheme : npipe } 
func Get Boot Time ( ) ( time . Time , error ) { current output , _ , err := tick return current } 
func New } 
func Int64Key Set ( the Map interface { } ) Int64 { v := reflect . Value Of ( the for _ , key Value := range v . Map Keys ( ) { ret . Insert ( key } 
} 
} 
} 
func ( s Int64 ) Has } 
func ( s Int64 ) Difference ( s2 Int64 ) Int64 { result := New } 
func ( s1 Int64 ) Union ( s2 Int64 ) Int64 { result := New } 
result := New } 
func ( s1 Int64 ) Equal ( s2 Int64 ) bool { return len ( s1 ) == len ( s2 ) && s1 . Is } 
func ( s Int64 ) List ( ) [ ] int64 { res := make ( sortable Slice Of } 
func ( s Int64 ) Unsorted } 
func ( s Int64 ) Pop var zero return zero } 
func ( s * legacy Root API Handler ) Web Service ( ) * restful . Web Service { media Types , _ := negotiation . Media Types For ws := new ( restful . Web ws . Path ( s . api ws . Route ( ws . GET ( " " ) . To ( s . handle ) . Doc ( " " ) . Operation ( " " ) . Produces ( media Types ... ) . Consumes ( media Types ... ) . Writes ( metav1 . API } 
func ( cm * container Manager Impl ) create Node Allocatable Cgroups ( ) error { cgroup Config := & Cgroup Config { Name : cm . cgroup Root , // The default limits for cpu shares can be very low which can lead to CPU starvation for pods. Resource Parameters : get Cgroup Config ( cm . internal if cm . cgroup Manager . Exists ( cgroup if err := cm . cgroup Manager . Create ( cgroup Config ) ; err != nil { klog . Errorf ( " " , cm . cgroup } 
func ( cm * container Manager Impl ) enforce Node Allocatable Cgroups ( ) error { nc := cm . Node Config . Node Allocatable // We need to update limits on node allocatable cgroup no matter what because // default cpu shares on cgroups are low and can cause cpu starvation. node Allocatable := cm . internal // Use Node Allocatable limits instead of capacity if the user requested enforcing node allocatable. if cm . Cgroups Per QOS && nc . Enforce Node Allocatable . Has ( kubetypes . Node Allocatable Enforcement Key ) { node Allocatable = cm . get Node Allocatable Internal cgroup Config := & Cgroup Config { Name : cm . cgroup Root , Resource Parameters : get Cgroup Config ( node // Using Object Reference for events as the node maybe not cached; refer to #42701 for detail. node Ref := & v1 . Object Reference { Kind : " " , Name : cm . node Info . Name , UID : types . UID ( cm . node // If Node Allocatable is enforced on a node that has not been drained or is updated on an existing node to a lower value, // existing memory usage across pods might be higher than current Node Allocatable Memory Limits. // Pod Evictions are expected to bring down memory usage to below Node Allocatable limits. // Until evictions happen retry cgroup updates. // Update limits on non root cgroup-root to be safe since the default limits for CPU can be too low. // Check if cgroup Root is set to a non-empty value (empty would be the root container) if len ( cm . cgroup Root ) > 0 { go func ( ) { for { err := cm . cgroup Manager . Update ( cgroup if err == nil { cm . recorder . Event ( node Ref , v1 . Event Type Normal , events . Successful Node Allocatable message := fmt . Sprintf ( " " , cm . cgroup cm . recorder . Event ( node Ref , v1 . Event Type Warning , events . Failed Node Allocatable // Now apply kube reserved and system reserved limits if required. if nc . Enforce Node Allocatable . Has ( kubetypes . System Reserved Enforcement Key ) { klog . V ( 2 ) . Infof ( " " , nc . System Reserved Cgroup Name , nc . System if err := enforce Existing Cgroup ( cm . cgroup Manager , Parse Cgroupfs To Cgroup Name ( nc . System Reserved Cgroup Name ) , nc . System Reserved ) ; err != nil { message := fmt . Sprintf ( " " , nc . System Reserved Cgroup cm . recorder . Event ( node Ref , v1 . Event Type Warning , events . Failed Node Allocatable cm . recorder . Eventf ( node Ref , v1 . Event Type Normal , events . Successful Node Allocatable Enforcement , " " , nc . System Reserved Cgroup if nc . Enforce Node Allocatable . Has ( kubetypes . Kube Reserved Enforcement Key ) { klog . V ( 2 ) . Infof ( " " , nc . Kube Reserved Cgroup Name , nc . Kube if err := enforce Existing Cgroup ( cm . cgroup Manager , Parse Cgroupfs To Cgroup Name ( nc . Kube Reserved Cgroup Name ) , nc . Kube Reserved ) ; err != nil { message := fmt . Sprintf ( " " , nc . Kube Reserved Cgroup cm . recorder . Event ( node Ref , v1 . Event Type Warning , events . Failed Node Allocatable cm . recorder . Eventf ( node Ref , v1 . Event Type Normal , events . Successful Node Allocatable Enforcement , " " , nc . Kube Reserved Cgroup } 
func enforce Existing Cgroup ( cgroup Manager Cgroup Manager , c Name Cgroup Name , rl v1 . Resource List ) error { cgroup Config := & Cgroup Config { Name : c Name , Resource Parameters : get Cgroup if cgroup Config . Resource Parameters == nil { return fmt . Errorf ( " " , cgroup klog . V ( 4 ) . Infof ( " " , c Name , cgroup Config . Resource Parameters . Cpu Shares , cgroup Config . Resource Parameters . Memory , cgroup Config . Resource Parameters . Pids if ! cgroup Manager . Exists ( cgroup Config . Name ) { return fmt . Errorf ( " " , cgroup if err := cgroup Manager . Update ( cgroup } 
func get Cgroup Config ( rl v1 . Resource List ) * Resource var rc Resource if q , exists := rl [ v1 . Resource if q , exists := rl [ v1 . Resource CPU ] ; exists { // CPU is defined in milli-cores. val := Milli CPU To Shares ( q . Milli rc . Cpu if q , exists := rl [ pidlimit . PI rc . Pids rc . Huge Page Limit = Huge Page } 
func ( cm * container Manager Impl ) Get Node Allocatable Reservation ( ) v1 . Resource List { eviction Reservation := hard Eviction Reservation ( cm . Hard Eviction result := make ( v1 . Resource for k := range cm . capacity { value := resource . New Quantity ( 0 , resource . Decimal if cm . Node Config . System Reserved != nil { value . Add ( cm . Node Config . System if cm . Node Config . Kube Reserved != nil { value . Add ( cm . Node Config . Kube if eviction Reservation != nil { value . Add ( eviction if ! value . Is } 
func ( cm * container Manager Impl ) validate Node nar := cm . Get Node Allocatable for k , v := range nar { value := cm . capacity [ k ] . Deep } 
func ( ed * empty Dir ) Set Up ( fs Group * int64 ) error { return ed . Set Up At ( ed . Get Path ( ) , fs } 
func ( ed * empty Dir ) Set Up At ( dir string , fs Group * int64 ) error { not Mnt , err := ed . mounter . Is Likely Not Mount // Getting an os.Is Not Exist err from is a contingency; the directory // may not exist yet, in which case, setup should run. if err != nil && ! os . Is Not // If the plugin readiness file is present for this volume, and the // storage medium is the default, then the volume is ready. If the // medium is memory, and a mountpoint is present, then the volume is // ready. if volumeutil . Is Ready ( ed . get Meta Dir ( ) ) { if ed . medium == v1 . Storage Medium Memory && ! not } else if ed . medium == v1 . Storage Medium switch ed . medium { case v1 . Storage Medium Default : err = ed . setup case v1 . Storage Medium Memory : err = ed . setup case v1 . Storage Medium Huge Pages : err = ed . setup volume . Set Volume Ownership ( ed , fs if err == nil { volumeutil . Set Ready ( ed . get Meta } 
func ( ed * empty Dir ) setup if err := ed . setup // Make Set Up idempotent. medium , is Mnt , err := ed . mount Detector . Get Mount // If the directory is a mountpoint with medium memory, there is no // work to do since we are already in the desired state. if is Mnt && medium == v1 . Storage Medium klog . V ( 3 ) . Infof ( " " , ed . pod . UID , ed . vol } 
func ( ed * empty Dir ) setup if err := ed . setup // Make Set Up idempotent. medium , is Mnt , err := ed . mount Detector . Get Mount // If the directory is a mountpoint with medium hugepages, there is no // work to do since we are already in the desired state. if is Mnt && medium == v1 . Storage Medium Huge page Size Mount Option , err := get Page Size Mount Option From klog . V ( 3 ) . Infof ( " " , ed . pod . UID , ed . vol return ed . mounter . Mount ( " " , dir , " " , [ ] string { page Size Mount } 
func get Page Size Mount Option From Pod ( pod * v1 . Pod ) ( string , error ) { page Size page // In some rare cases init containers can also consume Huge pages. containers := append ( pod . Spec . Containers , pod . Spec . Init for _ , container := range containers { // We can take request because limit and requests must match. for request Name := range container . Resources . Requests { if v1helper . Is Huge Page Resource Name ( request Name ) { current Page Size , err := v1helper . Huge Page Size From Resource Name ( request // Page Size for all volumes in a POD are equal, except for the first one discovered. if page Size Found && page Size . Cmp ( current Page page Size = current Page page Size if ! page Size return fmt . Sprintf ( " " , huge Pages Page Size Mount Option , page } 
func ( ed * empty Dir ) setup Dir ( dir string ) error { // Create the directory if it doesn't already exist. if err := os . Mkdir } 
func ( ed * empty Dir ) Tear Down At ( dir string ) error { if path Exists , path Err := mount . Path Exists ( dir ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path // Figure out the medium. medium , is Mnt , err := ed . mount Detector . Get Mount if is Mnt { if medium == v1 . Storage Medium Memory { ed . medium = v1 . Storage Medium return ed . teardown Tmpfs Or } else if medium == v1 . Storage Medium Huge Pages { ed . medium = v1 . Storage Medium Huge return ed . teardown Tmpfs Or // assume Storage Medium Default return ed . teardown } 
func ( s * Horizontal Pod Autoscaler Generator V1 ) Structured scaler := autoscalingv1 . Horizontal Pod Autoscaler { Object Meta : metav1 . Object Meta { Name : s . Name , } , Spec : autoscalingv1 . Horizontal Pod Autoscaler Spec { Scale Target Ref : autoscalingv1 . Cross Version Object Reference { Kind : s . Scale Ref Kind , Name : s . Scale Ref Name , API Version : s . Scale Ref API Version , } , Max Replicas : s . Max if s . Min Replicas > 0 { v := int32 ( s . Min scaler . Spec . Min if s . CPU Percent >= 0 { c := int32 ( s . CPU scaler . Spec . Target CPU Utilization } 
func ( s Horizontal Pod Autoscaler Generator if s . Max if s . Min Replicas > s . Max } 
func ( p * TTL Policy ) Is Expired ( obj * Timestamped } 
func ( c * Expiration Cache ) get Or Expire ( key string ) ( interface { } , bool ) { // Prevent all inserts from the time we deem an item as "expired" to when we // delete it, so an un-expired item doesn't sneak in under the same key, just // before the Delete. c . expiration defer c . expiration timestamped Item , exists := c . get Timestamped if c . expiration Policy . Is Expired ( timestamped Item ) { klog . V ( 4 ) . Infof ( " " , key , timestamped c . cache return timestamped } 
func ( c * Expiration Cache ) Get By Key ( key string ) ( interface { } , bool , error ) { obj , exists := c . get Or } 
func ( c * Expiration Cache ) Get ( obj interface { } ) ( interface { } , bool , error ) { key , err := c . key if err != nil { return nil , false , Key obj , exists := c . get Or } 
func ( c * Expiration Cache ) List ( ) [ ] interface { } { items := c . cache for _ , item := range items { obj := item . ( * Timestamped if key , err := c . key } else if obj , exists := c . get Or } 
func ( c * Expiration Cache ) Add ( obj interface { } ) error { key , err := c . key if err != nil { return Key c . expiration defer c . expiration c . cache Storage . Add ( key , & Timestamped } 
func ( c * Expiration Cache ) Delete ( obj interface { } ) error { key , err := c . key if err != nil { return Key c . expiration defer c . expiration c . cache } 
func ( c * Expiration Cache ) Replace ( list [ ] interface { } , resource for _ , item := range list { key , err := c . key if err != nil { return Key items [ key ] = & Timestamped c . expiration defer c . expiration c . cache Storage . Replace ( items , resource } 
func New TTL Store ( key Func Key Func , ttl time . Duration ) Store { return New Expiration Store ( key Func , & TTL Policy { ttl , clock . Real } 
func New Expiration Store ( key Func Key Func , expiration Policy Expiration Policy ) Store { return & Expiration Cache { cache Storage : New Thread Safe Store ( Indexers { } , Indices { } ) , key Func : key Func , clock : clock . Real Clock { } , expiration Policy : expiration } 
func New Node Status Updater ( kube Client clientset . Interface , node Lister corelisters . Node Lister , actual State Of World cache . Actual State Of World ) Node Status Updater { return & node Status Updater { actual State Of World : actual State Of World , node Lister : node Lister , kube Client : kube } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Admission Configuration ) ( nil ) , ( * apiserver . Admission Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Admission Configuration_To_apiserver_Admission Configuration ( a . ( * Admission Configuration ) , b . ( * apiserver . Admission if err := s . Add Generated Conversion Func ( ( * apiserver . Admission Configuration ) ( nil ) , ( * Admission Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiserver_Admission Configuration_To_v1alpha1_Admission Configuration ( a . ( * apiserver . Admission Configuration ) , b . ( * Admission if err := s . Add Generated Conversion Func ( ( * Admission Plugin Configuration ) ( nil ) , ( * apiserver . Admission Plugin Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Admission Plugin Configuration_To_apiserver_Admission Plugin Configuration ( a . ( * Admission Plugin Configuration ) , b . ( * apiserver . Admission Plugin if err := s . Add Generated Conversion Func ( ( * apiserver . Admission Plugin Configuration ) ( nil ) , ( * Admission Plugin Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiserver_Admission Plugin Configuration_To_v1alpha1_Admission Plugin Configuration ( a . ( * apiserver . Admission Plugin Configuration ) , b . ( * Admission Plugin } 
func Convert_v1alpha1_Admission Configuration_To_apiserver_Admission Configuration ( in * Admission Configuration , out * apiserver . Admission Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Admission Configuration_To_apiserver_Admission } 
func Convert_apiserver_Admission Configuration_To_v1alpha1_Admission Configuration ( in * apiserver . Admission Configuration , out * Admission Configuration , s conversion . Scope ) error { return auto Convert_apiserver_Admission Configuration_To_v1alpha1_Admission } 
func Convert_v1alpha1_Admission Plugin Configuration_To_apiserver_Admission Plugin Configuration ( in * Admission Plugin Configuration , out * apiserver . Admission Plugin Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Admission Plugin Configuration_To_apiserver_Admission Plugin } 
func Convert_apiserver_Admission Plugin Configuration_To_v1alpha1_Admission Plugin Configuration ( in * apiserver . Admission Plugin Configuration , out * Admission Plugin Configuration , s conversion . Scope ) error { return auto Convert_apiserver_Admission Plugin Configuration_To_v1alpha1_Admission Plugin } 
func Pod Condition By Kubelet ( condition Type v1 . Pod Condition Type ) bool { for _ , c := range Pod Conditions By Kubelet { if c == condition } 
func ( s * network Policy Lister ) Network Policies ( namespace string ) Network Policy Namespace Lister { return network Policy Namespace } 
func ( s network Policy Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Network Policy , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Network } 
func New Path Element ( s string ) ( fieldpath . Path Element , error ) { split := strings . Split if len ( split ) < 2 { return fieldpath . Path switch split [ 0 ] { case Field : return fieldpath . Path Element { Field case Value : val , err := value . From if err != nil { return fieldpath . Path return fieldpath . Path if err != nil { return fieldpath . Path return fieldpath . Path case Key : kv := map [ string ] json . Raw if err != nil { return fieldpath . Path if err != nil { return fieldpath . Path val , err := value . From if err != nil { return fieldpath . Path return fieldpath . Path default : // Ignore unknown key types return fieldpath . Path } 
func Path Element String ( pe fieldpath . Path Element ) ( string , error ) { switch { case pe . Field Name != nil : return Field + Separator + * pe . Field case len ( pe . Key ) > 0 : kv := map [ string ] json . Raw for _ , k := range pe . Key { b , err := k . Value . To m := json . Raw case pe . Value != nil : b , err := pe . Value . To } 
func ( d * namespaced Resources Deleter ) Delete ( ns Name string ) error { // Multiple controllers may edit a namespace during termination // first get the latest state of the namespace before proceeding // if the namespace was deleted already, don't do anything namespace , err := d . ns Client . Get ( ns Name , metav1 . Get if err != nil { if errors . Is Not if namespace . Deletion klog . V ( 5 ) . Infof ( " " , namespace . Name , d . finalizer // ensure that the status is up to date on the namespace // if we get a not found error, we assume the namespace is truly gone namespace , err = d . retry On Conflict Error ( namespace , d . update Namespace Status if err != nil { if errors . Is Not // the latest view of the namespace asserts that namespace is no longer deleting.. if namespace . Deletion Timestamp . Is // Delete the namespace if it is already finalized. if d . delete Namespace When Done && finalized ( namespace ) { return d . delete // there may still be content for us to remove estimate , err := d . delete All Content ( namespace . Name , * namespace . Deletion if estimate > 0 { return & Resources Remaining // we have removed content, so mark it finalized by us namespace , err = d . retry On Conflict Error ( namespace , d . finalize if err != nil { // in normal practice, this should not be possible, but if a deployment is running // two controllers to do namespace deletion that share a common finalizer token it's // possible that a not found could occur since the other controller would have finished the delete. if errors . Is Not // Check if we can delete now. if d . delete Namespace When Done && finalized ( namespace ) { return d . delete } 
func ( d * namespaced Resources Deleter ) delete Namespace ( namespace * v1 . Namespace ) error { var opts * metav1 . Delete if len ( uid ) > 0 { opts = & metav1 . Delete err := d . ns if err != nil && ! errors . Is Not } 
func ( o * operation Not Supported Cache ) is Supported ( key operation Key ) bool { o . lock . R defer o . lock . R } 
func ( d * namespaced Resources Deleter ) retry On Conflict Error ( namespace * v1 . Namespace , fn update Namespace Func ) ( result * v1 . Namespace , err error ) { latest for { result , err = fn ( latest if ! errors . Is prev Namespace := latest latest Namespace , err = d . ns Client . Get ( latest Namespace . Name , metav1 . Get if prev Namespace . UID != latest } 
func ( d * namespaced Resources Deleter ) update Namespace Status Func ( namespace * v1 . Namespace ) ( * v1 . Namespace , error ) { if namespace . Deletion Timestamp . Is Zero ( ) || namespace . Status . Phase == v1 . Namespace new new Namespace . Object Meta = namespace . Object new new Namespace . Status . Phase = v1 . Namespace return d . ns Client . Update Status ( & new } 
func ( d * namespaced Resources Deleter ) finalize Namespace ( namespace * v1 . Namespace ) ( * v1 . Namespace , error ) { namespace namespace Finalize . Object Meta = namespace . Object namespace finalizer Set := sets . New for i := range namespace . Spec . Finalizers { if namespace . Spec . Finalizers [ i ] != d . finalizer Token { finalizer namespace Finalize . Spec . Finalizers = make ( [ ] v1 . Finalizer Name , 0 , len ( finalizer for _ , value := range finalizer Set . List ( ) { namespace Finalize . Spec . Finalizers = append ( namespace Finalize . Spec . Finalizers , v1 . Finalizer namespace , err := d . ns Client . Finalize ( & namespace if err != nil { // it was removed already, so life is good if errors . Is Not } 
func ( d * namespaced Resources Deleter ) delete Collection ( gvr schema . Group Version key := operation Key { operation : operation Delete if ! d . op Cache . is // namespace controller does not want the garbage collector to insert the orphan finalizer since it calls // resource deletions generically. it will ensure all resources in the namespace are purged prior to releasing // namespace itself. background := metav1 . Delete Propagation opts := & metav1 . Delete Options { Propagation err := d . dynamic Client . Resource ( gvr ) . Namespace ( namespace ) . Delete Collection ( opts , metav1 . List // this is strange, but we need to special case for both Method Not Supported and Not Found errors // TODO: https://github.com/kubernetes/kubernetes/issues/22413 // we have a resource returned in the discovery API that supports no top-level verbs: // /apis/extensions/v1beta1/namespaces/default/replicationcontrollers // when working with this resource type, we will get a literal not found error rather than expected method not supported // remember next time that this resource does not support delete collection... if errors . Is Method Not Supported ( err ) || errors . Is Not d . op Cache . set Not } 
func ( d * namespaced Resources Deleter ) list Collection ( gvr schema . Group Version Resource , namespace string ) ( * unstructured . Unstructured key := operation Key { operation : operation if ! d . op Cache . is unstructured List , err := d . dynamic Client . Resource ( gvr ) . Namespace ( namespace ) . List ( metav1 . List if err == nil { return unstructured // this is strange, but we need to special case for both Method Not Supported and Not Found errors // TODO: https://github.com/kubernetes/kubernetes/issues/22413 // we have a resource returned in the discovery API that supports no top-level verbs: // /apis/extensions/v1beta1/namespaces/default/replicationcontrollers // when working with this resource type, we will get a literal not found error rather than expected method not supported // remember next time that this resource does not support delete collection... if errors . Is Method Not Supported ( err ) || errors . Is Not d . op Cache . set Not } 
func ( d * namespaced Resources Deleter ) delete Each Item ( gvr schema . Group Version unstructured List , list Supported , err := d . list if ! list for _ , item := range unstructured List . Items { background := metav1 . Delete Propagation opts := & metav1 . Delete Options { Propagation if err = d . dynamic Client . Resource ( gvr ) . Namespace ( namespace ) . Delete ( item . Get Name ( ) , opts ) ; err != nil && ! errors . Is Not Found ( err ) && ! errors . Is Method Not } 
func ( d * namespaced Resources Deleter ) delete All Content For Group Version Resource ( gvr schema . Group Version Resource , namespace string , namespace Deleted // estimate how long it will take for the resource to be deleted (needed for objects that support graceful delete) estimate , err := d . estimate Graceful Termination ( gvr , namespace , namespace Deleted // first try to delete the entire collection delete Collection Supported , err := d . delete // delete collection was not supported, so we list and delete each item... if ! delete Collection Supported { err = d . delete Each unstructured List , list Supported , err := d . list if ! list klog . V ( 5 ) . Infof ( " " , namespace , gvr , len ( unstructured if len ( unstructured List . Items ) != 0 && estimate == int64 ( 0 ) { // if any item has a finalizer, we treat that as a normal condition, and use a default estimation to allow for GC to complete. for _ , item := range unstructured List . Items { if len ( item . Get Finalizers ( ) ) > 0 { klog . V ( 5 ) . Infof ( " " , namespace , gvr , item . Get return finalizer Estimate } 
func ( d * namespaced Resources Deleter ) delete All Content ( namespace string , namespace Deleted resources , err := d . discover Resources // TODO(sttts): get rid of op Cache and pass the verbs (especially "deletecollection") down into the deleter deletable Resources := discovery . Filtered By ( discovery . Supports All group Version Resources , err := discovery . Group Version Resources ( deletable for gvr := range group Version Resources { gvr Estimate , err := d . delete All Content For Group Version Resource ( gvr , namespace , namespace Deleted if err != nil { // If there is an error, hold on to it but proceed with all the remaining // group Version if gvr Estimate > estimate { estimate = gvr if len ( errs ) > 0 { return estimate , utilerrors . New } 
func ( d * namespaced Resources Deleter ) estimate Graceful Termination ( gvr schema . Group Version Resource , ns string , namespace Deleted At metav1 . Time ) ( int64 , error ) { group Resource := gvr . Group klog . V ( 5 ) . Infof ( " " , group Resource . Group , group switch group Resource { case schema . Group Resource { Group : " " , Resource : " " } : estimate , err = d . estimate Graceful Termination For // determine if the estimate is greater than the deletion timestamp duration := time . Since ( namespace Deleted allowed if duration >= allowed } 
func ( d * namespaced Resources Deleter ) estimate Graceful Termination For pods Getter := d . pods if pods Getter == nil || reflect . Value Of ( pods Getter ) . Is items , err := pods Getter . Pods ( ns ) . List ( metav1 . List if v1 . Pod Succeeded == phase || v1 . Pod if pod . Spec . Termination Grace Period Seconds != nil { grace := * pod . Spec . Termination Grace Period } 
func ( in * Cluster ) Deep Copy if in . Certificate Authority Data != nil { in , out := & in . Certificate Authority Data , & out . Certificate Authority } else { ( * out ) [ key ] = val . Deep Copy } 
func ( in * Config ) Deep Copy in . Preferences . Deep Copy for key , val := range * in { var out } else { in , out := & val , & out ( * in ) . Deep Copy ( * out ) [ key ] = out if in . Auth Infos != nil { in , out := & in . Auth Infos , & out . Auth * out = make ( map [ string ] * Auth for key , val := range * in { var out Val * Auth } else { in , out := & val , & out * out = new ( Auth ( * in ) . Deep Copy ( * out ) [ key ] = out for key , val := range * in { var out } else { in , out := & val , & out ( * in ) . Deep Copy ( * out ) [ key ] = out } else { ( * out ) [ key ] = val . Deep Copy } 
func ( in * Context ) Deep Copy } else { ( * out ) [ key ] = val . Deep Copy } 
func Validate Conditional Network Policy ( np , old NP * networking . Network Policy ) field . Error List { var errs field . Error // If the SCTP Support feature is disabled, and the old object isn't using the SCTP feature, prevent the new object from using it if ! utilfeature . Default Feature Gate . Enabled ( features . SCTP Support ) && len ( sctp Fields ( old NP ) ) == 0 { for _ , f := range sctp Fields ( np ) { errs = append ( errs , field . Not Supported ( f , api . Protocol SCTP , [ ] string { string ( api . Protocol TCP ) , string ( api . Protocol } 
func ( rc Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { controller := obj . ( * api . Replication controller . Status = api . Replication Controller pod . Drop Disabled Template } 
func ( rc Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Controller := obj . ( * api . Replication old Controller := old . ( * api . Replication // update is not allowed to set status new Controller . Status = old pod . Drop Disabled Template Fields ( new Controller . Spec . Template , old // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. We push // the burden of managing the status onto the clients because we can't (in general) // know here what version of spec the writer of the status has seen. It may seem like // we can at first -- since obj contains spec -- but in the future we will probably make // status its own object, and even if we don't, writes may be the result of a // read-update-write loop, so the contents of spec may not actually be the spec that // the controller has *seen*. if ! apiequality . Semantic . Deep Equal ( old Controller . Spec , new Controller . Spec ) { new Controller . Generation = old } 
func ( rc Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { controller := obj . ( * api . Replication all Errs := validation . Validate Replication all Errs = append ( all Errs , validation . Validate Conditional Pod Template ( controller . Spec . Template , nil , field . New return all } 
func ( rc Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { old Rc := old . ( * api . Replication new Rc := obj . ( * api . Replication validation Error List := validation . Validate Replication Controller ( new update Error List := validation . Validate Replication Controller Update ( new Rc , old update Error List = append ( update Error List , validation . Validate Conditional Pod Template ( new Rc . Spec . Template , old Rc . Spec . Template , field . New errs := append ( validation Error List , update Error for key , value := range helper . Non Convertible Fields ( old broken switch { case strings . Contains ( broken Field , " " ) : if ! apiequality . Semantic . Deep Equal ( old Rc . Spec . Selector , new Rc . Spec . Selector ) { errs = append ( errs , field . Invalid ( field . New Path ( " " ) . Child ( " " ) , new default : errs = append ( errs , & field . Error { Type : field . Error Type Not Found , Bad Value : value , Field : broken } 
func Controller To Selectable Fields ( controller * api . Replication Controller ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & controller . Object controller Specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , controller Specific Fields } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { rc , ok := obj . ( * api . Replication return labels . Set ( rc . Object Meta . Labels ) , Controller To Selectable } 
func ( b Base Deployment } 
func ( b Base Deployment Generator ) structured Generate ( ) ( pod Spec v1 . Pod Spec , labels map [ string ] string , selector metav1 . Label pod Spec = build Pod selector = metav1 . Label Selector { Match } 
func build Pod Spec ( images [ ] string ) v1 . Pod Spec { pod Spec := v1 . Pod for _ , image String := range images { // Retain just the image name image Split := strings . Split ( image name := image Split [ len ( image pod Spec . Containers = append ( pod Spec . Containers , v1 . Container { Name : name , Image : image return pod } 
func ( s * Deployment Basic Generator V1 ) Structured Generate ( ) ( runtime . Object , error ) { pod Spec , labels , selector , err := s . structured return & extensionsv1beta1 . Deployment { Object Meta : metav1 . Object Meta { Name : s . Name , Labels : labels , } , Spec : extensionsv1beta1 . Deployment Spec { Replicas : & one , Selector : & selector , Template : v1 . Pod Template Spec { Object Meta : metav1 . Object Meta { Labels : labels , } , Spec : pod } 
func create Cloud Provider ( cloud Provider string , external Cloud Volume Plugin string , cloud Config File string , allow Untagged Cloud bool , shared Informers informers . Shared Informer Factory ) ( cloudprovider . Interface , Controller Loop var loop Mode Controller Loop if cloudprovider . Is External ( cloud Provider ) { loop Mode = External if external Cloud Volume Plugin == " " { // external Cloud Volume Plugin is temporary until we split all cloud providers out. // So we just tell the caller that we need to run External Loops without any cloud provider. return nil , loop cloud , err = cloudprovider . Init Cloud Provider ( external Cloud Volume Plugin , cloud Config } else { loop Mode = Include Cloud cloud , err = cloudprovider . Init Cloud Provider ( cloud Provider , cloud Config if err != nil { return nil , loop if cloud != nil && cloud . Has Cluster ID ( ) == false { if allow Untagged } else { return nil , loop if informer User Cloud , ok := cloud . ( cloudprovider . Informer User ) ; ok { informer User Cloud . Set Informers ( shared return cloud , loop } 
func ( c * cidrs ) String ( ) string { s := c . ipn . String } 
func ( c * cidrs ) Set ( value string ) error { // On first Set(), clear the original defaults if ! c . is Set { c . is c . ipn = make ( utilnet . IP Net for _ , cidr := range strings . Split ( value , " " ) { _ , ipnet , err := net . Parse } 
func ( g * Cloud ) Get Load Balancer ( ctx context . Context , cluster Name string , svc * v1 . Service ) ( * v1 . Load Balancer Status , bool , error ) { load Balancer Name := g . Get Load Balancer Name ( ctx , cluster fwd , err := g . Get Region Forwarding Rule ( load Balancer if err == nil { status := & v1 . Load Balancer status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : fwd . IP return nil , false , ignore Not } 
func ( g * Cloud ) Get Load Balancer Name ( ctx context . Context , cluster Name string , svc * v1 . Service ) string { // TODO: replace Default Load Balancer Name to generate more meaningful loadbalancer names. return cloudprovider . Default Load Balancer } 
func ( g * Cloud ) Ensure Load Balancer ( ctx context . Context , cluster Name string , svc * v1 . Service , nodes [ ] * v1 . Node ) ( * v1 . Load Balancer Status , error ) { load Balancer Name := g . Get Load Balancer Name ( ctx , cluster desired Scheme := get Svc cluster ID , err := g . Cluster ID . Get klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer Name , g . region , desired existing Fwd Rule , err := g . Get Region Forwarding Rule ( load Balancer if err != nil && ! is Not if existing Fwd Rule != nil { existing Scheme := cloud . Lb Scheme ( strings . To Upper ( existing Fwd Rule . Load Balancing // If the loadbalancer type changes between INTERNAL and EXTERNAL, the old load balancer should be deleted. if existing Scheme != desired Scheme { klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer Name , g . region , existing switch existing Scheme { case cloud . Scheme Internal : err = g . ensure Internal Load Balancer Deleted ( cluster Name , cluster default : err = g . ensure External Load Balancer Deleted ( cluster Name , cluster klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer Name , g . region , existing // Assume the ensure Deleted function successfully deleted the forwarding rule. existing Fwd var status * v1 . Load Balancer switch desired Scheme { case cloud . Scheme Internal : status , err = g . ensure Internal Load Balancer ( cluster Name , cluster ID , svc , existing Fwd default : status , err = g . ensure External Load Balancer ( cluster Name , cluster ID , svc , existing Fwd klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer } 
func ( g * Cloud ) Update Load Balancer ( ctx context . Context , cluster Name string , svc * v1 . Service , nodes [ ] * v1 . Node ) error { load Balancer Name := g . Get Load Balancer Name ( ctx , cluster scheme := get Svc cluster ID , err := g . Cluster ID . Get klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer switch scheme { case cloud . Scheme Internal : err = g . update Internal Load Balancer ( cluster Name , cluster default : err = g . update External Load Balancer ( cluster klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer } 
func ( g * Cloud ) Ensure Load Balancer Deleted ( ctx context . Context , cluster Name string , svc * v1 . Service ) error { load Balancer Name := g . Get Load Balancer Name ( ctx , cluster scheme := get Svc cluster ID , err := g . Cluster ID . Get klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer switch scheme { case cloud . Scheme Internal : err = g . ensure Internal Load Balancer Deleted ( cluster Name , cluster default : err = g . ensure External Load Balancer Deleted ( cluster Name , cluster klog . V ( 4 ) . Infof ( " " , cluster Name , svc . Namespace , svc . Name , load Balancer } 
func New PV Protection Controller ( pv Informer coreinformers . Persistent Volume Informer , cl clientset . Interface , storage Object In Use Protection Feature Enabled bool ) * Controller { e := & Controller { client : cl , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , storage Object In Use Protection Enabled : storage Object In Use Protection Feature if cl != nil && cl . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { metrics . Register Metric And Track Rate Limiter Usage ( " " , cl . Core V1 ( ) . REST Client ( ) . Get Rate e . pv Lister = pv e . pv Lister Synced = pv Informer . Informer ( ) . Has pv Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : e . pv Added Updated , Update Func : func ( old , new interface { } ) { e . pv Added } 
func ( c * Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer c . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , c . pv Lister for i := 0 ; i < workers ; i ++ { go wait . Until ( c . run Worker , time . Second , stop <- stop } 
func ( c * Controller ) process Next Work Item ( ) bool { pv defer c . queue . Done ( pv pv Name := pv err := c . process PV ( pv if err == nil { c . queue . Forget ( pv utilruntime . Handle Error ( fmt . Errorf ( " " , pv c . queue . Add Rate Limited ( pv } 
func ( c * Controller ) pv Added Updated ( obj interface { } ) { pv , ok := obj . ( * v1 . Persistent if ! ok { utilruntime . Handle if protectionutil . Need To Add Finalizer ( pv , volumeutil . PV Protection Finalizer ) || protectionutil . Is Deletion Candidate ( pv , volumeutil . PV Protection } 
func ( am * address Manager ) Hold Address ( ) ( string , error ) { // Hold Address starts with retrieving the address that we use for this load balancer (by name). // Retrieving an address by IP will indicate if the IP is reserved and if reserved by the user // or the controller, but won't tell us the current state of the controller's IP. The address // could be reserving another address; therefore, it would need to be deleted. In the normal // case of using a controller address, retrieving the address by name results in the fewest API // calls since it indicates whether a Delete is necessary before Reserve. klog . V ( 4 ) . Infof ( " " , am . log Prefix , am . target IP , am . address // Get the address in case it was orphaned earlier addr , err := am . svc . Get Region if err != nil && ! is Not if addr != nil { // If address exists, check if the address had the expected attributes. validation Error := am . validate if validation Error == nil { klog . V ( 4 ) . Infof ( " " , am . log Prefix , addr . Name , addr . Address , addr . Address klog . V ( 2 ) . Infof ( " " , am . log Prefix , validation err := am . svc . Delete Region if err != nil { if is Not Found ( err ) { klog . V ( 4 ) . Infof ( " " , am . log } else { klog . V ( 4 ) . Infof ( " " , am . log return am . ensure Address } 
func ( am * address Manager ) Release Address ( ) error { if ! am . try Release { klog . V ( 4 ) . Infof ( " " , am . log Prefix , am . target klog . V ( 4 ) . Infof ( " " , am . log Prefix , am . target // Controller only ever tries to unreserve the address named with the load balancer's name. err := am . svc . Delete Region if err != nil { if is Not Found ( err ) { klog . Warningf ( " " , am . log klog . V ( 4 ) . Infof ( " " , am . log Prefix , am . target } 
func ( vmdisk vm Disk Manager ) Create ( ctx context . Context , datastore * vclib . Datastore ) ( canonical Disk Path string , err error ) { if vmdisk . volume Options . SCSI Controller Type == " " { vmdisk . volume Options . SCSI Controller Type = vclib . PVSCSI Controller pbm Client , err := vclib . New Pbm if vmdisk . volume Options . Storage Policy ID == " " && vmdisk . volume Options . Storage Policy Name != " " { vmdisk . volume Options . Storage Policy ID , err = pbm Client . Profile ID By Name ( ctx , vmdisk . volume Options . Storage Policy if err != nil { klog . Errorf ( " " , vmdisk . volume Options . Storage Policy if vmdisk . volume Options . Storage Policy ID != " " { compatible , fault Message , err := datastore . Is Compatible With Storage Policy ( ctx , vmdisk . volume Options . Storage Policy if err != nil { klog . Errorf ( " " , vmdisk . volume Options . Storage Policy if ! compatible { klog . Errorf ( " " , datastore . Name ( ) , vmdisk . volume Options . Storage Policy return " " , fmt . Errorf ( " " , vmdisk . volume Options . Storage Policy Name , fault storage Profile Spec := & types . Virtual Machine Defined Profile // Is PBM storage policy ID is present, set the storage spec profile ID, // else, set raw the VSAN policy string. if vmdisk . volume Options . Storage Policy ID != " " { storage Profile Spec . Profile Id = vmdisk . volume Options . Storage Policy } else if vmdisk . volume Options . VSAN Storage Profile Data != " " { // Check Datastore type - VSAN Storage Profile Data is only applicable to v SAN Datastore ds Type , err := datastore . Get if ds Type != vclib . VSAN Datastore storage Profile Spec . Profile storage Profile Spec . Profile Data = & types . Virtual Machine Profile Raw Data { Extension Key : " " , Object Data : vmdisk . volume Options . VSAN Storage Profile var dummy VM * vclib . Virtual // Check if VM already exist in the folder. // If VM is already present, use it, else create a new dummy VM. fnv fnv Hash . Write ( [ ] byte ( vmdisk . volume dummy VM Full Name := vclib . Dummy VM Prefix Name + " " + fmt . Sprint ( fnv dummy VM , err = datastore . Datacenter . Get VM By Path ( ctx , vmdisk . vm Options . VM Folder . Inventory Path + " " + dummy VM Full if err != nil { // Create a dummy VM klog . V ( 1 ) . Infof ( " " , dummy VM Full dummy VM , err = vmdisk . create Dummy VM ( ctx , datastore . Datacenter , dummy VM Full // Reconfigure the VM to attach the disk with the VSAN policy configured virtual Machine Config Spec := types . Virtual Machine Config disk , _ , err := dummy VM . Create Disk Spec ( ctx , vmdisk . disk Path , datastore , vmdisk . volume device Config Spec := & types . Virtual Device Config Spec { Device : disk , Operation : types . Virtual Device Config Spec Operation Add , File Operation : types . Virtual Device Config Spec File Operation device Config Spec . Profile = append ( device Config Spec . Profile , storage Profile virtual Machine Config Spec . Device Change = append ( virtual Machine Config Spec . Device Change , device Config file Already task , err := dummy VM . Reconfigure ( ctx , virtual Machine Config if err != nil { file Already Exist = is Already Exists ( vmdisk . disk if file Already Exist { //Skip error and continue to detach the disk as the disk was already created on the datastore. klog . V ( vclib . Log Level ) . Infof ( " " , vmdisk . disk } else { klog . Errorf ( " " , dummy VM Full // Detach the disk from the dummy VM. err = dummy VM . Detach Disk ( ctx , vmdisk . disk if err != nil { if vclib . Disk Not Found Err Msg == err . Error ( ) && file Already Exist { // Skip error if disk was already detached from the dummy VM but still present on the datastore. klog . V ( vclib . Log Level ) . Infof ( " " , vmdisk . disk } else { klog . Errorf ( " " , vmdisk . disk Path , dummy VM Full // Delete the dummy VM err = dummy VM . Delete if err != nil { klog . Errorf ( " " , dummy VM Full return vmdisk . disk } 
func ( vmdisk vm Disk Manager ) create Dummy VM ( ctx context . Context , datacenter * vclib . Datacenter , vm Name string ) ( * vclib . Virtual Machine , error ) { // Create a virtual machine config spec with 1 SCSI adapter. virtual Machine Config Spec := types . Virtual Machine Config Spec { Name : vm Name , Files : & types . Virtual Machine File Info { Vm Path Name : " " + vmdisk . volume Options . Datastore + " " , } , Num CP Us : 1 , Memory MB : 4 , Device Change : [ ] types . Base Virtual Device Config Spec { & types . Virtual Device Config Spec { Operation : types . Virtual Device Config Spec Operation Add , Device : & types . Para Virtual SCSI Controller { Virtual SCSI Controller : types . Virtual SCSI Controller { Shared Bus : types . Virtual SCSI Sharing No Sharing , Virtual Controller : types . Virtual Controller { Bus Number : 0 , Virtual Device : types . Virtual task , err := vmdisk . vm Options . VM Folder . Create VM ( ctx , virtual Machine Config Spec , vmdisk . vm Options . VM Resource dummy VM Task Info , err := task . Wait For vm Ref := dummy VM Task dummy VM := object . New Virtual Machine ( datacenter . Client ( ) , vm return & vclib . Virtual Machine { Virtual Machine : dummy } 
func Clean Up Dummy V Ms ( ctx context . Context , folder * vclib . Folder , dc * vclib . Datacenter ) error { vm List , err := folder . Get Virtual if err != nil { klog . V ( 4 ) . Infof ( " " , folder . Inventory if vm List == nil || len ( vm List ) == 0 { klog . Errorf ( " " , folder . Inventory return fmt . Errorf ( " " , folder . Inventory var dummy VM List [ ] * vclib . Virtual // Loop through VM's in the Kubernetes cluster to find dummy VM's for _ , vm := range vm List { vm Name , err := vm . Object if strings . Has Prefix ( vm Name , vclib . Dummy VM Prefix Name ) { vm Obj := vclib . Virtual Machine { Virtual Machine : object . New Virtual dummy VM List = append ( dummy VM List , & vm for _ , vm := range dummy VM List { err = vm . Delete } 
func Open ( dirpath string , snap walpb . Snapshot ) ( * WAL , error ) { return open At } 
func Open For Read ( dirpath string , snap walpb . Snapshot ) ( * WAL , error ) { return open At } 
func ( w * WAL ) Read All ( ) ( metadata [ ] byte , state raftpb . Hard for err = decoder . decode ( rec ) ; err == nil ; err = decoder . decode ( rec ) { switch rec . Type { case entry Type : e := must Unmarshal case state Type : state = must Unmarshal case metadata Type : if metadata != nil && ! reflect . Deep return nil , state , nil , Err Metadata case crc return nil , state , nil , Err CRC decoder . update case snapshot pbutil . Must return nil , state , nil , Err Snapshot switch w . f { case nil : // We do not have to read out all entries in read mode. // The last record maybe a partial written one, so // Errunexpected EOF might be returned. if err != io . EOF && err != io . Err Unexpected if ! match { err = Err Snapshot Not if w . f != nil { // create encoder (chain crc with the decoder), enable appending w . encoder = new Encoder ( w . f , w . decoder . last last Index } 
fpath := path . Join ( w . dir , wal // create a temp wal file with name sequence + 1, or tuncate the existing one ft , err := os . Open prev w . encoder = new Encoder ( w . f , prev if err := w . save Crc ( prev if err := w . encoder . encode ( & walpb . Record { Type : metadata if err := w . save // open the wal file and update writer again f , err := os . Open err = fileutil . Preallocate ( f , segment Size prev w . encoder = new Encoder ( w . f , prev // lock the new wal file l , err := fileutil . New } 
func Is Huge Page Resource Name ( name core . Resource Name ) bool { return strings . Has Prefix ( string ( name ) , core . Resource Huge Pages } 
func Is Quota Huge Page Resource Name ( name core . Resource Name ) bool { return strings . Has Prefix ( string ( name ) , core . Resource Huge Pages Prefix ) || strings . Has Prefix ( string ( name ) , core . Resource Requests Huge Pages } 
func Huge Page Resource Name ( page Size resource . Quantity ) core . Resource Name { return core . Resource Name ( fmt . Sprintf ( " " , core . Resource Huge Pages Prefix , page } 
func Huge Page Size From Resource Name ( name core . Resource Name ) ( resource . Quantity , error ) { if ! Is Huge Page Resource page Size := strings . Trim Prefix ( string ( name ) , core . Resource Huge Pages return resource . Parse Quantity ( page } 
func Non Convertible Fields ( annotations map [ string ] string ) map [ string ] string { non Convertible for key , value := range annotations { if strings . Has Prefix ( key , core . Non Convertible Annotation Prefix ) { non Convertible return non Convertible } 
func Is Resource Quota Scope Valid For Resource ( scope core . Resource Quota Scope , resource string ) bool { switch scope { case core . Resource Quota Scope Terminating , core . Resource Quota Scope Not Terminating , core . Resource Quota Scope Not Best Effort , core . Resource Quota Scope Priority Class : return pod Object Count Quota Resources . Has ( resource ) || pod Compute Quota case core . Resource Quota Scope Best Effort : return pod Object Count Quota } 
func Is Native Resource ( name core . Resource Name ) bool { return ! strings . Contains ( string ( name ) , " " ) || strings . Contains ( string ( name ) , core . Resource Default Namespace } 
func Is Standard Quota Resource Name ( str string ) bool { return standard Quota Resources . Has ( str ) || Is Quota Huge Page Resource Name ( core . Resource } 
func Is Standard Resource Name ( str string ) bool { return standard Resources . Has ( str ) || Is Quota Huge Page Resource Name ( core . Resource } 
func Is Integer Resource Name ( str string ) bool { return integer Resources . Has ( str ) || Is Extended Resource Name ( core . Resource } 
func Load Balancer Status Equal ( l , r * core . Load Balancer Status ) bool { return ingress Slice } 
func Get Access Modes From String ( modes string ) [ ] core . Persistent Volume Access access Modes := [ ] core . Persistent Volume Access switch { case s == " " : access Modes = append ( access Modes , core . Read Write case s == " " : access Modes = append ( access Modes , core . Read Only case s == " " : access Modes = append ( access Modes , core . Read Write return access } 
func Node Selector Requirements As Selector ( nsm [ ] core . Node Selector selector := labels . New switch expr . Operator { case core . Node Selector Op case core . Node Selector Op Not In : op = selection . Not case core . Node Selector Op case core . Node Selector Op Does Not Exist : op = selection . Does Not case core . Node Selector Op Gt : op = selection . Greater case core . Node Selector Op Lt : op = selection . Less r , err := labels . New } 
func Node Selector Requirements As Field Selector ( nsm [ ] core . Node Selector for _ , expr := range nsm { switch expr . Operator { case core . Node Selector Op selectors = append ( selectors , fields . One Term Equal case core . Node Selector Op Not selectors = append ( selectors , fields . One Term Not Equal return fields . And } 
func Get Tolerations From Pod if len ( annotations ) > 0 && annotations [ core . Tolerations Annotation Key ] != " " { err := json . Unmarshal ( [ ] byte ( annotations [ core . Tolerations Annotation } 
func Add Or Update Toleration In Pod ( pod * core . Pod , toleration * core . Toleration ) bool { pod var new for i := range pod Tolerations { if toleration . Match Toleration ( & pod Tolerations [ i ] ) { if Semantic . Deep Equal ( toleration , pod new Tolerations = append ( new new Tolerations = append ( new Tolerations , pod if ! updated { new Tolerations = append ( new pod . Spec . Tolerations = new } 
func Get Taints From Node if len ( annotations ) > 0 && annotations [ core . Taints Annotation Key ] != " " { err := json . Unmarshal ( [ ] byte ( annotations [ core . Taints Annotation } 
func Get Persistent Volume Claim Class ( claim * core . Persistent Volume Claim ) string { // Use beta annotation first if class , found := claim . Annotations [ core . Beta Storage Class if claim . Spec . Storage Class Name != nil { return * claim . Spec . Storage Class } 
func Persistent Volume Claim Has Class ( claim * core . Persistent Volume Claim ) bool { // Use beta annotation first if _ , found := claim . Annotations [ core . Beta Storage Class if claim . Spec . Storage Class } 
func Reset Cluster Status For Node ( node Name string , client clientset . Interface ) error { fmt . Printf ( " \n " , node Name , kubeadmconstants . Kubeadm Config Config Map , metav1 . Namespace return apiclient . Mutate Config Map ( client , metav1 . Object Meta { Name : kubeadmconstants . Kubeadm Config Config Map , Namespace : metav1 . Namespace System , } , func ( cm * v1 . Config Map ) error { return mutate Cluster Status ( cm , func ( cs * kubeadmapi . Cluster Status ) error { // Handle a nil API Endpoints map. Should only happen if someone manually // interacted with the Config Map. if cs . API Endpoints == nil { return errors . Errorf ( " " , kubeadmconstants . Kubeadm Config Config Map , metav1 . Namespace klog . V ( 2 ) . Infof ( " " , node delete ( cs . API Endpoints , node } 
func Upload Configuration ( cfg * kubeadmapi . Init Configuration , client clientset . Interface ) error { fmt . Printf ( " \n " , kubeadmconstants . Kubeadm Config Config Map , metav1 . Namespace // Prepare the Cluster Configuration for upload // The components store their config in their own Config Maps, then reset the .Component Config struct; // We don't want to mutate the cfg itself, so create a copy of it using .Deep Copy of it first cluster Configuration To Upload := cfg . Cluster Configuration . Deep cluster Configuration To Upload . Component Configs = kubeadmapi . Component // Marshal the Cluster Configuration into YAML cluster Configuration Yaml , err := configutil . Marshal Kubeadm Config Object ( cluster Configuration To // Prepare the Cluster Status for upload cluster Status := & kubeadmapi . Cluster Status { API Endpoints : map [ string ] kubeadmapi . API Endpoint { cfg . Node Registration . Name : cfg . Local API // Marshal the Cluster Status into YAML cluster Status Yaml , err := configutil . Marshal Kubeadm Config Object ( cluster err = apiclient . Create Or Mutate Config Map ( client , & v1 . Config Map { Object Meta : metav1 . Object Meta { Name : kubeadmconstants . Kubeadm Config Config Map , Namespace : metav1 . Namespace System , } , Data : map [ string ] string { kubeadmconstants . Cluster Configuration Config Map Key : string ( cluster Configuration Yaml ) , kubeadmconstants . Cluster Status Config Map Key : string ( cluster Status Yaml ) , } , } , func ( cm * v1 . Config Map ) error { return mutate Cluster Status ( cm , func ( cs * kubeadmapi . Cluster Status ) error { // Handle a nil API Endpoints map. Should only happen if someone manually // interacted with the Config Map. if cs . API Endpoints == nil { return errors . Errorf ( " " , kubeadmconstants . Kubeadm Config Config Map , metav1 . Namespace cs . API Endpoints [ cfg . Node Registration . Name ] = cfg . Local API // Ensure that the Nodes Kubeadm Config Cluster Role Name exists err = apiclient . Create Or Update Role ( client , & rbac . Role { Object Meta : metav1 . Object Meta { Name : Nodes Kubeadm Config Cluster Role Name , Namespace : metav1 . Namespace System , } , Rules : [ ] rbac . Policy Rule { rbachelper . New Rule ( " " ) . Groups ( " " ) . Resources ( " " ) . Names ( kubeadmconstants . Kubeadm Config Config Map ) . Rule Or // Binds the Nodes Kubeadm Config Cluster Role Name to all the bootstrap tokens // that are members of the system:bootstrappers:kubeadm:default-node-token group // and to all nodes return apiclient . Create Or Update Role Binding ( client , & rbac . Role Binding { Object Meta : metav1 . Object Meta { Name : Nodes Kubeadm Config Cluster Role Name , Namespace : metav1 . Namespace System , } , Role Ref : rbac . Role Ref { API Group : rbac . Group Name , Kind : " " , Name : Nodes Kubeadm Config Cluster Role Name , } , Subjects : [ ] rbac . Subject { { Kind : rbac . Group Kind , Name : kubeadmconstants . Node Bootstrap Token Auth Group , } , { Kind : rbac . Group Kind , Name : kubeadmconstants . Nodes } 
func ( s * run As Any ) Validate ( fld Path * field . Path , _ * api . Pod , _ * api . Container , options * api . SE Linux Options ) field . Error List { return field . Error } 
func With Request Info ( handler http . Handler , resolver request . Request Info Resolver ) http . Handler { return http . Handler Func ( func ( w http . Response info , err := resolver . New Request if err != nil { responsewriters . Internal req = req . With Context ( request . With Request handler . Serve } 
func New Getter From Client ( c clientset . Interface , secret Lister v1listers . Secret Lister , service Account Lister v1listers . Service Account Lister , pod Lister v1listers . Pod Lister ) serviceaccount . Service Account Token Getter { return client Getter { c , secret Lister , service Account Lister , pod } 
func New File State ( file Path string , policy Name string ) State { state File := & state File { state File Path : file Path , cache : New Memory State ( ) , policy Name : policy if err := state File . try Restore State ( ) ; err != nil { // could not restore state, init new state file msg := fmt . Sprintf ( " \n " , err . Error ( ) ) + " \n " + fmt . Sprintf ( " \" \" " , state File . state File return state } 
func ( sf * state File ) try Restore // used when all parsing is ok tmp Assignments := make ( Container CPU tmp Default CPU Set := cpuset . New CPU tmp Container CPU Set := cpuset . New CPU content , err = ioutil . Read File ( sf . state File // If the state file does not exist or has zero length, write a new file. if os . Is Not Exist ( err ) || len ( content ) == 0 { sf . store klog . Infof ( " \" \" " , sf . state File // File exists; try to read it. var read State state File if err = json . Unmarshal ( content , & read State ) ; err != nil { klog . Errorf ( " \" \" " , sf . state File if sf . policy Name != read State . Policy Name { return fmt . Errorf ( " \" \" \" \" " , sf . policy Name , read State . Policy if tmp Default CPU Set , err = cpuset . Parse ( read State . Default CPU Set ) ; err != nil { klog . Errorf ( " \" \" " , read State . Default CPU for container ID , cpu String := range read State . Entries { if tmp Container CPU Set , err = cpuset . Parse ( cpu String ) ; err != nil { klog . Errorf ( " \" \" " , container ID , cpu tmp Assignments [ container ID ] = tmp Container CPU sf . cache . Set Default CPU Set ( tmp Default CPU sf . cache . Set CPU Assignments ( tmp klog . V ( 2 ) . Infof ( " \" \" " , sf . state File klog . V ( 2 ) . Infof ( " " , tmp Default CPU } 
func ( sf * state File ) store data := state File Data { Policy Name : sf . policy Name , Default CPU Set : sf . cache . Get Default CPU for container ID , cset := range sf . cache . Get CPU Assignments ( ) { data . Entries [ container if err = ioutil . Write File ( sf . state File } 
func rewrite Optional Methods ( decl ast . Decl , is Optional Optional Func ) { switch t := decl . ( type ) { case * ast . Func // correct initialization of the form `m.Field = &Optional Type{}` to // `m.Field = Optional Type{}` if t . Name . Name == " " { ast . Walk ( optional Assignment Visitor { fn : is if ! is switch t . Name . Name { case " " : ast . Walk ( & optional Items case " " , " " , " " : ast . Walk ( & optional Items } 
func ( v optional Assignment Visitor ) Visit ( n ast . Node ) ast . Visitor { switch t := n . ( type ) { case * ast . Assign Stmt : if len ( t . Lhs ) == 1 && len ( t . Rhs ) == 1 { if ! is Field unary , ok := t . Rhs [ 0 ] . ( * ast . Unary composite , ok := unary . X . ( * ast . Composite } 
func ( v * optional Items Visitor ) Visit ( n ast . Node ) ast . Visitor { switch t := n . ( type ) { case * ast . Range Stmt : if is Field case * ast . Assign Stmt : if len ( t . Lhs ) == 1 && len ( t . Rhs ) == 1 { switch lhs := t . Lhs [ 0 ] . ( type ) { case * ast . Index Expr : if is Field Selector ( lhs . X , " " , " " ) { lhs . X = & ast . Star default : if is Field Selector ( t . Lhs [ 0 ] , " " , " " ) { t . Lhs [ 0 ] = & ast . Star switch rhs := t . Rhs [ 0 ] . ( type ) { case * ast . Call if len ( rhs . Args ) > 0 { switch arg := rhs . Args [ 0 ] . ( type ) { case * ast . Ident : if arg . Name == " " { rhs . Args [ 0 ] = & ast . Star case * ast . If Stmt : switch cond := t . Cond . ( type ) { case * ast . Binary Expr : if cond . Op == token . EQL { if is Field Selector ( cond . X , " " , " " ) && is Ident ( cond . Y , " " ) { cond . X = & ast . Star if t . Init != nil { // Find form: // if err := m[len(m.Items)-1].Unmarshal(data[i Nd Ex:post Index]); err != nil { // return err // } switch s := t . Init . ( type ) { case * ast . Assign Stmt : if call , ok := s . Rhs [ 0 ] . ( * ast . Call Expr ) ; ok { if sel , ok := call . Fun . ( * ast . Selector Expr ) ; ok { if x , ok := sel . X . ( * ast . Index Expr ) ; ok { // m[] -> (*m)[] if sel2 , ok := x . X . ( * ast . Selector Expr ) ; ok { if ident , ok := sel2 . X . ( * ast . Ident ) ; ok && ident . Name == " " { x . X = & ast . Star // len(m.Items) -> len(*m) if bin , ok := x . Index . ( * ast . Binary Expr ) ; ok { if call2 , ok := bin . X . ( * ast . Call Expr ) ; ok && len ( call2 . Args ) == 1 { if is Field Selector ( call2 . Args [ 0 ] , " " , " " ) { call2 . Args [ 0 ] = & ast . Star case * ast . Index Expr : if is Field case * ast . Call for i := range t . Args { if is Field } 
func drop Existing Type Declarations ( decl ast . Decl , extract Fn Extract Func ) bool { switch t := decl . ( type ) { case * ast . Gen for _ , s := range t . Specs { switch spec := s . ( type ) { case * ast . Type Spec : if extract } 
func drop Empty Import Declarations ( decl ast . Decl ) bool { switch t := decl . ( type ) { case * ast . Gen for _ , s := range t . Specs { switch spec := s . ( type ) { case * ast . Import } 
func New Example Informer ( client versioned . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Example Informer ( client , namespace , resync } 
func New Filtered Example Informer ( client versioned . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options metav1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Cr } , Watch Func : func ( options metav1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Cr } , } , & crv1 . Example { } , resync } 
func ( s * custom Resource Definition Lister ) List ( selector labels . Selector ) ( ret [ ] * apiextensions . Custom Resource Definition , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * apiextensions . Custom Resource } 
func ( s * custom Resource Definition Lister ) Get ( name string ) ( * apiextensions . Custom Resource Definition , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * apiextensions . Custom Resource } 
func ( s * example Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Example , err error ) { err = cache . List } 
func ( s * example Lister ) Examples ( namespace string ) Example Namespace Lister { return example Namespace } 
func ( s example Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Example , err error ) { err = cache . List All By } 
func ( s example Namespace Lister ) Get ( name string ) ( * v1 . Example , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not } 
func find Disk ( wwn , lun string , io io Handler , device Util volumeutil . Device Util ) ( string , string ) { fc dev Path := by if dirs , err := io . Read Dir ( dev if strings . Contains ( name , fc Path ) { if disk , err1 := io . Eval Symlinks ( dev Path + name ) ; err1 == nil { dm := device Util . Find Multipath Device For } 
func find Disk WWI Ds ( wwid string , io io Handler , device Util volumeutil . Device Util ) ( string , string ) { // Example wwid format: // 3600508b400105e210000900000490000 // <VENDOR NAME> <IDENTIFIER NUMBER> // Example of symlink under by-id: // /dev/by-id/scsi-3600508b400105e210000900000490000 // /dev/by-id/scsi-<VENDOR NAME>_<IDENTIFIER NUMBER> // The wwid could contain white space and it will be replaced // underscore when wwid is exposed under /dev/by-id. fc dev ID := by if dirs , err := io . Read Dir ( dev if name == fc Path { disk , err := io . Eval Symlinks ( dev if err != nil { klog . V ( 2 ) . Infof ( " " , dev dm := device Util . Find Multipath Device For klog . V ( 2 ) . Infof ( " " , dev ID + fc } 
func remove From Scsi Subsystem ( device Name string , io io Handler ) { file Name := " " + device klog . V ( 4 ) . Infof ( " " , file io . Write File ( file } 
func make PD Name Internal ( host volume . Volume return filepath . Join ( host . Get Plugin Dir ( fc Plugin return filepath . Join ( host . Get Plugin Dir ( fc Plugin } 
func make VDPD Name Internal ( host volume . Volume return filepath . Join ( host . Get Volume Device Plugin Dir ( fc Plugin return filepath . Join ( host . Get Volume Device Plugin Dir ( fc Plugin } 
func ( util * fc Util ) Make Global VDPD Name ( fc fc Disk ) string { return make VDPD Name } 
func ( util * fc Util ) Detach Disk ( c fc Disk Unmounter , device // device Path might be like /dev/mapper/mpath X. Find destination. dst Path , err := c . io . Eval Symlinks ( device // Find slave if strings . Has Prefix ( dst Path , " " ) { devices = c . device Util . Find Slave Devices On Multipath ( dst } else { // Add single devicepath to devices devices = append ( devices , dst klog . V ( 4 ) . Infof ( " " , device Path , dst var last for _ , device := range devices { err := util . detach FC last if last Err != nil { klog . Errorf ( " \n " , last return last } 
func ( util * fc Util ) detach FC Disk ( io io Handler , device Path string ) error { // Remove scsi device from the node. if ! strings . Has Prefix ( device Path , " " ) { return fmt . Errorf ( " " , device arr := strings . Split ( device remove From Scsi } 
func ( util * fc Util ) Detach Block FC Disk ( c fc Disk Unmapper , map Path , device Path string ) error { // Check if device Path is valid if len ( device Path ) != 0 { if path Exists , path Err := check Path Exists ( device Path ) ; ! path Exists || path Err != nil { return path } else { // TODO: FC plugin can't obtain the device Path from kubelet because device Path // in volume object isn't updated when volume is attached to kubelet node. klog . Infof ( " " , map // Check if global map path is valid // global map path examples: // wwn+lun: plugins/kubernetes.io/fc/volume Devices/50060e801049cfd1-lun-0/ // wwid: plugins/kubernetes.io/fc/volume Devices/3600508b400105e210000900000490000/ if path Exists , path Err := check Path Exists ( map Path ) ; ! path Exists || path Err != nil { return path // Retrieve volume plugin dependent path like '50060e801049cfd1-lun-0' from global map path arr := strings . Split ( map if len ( arr ) < 1 { return fmt . Errorf ( " " , map volume // Search symbolic link which matches volume Info under /dev/disk/by-path or /dev/disk/by-id // then find destination device path from the link search Path := by if strings . Contains ( volume Info , " " ) { search Path = by fis , err := ioutil . Read Dir ( search for _ , fi := range fis { if strings . Contains ( fi . Name ( ) , volume Info ) { device Path = filepath . Join ( search klog . V ( 5 ) . Infof ( " " , device if len ( device Path ) == 0 { return fmt . Errorf ( " " , search dst Path , err := c . io . Eval Symlinks ( device klog . V ( 4 ) . Infof ( " " , dst dm := c . device Util . Find Multipath Device For Device ( dst if len ( dm ) != 0 { dst // Detach volume from kubelet node if len ( dm ) != 0 { // Find all devices which are managed by multipath devices = c . device Util . Find Slave Devices On } else { // Add single device path to devices devices = append ( devices , dst var last for _ , device := range devices { err = util . detach FC last if last Err != nil { klog . Errorf ( " \n " , last return last } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Deployment Controller Configuration ) ( nil ) , ( * config . Deployment Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Deployment Controller Configuration_To_config_Deployment Controller Configuration ( a . ( * v1alpha1 . Deployment Controller Configuration ) , b . ( * config . Deployment Controller if err := s . Add Generated Conversion Func ( ( * config . Deployment Controller Configuration ) ( nil ) , ( * v1alpha1 . Deployment Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Deployment Controller Configuration_To_v1alpha1_Deployment Controller Configuration ( a . ( * config . Deployment Controller Configuration ) , b . ( * v1alpha1 . Deployment Controller if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Conversion Func ( ( * config . Deployment Controller Configuration ) ( nil ) , ( * v1alpha1 . Deployment Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Deployment Controller Configuration_To_v1alpha1_Deployment Controller Configuration ( a . ( * config . Deployment Controller Configuration ) , b . ( * v1alpha1 . Deployment Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Deployment Controller Configuration ) ( nil ) , ( * config . Deployment Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Deployment Controller Configuration_To_config_Deployment Controller Configuration ( a . ( * v1alpha1 . Deployment Controller Configuration ) , b . ( * config . Deployment Controller } 
func New CIDR Range Allocator ( client clientset . Interface , node Informer informers . Node Informer , cluster CIDR * net . IP Net , service CIDR * net . IP Net , sub Net Mask Size int , node List * v1 . Node List ) ( CIDR event Broadcaster := record . New recorder := event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : client . Core set , err := cidrset . New CIDR Set ( cluster CIDR , sub Net Mask ra := & range Allocator { client : client , cidrs : set , cluster CIDR : cluster CIDR , node Lister : node Informer . Lister ( ) , nodes Synced : node Informer . Informer ( ) . Has Synced , node CIDR Update Channel : make ( chan node And CIDR , cidr Update Queue Size ) , recorder : recorder , nodes In Processing : sets . New if service CIDR != nil { ra . filter Out Service Range ( service if node List != nil { for _ , node := range node List . Items { if node . Spec . Pod } else { klog . Infof ( " " , node . Name , node . Spec . Pod if err := ra . occupy CIDR ( & node ) ; err != nil { // This will happen if: // 1. We find garbage in the pod node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : nodeutil . Create Add Node Handler ( ra . Allocate Or Occupy CIDR ) , Update Func : nodeutil . Create Update Node Handler ( func ( _ , new Node * v1 . Node ) error { // If the Pod CIDR is not empty we either: // - already processed a Node that already had a CIDR after NC restarted // (cidr is marked as used), // - already processed a Node successfully and allocated a CIDR for it // (cidr is marked as used), // - already processed a Node but we did saw a "timeout" response and // request eventually got through in this case we haven't released // the allocated CIDR (cidr is still marked as used). // There's a possible error here: // - NC sees a new Node and assigns a CIDR X to it, // - Update Node call fails with a timeout, // - Node is updated by some other component, NC sees an update and // assigns CIDR Y to the Node, // - Both CIDR X and CIDR Y are marked as used in the local cache, // even though Node sees only CIDR Y // The problem here is that in in-memory cache we see CIDR X as marked, // which prevents it from being assigned to any new node. The cluster // state is correct. // Restart of NC fixes the issue. if new Node . Spec . Pod CIDR == " " { return ra . Allocate Or Occupy CIDR ( new } ) , Delete Func : nodeutil . Create Delete Node Handler ( ra . Release } 
func ( r * range Allocator ) Allocate Or Occupy if ! r . insert Node To if node . Spec . Pod CIDR != " " { return r . occupy pod CIDR , err := r . cidrs . Allocate if err != nil { r . remove Node From nodeutil . Record Node Status klog . V ( 4 ) . Infof ( " " , node . Name , pod r . node CIDR Update Channel <- node And CIDR { node Name : node . Name , cidr : pod } 
func ( r * range Allocator ) filter Out Service Range ( service CIDR * net . IP Net ) { // Checks if service CIDR has a nonempty intersection with cluster // CIDR. It is the case if either cluster CIDR contains service CIDR with // cluster CIDR's Mask applied (this means that cluster CIDR contains // service CIDR) or vice versa (which means that service CIDR contains // cluster CIDR). if ! r . cluster CIDR . Contains ( service CIDR . IP . Mask ( r . cluster CIDR . Mask ) ) && ! service CIDR . Contains ( r . cluster CIDR . IP . Mask ( service if err := r . cidrs . Occupy ( service CIDR ) ; err != nil { klog . Errorf ( " " , service } 
func ( r * range Allocator ) update CIDR Allocation ( data node And defer r . remove Node From Processing ( data . node pod node , err = r . node Lister . Get ( data . node if err != nil { klog . Errorf ( " " , data . node if node . Spec . Pod CIDR == pod CIDR { klog . V ( 4 ) . Infof ( " " , node . Name , pod if node . Spec . Pod CIDR != " " { klog . Errorf ( " " , node . Name , node . Spec . Pod CIDR , pod if err := r . cidrs . Release ( data . cidr ) ; err != nil { klog . Errorf ( " " , pod // If we reached here, it means that the node has no CIDR currently assigned. So we set it. for i := 0 ; i < cidr Update Retries ; i ++ { if err = utilnode . Patch Node CIDR ( r . client , types . Node Name ( node . Name ) , pod CIDR ) ; err == nil { klog . Infof ( " " , node . Name , pod klog . Errorf ( " " , node . Name , pod nodeutil . Record Node Status // We accept the fact that we may leak CID Rs here. This is safer than releasing // them in case when we don't know if request went through. // Node Controller restart will return all falsely allocated CID Rs to the pool. if ! apierrors . Is Server if release Err := r . cidrs . Release ( data . cidr ) ; release Err != nil { klog . Errorf ( " " , node . Name , release } 
func ( r * real IP Getter ) Node I Ps ( ) ( ips [ ] net . IP , err error ) { // Pass in empty filter device name for list all LOCAL type addresses. node Address , err := r . nl . Get Local Addresses ( " " , Default Dummy // translate ip string to IP for _ , ip Str := range node Address . Unsorted List ( ) { ips = append ( ips , net . Parse IP ( ip } 
func parse Excluded CID Rs ( exclude CID Rs [ ] string ) [ ] * net . IP Net { var cidr Exclusions [ ] * net . IP for _ , excluded CIDR := range exclude CID Rs { _ , n , err := net . Parse CIDR ( excluded if err != nil { klog . Errorf ( " " , excluded cidr Exclusions = append ( cidr return cidr } 
func New Proxier ( ipt utiliptables . Interface , ipvs utilipvs . Interface , ipset utilipset . Interface , sysctl utilsysctl . Interface , exec utilexec . Interface , sync Period time . Duration , min Sync Period time . Duration , exclude CID Rs [ ] string , strict ARP bool , masquerade All bool , masquerade Bit int , cluster CIDR string , hostname string , node IP net . IP , recorder record . Event Recorder , healthz Server healthcheck . Healthz Updater , scheduler string , node Port Addresses [ ] string , ) ( * Proxier , error ) { // Set the route_localnet sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Route Localnet ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl Route Localnet , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Route // Proxy needs br_netfilter and bridge-nf-call-iptables=1 when containers // are connected to a Linux bridge (but not SDN bridges). Until most // plugins handle this, log when config is missing if val , err := sysctl . Get Sysctl ( sysctl Bridge Call IP // Set the conntrack sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl VS Conn Track ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl VS Conn Track , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl VS Conn // Set the connection reuse mode if val , _ := sysctl . Get Sysctl ( sysctl Conn Reuse ) ; val != 0 { if err := sysctl . Set Sysctl ( sysctl Conn Reuse , 0 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Conn // Set the expire_nodest_conn sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Expire No Dest Conn ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl Expire No Dest Conn , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Expire No Dest // Set the expire_quiescent_template sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Expire Quiescent Template ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl Expire Quiescent Template , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Expire Quiescent // Set the ip_forward sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Forward ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl Forward , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl if strict ARP { // Set the arp_ignore sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Arp Ignore ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl Arp Ignore , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Arp // Set the arp_announce sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Arp Announce ) ; val != 2 { if err := sysctl . Set Sysctl ( sysctl Arp Announce , 2 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Arp // Generate the masquerade mark to use for SNAT rules. masquerade Value := 1 << uint ( masquerade masquerade Mark := fmt . Sprintf ( " " , masquerade Value , masquerade if node node IP = net . Parse is I Pv6 := utilnet . Is I Pv6 ( node klog . V ( 2 ) . Infof ( " " , node IP , is I if len ( cluster } else if utilnet . Is I Pv6CIDR String ( cluster CIDR ) != is I Pv6 { return nil , fmt . Errorf ( " " , cluster CIDR , is I if len ( scheduler ) == 0 { klog . Warningf ( " " , Default scheduler = Default health Checker := healthcheck . New proxier := & Proxier { ports Map : make ( map [ utilproxy . Local Port ] utilproxy . Closeable ) , service Map : make ( proxy . Service Map ) , service Changes : proxy . New Service Change Tracker ( new Service Info , & is I Pv6 , recorder ) , endpoints Map : make ( proxy . Endpoints Map ) , endpoints Changes : proxy . New Endpoint Change Tracker ( hostname , nil , & is I Pv6 , recorder ) , sync Period : sync Period , min Sync Period : min Sync Period , exclude CID Rs : parse Excluded CID Rs ( exclude CID Rs ) , iptables : ipt , masquerade All : masquerade All , masquerade Mark : masquerade Mark , exec : exec , cluster CIDR : cluster CIDR , hostname : hostname , node IP : node IP , port Mapper : & listen Port Opener { } , recorder : recorder , health Checker : health Checker , healthz Server : healthz Server , ipvs : ipvs , ipvs Scheduler : scheduler , ip Getter : & real IP Getter { nl : New Net Link Handle ( is I Pv6 ) } , iptables Data : bytes . New Buffer ( nil ) , filter Chains Data : bytes . New Buffer ( nil ) , nat Chains : bytes . New Buffer ( nil ) , nat Rules : bytes . New Buffer ( nil ) , filter Chains : bytes . New Buffer ( nil ) , filter Rules : bytes . New Buffer ( nil ) , netlink Handle : New Net Link Handle ( is I Pv6 ) , ipset : ipset , node Port Addresses : node Port Addresses , network Interfacer : utilproxy . Real Network { } , gracefuldelete Manager : New Graceful Termination // initialize ipset List with all sets we needed proxier . ipset List = make ( map [ string ] * IP for _ , is := range ipset Info { proxier . ipset List [ is . name ] = New IP Set ( ipset , is . name , is . set Type , is I burst klog . V ( 3 ) . Infof ( " " , min Sync Period , sync Period , burst proxier . sync Runner = async . New Bounded Frequency Runner ( " " , proxier . sync Proxy Rules , min Sync Period , sync Period , burst proxier . gracefuldelete } 
func new Service Info ( port * v1 . Service Port , service * v1 . Service , base Info * proxy . Base Service Info ) proxy . Service Port { info := & service Info { Base Service Info : base // Store the following for performance reasons. svc Name := types . Namespaced svc Port Name := proxy . Service Port Name { Namespaced Name : svc info . service Name String = svc Port } 
func ( handle * Linux Kernel Handler ) Get Modules ( ) ( [ ] string , error ) { // Check whether IPVS required kernel modules are built-in kernel Version , ipvs Modules , err := utilipvs . Get Kernel Version And IPVS builtin Mods File Path := fmt . Sprintf ( " " , kernel b , err := ioutil . Read File ( builtin Mods File if err != nil { klog . Warningf ( " " , builtin Mods File for _ , module := range ipvs // Try to load IPVS required kernel modules using modprobe first for _ , kmod := range ipvs // Find out loaded kernel modules out , err := handle . executor . Command ( " " , " " , " " , " " , " " ) . Combined } 
func Can Use IPVS Proxier ( handle Kernel Handler , ipsetver IP Set Versioner ) ( bool , error ) { mods , err := handle . Get want Modules := sets . New load Modules := sets . New linux Kernel Handler := New Linux Kernel _ , ipvs Modules , _ := utilipvs . Get Kernel Version And IPVS Mods ( linux Kernel want Modules . Insert ( ipvs load modules := want Modules . Difference ( load Modules ) . Unsorted var missing Conntracki Missing for _ , mod := range modules { if strings . Contains ( mod , " " ) { Conntracki Missing } else { missing Mods = append ( missing if Conntracki Missing Counter == 2 { missing Mods = append ( missing if len ( missing Mods ) != 0 { return false , fmt . Errorf ( " " , missing // Check ipset version version String , err := ipsetver . Get if ! check Min Version ( version String ) { return false , fmt . Errorf ( " " , version String , Min IP Set Check } 
func cleanup Iptables Leftovers ( ipt utiliptables . Interface ) ( encountered Error bool ) { // Unlink the iptables chains created by ipvs Proxier for _ , jc := range iptables Jump if err := ipt . Delete Rule ( jc . table , jc . from , args ... ) ; err != nil { if ! utiliptables . Is Not Found encountered // Flush and remove all of our chains. Flushing all chains before removing them also removes all links between chains first. for _ , ch := range iptables Chains { if err := ipt . Flush Chain ( ch . table , ch . chain ) ; err != nil { if ! utiliptables . Is Not Found encountered // Remove all of our chains. for _ , ch := range iptables Chains { if err := ipt . Delete Chain ( ch . table , ch . chain ) ; err != nil { if ! utiliptables . Is Not Found encountered return encountered } 
func Cleanup Leftovers ( ipvs utilipvs . Interface , ipt utiliptables . Interface , ipset utilipset . Interface , cleanup IPVS bool ) ( encountered Error bool ) { if cleanup encountered encountered // Delete dummy interface created by ipvs Proxier. nl := New Net Link err := nl . Delete Dummy Device ( Default Dummy if err != nil { klog . Errorf ( " " , Default Dummy encountered // Clear iptables created by ipvs Proxier. encountered Error = cleanup Iptables Leftovers ( ipt ) || encountered // Destroy ip sets created by ipvs Proxier. We should call it after cleaning up // iptables since we can NOT delete ip set which is still referenced by iptables. for _ , set := range ipset Info { err = ipset . Destroy if err != nil { if ! utilipset . Is Not Found encountered return encountered } 
func ( proxier * Proxier ) On Service Add ( service * v1 . Service ) { proxier . On Service } 
func ( proxier * Proxier ) On Service Update ( old Service , service * v1 . Service ) { if proxier . service Changes . Update ( old Service , service ) && proxier . is Initialized ( ) { proxier . sync } 
func ( proxier * Proxier ) On Endpoints Add ( endpoints * v1 . Endpoints ) { proxier . On Endpoints } 
func ( proxier * Proxier ) On Endpoints Update ( old Endpoints , endpoints * v1 . Endpoints ) { if proxier . endpoints Changes . Update ( old Endpoints , endpoints ) && proxier . is Initialized ( ) { proxier . sync } 
func ( proxier * Proxier ) On Endpoints Delete ( endpoints * v1 . Endpoints ) { proxier . On Endpoints } 
func ( proxier * Proxier ) sync Proxy defer func ( ) { metrics . Sync Proxy Rules Latency . Observe ( metrics . Since In metrics . Deprecated Sync Proxy Rules Latency . Observe ( metrics . Since In // don't sync rules till we've received services and endpoints if ! proxier . endpoints Synced || ! proxier . services // We assume that if this was called, we really want to sync them, // even if nothing changed in the meantime. In other words, callers are // responsible for detecting no-op changes and not calling this function. service Update Result := proxy . Update Service Map ( proxier . service Map , proxier . service endpoint Update Result := proxy . Update Endpoints Map ( proxier . endpoints Map , proxier . endpoints stale Services := service Update Result . UDP Stale Cluster // merge stale services gathered from update Endpoints Map for _ , svc Port Name := range endpoint Update Result . Stale Service Names { if svc Info , ok := proxier . service Map [ svc Port Name ] ; ok && svc Info != nil && svc Info . Get Protocol ( ) == v1 . Protocol UDP { klog . V ( 2 ) . Infof ( " " , svc Port Name , svc Info . Cluster IP stale Services . Insert ( svc Info . Cluster IP for _ , ext IP := range svc Info . External IP Strings ( ) { stale Services . Insert ( ext // Begin install iptables // Reset all buffers used later. // This is to avoid memory reallocations and thus improve performance. proxier . nat proxier . nat proxier . filter proxier . filter // Write table headers. write Line ( proxier . filter write Line ( proxier . nat proxier . create And Linke Kube // make sure dummy interface exists in the system where ipvs Proxier will bind service address on it _ , err := proxier . netlink Handle . Ensure Dummy Device ( Default Dummy if err != nil { klog . Errorf ( " " , Default Dummy // make sure ip sets exists in the system. for _ , set := range proxier . ipset List { if err := ensure IP set . reset // Accumulate the set of local ports that we will be holding open once this update is complete replacement Ports Map := map [ utilproxy . Local // active IPVS Services represents IPVS service successfully created in this round of sync active IPVS // current IPVS Services represent IPVS services listed from the system current IPVS Services := make ( map [ string ] * utilipvs . Virtual // active Bind Addrs represents ip address successfully bind to Default Dummy Device in this round of sync active Bind // Build IPVS rules for each service. for svc Name , svc := range proxier . service Map { svc Info , ok := svc . ( * service if ! ok { klog . Errorf ( " " , svc protocol := strings . To Lower ( string ( svc // Precompute svc Name String; with many services the many calls // to Service Port Name.String() show up in CPU profiles. svc Name String := svc // Handle traffic that loops back to the originator with SNAT. for _ , e := range proxier . endpoints Map [ svc Name ] { ep , ok := e . ( * proxy . Base Endpoint if ! ep . Is ep ep // Error parsing this endpoint has been logged. Skip to next endpoint. if ep entry := & utilipset . Entry { IP : ep IP , Port : ep Port , Protocol : protocol , IP2 : ep IP , Set Type : utilipset . Hash IP Port if valid := proxier . ipset List [ kube Loop Back IP Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Loop Back IP proxier . ipset List [ kube Loop Back IP Set ] . active // Capture the cluster IP. // ipset call entry := & utilipset . Entry { IP : svc Info . Cluster IP . String ( ) , Port : svc Info . Port , Protocol : protocol , Set Type : utilipset . Hash IP // add service Cluster IP:Port to kube Service Access ip set for the purpose of solving hairpin. // proxier.kube Service Access Set.active Entries.Insert(entry.String()) if valid := proxier . ipset List [ kube Cluster IP Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Cluster IP proxier . ipset List [ kube Cluster IP Set ] . active // ipvs call serv := & utilipvs . Virtual Server { Address : svc Info . Cluster IP , Port : uint16 ( svc Info . Port ) , Protocol : string ( svc Info . Protocol ) , Scheduler : proxier . ipvs // Set session affinity flag and timeout for IPVS service if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { serv . Flags |= utilipvs . Flag serv . Timeout = uint32 ( svc Info . Sticky Max Age // We need to bind Cluster IP to dummy interface, so set `bind Addr` parameter to `true` in sync Service() if err := proxier . sync Service ( svc Name String , serv , true ) ; err == nil { active IPVS active Bind // External Traffic Policy only works for Node Port and external LB traffic, does not affect Cluster IP // So we still need cluster IP rules in only Node Local Endpoints mode. if err := proxier . sync Endpoint ( svc // Capture external I Ps. for _ , external IP := range svc Info . External I Ps { if local , err := utilproxy . Is Local IP ( external // We do not start listening on SCTP ports, according to our agreement in the // SCTP support KEP } else if local && ( svc Info . Get Protocol ( ) != v1 . Protocol SCTP ) { lp := utilproxy . Local Port { Description : " " + svc Name String , IP : external IP , Port : svc if proxier . ports replacement Ports Map [ lp ] = proxier . ports } else { socket , err := proxier . port Mapper . Open Local proxier . recorder . Eventf ( & v1 . Object Reference { Kind : " " , Name : proxier . hostname , UID : types . UID ( proxier . hostname ) , Namespace : " " , } , v1 . Event Type replacement Ports // ipset call entry := & utilipset . Entry { IP : external IP , Port : svc Info . Port , Protocol : protocol , Set Type : utilipset . Hash IP // We have to SNAT packets to external I Ps. if valid := proxier . ipset List [ kube External IP Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube External IP proxier . ipset List [ kube External IP Set ] . active // ipvs call serv := & utilipvs . Virtual Server { Address : net . Parse IP ( external IP ) , Port : uint16 ( svc Info . Port ) , Protocol : string ( svc Info . Protocol ) , Scheduler : proxier . ipvs if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { serv . Flags |= utilipvs . Flag serv . Timeout = uint32 ( svc Info . Sticky Max Age if err := proxier . sync Service ( svc Name String , serv , true ) ; err == nil { active IPVS active Bind if err := proxier . sync Endpoint ( svc // Capture load-balancer ingress. for _ , ingress := range svc Info . Load Balancer Status . Ingress { if ingress . IP != " " { // ipset call entry = & utilipset . Entry { IP : ingress . IP , Port : svc Info . Port , Protocol : protocol , Set Type : utilipset . Hash IP // add service load balancer ingress IP:Port to kube Service Access ip set for the purpose of solving hairpin. // proxier.kube Service Access Set.active Entries.Insert(entry.String()) // If we are proxying globally, we need to masquerade in case we cross nodes. // If we are proxying only locally, we can retain the source IP. if valid := proxier . ipset List [ kube Load Balancer Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Load Balancer proxier . ipset List [ kube Load Balancer Set ] . active // insert loadbalancer entry to lb Ingress Local Set if service externaltrafficpolicy=local if svc Info . Only Node Local Endpoints { if valid := proxier . ipset List [ kube Load Balancer Local Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Load Balancer Local proxier . ipset List [ kube Load Balancer Local Set ] . active if len ( svc Info . Load Balancer Source Ranges ) != 0 { // The service firewall rules are created based on Service Spec.load Balancer Source Ranges field. // This currently works for loadbalancers that preserves source ips. // For loadbalancers which direct traffic to service Node Port, the firewall rules will not apply. if valid := proxier . ipset List [ kube Loadbalancer FW Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Loadbalancer FW proxier . ipset List [ kube Loadbalancer FW Set ] . active allow From for _ , src := range svc Info . Load Balancer Source Ranges { // ipset call entry = & utilipset . Entry { IP : ingress . IP , Port : svc Info . Port , Protocol : protocol , Net : src , Set Type : utilipset . Hash IP Port // enumerate all white list source cidr if valid := proxier . ipset List [ kube Load Balancer Source CIDR Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Load Balancer Source CIDR proxier . ipset List [ kube Load Balancer Source CIDR Set ] . active // ignore error because it has been validated _ , cidr , _ := net . Parse if cidr . Contains ( proxier . node IP ) { allow From // generally, ip route rule was added to intercept request to loadbalancer vip from the // loadbalancer's backend hosts. In this case, request will not hit the loadbalancer but loop back directly. // Need to add the following rule to allow request on host. if allow From Node { entry = & utilipset . Entry { IP : ingress . IP , Port : svc Info . Port , Protocol : protocol , IP2 : ingress . IP , Set Type : utilipset . Hash IP Port // enumerate all white list source ip if valid := proxier . ipset List [ kube Load Balancer Source IP Set ] . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , proxier . ipset List [ kube Load Balancer Source IP proxier . ipset List [ kube Load Balancer Source IP Set ] . active // ipvs call serv := & utilipvs . Virtual Server { Address : net . Parse IP ( ingress . IP ) , Port : uint16 ( svc Info . Port ) , Protocol : string ( svc Info . Protocol ) , Scheduler : proxier . ipvs if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { serv . Flags |= utilipvs . Flag serv . Timeout = uint32 ( svc Info . Sticky Max Age if err := proxier . sync Service ( svc Name String , serv , true ) ; err == nil { active IPVS active Bind if err := proxier . sync Endpoint ( svc Name , svc Info . Only Node Local if svc Info . Node Port != 0 { addresses , err := utilproxy . Get Node Addresses ( proxier . node Port Addresses , proxier . network var lps [ ] utilproxy . Local for address := range addresses { lp := utilproxy . Local Port { Description : " " + svc Name String , IP : address , Port : svc Info . Node if utilproxy . Is Zero // For ports on node I Ps, open the actual port and hold it. for _ , lp := range lps { if proxier . ports replacement Ports Map [ lp ] = proxier . ports // We do not start listening on SCTP ports, according to our agreement in the // SCTP support KEP } else if svc Info . Get Protocol ( ) != v1 . Protocol SCTP { socket , err := proxier . port Mapper . Open Local if lp . Protocol == " " { is I Pv6 := utilnet . Is I Pv6 ( svc Info . Cluster conntrack . Clear Entries For Port ( proxier . exec , lp . Port , is I Pv6 , v1 . Protocol replacement Ports // Nodeports need SNAT, unless they're local. // ipset call var node Port Set * IP switch protocol { case " " : node Port Set = proxier . ipset List [ kube Node Port Set entry = & utilipset . Entry { // No need to provide ip info Port : svc Info . Node Port , Protocol : protocol , Set Type : utilipset . Bitmap case " " : node Port Set = proxier . ipset List [ kube Node Port Set entry = & utilipset . Entry { // No need to provide ip info Port : svc Info . Node Port , Protocol : protocol , Set Type : utilipset . Bitmap case " " : node Port Set = proxier . ipset List [ kube Node Port Set entry = & utilipset . Entry { IP : proxier . node IP . String ( ) , Port : svc Info . Node Port , Protocol : protocol , Set Type : utilipset . Hash IP if node Port Set != nil { if valid := node Port Set . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , node Port node Port Set . active // Add externaltrafficpolicy=local type nodeport entry if svc Info . Only Node Local Endpoints { var node Port Local Set * IP switch protocol { case " " : node Port Local Set = proxier . ipset List [ kube Node Port Local Set case " " : node Port Local Set = proxier . ipset List [ kube Node Port Local Set case " " : node Port Local Set = proxier . ipset List [ kube Node Port Local Set if node Port Local Set != nil { if valid := node Port Local Set . validate Entry ( entry ) ; ! valid { klog . Errorf ( " " , fmt . Sprintf ( Entry Invalid Err , entry , node Port Local node Port Local Set . active // Build ipvs kernel routes for each node ip address var node I for address := range addresses { if ! utilproxy . Is Zero CIDR ( address ) { node I Ps = append ( node I Ps , net . Parse // zero cidr node I Ps , err = proxier . ip Getter . Node I for _ , node IP := range node I Ps { // ipvs call serv := & utilipvs . Virtual Server { Address : node IP , Port : uint16 ( svc Info . Node Port ) , Protocol : string ( svc Info . Protocol ) , Scheduler : proxier . ipvs if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { serv . Flags |= utilipvs . Flag serv . Timeout = uint32 ( svc Info . Sticky Max Age // There is no need to bind Node IP to dummy interface, so set parameter `bind Addr` to `false`. if err := proxier . sync Service ( svc Name String , serv , false ) ; err == nil { active IPVS if err := proxier . sync Endpoint ( svc Name , svc Info . Only Node Local // sync ipset entries for _ , set := range proxier . ipset List { set . sync IP Set // Tail call iptables rules for ipset, make sure only call iptables once // in a single loop per ip set. proxier . write Iptables // Sync iptables rules. // NOTE: No Flush Tables is used so we don't flush non-kubernetes chains in the table. proxier . iptables proxier . iptables Data . Write ( proxier . nat proxier . iptables Data . Write ( proxier . nat proxier . iptables Data . Write ( proxier . filter proxier . iptables Data . Write ( proxier . filter klog . V ( 5 ) . Infof ( " " , proxier . iptables err = proxier . iptables . Restore All ( proxier . iptables Data . Bytes ( ) , utiliptables . No Flush Tables , utiliptables . Restore if err != nil { klog . Errorf ( " \n \n " , err , proxier . iptables // Revert new local ports. utilproxy . Revert Ports ( replacement Ports Map , proxier . ports for _ , last Change Trigger Time := range endpoint Update Result . Last Change Trigger Times { latency := metrics . Since In Seconds ( last Change Trigger metrics . Network Programming // Close old local ports and save new ones. for k , v := range proxier . ports Map { if replacement Ports proxier . ports Map = replacement Ports // Get legacy bind address // current Bind Addrs represents ip addresses bind to Default Dummy Device from the system current Bind Addrs , err := proxier . netlink Handle . List Bind Address ( Default Dummy legacy Bind Addrs := proxier . get Legacy Bind Addr ( active Bind Addrs , current Bind // Clean up legacy IPVS services and unbind addresses applied Svcs , err := proxier . ipvs . Get Virtual if err == nil { for _ , applied Svc := range applied Svcs { current IPVS Services [ applied Svc . String ( ) ] = applied proxier . clean Legacy Service ( active IPVS Services , current IPVS Services , legacy Bind // Update healthz timestamp if proxier . healthz Server != nil { proxier . healthz Server . Update // Update healthchecks. The endpoints list might include services that are // not "Only Local", but the services list will not, and the health Checker // will just drop those endpoints. if err := proxier . health Checker . Sync Services ( service Update Result . HC Service Node if err := proxier . health Checker . Sync Endpoints ( endpoint Update Result . HC Endpoints Local IP // Finish housekeeping. // TODO: these could be made more consistent. for _ , svc IP := range stale Services . Unsorted List ( ) { if err := conntrack . Clear Entries For IP ( proxier . exec , svc IP , v1 . Protocol UDP ) ; err != nil { klog . Errorf ( " " , svc proxier . delete Endpoint Connections ( endpoint Update Result . Stale } 
func ( proxier * Proxier ) write Iptables for _ , set := range ipset With Iptables Chain { if _ , find := proxier . ipset List [ set . name ] ; find && ! proxier . ipset List [ set . name ] . is if set . protocol Match != " " { args = append ( args , " " , set . protocol args = append ( args , " " , " " , " " , proxier . ipset List [ set . name ] . get Comment ( ) , " " , " " , " " , set . name , set . match write Line ( proxier . nat if ! proxier . ipset List [ kube Cluster IP Set ] . is Empty ( ) { args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , proxier . ipset List [ kube Cluster IP Set ] . get Comment ( ) , " " , " " , " " , kube Cluster IP if proxier . masquerade All { write Line ( proxier . nat Rules , append ( args , " " , " " , string ( Kube Mark Masq } else if len ( proxier . cluster CIDR ) > 0 { // This masquerades off-cluster traffic to a service VIP. The idea // is that you can establish a static route for your Service range, // routing to any node, and that node will bridge into the Service // for you. Since that might bounce off-node, we masquerade here. // If/when we support "Local" policy for VI Ps, we should update this. write Line ( proxier . nat Rules , append ( args , " " , " " , proxier . cluster CIDR , " " , string ( Kube Mark Masq } else { // Masquerade all OUTPUT traffic coming from a service ip. // The kube dummy interface has all service VI Ps assigned which // results in the service VIP being picked as the source IP to reach // a VIP. This leads to a connection from VIP:<random port> to // VIP:<service port>. // Always masquerading OUTPUT (node-originating) traffic with a VIP // source ip and service port destination fixes the outgoing connections. write Line ( proxier . nat Rules , append ( args , " " , " " , string ( Kube Mark Masq if ! proxier . ipset List [ kube External IP Set ] . is Empty ( ) { // Build masquerade rules for packets to external I Ps. args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , proxier . ipset List [ kube External IP Set ] . get Comment ( ) , " " , " " , " " , kube External IP write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Mark Masq // Allow traffic for external I Ps that does not come from a bridge (i.e. not from a container) // nor from a local process to be forwarded to the service. // This rule roughly translates to "all traffic from off-machine". // This is imperfect in the face of network plugins that might not use a bridge, but we can revisit that later. external Traffic Only write Line ( proxier . nat Rules , append ( external Traffic Only dst Local Only // Allow traffic bound for external I Ps that happen to be recognized as local I Ps to stay local. // This covers cases like GCE load-balancers which get added to the local routing table. write Line ( proxier . nat Rules , append ( dst Local Only // -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT args = append ( args [ : 0 ] , " " , string ( kube Services write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Node Port // mark drop for KUBE-LOAD-BALANCER write Line ( proxier . nat Rules , [ ] string { " " , string ( Kube Load Balancer Chain ) , " " , string ( Kube Mark Masq // mark drop for KUBE-FIRE-WALL write Line ( proxier . nat Rules , [ ] string { " " , string ( Kube Fire Wall Chain ) , " " , string ( Kube Mark Drop // Accept all traffic with destination of ipvs virtual service, in case other iptables rules // block the traffic, that may result in ipvs rules invalid. // Those rules must be in the end of KUBE-SERVICE chain proxier . accept IPVS // If the masquerade Mark has been added then we want to forward that same // traffic, this allows Node Port traffic to be forwarded even if the default // FORWARD policy is not accept. write Line ( proxier . filter Rules , " " , string ( Kube Forward Chain ) , " " , " " , " " , `"kubernetes forwarding rules"` , " " , " " , " " , proxier . masquerade // The following rules can only be set if cluster CIDR has been defined. if len ( proxier . cluster CIDR ) != 0 { // The following two rules ensure the traffic after the initial packet // accepted by the "kubernetes forwarding rules" rule above will be // accepted, to be as specific as possible the traffic must be sourced // or destined to the cluster CIDR (to/from a pod). write Line ( proxier . filter Rules , " " , string ( Kube Forward Chain ) , " " , proxier . cluster write Line ( proxier . filter Rules , " " , string ( Kube Forward Chain ) , " " , " " , " " , `"kubernetes forwarding conntrack pod destination rule"` , " " , proxier . cluster // Write the end-of-table markers. write Line ( proxier . filter write Line ( proxier . nat } 
func ( proxier * Proxier ) create And Linke Kube Chain ( ) { existing Filter Chains := proxier . get Existing Chains ( proxier . filter Chains Data , utiliptables . Table existing NAT Chains := proxier . get Existing Chains ( proxier . iptables Data , utiliptables . Table // Make sure we keep stats for the top-level chains for _ , ch := range iptables Chains { if _ , err := proxier . iptables . Ensure if ch . table == utiliptables . Table NAT { if chain , ok := existing NAT Chains [ ch . chain ] ; ok { write Bytes Line ( proxier . nat } else { write Line ( proxier . nat Chains , utiliptables . Make Chain Line ( kube Postrouting } else { if chain , ok := existing Filter Chains [ Kube Forward Chain ] ; ok { write Bytes Line ( proxier . filter } else { write Line ( proxier . filter Chains , utiliptables . Make Chain Line ( Kube Forward for _ , jc := range iptables Jump if _ , err := proxier . iptables . Ensure // Install the kubernetes-specific postrouting rules. We use a whole chain for // this so that it is easier to flush and change, for example if the mark // value should ever change. write Line ( proxier . nat Rules , [ ] string { " " , string ( kube Postrouting Chain ) , " " , " " , " " , `"kubernetes service traffic requiring SNAT"` , " " , " " , " " , proxier . masquerade // Install the kubernetes-specific masquerade mark rule. We use a whole chain for // this so that it is easier to flush and change, for example if the mark // value should ever change. write Line ( proxier . nat Rules , [ ] string { " " , string ( Kube Mark Masq Chain ) , " " , " " , " " , proxier . masquerade } 
func ( proxier * Proxier ) get Existing err := proxier . iptables . Save } else { // otherwise parse the output return utiliptables . Get Chain } 
func write Line ( buf * bytes . Buffer , words ... string ) { // We avoid strings.Join for performance reasons. for i := range words { buf . Write if i < len ( words ) - 1 { buf . Write } else { buf . Write } 
func ( l * listen Port Opener ) Open Local Port ( lp * utilproxy . Local Port ) ( utilproxy . Closeable , error ) { return open Local } 
func Serve Attach ( w http . Response Writer , req * http . Request , attacher Attacher , pod Name string , uid types . UID , container string , stream Opts * Options , idle Timeout , stream Creation Timeout time . Duration , supported Protocols [ ] string ) { ctx , ok := create Streams ( req , w , stream Opts , supported Protocols , idle Timeout , stream Creation if ! ok { // error is handled by create err := attacher . Attach Container ( pod Name , uid , container , ctx . stdin Stream , ctx . stdout Stream , ctx . stderr Stream , ctx . tty , ctx . resize runtime . Handle ctx . write Status ( apierrors . New Internal } else { ctx . write Status ( & apierrors . Status Error { Err Status : metav1 . Status { Status : metav1 . Status } 
func init ( ) { tr := utilnet . Set Transport metadata HTTP Client http Client := & http . Client { Transport : tr , Timeout : metadata HTTP Client credentialprovider . Register Credential Provider ( " " , & credentialprovider . Caching Docker Config Provider { Provider : & docker Config Key Provider { metadata Provider { Client : http credentialprovider . Register Credential Provider ( " " , & credentialprovider . Caching Docker Config Provider { Provider : & docker Config Url Key Provider { metadata Provider { Client : http credentialprovider . Register Credential Provider ( " " , // Never cache this. The access token is already // cached by the metadata service. & container Registry Provider { metadata Provider { Client : http } 
func on GCEVM ( ) bool { data , err := ioutil . Read File ( gce Product Name name := strings . Trim } 
func ( g * docker Config Key Provider ) Provide ( image string ) credentialprovider . Docker Config { // Read the contents of the google-dockercfg metadata key and // parse them as an alternate .dockercfg if cfg , err := credentialprovider . Read Docker Config File From Url ( docker Config Key , g . Client , metadata return credentialprovider . Docker } 
func ( g * docker Config Url Key Provider ) Provide ( image string ) credentialprovider . Docker Config { // Read the contents of the google-dockercfg-url key and load a .dockercfg from there if url , err := credentialprovider . Read Url ( docker Config Url Key , g . Client , metadata } else { if strings . Has Prefix ( string ( url ) , " " ) { if cfg , err := credentialprovider . Read Docker Config File From } else { // TODO(mattmoor): support reading alternate scheme UR return credentialprovider . Docker } 
func run With const max if backoff > max Backoff { backoff = max } 
func ( g * container Registry Provider ) Enabled ( ) bool { if ! on // Given that we are on GCE, we should keep retrying until the metadata server responds. value := run With Backoff ( func ( ) ( [ ] byte , error ) { value , err := credentialprovider . Read Url ( service Accounts , g . Client , metadata // We expect the service account to return a list of account directories separated by newlines, e.g., // sv-account-name1/ // sv-account-name2/ // ref: https://cloud.google.com/compute/docs/storing-retrieving-metadata default Service Account for _ , sa := range strings . Split ( string ( value ) , " \n " ) { if strings . Trim Space ( sa ) == default Service Account { default Service Account if ! default Service Account url := metadata value = run With Backoff ( func ( ) ( [ ] byte , error ) { value , err := credentialprovider . Read Url ( url , g . Client , metadata for _ , v := range scopes { // cloud Platform Scope implies storage scope. if strings . Has Prefix ( v , storage Scope Prefix ) || strings . Has Prefix ( v , cloud Platform Scope } 
func ( g * container Registry Provider ) Provide ( image string ) credentialprovider . Docker Config { cfg := credentialprovider . Docker token Json Blob , err := credentialprovider . Read Url ( metadata Token , g . Client , metadata email , err := credentialprovider . Read Url ( metadata Email , g . Client , metadata var parsed Blob token if err := json . Unmarshal ( [ ] byte ( token Json Blob ) , & parsed Blob ) ; err != nil { klog . Errorf ( " " , token Json entry := credentialprovider . Docker Config Entry { Username : " " , Password : parsed Blob . Access // Add our entry for each of the supported container registry UR Ls for _ , k := range container Registry } 
func ( w * real Timeout Factory ) Timeout Ch ( ) ( <- chan time . Time , func ( ) bool ) { if w . timeout == 0 { return never Exit t := time . New } 
func serve Watch ( watcher watch . Interface , scope * Request Scope , media Type Options negotiation . Media Type Options , req * http . Request , w http . Response Writer , timeout time . Duration ) { options , err := options For Transform ( media Type // negotiate for the stream serializer from the scope's serializer serializer , err := negotiation . Negotiate Output Media Type framer := serializer . Stream stream Serializer := serializer . Stream encoder := scope . Serializer . Encoder For Version ( stream Serializer , scope . Kind . Group use Text Framing := serializer . Encodes As if framer == nil { scope . err ( fmt . Errorf ( " " , serializer . Media // TODO: next step, get back media Type Options from negotiate and return the exact value here media Type := serializer . Media if media Type != runtime . Content Type JSON { media // locate the appropriate embedded encoder based on the transform var embedded content Kind , content Serializer , transform := target Encoding For Transform ( scope , media Type if transform { info , ok := runtime . Serializer Info For Media Type ( content Serializer . Supported Media Types ( ) , serializer . Media if ! ok { scope . err ( fmt . Errorf ( " " , serializer . Media Type , content embedded Encoder = content Serializer . Encoder For Version ( info . Serializer , content Kind . Group } else { embedded Encoder = scope . Serializer . Encoder For Version ( serializer . Serializer , content Kind . Group server := & Watch Server { Watching : watcher , Scope : scope , Use Text Framing : use Text Framing , Media Type : media Type , Framer : framer , Encoder : encoder , Embedded Encoder : embedded Encoder , Fixup : func ( obj runtime . Object ) runtime . Object { result , err := transform Object ( ctx , obj , options , media Type if err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , reflect . Type // When we are transformed to a table, use the table options as the state for whether we // should print headers - on watch, we only want to print table headers on the first object // and omit them on subsequent events. if table Options , ok := options . ( * metav1beta1 . Table Options ) ; ok { table Options . No } , Timeout Factory : & real Timeout server . Serve } 
func ( s * Watch Server ) Serve HTTP ( w http . Response metrics . Registered Watchers . With Label defer metrics . Registered Watchers . With Label if wsstream . Is Web Socket Request ( req ) { w . Header ( ) . Set ( " " , s . Media websocket . Handler ( s . Handle WS ) . Serve cn , ok := w . ( http . Close utilruntime . Handle s . Scope . err ( errors . New Internal utilruntime . Handle s . Scope . err ( errors . New Internal framer := s . Framer . New Frame if framer == nil { // programmer error err := fmt . Errorf ( " " , s . Media utilruntime . Handle s . Scope . err ( errors . New Bad e := streaming . New // ensure the connection times out timeout Ch , cleanup := s . Timeout Factory . Timeout // begin the stream w . Header ( ) . Set ( " " , s . Media w . Write Header ( http . Status internal Event := & metav1 . Internal out Event := & metav1 . Watch ch := s . Watching . Result for { select { case <- cn . Close case <- timeout if err := s . Embedded Encoder . Encode ( obj , buf ) ; err != nil { // unexpected error utilruntime . Handle // Content * out Event = metav1 . Watch // create the external type directly and encode it. Clients will only recognize the serialization we provide. // The internal event is being reused, not reallocated so its just a few extra assignments to do it this way // and we get the benefit of using conversion functions which already have to stay in sync * internal Event = metav1 . Internal err := metav1 . Convert_v1_Internal Event_To_v1_Watch Event ( internal Event , out if err != nil { utilruntime . Handle if err := e . Encode ( out Event ) ; err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , out } 
func ( s * Watch Server ) Handle go func ( ) { defer utilruntime . Handle // This blocks until the connection is closed. // Client should not send anything. wsstream . Ignore internal Event := & metav1 . Internal stream ch := s . Watching . Result if err := s . Embedded Encoder . Encode ( obj , buf ) ; err != nil { // unexpected error utilruntime . Handle // Content // the internal event will be versioned by the encoder // create the external type directly and encode it. Clients will only recognize the serialization we provide. // The internal event is being reused, not reallocated so its just a few extra assignments to do it this way // and we get the benefit of using conversion functions which already have to stay in sync out Event := & metav1 . Watch * internal Event = metav1 . Internal err := metav1 . Convert_v1_Internal Event_To_v1_Watch Event ( internal Event , out if err != nil { utilruntime . Handle if err := s . Encoder . Encode ( out Event , stream Buf ) ; err != nil { // encoding error utilruntime . Handle if s . Use Text Framing { if err := websocket . Message . Send ( ws , stream } else { if err := websocket . Message . Send ( ws , stream stream } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Event List ) ( nil ) , ( * audit . Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Event List_To_audit_Event List ( a . ( * Event List ) , b . ( * audit . Event if err := s . Add Generated Conversion Func ( ( * audit . Event List ) ( nil ) , ( * Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Event List_To_v1beta1_Event List ( a . ( * audit . Event List ) , b . ( * Event if err := s . Add Generated Conversion Func ( ( * Group Resources ) ( nil ) , ( * audit . Group Resources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Group Resources_To_audit_Group Resources ( a . ( * Group Resources ) , b . ( * audit . Group if err := s . Add Generated Conversion Func ( ( * audit . Group Resources ) ( nil ) , ( * Group Resources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Group Resources_To_v1beta1_Group Resources ( a . ( * audit . Group Resources ) , b . ( * Group if err := s . Add Generated Conversion Func ( ( * Object Reference ) ( nil ) , ( * audit . Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Object Reference_To_audit_Object Reference ( a . ( * Object Reference ) , b . ( * audit . Object if err := s . Add Generated Conversion Func ( ( * audit . Object Reference ) ( nil ) , ( * Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Object Reference_To_v1beta1_Object Reference ( a . ( * audit . Object Reference ) , b . ( * Object if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Policy List ) ( nil ) , ( * audit . Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Policy List_To_audit_Policy List ( a . ( * Policy List ) , b . ( * audit . Policy if err := s . Add Generated Conversion Func ( ( * audit . Policy List ) ( nil ) , ( * Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Policy List_To_v1beta1_Policy List ( a . ( * audit . Policy List ) , b . ( * Policy if err := s . Add Generated Conversion Func ( ( * Policy Rule ) ( nil ) , ( * audit . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Policy Rule_To_audit_Policy Rule ( a . ( * Policy Rule ) , b . ( * audit . Policy if err := s . Add Generated Conversion Func ( ( * audit . Policy Rule ) ( nil ) , ( * Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Policy Rule_To_v1beta1_Policy Rule ( a . ( * audit . Policy Rule ) , b . ( * Policy if err := s . Add Conversion if err := s . Add Conversion } 
func Convert_v1beta1_Event List_To_audit_Event List ( in * Event List , out * audit . Event List , s conversion . Scope ) error { return auto Convert_v1beta1_Event List_To_audit_Event } 
func Convert_audit_Event List_To_v1beta1_Event List ( in * audit . Event List , out * Event List , s conversion . Scope ) error { return auto Convert_audit_Event List_To_v1beta1_Event } 
func Convert_v1beta1_Group Resources_To_audit_Group Resources ( in * Group Resources , out * audit . Group Resources , s conversion . Scope ) error { return auto Convert_v1beta1_Group Resources_To_audit_Group } 
func Convert_audit_Group Resources_To_v1beta1_Group Resources ( in * audit . Group Resources , out * Group Resources , s conversion . Scope ) error { return auto Convert_audit_Group Resources_To_v1beta1_Group } 
func Convert_v1beta1_Object Reference_To_audit_Object Reference ( in * Object Reference , out * audit . Object Reference , s conversion . Scope ) error { return auto Convert_v1beta1_Object Reference_To_audit_Object } 
func Convert_audit_Object Reference_To_v1beta1_Object Reference ( in * audit . Object Reference , out * Object Reference , s conversion . Scope ) error { return auto Convert_audit_Object Reference_To_v1beta1_Object } 
func Convert_v1beta1_Policy_To_audit_Policy ( in * Policy , out * audit . Policy , s conversion . Scope ) error { return auto } 
func Convert_audit_Policy_To_v1beta1_Policy ( in * audit . Policy , out * Policy , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Policy List_To_audit_Policy List ( in * Policy List , out * audit . Policy List , s conversion . Scope ) error { return auto Convert_v1beta1_Policy List_To_audit_Policy } 
func Convert_audit_Policy List_To_v1beta1_Policy List ( in * audit . Policy List , out * Policy List , s conversion . Scope ) error { return auto Convert_audit_Policy List_To_v1beta1_Policy } 
func Convert_v1beta1_Policy Rule_To_audit_Policy Rule ( in * Policy Rule , out * audit . Policy Rule , s conversion . Scope ) error { return auto Convert_v1beta1_Policy Rule_To_audit_Policy } 
func Convert_audit_Policy Rule_To_v1beta1_Policy Rule ( in * audit . Policy Rule , out * Policy Rule , s conversion . Scope ) error { return auto Convert_audit_Policy Rule_To_v1beta1_Policy } 
func Register ( plugins * admission . Plugins ) { plugins . Register ( Plugin Name , func ( config io . Reader ) ( admission . Interface , error ) { // load the configuration provided (if any) configuration , err := Load // validate the configuration (if any) if configuration != nil { if errs := validation . Validate Configuration ( configuration ) ; len ( errs ) != 0 { return nil , errs . To return New Resource } 
func New Resource Quota ( config * resourcequotaapi . Configuration , num Evaluators int , stop Ch <- chan struct { } ) ( * Quota Admission , error ) { quota Accessor , err := new Quota return & Quota Admission { Handler : admission . New Handler ( admission . Create , admission . Update ) , stop Ch : stop Ch , num Evaluators : num Evaluators , config : config , quota Accessor : quota } 
func ( a * Quota Admission ) Validate Initialization ( ) error { if a . quota if a . quota if a . quota if a . quota } 
func ( a * Quota Admission ) Validate ( attr admission . Attributes , o admission . Object Interfaces ) ( err error ) { // ignore all operations that correspond to sub-resource actions if attr . Get // ignore all operations that are not namespaced if attr . Get } 
} 
} 
func ( ps * Plugins ) get config1 , config2 , err := split if ! Plugin Enabled } 
func split Stream ( config io . Reader ) ( io . Reader , io . Reader , error ) { if config == nil || reflect . Value Of ( config ) . Is config Bytes , err := ioutil . Read return bytes . New Buffer ( config Bytes ) , bytes . New Buffer ( config } 
func ( ps * Plugins ) New From Plugins ( plugin Names [ ] string , config Provider Config Provider , plugin Initializer Plugin mutation validation for _ , plugin Name := range plugin Names { plugin Config , err := config Provider . Config For ( plugin plugin , err := ps . Init Plugin ( plugin Name , plugin Config , plugin if plugin != nil { if decorator != nil { handlers = append ( handlers , decorator . Decorate ( plugin , plugin if _ , ok := plugin . ( Mutation Interface ) ; ok { mutation Plugins = append ( mutation Plugins , plugin if _ , ok := plugin . ( Validation Interface ) ; ok { validation Plugins = append ( validation Plugins , plugin if len ( mutation Plugins ) != 0 { klog . Infof ( " " , len ( mutation Plugins ) , strings . Join ( mutation if len ( validation Plugins ) != 0 { klog . Infof ( " " , len ( validation Plugins ) , strings . Join ( validation return chain Admission } 
func ( ps * Plugins ) Init Plugin ( name string , config io . Reader , plugin Initializer Plugin plugin , found , err := ps . get plugin // ensure that plugins have been properly initialized if err := Validate } 
func Validate Initialization ( plugin Interface ) error { if validater , ok := plugin . ( Initialization Validator ) ; ok { err := validater . Validate } 
func ( con Etcd Connection ) Check Etcd Servers ( ) ( done bool , err error ) { // Attempt to reach every Etcd server randomly. server Number := len ( con . Server server Perms := rand . Perm ( server for _ , index := range server Perms { host , err := parse Server URI ( con . Server if con . server } 
func New ( follow Non Local Redirects bool ) Prober { tls Config := & tls . Config { Insecure Skip return New With TLS Config ( tls Config , follow Non Local } 
func New With TLS Config ( config * tls . Config , follow Non Local Redirects bool ) Prober { // We do not want the probe use node's local proxy set. transport := utilnet . Set Transport Defaults ( & http . Transport { TLS Client Config : config , Disable Keep Alives : true , Proxy : http . Proxy return http Prober { transport , follow Non Local } 
func ( pr http Prober ) Probe ( url * url . URL , headers http . Header , timeout time . Duration ) ( probe . Result , string , error ) { client := & http . Client { Timeout : timeout , Transport : pr . transport , Check Redirect : redirect Checker ( pr . follow Non Local return Do HTTP } 
func Do HTTP Probe ( url * url . URL , headers http . Header , client Get HTTP Interface ) ( probe . Result , string , error ) { req , err := http . New b , err := ioutil . Read if res . Status Code >= http . Status OK && res . Status Code < http . Status Bad Request { if res . Status Code >= http . Status Multiple return probe . Failure , fmt . Sprintf ( " " , res . Status } 
func Validate Runtime Class ( rc * node . Runtime Class ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & rc . Object Meta , false , apivalidation . Name Is DNS Subdomain , field . New for _ , msg := range apivalidation . Name Is DNS Label ( rc . Handler , false ) { all Errs = append ( all Errs , field . Invalid ( field . New return all } 
func Validate Runtime Class Update ( new , old * node . Runtime Class ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & new . Object Meta , & old . Object Meta , field . New all Errs = append ( all Errs , apivalidation . Validate Immutable Field ( new . Handler , old . Handler , field . New return all } 
func ( in * Controller Revision ) Deep Copy Into ( out * Controller out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object if in . Data != nil { out . Data = in . Data . Deep Copy } 
func ( in * Controller Revision ) Deep Copy ( ) * Controller out := new ( Controller in . Deep Copy } 
func ( in * Controller Revision ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Controller Revision List ) Deep Copy Into ( out * Controller Revision out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Controller for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Controller Revision List ) Deep Copy ( ) * Controller Revision out := new ( Controller Revision in . Deep Copy } 
func ( in * Controller Revision List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Daemon Set ) Deep Copy Into ( out * Daemon out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Daemon Set ) Deep Copy ( ) * Daemon out := new ( Daemon in . Deep Copy } 
func ( in * Daemon Set ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Daemon Set Condition ) Deep Copy Into ( out * Daemon Set in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Daemon Set Condition ) Deep Copy ( ) * Daemon Set out := new ( Daemon Set in . Deep Copy } 
func ( in * Daemon Set List ) Deep Copy Into ( out * Daemon Set out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Daemon for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Daemon Set List ) Deep Copy ( ) * Daemon Set out := new ( Daemon Set in . Deep Copy } 
func ( in * Daemon Set List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Daemon Set Spec ) Deep Copy Into ( out * Daemon Set * out = new ( v1 . Label ( * in ) . Deep Copy in . Template . Deep Copy in . Update Strategy . Deep Copy Into ( & out . Update if in . Revision History Limit != nil { in , out := & in . Revision History Limit , & out . Revision History } 
func ( in * Daemon Set Spec ) Deep Copy ( ) * Daemon Set out := new ( Daemon Set in . Deep Copy } 
func ( in * Daemon Set Status ) Deep Copy Into ( out * Daemon Set if in . Collision Count != nil { in , out := & in . Collision Count , & out . Collision * out = make ( [ ] Daemon Set for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Daemon Set Status ) Deep Copy ( ) * Daemon Set out := new ( Daemon Set in . Deep Copy } 
func ( in * Daemon Set Update Strategy ) Deep Copy Into ( out * Daemon Set Update if in . Rolling Update != nil { in , out := & in . Rolling Update , & out . Rolling * out = new ( Rolling Update Daemon } 
func ( in * Daemon Set Update Strategy ) Deep Copy ( ) * Daemon Set Update out := new ( Daemon Set Update in . Deep Copy } 
func ( in * Deployment ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Deployment ) Deep in . Deep Copy } 
func ( in * Deployment ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Deployment Condition ) Deep Copy Into ( out * Deployment in . Last Update Time . Deep Copy Into ( & out . Last Update in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Deployment Condition ) Deep Copy ( ) * Deployment out := new ( Deployment in . Deep Copy } 
func ( in * Deployment List ) Deep Copy Into ( out * Deployment out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Deployment List ) Deep Copy ( ) * Deployment out := new ( Deployment in . Deep Copy } 
func ( in * Deployment List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Deployment Rollback ) Deep Copy Into ( out * Deployment out . Type Meta = in . Type if in . Updated Annotations != nil { in , out := & in . Updated Annotations , & out . Updated out . Rollback To = in . Rollback } 
func ( in * Deployment Rollback ) Deep Copy ( ) * Deployment out := new ( Deployment in . Deep Copy } 
func ( in * Deployment Rollback ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Deployment Spec ) Deep Copy Into ( out * Deployment * out = new ( v1 . Label ( * in ) . Deep Copy in . Template . Deep Copy in . Strategy . Deep Copy if in . Revision History Limit != nil { in , out := & in . Revision History Limit , & out . Revision History if in . Rollback To != nil { in , out := & in . Rollback To , & out . Rollback * out = new ( Rollback if in . Progress Deadline Seconds != nil { in , out := & in . Progress Deadline Seconds , & out . Progress Deadline } 
func ( in * Deployment Spec ) Deep Copy ( ) * Deployment out := new ( Deployment in . Deep Copy } 
func ( in * Deployment Status ) Deep Copy Into ( out * Deployment * out = make ( [ ] Deployment for i := range * in { ( * in ) [ i ] . Deep Copy if in . Collision Count != nil { in , out := & in . Collision Count , & out . Collision } 
func ( in * Deployment Status ) Deep Copy ( ) * Deployment out := new ( Deployment in . Deep Copy } 
func ( in * Deployment Strategy ) Deep Copy ( ) * Deployment out := new ( Deployment in . Deep Copy } 
func ( in * Replica Set ) Deep Copy ( ) * Replica out := new ( Replica in . Deep Copy } 
func ( in * Replica Set ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Replica Set Condition ) Deep Copy Into ( out * Replica Set in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Replica Set Condition ) Deep Copy ( ) * Replica Set out := new ( Replica Set in . Deep Copy } 
func ( in * Replica Set List ) Deep Copy Into ( out * Replica Set out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Replica for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Replica Set List ) Deep Copy ( ) * Replica Set out := new ( Replica Set in . Deep Copy } 
func ( in * Replica Set List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Replica Set Spec ) Deep Copy Into ( out * Replica Set * out = new ( v1 . Label ( * in ) . Deep Copy in . Template . Deep Copy } 
func ( in * Replica Set Spec ) Deep Copy ( ) * Replica Set out := new ( Replica Set in . Deep Copy } 
func ( in * Replica Set Status ) Deep Copy Into ( out * Replica Set * out = make ( [ ] Replica Set for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Replica Set Status ) Deep Copy ( ) * Replica Set out := new ( Replica Set in . Deep Copy } 
func ( in * Rollback Config ) Deep Copy ( ) * Rollback out := new ( Rollback in . Deep Copy } 
func ( in * Rolling Update Daemon Set ) Deep Copy Into ( out * Rolling Update Daemon out . Max Unavailable = in . Max } 
func ( in * Rolling Update Daemon Set ) Deep Copy ( ) * Rolling Update Daemon out := new ( Rolling Update Daemon in . Deep Copy } 
func ( in * Rolling Update Deployment ) Deep Copy Into ( out * Rolling Update out . Max Unavailable = in . Max out . Max Surge = in . Max } 
func ( in * Rolling Update Deployment ) Deep Copy ( ) * Rolling Update out := new ( Rolling Update in . Deep Copy } 
func ( in * Rolling Update Stateful Set Strategy ) Deep Copy ( ) * Rolling Update Stateful Set out := new ( Rolling Update Stateful Set in . Deep Copy } 
func ( in * Stateful Set ) Deep Copy Into ( out * Stateful out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Stateful Set ) Deep Copy ( ) * Stateful out := new ( Stateful in . Deep Copy } 
func ( in * Stateful Set ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Stateful Set Condition ) Deep Copy Into ( out * Stateful Set in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Stateful Set Condition ) Deep Copy ( ) * Stateful Set out := new ( Stateful Set in . Deep Copy } 
func ( in * Stateful Set List ) Deep Copy Into ( out * Stateful Set out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Stateful for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Stateful Set List ) Deep Copy ( ) * Stateful Set out := new ( Stateful Set in . Deep Copy } 
func ( in * Stateful Set List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Stateful Set Spec ) Deep Copy ( ) * Stateful Set out := new ( Stateful Set in . Deep Copy } 
func ( in * Stateful Set Status ) Deep Copy Into ( out * Stateful Set if in . Observed Generation != nil { in , out := & in . Observed Generation , & out . Observed if in . Collision Count != nil { in , out := & in . Collision Count , & out . Collision * out = make ( [ ] Stateful Set for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Stateful Set Status ) Deep Copy ( ) * Stateful Set out := new ( Stateful Set in . Deep Copy } 
func ( in * Stateful Set Update Strategy ) Deep Copy ( ) * Stateful Set Update out := new ( Stateful Set Update in . Deep Copy } 
func ( transient Sched Info * Transient Scheduler Info ) Reset Transient Scheduler Info ( ) { transient Sched Info . Transient defer transient Sched Info . Transient // Reset Transient Node Info. transient Sched Info . Trans Node Info . Allocatable Volumes transient Sched Info . Trans Node Info . Requested } 
func New Resource ( rl v1 . Resource } 
func ( r * Resource ) Add ( rl v1 . Resource for r Name , r Quant := range rl { switch r Name { case v1 . Resource CPU : r . Milli CPU += r Quant . Milli case v1 . Resource Memory : r . Memory += r case v1 . Resource Pods : r . Allowed Pod Number += int ( r case v1 . Resource Ephemeral Storage : r . Ephemeral Storage += r default : if v1helper . Is Scalar Resource Name ( r Name ) { r . Add Scalar ( r Name , r } 
func ( r * Resource ) Resource List ( ) v1 . Resource List { result := v1 . Resource List { v1 . Resource CPU : * resource . New Milli Quantity ( r . Milli CPU , resource . Decimal SI ) , v1 . Resource Memory : * resource . New Quantity ( r . Memory , resource . Binary SI ) , v1 . Resource Pods : * resource . New Quantity ( int64 ( r . Allowed Pod Number ) , resource . Binary SI ) , v1 . Resource Ephemeral Storage : * resource . New Quantity ( r . Ephemeral Storage , resource . Binary for r Name , r Quant := range r . Scalar Resources { if v1helper . Is Huge Page Resource Name ( r Name ) { result [ r Name ] = * resource . New Quantity ( r Quant , resource . Binary } else { result [ r Name ] = * resource . New Quantity ( r Quant , resource . Decimal } 
func ( r * Resource ) Clone ( ) * Resource { res := & Resource { Milli CPU : r . Milli CPU , Memory : r . Memory , Allowed Pod Number : r . Allowed Pod Number , Ephemeral Storage : r . Ephemeral if r . Scalar Resources != nil { res . Scalar Resources = make ( map [ v1 . Resource for k , v := range r . Scalar Resources { res . Scalar } 
func ( r * Resource ) Add Scalar ( name v1 . Resource Name , quantity int64 ) { r . Set Scalar ( name , r . Scalar } 
func ( r * Resource ) Set Scalar ( name v1 . Resource Name , quantity int64 ) { // Lazily allocate scalar resource map. if r . Scalar Resources == nil { r . Scalar Resources = map [ v1 . Resource r . Scalar } 
func ( r * Resource ) Set Max Resource ( rl v1 . Resource for r Name , r Quantity := range rl { switch r Name { case v1 . Resource Memory : if mem := r case v1 . Resource CPU : if cpu := r Quantity . Milli Value ( ) ; cpu > r . Milli CPU { r . Milli case v1 . Resource Ephemeral Storage : if ephemeral Storage := r Quantity . Value ( ) ; ephemeral Storage > r . Ephemeral Storage { r . Ephemeral Storage = ephemeral default : if v1helper . Is Scalar Resource Name ( r Name ) { value := r if value > r . Scalar Resources [ r Name ] { r . Set Scalar ( r } 
func New Node Info ( pods ... * v1 . Pod ) * Node Info { ni := & Node Info { requested Resource : & Resource { } , nonzero Request : & Resource { } , allocatable Resource : & Resource { } , Transient Info : New Transient Scheduler Info ( ) , generation : next Generation ( ) , used Ports : make ( Host Port Info ) , image States : make ( map [ string ] * Image State for _ , pod := range pods { ni . Add } 
func ( n * Node } 
func ( n * Node } 
func ( n * Node Info ) Image States ( ) map [ string ] * Image State return n . image } 
func ( n * Node Info ) Pods With return n . pods With } 
func ( n * Node Info ) Allowed Pod Number ( ) int { if n == nil || n . allocatable return n . allocatable Resource . Allowed Pod } 
func ( n * Node return n . taints , n . taints } 
func ( n * Node Info ) Memory Pressure Condition ( ) v1 . Condition Status { if n == nil { return v1 . Condition return n . memory Pressure } 
func ( n * Node Info ) Disk Pressure Condition ( ) v1 . Condition Status { if n == nil { return v1 . Condition return n . disk Pressure } 
func ( n * Node Info ) PID Pressure Condition ( ) v1 . Condition Status { if n == nil { return v1 . Condition return n . pid Pressure } 
func ( n * Node Info ) Set Allocatable Resource ( allocatable Resource * Resource ) { n . allocatable Resource = allocatable n . generation = next } 
func ( n * Node Info ) Clone ( ) * Node Info { clone := & Node Info { node : n . node , requested Resource : n . requested Resource . Clone ( ) , nonzero Request : n . nonzero Request . Clone ( ) , allocatable Resource : n . allocatable Resource . Clone ( ) , taints Err : n . taints Err , Transient Info : n . Transient Info , memory Pressure Condition : n . memory Pressure Condition , disk Pressure Condition : n . disk Pressure Condition , pid Pressure Condition : n . pid Pressure Condition , used Ports : make ( Host Port Info ) , image States : n . image if len ( n . used Ports ) > 0 { // Host Port Info is a map-in-map struct // make sure it's deep copied for ip , port Map := range n . used Ports { clone . used Ports [ ip ] = make ( map [ Protocol for protocol Port , v := range port Map { clone . used Ports [ ip ] [ protocol if len ( n . pods With Affinity ) > 0 { clone . pods With Affinity = append ( [ ] * v1 . Pod ( nil ) , n . pods With } 
func ( n * Node Info ) Volume Limits ( ) map [ v1 . Resource Name ] int64 { volume Limits := map [ v1 . Resource for k , v := range n . Allocatable Resource ( ) . Scalar Resources { if v1helper . Is Attachable Volume Resource Name ( k ) { volume return volume } 
func ( n * Node Info ) String ( ) string { pod for i , pod := range n . pods { pod return fmt . Sprintf ( " " , pod Keys , n . requested Resource , n . nonzero Request , n . used Ports , n . allocatable } 
func ( n * Node Info ) Add Pod ( pod * v1 . Pod ) { res , non0CPU , non0Mem := calculate n . requested Resource . Milli CPU += res . Milli n . requested n . requested Resource . Ephemeral Storage += res . Ephemeral if n . requested Resource . Scalar Resources == nil && len ( res . Scalar Resources ) > 0 { n . requested Resource . Scalar Resources = map [ v1 . Resource for r Name , r Quant := range res . Scalar Resources { n . requested Resource . Scalar Resources [ r Name ] += r n . nonzero Request . Milli n . nonzero if has Pod Affinity Constraints ( pod ) { n . pods With Affinity = append ( n . pods With // Consume ports when pods added. n . Update Used n . generation = next } 
func ( n * Node Info ) Remove Pod ( pod * v1 . Pod ) error { k1 , err := Get Pod for i := range n . pods With Affinity { k2 , err := Get Pod Key ( n . pods With if k1 == k2 { // delete the element n . pods With Affinity [ i ] = n . pods With Affinity [ len ( n . pods With n . pods With Affinity = n . pods With Affinity [ : len ( n . pods With for i := range n . pods { k2 , err := Get Pod // reduce the resource data res , non0CPU , non0Mem := calculate n . requested Resource . Milli CPU -= res . Milli n . requested n . requested Resource . Ephemeral Storage -= res . Ephemeral if len ( res . Scalar Resources ) > 0 && n . requested Resource . Scalar Resources == nil { n . requested Resource . Scalar Resources = map [ v1 . Resource for r Name , r Quant := range res . Scalar Resources { n . requested Resource . Scalar Resources [ r Name ] -= r n . nonzero Request . Milli n . nonzero // Release ports when remove Pods. n . Update Used n . generation = next } 
func ( n * Node Info ) Update Used for k := range container . Ports { pod if add { n . used Ports . Add ( pod Port . Host IP , string ( pod Port . Protocol ) , pod Port . Host } else { n . used Ports . Remove ( pod Port . Host IP , string ( pod Port . Protocol ) , pod Port . Host } 
func ( n * Node Info ) Set n . allocatable Resource = New switch cond . Type { case v1 . Node Memory Pressure : n . memory Pressure case v1 . Node Disk Pressure : n . disk Pressure case v1 . Node PID Pressure : n . pid Pressure n . Transient Info = New Transient Scheduler n . generation = next } 
func ( n * Node Info ) Remove Node ( node * v1 . Node ) error { // We don't remove Node n . allocatable n . taints , n . taints n . memory Pressure Condition = v1 . Condition n . disk Pressure Condition = v1 . Condition n . pid Pressure Condition = v1 . Condition n . image States = make ( map [ string ] * Image State n . generation = next } 
func ( n * Node Info ) Filter Out for _ , p := range pods { if p . Spec . Node // If pod is on the given node, add it to 'filtered' only if it is present in node Info. pod Key , _ := Get Pod for _ , np := range n . Pods ( ) { npodkey , _ := Get Pod if npodkey == pod } 
func Get Pod } 
func ( n * Node Info ) Filter ( pod * v1 . Pod ) bool { if pod . Spec . Node } 
root := filepath . From } 
func ( s * Secure Serving Options With Loopback ) Apply To ( secure Serving Info * * server . Secure Serving Info , loopback Client Config * * rest . Config ) error { if s == nil || s . Secure Serving Options == nil || secure Serving if err := s . Secure Serving Options . Apply To ( secure Serving if * secure Serving Info == nil || loopback Client // create self-signed cert+key with the fake server.Loopback Client Server Name Override and // let the server return it when the loopback client connects. cert Pem , key Pem , err := certutil . Generate Self Signed Cert Key ( server . Loopback Client Server Name tls Cert , err := tls . X509Key Pair ( cert Pem , key secure Loopback Client Config , err := ( * secure Serving Info ) . New Loopback Client Config ( uuid . New Random ( ) . String ( ) , cert switch { // if we failed and there's no fallback loopback client config, we need to fail case err != nil && * loopback Client // if we failed, but we already have a fallback loopback client config (usually insecure), allow it case err != nil && * loopback Client Config != nil : default : * loopback Client Config = secure Loopback Client ( * secure Serving Info ) . SNI Certs [ server . Loopback Client Server Name Override ] = & tls } 
func ( r * Role Binding Builder ) Users ( users ... string ) * Role Binding Builder { for _ , user := range users { r . Role Binding . Subjects = append ( r . Role Binding . Subjects , Subject { Kind : User Kind , API Group : Group } 
func ( r * Role Binding Builder ) S As ( namespace string , service Account Names ... string ) * Role Binding Builder { for _ , sa Name := range service Account Names { r . Role Binding . Subjects = append ( r . Role Binding . Subjects , Subject { Kind : Service Account Kind , Namespace : namespace , Name : sa } 
func ( r * Role Binding Builder ) Binding Or Die ( ) Role } 
func ( r * Role Binding Builder ) Binding ( ) ( Role Binding , error ) { if len ( r . Role Binding . Subjects ) == 0 { return Role Binding { } , fmt . Errorf ( " " , r . Role return r . Role } 
func New NS Enter ( mounter mount . Interface , ne * nsenter . Nsenter , root } 
func lock And Check Sub Path ( volume Path , host Path string ) ( [ ] uintptr , error ) { if len ( volume Path ) == 0 || len ( host final Sub Path , err := filepath . Eval Symlinks ( host if err != nil { return [ ] uintptr { } , fmt . Errorf ( " " , host final Volume Path , err := filepath . Eval Symlinks ( volume if err != nil { return [ ] uintptr { } , fmt . Errorf ( " " , volume return lock And Check Sub Path Without Symlink ( final Volume Path , final Sub } 
func lock And Check Sub Path Without Symlink ( volume Path , sub Path string ) ( [ ] uintptr , error ) { if len ( volume Path ) == 0 || len ( sub // get relative path to volume Path rel Sub Path , err := filepath . Rel ( volume Path , sub if err != nil { return [ ] uintptr { } , fmt . Errorf ( " " , volume Path , sub if mount . Starts With Backstep ( rel Sub Path ) { return [ ] uintptr { } , fmt . Errorf ( " " , sub Path , volume if rel Sub Path == " " { // volume Path and sub file var error current Full Path := volume dirs := strings . Split ( rel Sub Path , string ( os . Path for _ , dir := range dirs { // lock intermediate sub Path directory first current Full Path = filepath . Join ( current Full handle , err := lock Path ( current Full if err != nil { error Result = fmt . Errorf ( " " , current Full file Handles = append ( file // make sure intermediate sub Path directory does not contain symlink any more stat , err := os . Lstat ( current Full if err != nil { error Result = fmt . Errorf ( " " , current Full if stat . Mode ( ) & os . Mode Symlink != 0 { error Result = fmt . Errorf ( " " , current Full if ! mount . Path Within Base ( current Full Path , volume Path ) { error Result = fmt . Errorf ( " " , current Full Path , volume return file Handles , error } 
func unlock Path ( file Handles [ ] uintptr ) { if file Handles != nil { for _ , handle := range file Handles { syscall . Close } 
func lock Path ( path string ) ( uintptr , error ) { if len ( path ) == 0 { return uintptr ( syscall . Invalid pathp , err := syscall . UTF16Ptr From if err != nil { return uintptr ( syscall . Invalid fd , err := syscall . Create } 
func ( sp * subpath ) Prepare Safe Subpath ( sub Path Subpath ) ( new Host Path string , cleanup Action func ( ) , err error ) { handles , err := lock And Check Sub Path ( sub Path . Volume Path , sub // Unlock the directories when the container starts cleanup Action = func ( ) { unlock return sub Path . Path , cleanup } 
func find Existing if mount . Starts With current for i , dir := range dirs { parent = current current if _ , err := os . Lstat ( current Path ) ; err != nil { if os . Is Not } 
func New Cmd cmd . Add Command ( new Cmd Certs cmd . Add Command ( new Cmd Kubelet cmd . Add Command ( new Cmd Kube Config cmd . Add Command ( New Cmd // TODO: This command should be removed as soon as the kubeadm init phase refactoring is completed. // current phases implemented as cobra.Commands should become workflow.Phases, while other utilities // hosted under kubeadm alpha phases command should found a new home under kubeadm alpha (without phases) cmd . Add Command ( new Cmd } 
func ( in * Event ) Deep Copy if in . Object != nil { out . Object = in . Object . Deep Copy } 
// Sort for determinism. sort . String } 
} 
func New Storage ( opts Getter generic . REST Options Getter ) * CSI Node Storage { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & storageapi . CSI Node { } } , New List Func : func ( ) runtime . Object { return & storageapi . CSI Node List { } } , Default Qualified Resource : storageapi . Resource ( " " ) , Create Strategy : csinode . Strategy , Update Strategy : csinode . Strategy , Delete Strategy : csinode . Strategy , Return Deleted options := & generic . Store Options { REST Options : opts if err := store . Complete With return & CSI Node Storage { CSI } 
func ( o * HPA Controller Options ) Add Flags ( fs * pflag . Flag fs . Duration Var ( & o . Horizontal Pod Autoscaler Sync Period . Duration , " " , o . Horizontal Pod Autoscaler Sync fs . Duration Var ( & o . Horizontal Pod Autoscaler Upscale Forbidden Window . Duration , " " , o . Horizontal Pod Autoscaler Upscale Forbidden fs . Mark fs . Duration Var ( & o . Horizontal Pod Autoscaler Downscale Stabilization Window . Duration , " " , o . Horizontal Pod Autoscaler Downscale Stabilization fs . Duration Var ( & o . Horizontal Pod Autoscaler Downscale Forbidden Window . Duration , " " , o . Horizontal Pod Autoscaler Downscale Forbidden fs . Mark fs . Float64Var ( & o . Horizontal Pod Autoscaler Tolerance , " " , o . Horizontal Pod Autoscaler fs . Bool Var ( & o . Horizontal Pod Autoscaler Use REST Clients , " " , o . Horizontal Pod Autoscaler Use REST fs . Duration Var ( & o . Horizontal Pod Autoscaler CPU Initialization Period . Duration , " " , o . Horizontal Pod Autoscaler CPU Initialization fs . Mark fs . Duration Var ( & o . Horizontal Pod Autoscaler Initial Readiness Delay . Duration , " " , o . Horizontal Pod Autoscaler Initial Readiness } 
func ( o * HPA Controller Options ) Apply To ( cfg * poautosclerconfig . HPA Controller cfg . Horizontal Pod Autoscaler Sync Period = o . Horizontal Pod Autoscaler Sync cfg . Horizontal Pod Autoscaler Downscale Stabilization Window = o . Horizontal Pod Autoscaler Downscale Stabilization cfg . Horizontal Pod Autoscaler Tolerance = o . Horizontal Pod Autoscaler cfg . Horizontal Pod Autoscaler Use REST Clients = o . Horizontal Pod Autoscaler Use REST cfg . Horizontal Pod Autoscaler CPU Initialization Period = o . Horizontal Pod Autoscaler CPU Initialization cfg . Horizontal Pod Autoscaler Initial Readiness Delay = o . Horizontal Pod Autoscaler Initial Readiness } 
func ( o * HPA Controller } 
func Recommended Default HPA Controller Configuration ( obj * kubectrlmgrconfigv1alpha1 . HPA Controller if obj . Horizontal Pod Autoscaler Use REST Clients == nil { obj . Horizontal Pod Autoscaler Use REST Clients = utilpointer . Bool if obj . Horizontal Pod Autoscaler Sync Period == zero { obj . Horizontal Pod Autoscaler Sync if obj . Horizontal Pod Autoscaler Upscale Forbidden Window == zero { obj . Horizontal Pod Autoscaler Upscale Forbidden if obj . Horizontal Pod Autoscaler Downscale Stabilization Window == zero { obj . Horizontal Pod Autoscaler Downscale Stabilization if obj . Horizontal Pod Autoscaler CPU Initialization Period == zero { obj . Horizontal Pod Autoscaler CPU Initialization if obj . Horizontal Pod Autoscaler Initial Readiness Delay == zero { obj . Horizontal Pod Autoscaler Initial Readiness if obj . Horizontal Pod Autoscaler Downscale Forbidden Window == zero { obj . Horizontal Pod Autoscaler Downscale Forbidden if obj . Horizontal Pod Autoscaler Tolerance == 0 { obj . Horizontal Pod Autoscaler } 
func New For Config ( c * rest . Config ) ( * Clientset , error ) { config Shallow if config Shallow Copy . Rate Limiter == nil && config Shallow Copy . QPS > 0 { config Shallow Copy . Rate Limiter = flowcontrol . New Token Bucket Rate Limiter ( config Shallow Copy . QPS , config Shallow cs . example , err = exampleinternalversion . New For Config ( & config Shallow cs . second Example , err = secondexampleinternalversion . New For Config ( & config Shallow cs . Discovery Client , err = discovery . New Discovery Client For Config ( & config Shallow } 
func New For Config Or cs . example = exampleinternalversion . New For Config Or cs . second Example = secondexampleinternalversion . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . second cs . Discovery Client = discovery . New Discovery } 
func ( c * Clientset ) Cr V1 ( ) crv1 . Cr V1Interface { return & fakecrv1 . Fake Cr } 
func ( in * Cluster Role Binding Adapter ) Deep Copy Into ( out * Cluster Role Binding if in . Cluster Role Binding != nil { in , out := & in . Cluster Role Binding , & out . Cluster Role * out = new ( v1 . Cluster Role ( * in ) . Deep Copy } 
func ( in * Cluster Role Binding Adapter ) Deep Copy ( ) * Cluster Role Binding out := new ( Cluster Role Binding in . Deep Copy } 
func ( in * Cluster Role Rule Owner ) Deep Copy Into ( out * Cluster Role Rule if in . Cluster Role != nil { in , out := & in . Cluster Role , & out . Cluster * out = new ( v1 . Cluster ( * in ) . Deep Copy } 
func ( in * Cluster Role Rule Owner ) Deep Copy ( ) * Cluster Role Rule out := new ( Cluster Role Rule in . Deep Copy } 
func ( in * Role Binding Adapter ) Deep Copy Into ( out * Role Binding if in . Role Binding != nil { in , out := & in . Role Binding , & out . Role * out = new ( v1 . Role ( * in ) . Deep Copy } 
func ( in * Role Binding Adapter ) Deep Copy ( ) * Role Binding out := new ( Role Binding in . Deep Copy } 
func ( in * Role Rule Owner ) Deep Copy Into ( out * Role Rule ( * in ) . Deep Copy } 
func ( in * Role Rule Owner ) Deep Copy ( ) * Role Rule out := new ( Role Rule in . Deep Copy } 
func ( e Map Element ) Merge ( v Strategy ) ( Result , error ) { return v . Merge } 
func New Flunder Informer ( client versioned . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Flunder Informer ( client , namespace , resync } 
func ( i * Import Restriction ) Forbidden Imports For ( pkg Package ) ( [ ] string , error ) { if restricted , err := i . is Restricted return i . forbidden Imports } 
func ( i * Import Restriction ) is Restricted Dir ( dir string ) ( bool , error ) { if under , err := is Path Under ( i . Base for _ , ignored := range i . Ignored Sub Trees { if under , err := is Path } 
func is Path Under ( base , path string ) ( bool , error ) { abs abs rel Path , err := filepath . Rel ( abs Base , abs // if path is below base, the relative path // from base to path will not start with `../` return ! strings . Has Prefix ( rel } 
func ( i * Import Restriction ) forbidden Imports For ( pkg Package ) [ ] string { forbidden Import if ! i . Exclude Tests { imports = append ( imports , append ( pkg . Test Imports , pkg . X Test for _ , imp := range imports { path := extract Vendor if i . is Forbidden ( path ) { forbidden Import var forbidden for imp := range forbidden Import Set { forbidden Imports = append ( forbidden return forbidden } 
func extract Vendor Path ( path string ) string { vendor if ! strings . Contains ( path , vendor return path [ strings . Index ( path , vendor Path ) + len ( vendor } 
func ( i * Import Restriction ) is Forbidden ( imp string ) bool { imports Below Root := strings . Has Prefix ( imp , root imports Below Base := strings . Has Prefix ( imp , i . Base imports for _ , allowed := range i . Allowed Imports { exactly Imports imports Below Allowed := strings . Has imports Allowed = imports Allowed || ( imports Below Allowed || exactly Imports return imports Below Root && ! imports Below Base && ! imports } 
func ( in * Deployment Strategy ) Deep Copy Into ( out * Deployment if in . Rolling Update != nil { in , out := & in . Rolling Update , & out . Rolling * out = new ( Rolling Update ( * in ) . Deep Copy } 
func ( in * HTTP Ingress Path ) Deep Copy Into ( out * HTTP Ingress } 
func ( in * HTTP Ingress Path ) Deep Copy ( ) * HTTP Ingress out := new ( HTTP Ingress in . Deep Copy } 
func ( in * HTTP Ingress Rule Value ) Deep Copy Into ( out * HTTP Ingress Rule * out = make ( [ ] HTTP Ingress } 
func ( in * HTTP Ingress Rule Value ) Deep Copy ( ) * HTTP Ingress Rule out := new ( HTTP Ingress Rule in . Deep Copy } 
func ( in * IP Block ) Deep Copy Into ( out * IP } 
func ( in * IP Block ) Deep Copy ( ) * IP out := new ( IP in . Deep Copy } 
func ( in * Ingress ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Ingress ) Deep in . Deep Copy } 
func ( in * Ingress ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Ingress Backend ) Deep Copy Into ( out * Ingress out . Service Port = in . Service } 
func ( in * Ingress Backend ) Deep Copy ( ) * Ingress out := new ( Ingress in . Deep Copy } 
func ( in * Ingress List ) Deep Copy Into ( out * Ingress out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Ingress List ) Deep Copy ( ) * Ingress out := new ( Ingress in . Deep Copy } 
func ( in * Ingress List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Ingress Rule ) Deep Copy Into ( out * Ingress in . Ingress Rule Value . Deep Copy Into ( & out . Ingress Rule } 
func ( in * Ingress Rule ) Deep Copy ( ) * Ingress out := new ( Ingress in . Deep Copy } 
func ( in * Ingress Rule Value ) Deep Copy Into ( out * Ingress Rule * out = new ( HTTP Ingress Rule ( * in ) . Deep Copy } 
func ( in * Ingress Rule Value ) Deep Copy ( ) * Ingress Rule out := new ( Ingress Rule in . Deep Copy } 
func ( in * Ingress Spec ) Deep Copy Into ( out * Ingress * out = new ( Ingress * out = make ( [ ] Ingress for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Ingress for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Ingress Spec ) Deep Copy ( ) * Ingress out := new ( Ingress in . Deep Copy } 
func ( in * Ingress Status ) Deep Copy Into ( out * Ingress in . Load Balancer . Deep Copy Into ( & out . Load } 
func ( in * Ingress Status ) Deep Copy ( ) * Ingress out := new ( Ingress in . Deep Copy } 
func ( in * Ingress TLS ) Deep Copy Into ( out * Ingress } 
func ( in * Ingress TLS ) Deep Copy ( ) * Ingress out := new ( Ingress in . Deep Copy } 
func ( in * Network Policy ) Deep Copy Into ( out * Network out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Network Policy ) Deep Copy ( ) * Network out := new ( Network in . Deep Copy } 
func ( in * Network Policy ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Network Policy Egress Rule ) Deep Copy Into ( out * Network Policy Egress * out = make ( [ ] Network Policy for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Network Policy for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Network Policy Egress Rule ) Deep Copy ( ) * Network Policy Egress out := new ( Network Policy Egress in . Deep Copy } 
func ( in * Network Policy Ingress Rule ) Deep Copy Into ( out * Network Policy Ingress * out = make ( [ ] Network Policy for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Network Policy for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Network Policy Ingress Rule ) Deep Copy ( ) * Network Policy Ingress out := new ( Network Policy Ingress in . Deep Copy } 
func ( in * Network Policy List ) Deep Copy Into ( out * Network Policy out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Network for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Network Policy List ) Deep Copy ( ) * Network Policy out := new ( Network Policy in . Deep Copy } 
func ( in * Network Policy List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Network Policy Peer ) Deep Copy ( ) * Network Policy out := new ( Network Policy in . Deep Copy } 
func ( in * Network Policy Port ) Deep Copy ( ) * Network Policy out := new ( Network Policy in . Deep Copy } 
func ( in * Network Policy Spec ) Deep Copy Into ( out * Network Policy in . Pod Selector . Deep Copy Into ( & out . Pod * out = make ( [ ] Network Policy Ingress for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Network Policy Egress for i := range * in { ( * in ) [ i ] . Deep Copy if in . Policy Types != nil { in , out := & in . Policy Types , & out . Policy * out = make ( [ ] Policy } 
func ( in * Network Policy Spec ) Deep Copy ( ) * Network Policy out := new ( Network Policy in . Deep Copy } 
func ( in * Pod Security Policy Spec ) Deep Copy Into ( out * Pod Security Policy if in . Default Add Capabilities != nil { in , out := & in . Default Add Capabilities , & out . Default Add if in . Required Drop Capabilities != nil { in , out := & in . Required Drop Capabilities , & out . Required Drop if in . Allowed Capabilities != nil { in , out := & in . Allowed Capabilities , & out . Allowed * out = make ( [ ] FS if in . Host Ports != nil { in , out := & in . Host Ports , & out . Host * out = make ( [ ] Host Port in . SE Linux . Deep Copy Into ( & out . SE in . Run As User . Deep Copy Into ( & out . Run As if in . Run As Group != nil { in , out := & in . Run As Group , & out . Run As * out = new ( Run As Group Strategy ( * in ) . Deep Copy in . Supplemental Groups . Deep Copy Into ( & out . Supplemental in . FS Group . Deep Copy Into ( & out . FS if in . Default Allow Privilege Escalation != nil { in , out := & in . Default Allow Privilege Escalation , & out . Default Allow Privilege if in . Allow Privilege Escalation != nil { in , out := & in . Allow Privilege Escalation , & out . Allow Privilege if in . Allowed Host Paths != nil { in , out := & in . Allowed Host Paths , & out . Allowed Host * out = make ( [ ] Allowed Host if in . Allowed Flex Volumes != nil { in , out := & in . Allowed Flex Volumes , & out . Allowed Flex * out = make ( [ ] Allowed Flex if in . Allowed CSI Drivers != nil { in , out := & in . Allowed CSI Drivers , & out . Allowed CSI * out = make ( [ ] Allowed CSI if in . Allowed Unsafe Sysctls != nil { in , out := & in . Allowed Unsafe Sysctls , & out . Allowed Unsafe if in . Forbidden Sysctls != nil { in , out := & in . Forbidden Sysctls , & out . Forbidden if in . Allowed Proc Mount Types != nil { in , out := & in . Allowed Proc Mount Types , & out . Allowed Proc Mount * out = make ( [ ] corev1 . Proc Mount if in . Runtime Class != nil { in , out := & in . Runtime Class , & out . Runtime * out = new ( Runtime Class Strategy ( * in ) . Deep Copy } 
func ( in * Replica Set Spec ) Deep Copy Into ( out * Replica Set * out = new ( v1 . Label ( * in ) . Deep Copy in . Template . Deep Copy } 
func ( in * Rolling Update Daemon Set ) Deep Copy Into ( out * Rolling Update Daemon if in . Max Unavailable != nil { in , out := & in . Max Unavailable , & out . Max * out = new ( intstr . Int Or } 
func ( in * Rolling Update Deployment ) Deep Copy Into ( out * Rolling Update if in . Max Unavailable != nil { in , out := & in . Max Unavailable , & out . Max * out = new ( intstr . Int Or if in . Max Surge != nil { in , out := & in . Max Surge , & out . Max * out = new ( intstr . Int Or } 
func ( in * SE Linux Strategy Options ) Deep Copy Into ( out * SE Linux Strategy if in . SE Linux Options != nil { in , out := & in . SE Linux Options , & out . SE Linux * out = new ( corev1 . SE Linux } 
func ( in * Scale Status ) Deep Copy Into ( out * Scale } 
func new Foos ( c * Samplecontroller V1alpha1Client , namespace string ) * foos { return & foos { client : c . REST } 
} 
} 
func ( c * Fake Node Metricses ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Node Metrics , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( nodemetricses Resource , name ) , & v1beta1 . Node return obj . ( * v1beta1 . Node } 
func ( c * Fake Node Metricses ) List ( opts v1 . List Options ) ( result * v1beta1 . Node Metrics List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( nodemetricses Resource , nodemetricses Kind , opts ) , & v1beta1 . Node Metrics label , _ , _ := testing . Extract From List list := & v1beta1 . Node Metrics List { List Meta : obj . ( * v1beta1 . Node Metrics List ) . List for _ , item := range obj . ( * v1beta1 . Node Metrics } 
func ( c * Fake Node Metricses ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( nodemetricses } 
func Update Transport ( stop Ch <- chan struct { } , client Config * restclient . Config , client Certificate Manager certificate . Manager , exit After time . Duration ) ( func ( ) , error ) { return update Transport ( stop Ch , 10 * time . Second , client Config , client Certificate Manager , exit } 
func update Transport ( stop Ch <- chan struct { } , period time . Duration , client Config * restclient . Config , client Certificate Manager certificate . Manager , exit After time . Duration ) ( func ( ) , error ) { if client Config . Transport != nil || client d := connrotation . New Dialer ( ( & net . Dialer { Timeout : 30 * time . Second , Keep Alive : 30 * time . Second } ) . Dial if client Certificate Manager != nil { if err := add Cert Rotation ( stop Ch , period , client Config , client Certificate Manager , exit } else { client Config . Dial = d . Dial return d . Close } 
func create Rate Limit Err ( is Write bool , op Name string ) error { op if is Write { op return fmt . Errorf ( " " , op Type , op } 
func ( spc * real Stateful Pod Control ) record Pod Event ( verb string , set * apps . Stateful message := fmt . Sprintf ( " " , strings . To spc . recorder . Event ( set , v1 . Event Type message := fmt . Sprintf ( " " , strings . To spc . recorder . Event ( set , v1 . Event Type } 
func ( spc * real Stateful Pod Control ) create Persistent Volume Claims ( set * apps . Stateful for _ , claim := range get Persistent Volume Claims ( set , pod ) { _ , err := spc . pvc Lister . Persistent Volume switch { case apierrors . Is Not Found ( err ) : _ , err := spc . client . Core V1 ( ) . Persistent Volume if err == nil || ! apierrors . Is Already Exists ( err ) { spc . record Claim spc . record Claim return errorutils . New } 
func Recommended Default Persistent Volume Binder Controller Configuration ( obj * kubectrlmgrconfigv1alpha1 . Persistent Volume Binder Controller if obj . PV Claim Binder Sync Period == zero { obj . PV Claim Binder Sync // Use the default Volume Configuration options. Recommended Default Volume Configuration ( & obj . Volume } 
func Recommended Default Volume Configuration ( obj * kubectrlmgrconfigv1alpha1 . Volume Configuration ) { if obj . Enable Host Path Provisioning == nil { obj . Enable Host Path Provisioning = utilpointer . Bool if obj . Enable Dynamic Provisioning == nil { obj . Enable Dynamic Provisioning = utilpointer . Bool if obj . Flex Volume Plugin Dir == " " { obj . Flex Volume Plugin // Use the default Persistent Volume Recycler Configuration options. Recommended Default Persistent Volume Recycler Configuration ( & obj . Persistent Volume Recycler } 
func Recommended Default Persistent Volume Recycler Configuration ( obj * kubectrlmgrconfigv1alpha1 . Persistent Volume Recycler Configuration ) { if obj . Maximum Retry == 0 { obj . Maximum if obj . Minimum Timeout NFS == 0 { obj . Minimum Timeout if obj . Increment Timeout NFS == 0 { obj . Increment Timeout if obj . Minimum Timeout Host Path == 0 { obj . Minimum Timeout Host if obj . Increment Timeout Host Path == 0 { obj . Increment Timeout Host } 
func New Stateful Set Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Stateful Set Informer ( client , namespace , resync } 
func ( v Volume Path Handler ) Attach File Device ( path string ) ( string , error ) { block Device Path , err := v . Get Loop if err != nil && err . Error ( ) != Err Device Not // If no existing loop device for the path, create one if block Device block Device Path , err = make Loop return block Device } 
func ( v Volume Path Handler ) Get Loop if os . Is Not Exist ( err ) { return " " , errors . New ( Err Device Not cmd := exec . Command ( losetup out , err := cmd . Combined return parse Losetup Output For } 
func ( v Volume Path Handler ) Remove Loop cmd := exec . Command ( losetup out , err := cmd . Combined if err != nil { if _ , err := os . Stat ( device ) ; os . Is Not } 
func ( attacher * cinder Disk Attacher ) Mount Device ( spec * volume . Spec , device Path string , device Mount Path string ) error { mounter := attacher . host . Get Mounter ( cinder Volume Plugin not Mnt , err := mounter . Is Likely Not Mount Point ( device Mount if err != nil { if os . Is Not Exist ( err ) { if err := os . Mkdir All ( device Mount not _ , volume FS Type , read Only , err := get Volume if read if not Mnt { disk Mounter := volumeutil . New Safe Format And Mount From Host ( cinder Volume Plugin mount Options := volumeutil . Mount Option From err = disk Mounter . Format And Mount ( device Path , device Mount Path , volume FS Type , mount if err != nil { os . Remove ( device Mount } 
func ( c * Fake Priority Classes ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Priority Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( priorityclasses Resource , name ) , & v1beta1 . Priority return obj . ( * v1beta1 . Priority } 
func ( b * Backend ) Process Events ( ev ... * auditinternal . Event ) bool { if b . On Request != nil { b . On } 
func Register ( plugins * admission . Plugins ) { plugins . Register ( Plugin Name , func ( config io . Reader ) ( admission . Interface , error ) { plugin Config , err := load return New Pod Tolerations Plugin ( plugin } 
func ( p * pod Tolerations Plugin ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { if should if ! p . Wait For Ready ( ) { return admission . New pod := a . Get var final if a . Get Operation ( ) == admission . Create { ts , err := p . get Namespace Default Tolerations ( a . Get // If the namespace has not specified its default tolerations, // fall back to cluster's default tolerations. if ts == nil { ts = p . plugin if len ( ts ) > 0 { if len ( pod . Spec . Tolerations ) > 0 { if tolerations . Is // modified pod tolerations = namespace tolerations + current pod tolerations final Tolerations = tolerations . Merge } else { final } else { final } else { final if qoshelper . Get Pod QOS ( pod ) != api . Pod QOS Best Effort { final Tolerations = tolerations . Merge Tolerations ( final Tolerations , [ ] api . Toleration { { Key : schedulerapi . Taint Node Memory Pressure , Operator : api . Toleration Op Exists , Effect : api . Taint Effect No // Final merge of tolerations irrespective of pod type, if the user while creating pods gives // conflicting tolerations(with same key+effect), the existing ones should be overwritten by latest one pod . Spec . Tolerations = tolerations . Merge Tolerations ( final } 
func ( p * pod Tolerations Plugin ) get Namespace ( ns Name string ) ( * corev1 . Namespace , error ) { namespace , err := p . namespace Lister . Get ( ns if errors . Is Not Found ( err ) { // in case of latency in our caches, make a call direct to storage to verify that it truly exists or not namespace , err = p . client . Core V1 ( ) . Namespaces ( ) . Get ( ns Name , metav1 . Get if err != nil { if errors . Is Not return nil , errors . New Internal } else if err != nil { return nil , errors . New Internal } 
func extract NS // if NSWL Tolerations or NS Default } 
func ( c * Networking V1Client ) REST return c . rest } 
func New From Interface ( subject Access Review authorizationclient . Subject Access Review Interface , authorized TTL , unauthorized TTL time . Duration ) ( * Webhook Authorizer , error ) { return new With Backoff ( subject Access Review , authorized TTL , unauthorized TTL , retry } 
func New ( kube Config File string , authorized TTL , unauthorized TTL time . Duration ) ( * Webhook Authorizer , error ) { subject Access Review , err := subject Access Review Interface From Kubeconfig ( kube Config return new With Backoff ( subject Access Review , authorized TTL , unauthorized TTL , retry } 
func new With Backoff ( subject Access Review authorizationclient . Subject Access Review Interface , authorized TTL , unauthorized TTL , initial Backoff time . Duration ) ( * Webhook Authorizer , error ) { return & Webhook Authorizer { subject Access Review : subject Access Review , response Cache : cache . New LRU Expire Cache ( 1024 ) , authorized TTL : authorized TTL , unauthorized TTL : unauthorized TTL , initial Backoff : initial Backoff , decision On Error : authorizer . Decision No } 
func ( w * Webhook Authorizer ) Authorize ( attr authorizer . Attributes ) ( decision authorizer . Decision , reason string , err error ) { r := & authorization . Subject Access if user := attr . Get User ( ) ; user != nil { r . Spec = authorization . Subject Access Review Spec { User : user . Get Name ( ) , UID : user . Get UID ( ) , Groups : user . Get Groups ( ) , Extra : convert To SAR Extra ( user . Get if attr . Is Resource Request ( ) { r . Spec . Resource Attributes = & authorization . Resource Attributes { Namespace : attr . Get Namespace ( ) , Verb : attr . Get Verb ( ) , Group : attr . Get API Group ( ) , Version : attr . Get API Version ( ) , Resource : attr . Get Resource ( ) , Subresource : attr . Get Subresource ( ) , Name : attr . Get } else { r . Spec . Non Resource Attributes = & authorization . Non Resource Attributes { Path : attr . Get Path ( ) , Verb : attr . Get if err != nil { return w . decision On if entry , ok := w . response Cache . Get ( string ( key ) ) ; ok { r . Status = entry . ( authorization . Subject Access Review } else { var ( result * authorization . Subject Access webhook . With Exponential Backoff ( w . initial Backoff , func ( ) error { result , err = w . subject Access return w . decision On if should Cache ( attr ) { if r . Status . Allowed { w . response Cache . Add ( string ( key ) , r . Status , w . authorized } else { w . response Cache . Add ( string ( key ) , r . Status , w . unauthorized switch { case r . Status . Denied && r . Status . Allowed : return authorizer . Decision case r . Status . Denied : return authorizer . Decision case r . Status . Allowed : return authorizer . Decision default : return authorizer . Decision No } 
func ( w * Webhook Authorizer ) Rules For ( user user . Info , namespace string ) ( [ ] authorizer . Resource Rule Info , [ ] authorizer . Non Resource Rule Info , bool , error ) { var ( resource Rules [ ] authorizer . Resource Rule non Resource Rules [ ] authorizer . Non Resource Rule return resource Rules , non Resource } 
func subject Access Review Interface From Kubeconfig ( kube Config File string ) ( authorizationclient . Subject Access Review Interface , error ) { local Scheme := runtime . New if err := scheme . Add To Scheme ( local if err := local Scheme . Set Version Priority ( group gw , err := webhook . New Generic Webhook ( local Scheme , scheme . Codecs , kube Config File , group return & subject Access Review } 
func should Cache ( attr authorizer . Attributes ) bool { controlled Attr Size := int64 ( len ( attr . Get Namespace ( ) ) ) + int64 ( len ( attr . Get Verb ( ) ) ) + int64 ( len ( attr . Get API Group ( ) ) ) + int64 ( len ( attr . Get API Version ( ) ) ) + int64 ( len ( attr . Get Resource ( ) ) ) + int64 ( len ( attr . Get Subresource ( ) ) ) + int64 ( len ( attr . Get Name ( ) ) ) + int64 ( len ( attr . Get return controlled Attr Size < max Controlled Attr Cache } 
func ( Default Fs ) Stat ( name string ) ( os . File } 
func ( Default return & default } 
func ( Default } 
func ( Default Fs ) Temp Dir ( dir , prefix string ) ( string , error ) { return ioutil . Temp } 
func ( Default Fs ) Temp File ( dir , prefix string ) ( File , error ) { file , err := ioutil . Temp return & default } 
func ( Default Fs ) Read Dir ( dirname string ) ( [ ] os . File Info , error ) { return ioutil . Read } 
func ( Default Fs ) Walk ( root string , walk Fn filepath . Walk Func ) error { return filepath . Walk ( root , walk } 
func New Certificate Signing Request Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Certificate Signing Request Informer ( client , resync } 
func New Filtered Certificate Signing Request Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Certificates V1beta1 ( ) . Certificate Signing } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Certificates V1beta1 ( ) . Certificate Signing } , } , & certificatesv1beta1 . Certificate Signing Request { } , resync } 
func Validate Flunder ( f * wardle . Flunder ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Flunder Spec ( & f . Spec , field . New return all } 
func Validate Flunder Spec ( s * wardle . Flunder Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( s . Flunder Reference ) != 0 && len ( s . Fischer Reference ) != 0 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , s . Fischer } else if len ( s . Flunder Reference ) != 0 && s . Reference Type != wardle . Flunder Reference Type { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , s . Flunder } else if len ( s . Fischer Reference ) != 0 && s . Reference Type != wardle . Fischer Reference Type { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , s . Fischer } else if len ( s . Fischer Reference ) == 0 && s . Reference Type == wardle . Fischer Reference Type { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , s . Fischer } else if len ( s . Flunder Reference ) == 0 && s . Reference Type == wardle . Flunder Reference Type { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , s . Flunder if len ( s . Reference Type ) != 0 && s . Reference Type != wardle . Fischer Reference Type && s . Reference Type != wardle . Flunder Reference Type { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , s . Reference return all } 
func ( s * daemon Set Lister ) Daemon Sets ( namespace string ) Daemon Set Namespace Lister { return daemon Set Namespace } 
func Get Join Worker Command ( kube Config File , token string , skip Token Print bool ) ( string , error ) { return get Join Command ( kube Config File , token , " " , false , skip Token } 
func Get Join Control Plane Command ( kube Config File , token , key string , skip Token Print , skip Certificate Key Print bool ) ( string , error ) { return get Join Command ( kube Config File , token , key , true , skip Token Print , skip Certificate Key } 
func ( c * Fake Validating Webhook Configurations ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Validating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( validatingwebhookconfigurations Resource , name ) , & v1beta1 . Validating Webhook return obj . ( * v1beta1 . Validating Webhook } 
func ( c * Fake Validating Webhook Configurations ) List ( opts v1 . List Options ) ( result * v1beta1 . Validating Webhook Configuration List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( validatingwebhookconfigurations Resource , validatingwebhookconfigurations Kind , opts ) , & v1beta1 . Validating Webhook Configuration label , _ , _ := testing . Extract From List list := & v1beta1 . Validating Webhook Configuration List { List Meta : obj . ( * v1beta1 . Validating Webhook Configuration List ) . List for _ , item := range obj . ( * v1beta1 . Validating Webhook Configuration } 
func ( c * Fake Validating Webhook Configurations ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( validatingwebhookconfigurations } 
func ( c * Fake Validating Webhook Configurations ) Create ( validating Webhook Configuration * v1beta1 . Validating Webhook Configuration ) ( result * v1beta1 . Validating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( validatingwebhookconfigurations Resource , validating Webhook Configuration ) , & v1beta1 . Validating Webhook return obj . ( * v1beta1 . Validating Webhook } 
func ( c * Fake Validating Webhook Configurations ) Update ( validating Webhook Configuration * v1beta1 . Validating Webhook Configuration ) ( result * v1beta1 . Validating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( validatingwebhookconfigurations Resource , validating Webhook Configuration ) , & v1beta1 . Validating Webhook return obj . ( * v1beta1 . Validating Webhook } 
func ( c * Fake Validating Webhook Configurations ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( validatingwebhookconfigurations Resource , name ) , & v1beta1 . Validating Webhook } 
func ( c * Fake Validating Webhook Configurations ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( validatingwebhookconfigurations Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Validating Webhook Configuration } 
func ( c * Fake Validating Webhook Configurations ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Validating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( validatingwebhookconfigurations Resource , name , pt , data , subresources ... ) , & v1beta1 . Validating Webhook return obj . ( * v1beta1 . Validating Webhook } 
func New Cmd Config Delete Cluster ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : " " , Example : delete Cluster Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check Err ( run Delete Cluster ( out , config } 
func ( v * version ) Network Policies ( ) Network Policy Informer { return & network Policy Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( handler * device Handler ) Find Multipath Device For Device ( device string ) string { io := handler . get disk , err := find Device For sys if dirs , err := io . Read Dir ( sys if strings . Has Prefix ( name , " " ) { if _ , err1 := io . Lstat ( sys } 
func find Device For Path ( path string , io Io Util ) ( string , error ) { device Path , err := io . Eval // if path /dev/hd X split into "", "dev", "hd X" then we will // return just the last part parts := strings . Split ( device if len ( parts ) == 3 && strings . Has return " " , errors . New ( " " + device } 
func ( handler * device Handler ) Find Slave Devices On io := handler . get if len ( parts ) != 3 || ! strings . Has slaves if files , err := io . Read Dir ( slaves } 
func ( handler * device Handler ) Get ISCSI Portal Host Map For Target ( target Iqn string ) ( map [ string ] int , error ) { portal Host io := handler . get // Iterate over all the i SCSI hosts in sysfs sys host Dirs , err := io . Read Dir ( sys if err != nil { if os . Is Not Exist ( err ) { return portal Host for _ , host Dir := range host Dirs { // i SCSI hosts are always of the format "host%d" // See drivers/scsi/hosts.c in Linux host Name := host if ! strings . Has Prefix ( host host Number , err := strconv . Atoi ( strings . Trim Prefix ( host if err != nil { klog . Errorf ( " " , host // Iterate over the children of the iscsi_host device // We are looking for the associated session device Path := sys Path + " " + host device Dirs , err := io . Read Dir ( device for _ , device Dir := range device Dirs { // Skip over files that aren't the session // Sessions are of the format "session%u" // See drivers/scsi/scsi_transport_iscsi.c in Linux session Name := device if ! strings . Has Prefix ( session session Path := device Path + " " + session // Read the target name for the i SCSI session target Name Path := session Path + " " + session target Name , err := io . Read File ( target Name if err != nil { klog . Infof ( " " , session // Ignore hosts that don't matchthe target we were looking for. if strings . Trim Space ( string ( target Name ) ) != target // Iterate over the children of the i SCSI session looking // for the i SCSI connection. dirs2 , err := io . Read Dir ( session if err != nil { klog . Infof ( " " , session for _ , dir2 := range dirs2 { // Skip over files that aren't the connection // Connections are of the format "connection%d:%u" // See drivers/scsi/scsi_transport_iscsi.c in Linux dir if ! strings . Has Prefix ( dir connection Path := session Path + " " + dir Name + " " + dir // Read the current and persistent portal information for the connection. addr Path := connection addr , err := io . Read File ( addr if err != nil { klog . Infof ( " " , dir port Path := connection port , err := io . Read File ( port if err != nil { klog . Infof ( " " , dir persistent Addr Path := connection persistent Addr , err := io . Read File ( persistent Addr if err != nil { klog . Infof ( " " , dir persistent Port Path := connection persistent Port , err := io . Read File ( persistent Port if err != nil { klog . Infof ( " " , dir // Add entries to the map for both the current and persistent portals // pointing to the SCSI host for those connections portal := strings . Trim Space ( string ( addr ) ) + " " + strings . Trim portal Host Map [ portal ] = host persistent Portal := strings . Trim Space ( string ( persistent Addr ) ) + " " + strings . Trim Space ( string ( persistent portal Host Map [ persistent Portal ] = host return portal Host } 
func ( handler * device Handler ) Find Devices For ISCSI Lun ( target io := handler . get // Iterate over all the i SCSI hosts in sysfs sys host Dirs , err := io . Read Dir ( sys for _ , host Dir := range host Dirs { // i SCSI hosts are always of the format "host%d" // See drivers/scsi/hosts.c in Linux host Name := host if ! strings . Has Prefix ( host host Number , err := strconv . Atoi ( strings . Trim Prefix ( host if err != nil { klog . Errorf ( " " , host // Iterate over the children of the iscsi_host device // We are looking for the associated session device Path := sys Path + " " + host device Dirs , err := io . Read Dir ( device for _ , device Dir := range device Dirs { // Skip over files that aren't the session // Sessions are of the format "session%u" // See drivers/scsi/scsi_transport_iscsi.c in Linux session Name := device if ! strings . Has Prefix ( session // Read the target name for the i SCSI session target Name Path := device Path + " " + session Name + " " + session target Name , err := io . Read File ( target Name // Only if the session matches the target we were looking for, // add it to the map if strings . Trim Space ( string ( target Name ) ) != target // The list of block devices on the scsi bus will be in a // directory called "target%d:%d:%d". // See drivers/scsi/scsi_scan.c in Linux // We assume the channel/bus and device/controller are always zero for i SCSI target Path := device Path + " " + session Name + fmt . Sprintf ( " " , host // The block device for a given lun will be "%d:%d:%d:%d" -- // host:channel:bus:LUN block Device Path := target Path + fmt . Sprintf ( " " , host // If the LUN doesn't exist on this bus, continue on _ , err = io . Lstat ( block Device // Read the block directory, there should only be one child -- // the block device "sd*" path := block Device dirs , err := io . Read } 
func Funcs ( codecs runtimeserializer . Codec Factory ) [ ] interface { } { return [ ] interface { } { // provide non-empty values for fields with defaults, so the defaulter doesn't change values during round-trip func ( obj * kubeletconfig . Kubelet Configuration , c fuzz . Continue ) { c . Fuzz No obj . Authentication . Webhook . Cache obj . Authorization . Mode = kubeletconfig . Kubelet Authorization Mode Always obj . Authorization . Webhook . Cache Authorized obj . Authorization . Webhook . Cache Unauthorized obj . Volume Stats Agg obj . Runtime Request obj . CPUCFS obj . Event obj . Event Record obj . Enable Controller Attach obj . Enable Debugging obj . File Check obj . Healthz Bind obj . Healthz obj . HTTP Check obj . Image Minimum GC obj . Image GC High Threshold obj . Image GC Low Threshold obj . Max Open obj . Max obj . Pod Pids obj . Node Status Update obj . Node Status Report obj . Node Lease Duration obj . CPU Manager obj . CPU Manager Reconcile Period = obj . Node Status Update obj . QOS obj . OOM Score Adj = int32 ( qos . Kubelet OOM Score obj . Port = ports . Kubelet obj . Read Only Port = ports . Kubelet Read Only obj . Registry obj . Registry Pull obj . Resolver Config = kubetypes . Resolv Conf obj . Serialize Image obj . Streaming Connection Idle obj . Sync obj . Content obj . Kube obj . Kube API obj . Hairpin Mode = v1beta1 . Promiscuous obj . Eviction Hard = kubeletconfigv1beta1 . Default Eviction obj . Eviction Pressure Transition obj . Make IP Tables Util obj . IP Tables Masquerade Bit = kubeletconfigv1beta1 . Default IP Tables Masquerade obj . IP Tables Drop Bit = kubeletconfigv1beta1 . Default IP Tables Drop obj . Cgroups Per obj . Cgroup obj . Enforce Node Allocatable = kubeletconfigv1beta1 . Default Node Allocatable obj . Static Pod URL obj . Container Log Max obj . Container Log Max obj . Config Map And Secret Change Detection } 
func New Cmd Config Set Context ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { options := & create Context Options { config Access : config cmd := & cobra . Command { Use : fmt . Sprintf ( " " , clientcmd . Flag Cluster Name , clientcmd . Flag Auth Info Name , clientcmd . Flag Namespace ) , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : create Context Long , Example : create Context Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmd . Flags ( ) . Bool Var ( & options . curr Context , " " , options . curr cmd . Flags ( ) . Var ( & options . cluster , clientcmd . Flag Cluster Name , clientcmd . Flag Cluster cmd . Flags ( ) . Var ( & options . auth Info , clientcmd . Flag Auth Info Name , clientcmd . Flag Auth Info cmd . Flags ( ) . Var ( & options . namespace , clientcmd . Flag Namespace , clientcmd . Flag } 
func ( svc Strategy ) Prepare For service . Status = api . Service } 
func ( svc Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new old new Service . Status = old } 
func ( svc Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error all Errs := validation . Validate all Errs = append ( all Errs , validation . Validate Conditional return all } 
func ( service Status Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new old // status changes are not allowed to update spec new Service . Spec = old } 
func ( service Status Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return validation . Validate Service Status } 
func ( lb * Load Balancer RR ) new Service Internal ( svc Port proxy . Service Port Name , affinity Type v1 . Service Affinity , ttl Seconds int ) * balancer State { if ttl Seconds == 0 { ttl Seconds = int ( v1 . Default Client IP Service Affinity if _ , exists := lb . services [ svc Port ] ; ! exists { lb . services [ svc Port ] = & balancer State { affinity : * new Affinity Policy ( affinity Type , ttl klog . V ( 4 ) . Infof ( " " , svc } else if affinity Type != " " { lb . services [ svc Port ] . affinity . affinity Type = affinity return lb . services [ svc } 
func is Session Affinity ( affinity * affinity Policy ) bool { // Should never be empty string, but checking for it to be safe. if affinity . affinity Type == " " || affinity . affinity Type == v1 . Service Affinity } 
func ( lb * Load Balancer RR ) Next Endpoint ( svc Port proxy . Service Port Name , src Addr net . Addr , session Affinity state , exists := lb . services [ svc if ! exists || state == nil { return " " , Err Missing Service if len ( state . endpoints ) == 0 { return " " , Err Missing klog . V ( 4 ) . Infof ( " " , svc Port , src session Affinity Enabled := is Session if session Affinity ipaddr , _ , err = net . Split Host Port ( src if err != nil { return " " , fmt . Errorf ( " " , src if ! session Affinity Reset { session Affinity , exists := state . affinity . affinity if exists && int ( time . Since ( session Affinity . last Used ) . Seconds ( ) ) < state . affinity . ttl Seconds { // Affinity wins. endpoint := session session Affinity . last klog . V ( 4 ) . Infof ( " " , svc Port , ipaddr , session if session Affinity Enabled { var affinity * affinity affinity = state . affinity . affinity if affinity == nil { affinity = new ( affinity State ) //&affinity state . affinity . affinity affinity . last affinity . client klog . V ( 4 ) . Infof ( " " , ipaddr , state . affinity . affinity } 
func remove Session Affinity By Endpoint ( state * balancer State , svc Port proxy . Service Port Name , endpoint string ) { for _ , affinity := range state . affinity . affinity Map { if affinity . endpoint == endpoint { klog . V ( 4 ) . Infof ( " " , affinity . endpoint , svc delete ( state . affinity . affinity Map , affinity . client } 
func ( lb * Load Balancer RR ) update Affinity Map ( svc Port proxy . Service Port Name , new Endpoints [ ] string ) { all for _ , new Endpoint := range new Endpoints { all Endpoints [ new state , exists := lb . services [ svc for _ , existing Endpoint := range state . endpoints { all Endpoints [ existing Endpoint ] = all Endpoints [ existing for m Key , m Val := range all Endpoints { if m Val == 1 { klog . V ( 2 ) . Infof ( " " , m Key , svc remove Session Affinity By Endpoint ( state , svc Port , m } 
func build Ports To Endpoints Map ( endpoints * v1 . Endpoints ) map [ string ] [ ] host Port Pair { ports To Endpoints := map [ string ] [ ] host Port ports To Endpoints [ port . Name ] = append ( ports To Endpoints [ port . Name ] , host Port return ports To } 
func slices if reflect . Deep Equal ( slice . Sort Strings ( lhs ) , slice . Sort } 
func http Stream Received ( streams chan httpstream . Stream ) func ( httpstream . Stream , <- chan struct { } ) error { return func ( stream httpstream . Stream , reply Sent <- chan struct { } ) error { // make sure it has a valid port header port String := stream . Headers ( ) . Get ( api . Port if len ( port String ) == 0 { return fmt . Errorf ( " " , api . Port port , err := strconv . Parse Uint ( port if err != nil { return fmt . Errorf ( " " , port if port < 1 { return fmt . Errorf ( " " , port // make sure it has a valid stream type header stream Type := stream . Headers ( ) . Get ( api . Stream if len ( stream Type ) == 0 { return fmt . Errorf ( " " , api . Stream if stream Type != api . Stream Type Error && stream Type != api . Stream Type Data { return fmt . Errorf ( " " , stream } 
func ( h * http Stream Handler ) get Stream Pair ( request ID string ) ( * http Stream Pair , bool ) { h . stream Pairs defer h . stream Pairs if p , ok := h . stream Pairs [ request ID ] ; ok { klog . V ( 5 ) . Infof ( " " , h . conn , request klog . V ( 5 ) . Infof ( " " , h . conn , request p := new Port Forward Pair ( request h . stream Pairs [ request } 
func ( h * http Stream Handler ) monitor Stream Pair ( p * http Stream Pair , timeout <- chan time . Time ) { select { case <- timeout : err := fmt . Errorf ( " " , h . conn , p . request utilruntime . Handle p . print case <- p . complete : klog . V ( 5 ) . Infof ( " " , h . conn , p . request h . remove Stream Pair ( p . request } 
func ( h * http Stream Handler ) has Stream Pair ( request ID string ) bool { h . stream Pairs Lock . R defer h . stream Pairs Lock . R _ , ok := h . stream Pairs [ request } 
func ( h * http Stream Handler ) request ID ( stream httpstream . Stream ) string { request ID := stream . Headers ( ) . Get ( api . Port Forward Request ID if len ( request ID ) == 0 { klog . V ( 5 ) . Infof ( " " , h . conn , api . Port Forward Request ID // If we get here, it's because the connection came from an older client // that isn't generating the request id header // (https://github.com/kubernetes/kubernetes/blob/843134885e7e0b360eb5441e85b1410a8b1a7a0c/pkg/client/unversioned/portforward/portforward.go#L258-L287) // // This is a best-effort attempt at supporting older clients. // // When there aren't concurrent new forwarded connections, each connection // will have a pair of streams (data, error), and the stream I Ds will be // consecutive odd numbers, e.g. 1 and 3 for the first connection. Convert // the stream ID into a pseudo-request id by taking the stream type and // using id = stream.Identifier() when the stream type is error, // and id = stream.Identifier() - 2 when it's data. // // NOTE: this only works when there are not concurrent new streams from // multiple forwarded connections; it's a best-effort attempt at supporting // old clients that don't generate request ids. If there are concurrent // new connections, it's possible that 1 connection gets streams whose I Ds // are not consecutive (e.g. 5 and 9 instead of 5 and 7). stream Type := stream . Headers ( ) . Get ( api . Stream switch stream Type { case api . Stream Type Error : request case api . Stream Type Data : request klog . V ( 5 ) . Infof ( " " , h . conn , request ID , stream return request } 
func ( h * http Stream Loop : for { select { case <- h . conn . Close case stream := <- h . stream Chan : request ID := h . request stream Type := stream . Headers ( ) . Get ( api . Stream klog . V ( 5 ) . Infof ( " " , h . conn , request ID , stream p , created := h . get Stream Pair ( request if created { go h . monitor Stream Pair ( p , time . After ( h . stream Creation if complete , err := p . add ( stream ) ; err != nil { msg := fmt . Sprintf ( " " , request utilruntime . Handle p . print } else if complete { go h . port } 
func ( h * http Stream Handler ) port Forward ( p * http Stream Pair ) { defer p . data defer p . error port String := p . data Stream . Headers ( ) . Get ( api . Port port , _ := strconv . Parse Int ( port klog . V ( 5 ) . Infof ( " " , h . conn , p . request ID , port err := h . forwarder . Port Forward ( h . pod , h . uid , int32 ( port ) , p . data klog . V ( 5 ) . Infof ( " " , h . conn , p . request ID , port utilruntime . Handle fmt . Fprint ( p . error } 
func ( p * http Stream Pair ) print Error ( s string ) { p . lock . R defer p . lock . R if p . error Stream != nil { fmt . Fprint ( p . error } 
func Kubelet Config Ok Eq ( a , b * apiv1 . Node } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * External Metric Value ) ( nil ) , ( * externalmetrics . External Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_External Metric Value_To_external_metrics_External Metric Value ( a . ( * External Metric Value ) , b . ( * externalmetrics . External Metric if err := s . Add Generated Conversion Func ( ( * externalmetrics . External Metric Value ) ( nil ) , ( * External Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_external_metrics_External Metric Value_To_v1beta1_External Metric Value ( a . ( * externalmetrics . External Metric Value ) , b . ( * External Metric if err := s . Add Generated Conversion Func ( ( * External Metric Value List ) ( nil ) , ( * externalmetrics . External Metric Value List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_External Metric Value List_To_external_metrics_External Metric Value List ( a . ( * External Metric Value List ) , b . ( * externalmetrics . External Metric Value if err := s . Add Generated Conversion Func ( ( * externalmetrics . External Metric Value List ) ( nil ) , ( * External Metric Value List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_external_metrics_External Metric Value List_To_v1beta1_External Metric Value List ( a . ( * externalmetrics . External Metric Value List ) , b . ( * External Metric Value } 
func Convert_v1beta1_External Metric Value_To_external_metrics_External Metric Value ( in * External Metric Value , out * externalmetrics . External Metric Value , s conversion . Scope ) error { return auto Convert_v1beta1_External Metric Value_To_external_metrics_External Metric } 
func Convert_external_metrics_External Metric Value_To_v1beta1_External Metric Value ( in * externalmetrics . External Metric Value , out * External Metric Value , s conversion . Scope ) error { return auto Convert_external_metrics_External Metric Value_To_v1beta1_External Metric } 
func Convert_v1beta1_External Metric Value List_To_external_metrics_External Metric Value List ( in * External Metric Value List , out * externalmetrics . External Metric Value List , s conversion . Scope ) error { return auto Convert_v1beta1_External Metric Value List_To_external_metrics_External Metric Value } 
func Convert_external_metrics_External Metric Value List_To_v1beta1_External Metric Value List ( in * externalmetrics . External Metric Value List , out * External Metric Value List , s conversion . Scope ) error { return auto Convert_external_metrics_External Metric Value List_To_v1beta1_External Metric Value } 
func Get Control Plane Endpoint ( control Plane Endpoint string , local Endpoint * kubeadmapi . API Endpoint ) ( string , error ) { // parse the bind port bind Port String := strconv . Itoa ( int ( local Endpoint . Bind if _ , err := Parse Port ( bind Port String ) ; err != nil { return " " , errors . Wrapf ( err , " " , local Endpoint . Bind // parse the Advertise Address var ip = net . Parse IP ( local Endpoint . Advertise if ip == nil { return " " , errors . Errorf ( " " , local Endpoint . Advertise // set the control-plane url using local Endpoint.Advertise Address + the local Endpoint.Bind Port control Plane URL := & url . URL { Scheme : " " , Host : net . Join Host Port ( ip . String ( ) , bind Port // if the controlplane endpoint is defined if len ( control Plane if host , port , err = Parse Host Port ( control Plane Endpoint ) ; err != nil { return " " , errors . Wrapf ( err , " " , control Plane // if a port is provided within the control Plane Address warn the users we are using it, else use the bindport if port != " " { if port != bind Port } else { port = bind Port // overrides the control-plane url using the control Plane Address (and eventually the bindport) control Plane URL = & url . URL { Scheme : " " , Host : net . Join Host return control Plane } 
func Parse Host // try to split host and port if host , port , err = net . Split Host Port ( hostport ) ; err != nil { // if Split Host // if port is defined, parse and validate it if port != " " { if _ , err := Parse // if host is a valid IP, returns it if ip := net . Parse // if host is a validate RFC-1123 subdomain, returns it if errs := validation . Is } 
func Parse Port ( port string ) ( int , error ) { port if err == nil && ( 1 <= port Int && port Int <= 65535 ) { return port } 
func New For Config Or cs . apiextensions V1beta1 = apiextensionsv1beta1 . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . apiextensions cs . Discovery Client = discovery . New Discovery } 
func ( in * Cloud Provider Configuration ) Deep Copy ( ) * Cloud Provider out := new ( Cloud Provider in . Deep Copy } 
func ( in * Deprecated Controller Configuration ) Deep Copy ( ) * Deprecated Controller out := new ( Deprecated Controller in . Deep Copy } 
func ( in * Generic Controller Manager Configuration ) Deep Copy ( ) * Generic Controller Manager out := new ( Generic Controller Manager in . Deep Copy } 
func ( in * Kube Cloud Shared Configuration ) Deep Copy Into ( out * Kube Cloud Shared out . Cloud Provider = in . Cloud out . Route Reconciliation Period = in . Route Reconciliation out . Node Monitor Period = in . Node Monitor out . Node Sync Period = in . Node Sync } 
func ( in * Kube Cloud Shared Configuration ) Deep Copy ( ) * Kube Cloud Shared out := new ( Kube Cloud Shared in . Deep Copy } 
func ( in * Kube Controller Manager Configuration ) Deep Copy ( ) * Kube Controller Manager out := new ( Kube Controller Manager in . Deep Copy } 
func ( in * Kube Controller Manager Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( r * streaming Runtime ) exec ( container ID string , cmd [ ] string , in io . Reader , out , errw io . Write Closer , tty bool , resize <- chan remotecommand . Terminal Size , timeout time . Duration ) error { container , err := check Container Status ( r . client , container return r . exec Handler . Exec In } 
func ( ds * docker Service ) Exec Sync ( _ context . Context , req * runtimeapi . Exec Sync Request ) ( * runtimeapi . Exec Sync var stdout Buffer , stderr err := ds . streaming Runtime . exec ( req . Container Id , req . Cmd , nil , // in ioutils . Write Closer Wrapper ( & stdout Buffer ) , ioutils . Write Closer Wrapper ( & stderr var exit if err != nil { exit Error , ok := err . ( utilexec . Exit exit Code = int32 ( exit Error . Exit return & runtimeapi . Exec Sync Response { Stdout : stdout Buffer . Bytes ( ) , Stderr : stderr Buffer . Bytes ( ) , Exit Code : exit } 
func ( ds * docker Service ) Exec ( _ context . Context , req * runtimeapi . Exec Request ) ( * runtimeapi . Exec Response , error ) { if ds . streaming Server == nil { return nil , streaming . New Error Streaming _ , err := check Container Status ( ds . client , req . Container return ds . streaming Server . Get } 
func ( ds * docker Service ) Attach ( _ context . Context , req * runtimeapi . Attach Request ) ( * runtimeapi . Attach Response , error ) { if ds . streaming Server == nil { return nil , streaming . New Error Streaming _ , err := check Container Status ( ds . client , req . Container return ds . streaming Server . Get } 
func ( ds * docker Service ) Port Forward ( _ context . Context , req * runtimeapi . Port Forward Request ) ( * runtimeapi . Port Forward Response , error ) { if ds . streaming Server == nil { return nil , streaming . New Error Streaming _ , err := check Container Status ( ds . client , req . Pod Sandbox // TODO(tallclair): Verify that ports are exposed. return ds . streaming Server . Get Port } 
func New Upload Config Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Aliases : [ ] string { " " } , Short : " " , Long : cmdutil . Macro Command Long Description , Phases : [ ] workflow . Phase { { Name : " " , Short : " " , Run All Siblings : true , Inherit Flags : get Upload Config Phase Flags ( ) , } , { Name : " " , Short : " " , Long : upload Kubeadm Config Long Desc , Example : upload Kubeadm Config Example , Run : run Upload Kubeadm Config , Inherit Flags : get Upload Config Phase Flags ( ) , } , { Name : " " , Short : " " , Long : upload Kubelet Config Long Desc , Example : upload Kubelet Config Example , Run : run Upload Kubelet Config , Inherit Flags : get Upload Config Phase } 
func run Upload Kubeadm Config ( c workflow . Run Data ) error { cfg , client , err := get Upload Config if err := uploadconfig . Upload } 
func run Upload Kubelet Config ( c workflow . Run Data ) error { cfg , client , err := get Upload Config if err = kubeletphase . Create Config Map ( cfg . Cluster Configuration . Component Configs . Kubelet , cfg . Kubernetes if err := patchnodephase . Annotate CRI Socket ( client , cfg . Node Registration . Name , cfg . Node Registration . CRI } 
func Interpret List Error ( err error , qualified Resource schema . Group Resource ) error { switch { case storage . Is Not Found ( err ) : return errors . New Not Found ( qualified case storage . Is Unreachable ( err ) : return errors . New Server Timeout ( qualified case storage . Is Internal Error ( err ) : return errors . New Internal } 
func Interpret Get Error ( err error , qualified Resource schema . Group Resource , name string ) error { switch { case storage . Is Not Found ( err ) : return errors . New Not Found ( qualified case storage . Is Unreachable ( err ) : return errors . New Server Timeout ( qualified case storage . Is Internal Error ( err ) : return errors . New Internal } 
func Interpret Create Error ( err error , qualified Resource schema . Group Resource , name string ) error { switch { case storage . Is Node Exist ( err ) : return errors . New Already Exists ( qualified case storage . Is Unreachable ( err ) : return errors . New Server Timeout ( qualified case storage . Is Internal Error ( err ) : return errors . New Internal } 
func Interpret Update Error ( err error , qualified Resource schema . Group Resource , name string ) error { switch { case storage . Is Conflict ( err ) , storage . Is Node Exist ( err ) , storage . Is Invalid Obj ( err ) : return errors . New Conflict ( qualified case storage . Is Unreachable ( err ) : return errors . New Server Timeout ( qualified case storage . Is Not Found ( err ) : return errors . New Not Found ( qualified case storage . Is Internal Error ( err ) : return errors . New Internal } 
func Interpret Watch Error ( err error , resource schema . Group Resource , name string ) error { switch { case storage . Is Invalid Error ( err ) : invalid Error , _ := err . ( storage . Invalid return errors . New Invalid ( schema . Group Kind { Group : resource . Group , Kind : resource . Resource } , name , invalid case storage . Is Internal Error ( err ) : return errors . New Internal } 
func ( pfs * Proc FS ) Get Full Container Name ( pid int ) ( string , error ) { file content , err := ioutil . Read File ( file if err != nil { if os . Is Not Exist ( err ) { return " " , os . Err Not return container Name From Proc } 
func P pids := get err for _ , pid := range pids { if err = syscall . Kill ( pid , sig ) ; err != nil { err List = append ( err return utilerrors . New Aggregate ( err } 
func Pid return get } 
func Default Service IP Range ( passed Service Cluster IP Range net . IP Net ) ( net . IP Net , net . IP , error ) { service Cluster IP Range := passed Service Cluster IP if passed Service Cluster IP Range . IP == nil { klog . Infof ( " " , kubeoptions . Default Service service Cluster IP Range = kubeoptions . Default Service if size := ipallocator . Range Size ( & service Cluster IP Range ) ; size < 8 { return net . IP // Select the first valid IP from Service Cluster IP Range to use as the Generic API Server service IP. api Server Service IP , err := ipallocator . Get Indexed IP ( & service Cluster IP if err != nil { return net . IP klog . V ( 4 ) . Infof ( " " , api Server Service return service Cluster IP Range , api Server Service } 
func Marshal To Yaml ( obj runtime . Object , gv schema . Group Version ) ( [ ] byte , error ) { return Marshal To Yaml For } 
func Marshal To Yaml For Codecs ( obj runtime . Object , gv schema . Group Version , codecs serializer . Codec Factory ) ( [ ] byte , error ) { const media Type = runtime . Content Type info , ok := runtime . Serializer Info For Media Type ( codecs . Supported Media Types ( ) , media if ! ok { return [ ] byte { } , errors . Errorf ( " " , media encoder := codecs . Encoder For } 
func Unmarshal From Yaml ( buffer [ ] byte , gv schema . Group Version ) ( runtime . Object , error ) { return Unmarshal From Yaml For } 
func Unmarshal From Yaml For Codecs ( buffer [ ] byte , gv schema . Group Version , codecs serializer . Codec Factory ) ( runtime . Object , error ) { const media Type = runtime . Content Type info , ok := runtime . Serializer Info For Media Type ( codecs . Supported Media Types ( ) , media if ! ok { return nil , errors . Errorf ( " " , media decoder := codecs . Decoder To } 
func Split YAML Documents ( yaml Bytes [ ] byte ) ( map [ schema . Group Version Kind ] [ ] byte , error ) { gvkmap := map [ schema . Group Version known buf := bytes . New Buffer ( yaml reader := utilyaml . New YAML Reader ( bufio . New for { type Meta Info := runtime . Type // Deserialize the Type Meta information of this byte slice if err := yaml . Unmarshal ( b , & type Meta // Require Type Meta information to be present if len ( type Meta Info . API Version ) == 0 || len ( type Meta // Check whether the kind has been registered before. If it has, throw an error if known := known Kinds [ type Meta Info . Kind ] ; known { errs = append ( errs , errors . Errorf ( " " , type Meta known Kinds [ type Meta // Build a Group Version Kind object from the deserialized Type Meta object gv , err := schema . Parse Group Version ( type Meta Info . API gvk := gv . With Kind ( type Meta if err := errorsutil . New } 
func Group Version Kinds From Bytes ( b [ ] byte ) ( [ ] schema . Group Version Kind , error ) { gvkmap , err := Split YAML gvks := [ ] schema . Group Version } 
func Group Version Kinds Has Kind ( gvks [ ] schema . Group Version } 
func ( s * priority Class Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Priority Class , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Priority } 
func ( s * priority Class Lister ) Get ( name string ) ( * v1beta1 . Priority Class , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1beta1 . Priority } 
func New Certs Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Short : " " , Phases : new Cert Sub Phases ( ) , Run : run Certs , Long : cmdutil . Macro Command Long } 
func new Cert Sub Phases ( ) [ ] workflow . Phase { sub // All subphase all Phase := workflow . Phase { Name : " " , Short : " " , Inherit Flags : get Cert Phase Flags ( " " ) , Run All sub Phases = append ( sub Phases , all cert Tree , _ := certsphase . Get Default Cert List ( ) . As Map ( ) . Cert for ca , cert List := range cert Tree { ca Phase := new Cert Sub Phase ( ca , run CA sub Phases = append ( sub Phases , ca for _ , cert := range cert List { cert Phase := new Cert Sub Phase ( cert , run Cert cert Phase . Local Flags = local sub Phases = append ( sub Phases , cert // SA creates the private/public key pair, which doesn't use x509 at all sa Phase := workflow . Phase { Name : " " , Short : " " , Long : sa Key Long Desc , Run : run Certs Sa , Inherit Flags : [ ] string { options . Certificates sub Phases = append ( sub Phases , sa return sub } 
func ( elem Group Kind Element ) Accept ( visitor Kind Visitor ) error { switch { case elem . Group Match ( " " , " " ) && elem . Kind == " " : visitor . Visit Daemon case elem . Group Match ( " " , " " ) && elem . Kind == " " : visitor . Visit case elem . Group Match ( " " ) && elem . Kind == " " : visitor . Visit case elem . Group Match ( " " , " " ) && elem . Kind == " " : visitor . Visit case elem . Group Match ( " " , " " ) && elem . Kind == " " : visitor . Visit Replica case elem . Group Match ( " " , " " ) && elem . Kind == " " : visitor . Visit Replication case elem . Group Match ( " " ) && elem . Kind == " " : visitor . Visit Stateful case elem . Group Match ( " " ) && elem . Kind == " " : visitor . Visit Cron } 
func ( elem Group Kind Element ) Group } 
func With Failed Authentication Audit ( failed Handler http . Handler , sink audit . Sink , policy policy . Checker ) http . Handler { if sink == nil || policy == nil { return failed return http . Handler Func ( func ( w http . Response Writer , req * http . Request ) { req , ev , omit Stages , err := create Audit Event And Attach To if err != nil { utilruntime . Handle responsewriters . Internal if ev == nil { failed Handler . Serve ev . Response ev . Response Status . Message = get Auth ev . Stage = auditinternal . Stage Response rw := decorate Response Writer ( w , ev , sink , omit failed Handler . Serve } 
func ( c * Fake Flunders ) List ( opts v1 . List Options ) ( result * v1alpha1 . Flunder List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( flunders Resource , flunders Kind , c . ns , opts ) , & v1alpha1 . Flunder label , _ , _ := testing . Extract From List list := & v1alpha1 . Flunder List { List Meta : obj . ( * v1alpha1 . Flunder List ) . List for _ , item := range obj . ( * v1alpha1 . Flunder } 
func ( c * Fake Flunders ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( flunders } 
func ( c * Fake Flunders ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( flunders } 
func ( s * Cloud Provider Options ) Apply To ( cfg * kubectrlmgrconfig . Cloud Provider cfg . Cloud Config File = s . Cloud Config } 
func Read Dockercfg File ( search Paths [ ] string ) ( cfg Docker Config , err error ) { if len ( search Paths ) == 0 { search Paths = Default Dockercfg for _ , config Path := range search Paths { abs Docker Config File Location , err := filepath . Abs ( filepath . Join ( config Path , config File if err != nil { klog . Errorf ( " " , config klog . V ( 4 ) . Infof ( " " , abs Docker Config File contents , err := ioutil . Read File ( abs Docker Config File if os . Is Not if err != nil { klog . V ( 4 ) . Infof ( " " , abs Docker Config File cfg , err := read Docker Config File From if err == nil { klog . V ( 4 ) . Infof ( " " , abs Docker Config File return nil , fmt . Errorf ( " " , search } 
func Read Docker Config JSON File ( search Paths [ ] string ) ( cfg Docker Config , err error ) { if len ( search Paths ) == 0 { search Paths = Default Docker Config JSON for _ , config Path := range search Paths { abs Docker Config File Location , err := filepath . Abs ( filepath . Join ( config Path , config Json File if err != nil { klog . Errorf ( " " , config klog . V ( 4 ) . Infof ( " " , config Json File Name , abs Docker Config File cfg , err = Read Specific Docker Config Json File ( abs Docker Config File if err != nil { if ! os . Is Not Exist ( err ) { klog . V ( 4 ) . Infof ( " " , abs Docker Config File klog . V ( 4 ) . Infof ( " " , config Json File Name , abs Docker Config File return nil , fmt . Errorf ( " " , config Json File Name , search } 
func Read Specific Docker Config Json File ( file Path string ) ( cfg Docker if contents , err = ioutil . Read File ( file return read Docker Config Json File From } 
func ( he * Http Error ) Error ( ) string { return fmt . Sprintf ( " " , he . Status } 
func decode Docker Config Field Auth ( field string ) ( username , password string , err error ) { decoded , err := base64 . Std Encoding . Decode parts := strings . Split } 
func New Cmd Config ( f cmdutil . Factory , path Options * clientcmd . Path Options , streams genericclioptions . IO Streams ) * cobra . Command { if len ( path Options . Explicit File Flag ) == 0 { path Options . Explicit File Flag = clientcmd . Recommended Config Path cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : templates . Long 1. If the --` + path Options . Explicit File 2. If $` + path Options . Env 3. Otherwise, ` + path . Join ( " " , path Options . Global File Subpath ) + ` is used and no merging takes place.` ) , Run : cmdutil . Default Sub Command Run ( streams . Err // file paths are common to all sub commands cmd . Persistent Flags ( ) . String Var ( & path Options . Loading Rules . Explicit Path , path Options . Explicit File Flag , path Options . Loading Rules . Explicit // TODO(juanvallejo): update all subcommands to work with genericclioptions.IO Streams cmd . Add Command ( New Cmd Config View ( f , streams , path cmd . Add Command ( New Cmd Config Set Cluster ( streams . Out , path cmd . Add Command ( New Cmd Config Set Auth Info ( streams . Out , path cmd . Add Command ( New Cmd Config Set Context ( streams . Out , path cmd . Add Command ( New Cmd Config Set ( streams . Out , path cmd . Add Command ( New Cmd Config Unset ( streams . Out , path cmd . Add Command ( New Cmd Config Current Context ( streams . Out , path cmd . Add Command ( New Cmd Config Use Context ( streams . Out , path cmd . Add Command ( New Cmd Config Get Contexts ( streams , path cmd . Add Command ( New Cmd Config Get Clusters ( streams . Out , path cmd . Add Command ( New Cmd Config Delete Cluster ( streams . Out , path cmd . Add Command ( New Cmd Config Delete Context ( streams . Out , streams . Err Out , path cmd . Add Command ( New Cmd Config Rename Context ( streams . Out , path } 
func get Service Protocols ( spec corev1 . Service for _ , service Port := range spec . Ports { // Empty protocol must be defaulted (TCP) if len ( service Port . Protocol ) == 0 { service Port . Protocol = corev1 . Protocol result [ strconv . Itoa ( int ( service Port . Port ) ) ] = string ( service } 
func Create Handlers ( root Path string , provider Provider , summary Provider Summary Provider ) * restful . Web Service { h := & handler { provider , summary ws := & restful . Web ws . Path ( root handler restful . Route } { { " " , h . handle Stats } , { " " , h . handle Summary } , { " " , h . handle System Container } , { " " , h . handle Pod Container } , { " " , h . handle Pod } 
func ( h * handler ) handle Stats ( request * restful . Request , response * restful . Response ) { query , err := parse Stats if err != nil { handle // Root container stats. stats Map , err := h . provider . Get Raw Container Info ( " " , query . cadvisor if err != nil { handle write Response ( response , stats } 
func ( h * handler ) handle Summary ( request * restful . Request , response * restful . Response ) { only CPU And request . Request . Parse if only Clu And Memory Param , found := request . Request . Form [ " " ] ; found && len ( only Clu And Memory Param ) == 1 && only Clu And Memory Param [ 0 ] == " " { only CPU And if only CPU And Memory { summary , err = h . summary Provider . Get CPU And Memory } else { // external calls to the summary API use cached stats force Stats summary , err = h . summary Provider . Get ( force Stats if err != nil { handle } else { write } 
func ( h * handler ) handle System Container ( request * restful . Request , response * restful . Response ) { query , err := parse Stats if err != nil { handle // Non-Kubernetes container stats. container Name := path . Join ( " " , query . Container stats , err := h . provider . Get Raw Container Info ( container Name , query . cadvisor if err != nil { if _ , ok := stats [ container } else { handle write } 
func ( h * handler ) handle Pod Container ( request * restful . Request , response * restful . Response ) { query , err := parse Stats if err != nil { handle // Default parameters. params := map [ string ] string { " " : metav1 . Namespace for k , v := range request . Path if params [ " " ] == " " || params [ " " ] == " " { response . Write Error String ( http . Status Bad pod , ok := h . provider . Get Pod By response . Write Error ( http . Status Not Found , kubecontainer . Err Container Not stats , err := h . provider . Get Container Info ( kubecontainer . Get Pod Full Name ( pod ) , types . UID ( params [ " " ] ) , params [ " " ] , query . cadvisor if err != nil { handle write } 
func handle Error ( response * restful . Response , request string , err error ) { switch err { case kubecontainer . Err Container Not Found : response . Write Error ( http . Status Not response . Write Error String ( http . Status Internal Server } 
func ( c * Fake Network Policies ) Get ( name string , options v1 . Get Options ) ( result * networkingv1 . Network Policy , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( networkpolicies Resource , c . ns , name ) , & networkingv1 . Network return obj . ( * networkingv1 . Network } 
func ( c * Fake Network Policies ) List ( opts v1 . List Options ) ( result * networkingv1 . Network Policy List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( networkpolicies Resource , networkpolicies Kind , c . ns , opts ) , & networkingv1 . Network Policy label , _ , _ := testing . Extract From List list := & networkingv1 . Network Policy List { List Meta : obj . ( * networkingv1 . Network Policy List ) . List for _ , item := range obj . ( * networkingv1 . Network Policy } 
func ( c * Fake Network Policies ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( networkpolicies } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1alpha1 . Pod Preset { } , func ( obj interface { } ) { Set Object Defaults_Pod Preset ( obj . ( * v1alpha1 . Pod scheme . Add Type Defaulting Func ( & v1alpha1 . Pod Preset List { } , func ( obj interface { } ) { Set Object Defaults_Pod Preset List ( obj . ( * v1alpha1 . Pod Preset } 
func ( r * Runtime Sort ) Original Position ( ix int ) int { if ix < 0 || ix > len ( r . orig return r . orig } 
func ( c * Clientset ) Example V1 ( ) examplev1 . Example V1Interface { return & fakeexamplev1 . Fake Example } 
func ( c * Clientset ) Second Example V1 ( ) secondexamplev1 . Second Example V1Interface { return & fakesecondexamplev1 . Fake Second Example } 
func Validate Stateful Set Name ( name string , prefix bool ) [ ] string { // TODO: Validate that there's name for the suffix inserted by the pods. // Currently this is just "-index". In the future we may allow a user // specified list of suffixes and we need to validate the longest one. return apimachineryvalidation . Name Is DNS } 
func Validate Pod Template Spec For Stateful Set ( template * api . Pod Template Spec , selector labels . Selector , fld Path * field . Path ) field . Error List { all Errs := field . Error if template == nil { all Errs = append ( all Errs , field . Required ( fld } else { if ! selector . Empty ( ) { // Verify that the Stateful if ! selector . Matches ( labels ) { all Errs = append ( all Errs , field . Invalid ( fld // TODO: Add validation for Pod Spec, currently this will check volumes, which we know will // fail. We should really check that the union of the given volumes and volume Claims match // volume mounts in the containers. // all Errs = append(all Errs, apivalidation.Validate Pod Template Spec(template, fld Path)...) all Errs = append ( all Errs , unversionedvalidation . Validate Labels ( template . Labels , fld all Errs = append ( all Errs , apivalidation . Validate Annotations ( template . Annotations , fld all Errs = append ( all Errs , apivalidation . Validate Pod Specific Annotations ( template . Annotations , & template . Spec , fld return all } 
func Validate Stateful Set Spec ( spec * apps . Stateful Set Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error switch spec . Pod Management Policy { case " " : all Errs = append ( all Errs , field . Required ( fld case apps . Ordered Ready Pod Management , apps . Parallel Pod Management : default : all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , spec . Pod Management Policy , fmt . Sprintf ( " " , apps . Ordered Ready Pod Management , apps . Parallel Pod switch spec . Update Strategy . Type { case " " : all Errs = append ( all Errs , field . Required ( fld case apps . On Delete Stateful Set Strategy Type : if spec . Update Strategy . Rolling Update != nil { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) . Child ( " " ) , spec . Update Strategy . Rolling Update , fmt . Sprintf ( " " , apps . Rolling Update Stateful Set Strategy case apps . Rolling Update Stateful Set Strategy Type : if spec . Update Strategy . Rolling Update != nil { all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Update Strategy . Rolling Update . Partition ) , fld default : all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , spec . Update Strategy , fmt . Sprintf ( " " , apps . Rolling Update Stateful Set Strategy Type , apps . On Delete Stateful Set Strategy all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Replicas ) , fld if spec . Selector == nil { all Errs = append ( all Errs , field . Required ( fld } else { all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( spec . Selector , fld if len ( spec . Selector . Match Labels ) + len ( spec . Selector . Match Expressions ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld selector , err := metav1 . Label Selector As if err != nil { all Errs = append ( all Errs , field . Invalid ( fld } else { all Errs = append ( all Errs , Validate Pod Template Spec For Stateful Set ( & spec . Template , selector , fld if spec . Template . Spec . Restart Policy != api . Restart Policy Always { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " , " " , " " ) , spec . Template . Spec . Restart Policy , [ ] string { string ( api . Restart Policy if spec . Template . Spec . Active Deadline Seconds != nil { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func Validate Stateful Set ( stateful Set * apps . Stateful Set ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & stateful Set . Object Meta , true , Validate Stateful Set Name , field . New all Errs = append ( all Errs , Validate Stateful Set Spec ( & stateful Set . Spec , field . New return all } 
func Validate Stateful Set Update ( stateful Set , old Stateful Set * apps . Stateful Set ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & stateful Set . Object Meta , & old Stateful Set . Object Meta , field . New restore Replicas := stateful stateful Set . Spec . Replicas = old Stateful restore Template := stateful stateful Set . Spec . Template = old Stateful restore Strategy := stateful Set . Spec . Update stateful Set . Spec . Update Strategy = old Stateful Set . Spec . Update if ! apiequality . Semantic . Deep Equal ( stateful Set . Spec , old Stateful Set . Spec ) { all Errs = append ( all Errs , field . Forbidden ( field . New stateful Set . Spec . Replicas = restore stateful Set . Spec . Template = restore stateful Set . Spec . Update Strategy = restore all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( stateful Set . Spec . Replicas ) , field . New return all } 
func Validate Stateful Set Status ( status * apps . Stateful Set Status , field Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Replicas ) , field all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Ready Replicas ) , field all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Current Replicas ) , field all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Updated Replicas ) , field if status . Observed Generation != nil { all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * status . Observed Generation ) , field if status . Collision Count != nil { all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * status . Collision Count ) , field if status . Ready Replicas > status . Replicas { all Errs = append ( all Errs , field . Invalid ( field Path . Child ( " " ) , status . Ready if status . Current Replicas > status . Replicas { all Errs = append ( all Errs , field . Invalid ( field Path . Child ( " " ) , status . Current if status . Updated Replicas > status . Replicas { all Errs = append ( all Errs , field . Invalid ( field Path . Child ( " " ) , status . Updated return all } 
func Validate Stateful Set Status Update ( stateful Set , old Stateful Set * apps . Stateful Set ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Stateful Set Status ( & stateful Set . Status , field . New all Errs = append ( all Errs , apivalidation . Validate Object Meta Update ( & stateful Set . Object Meta , & old Stateful Set . Object Meta , field . New // TODO: Validate status. if apivalidation . Is Decremented ( stateful Set . Status . Collision Count , old Stateful Set . Status . Collision if stateful Set . Status . Collision Count != nil { value = * stateful Set . Status . Collision all Errs = append ( all Errs , field . Invalid ( field . New return all } 
func Validate Controller Revision ( revision * apps . Controller Revision ) field . Error List { errs := field . Error errs = append ( errs , apivalidation . Validate Object Meta ( & revision . Object Meta , true , Validate Controller Revision Name , field . New if revision . Data == nil { errs = append ( errs , field . Required ( field . New errs = append ( errs , apivalidation . Validate Nonnegative Field ( revision . Revision , field . New } 
func Validate Controller Revision Update ( new History , old History * apps . Controller Revision ) field . Error List { errs := field . Error errs = append ( errs , apivalidation . Validate Object Meta Update ( & new History . Object Meta , & old History . Object Meta , field . New errs = append ( errs , Validate Controller Revision ( new errs = append ( errs , apivalidation . Validate Immutable Field ( new History . Data , old History . Data , field . New } 
func Validate Daemon Set ( ds * apps . Daemon Set ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & ds . Object Meta , true , Validate Daemon Set Name , field . New all Errs = append ( all Errs , Validate Daemon Set Spec ( & ds . Spec , field . New return all } 
func Validate Daemon Set Update ( ds , old DS * apps . Daemon Set ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & ds . Object Meta , & old DS . Object Meta , field . New all Errs = append ( all Errs , Validate Daemon Set Spec Update ( & ds . Spec , & old DS . Spec , field . New all Errs = append ( all Errs , Validate Daemon Set Spec ( & ds . Spec , field . New return all } 
func validate Daemon Set Status ( status * apps . Daemon Set Status , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Current Number Scheduled ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Number Misscheduled ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Desired Number Scheduled ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Number Ready ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( status . Observed Generation , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Updated Number Scheduled ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Number Available ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Number Unavailable ) , fld if status . Collision Count != nil { all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * status . Collision Count ) , fld return all } 
func Validate Daemon Set Status Update ( ds , old DS * apps . Daemon Set ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & ds . Object Meta , & old DS . Object Meta , field . New all Errs = append ( all Errs , validate Daemon Set Status ( & ds . Status , field . New if apivalidation . Is Decremented ( ds . Status . Collision Count , old DS . Status . Collision if ds . Status . Collision Count != nil { value = * ds . Status . Collision all Errs = append ( all Errs , field . Invalid ( field . New return all } 
func Validate Daemon Set Spec ( spec * apps . Daemon Set Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( spec . Selector , fld selector , err := metav1 . Label Selector As if err == nil && ! selector . Matches ( labels . Set ( spec . Template . Labels ) ) { all Errs = append ( all Errs , field . Invalid ( fld if spec . Selector != nil && len ( spec . Selector . Match Labels ) + len ( spec . Selector . Match Expressions ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld all Errs = append ( all Errs , apivalidation . Validate Pod Template Spec ( & spec . Template , fld // Daemons typically run on more than one node, so mark Read-Write persistent disks as invalid. all Errs = append ( all Errs , apivalidation . Validate Read Only Persistent Disks ( spec . Template . Spec . Volumes , fld // Restart Policy has already been first-order validated as per Validate Pod Template Spec(). if spec . Template . Spec . Restart Policy != api . Restart Policy Always { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " , " " , " " ) , spec . Template . Spec . Restart Policy , [ ] string { string ( api . Restart Policy if spec . Template . Spec . Active Deadline Seconds != nil { all Errs = append ( all Errs , field . Forbidden ( fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Min Ready Seconds ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Template Generation ) , fld all Errs = append ( all Errs , Validate Daemon Set Update Strategy ( & spec . Update Strategy , fld if spec . Revision History Limit != nil { // zero is a valid Revision History Limit all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * spec . Revision History Limit ) , fld return all } 
func Validate Deployment Spec ( spec * apps . Deployment Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Replicas ) , fld if spec . Selector == nil { all Errs = append ( all Errs , field . Required ( fld } else { all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( spec . Selector , fld if len ( spec . Selector . Match Labels ) + len ( spec . Selector . Match Expressions ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld selector , err := metav1 . Label Selector As if err != nil { all Errs = append ( all Errs , field . Invalid ( fld } else { all Errs = append ( all Errs , Validate Pod Template Spec For Replica Set ( & spec . Template , selector , spec . Replicas , fld all Errs = append ( all Errs , Validate Deployment Strategy ( & spec . Strategy , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Min Ready Seconds ) , fld if spec . Revision History Limit != nil { // zero is a valid Revision History Limit all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * spec . Revision History Limit ) , fld if spec . Rollback To != nil { all Errs = append ( all Errs , Validate Rollback ( spec . Rollback To , fld if spec . Progress Deadline Seconds != nil { all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * spec . Progress Deadline Seconds ) , fld if * spec . Progress Deadline Seconds <= spec . Min Ready Seconds { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , spec . Progress Deadline return all } 
func Validate Deployment Status ( status * apps . Deployment Status , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( status . Observed Generation , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Replicas ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Updated Replicas ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Ready Replicas ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Available Replicas ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( status . Unavailable Replicas ) , fld if status . Collision Count != nil { all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( * status . Collision Count ) , fld if status . Updated Replicas > status . Replicas { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , status . Updated if status . Ready Replicas > status . Replicas { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , status . Ready if status . Available Replicas > status . Replicas { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , status . Available if status . Available Replicas > status . Ready Replicas { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , status . Available return all } 
func Validate Replica Set ( rs * apps . Replica Set ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & rs . Object Meta , true , Validate Replica Set Name , field . New all Errs = append ( all Errs , Validate Replica Set Spec ( & rs . Spec , field . New return all } 
func Validate Replica Set Update ( rs , old Rs * apps . Replica Set ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Object Meta Update ( & rs . Object Meta , & old Rs . Object Meta , field . New all Errs = append ( all Errs , Validate Replica Set Spec ( & rs . Spec , field . New return all } 
func Validate Replica Set Status Update ( rs , old Rs * apps . Replica Set ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Object Meta Update ( & rs . Object Meta , & old Rs . Object Meta , field . New all Errs = append ( all Errs , Validate Replica Set Status ( rs . Status , field . New return all } 
func Validate Replica Set Spec ( spec * apps . Replica Set Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Replicas ) , fld all Errs = append ( all Errs , apivalidation . Validate Nonnegative Field ( int64 ( spec . Min Ready Seconds ) , fld if spec . Selector == nil { all Errs = append ( all Errs , field . Required ( fld } else { all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( spec . Selector , fld if len ( spec . Selector . Match Labels ) + len ( spec . Selector . Match Expressions ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld selector , err := metav1 . Label Selector As if err != nil { all Errs = append ( all Errs , field . Invalid ( fld } else { all Errs = append ( all Errs , Validate Pod Template Spec For Replica Set ( & spec . Template , selector , spec . Replicas , fld return all } 
func Validate Pod Template Spec For Replica Set ( template * api . Pod Template Spec , selector labels . Selector , replicas int32 , fld Path * field . Path ) field . Error List { all Errs := field . Error if template == nil { all Errs = append ( all Errs , field . Required ( fld } else { if ! selector . Empty ( ) { // Verify that the Replica if ! selector . Matches ( labels ) { all Errs = append ( all Errs , field . Invalid ( fld all Errs = append ( all Errs , apivalidation . Validate Pod Template Spec ( template , fld if replicas > 1 { all Errs = append ( all Errs , apivalidation . Validate Read Only Persistent Disks ( template . Spec . Volumes , fld // Restart Policy has already been first-order validated as per Validate Pod Template Spec(). if template . Spec . Restart Policy != api . Restart Policy Always { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " , " " ) , template . Spec . Restart Policy , [ ] string { string ( api . Restart Policy if template . Spec . Active Deadline Seconds != nil { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func Get Load Balancer Annotation Type ( service * v1 . Service ) ( Load Balancer Type , bool ) { v := Load Balancer if service . Spec . Type != v1 . Service Type Load l , ok := service . Annotations [ Service Annotation Load Balancer v = Load Balancer switch v { case LB Type Internal , deprecated Type Internal Lower Case : return LB Type } 
func Get Load Balancer Annotation Backend Share ( service * v1 . Service ) bool { if l , exists := service . Annotations [ Service Annotation ILB Backend // Check for deprecated annotation key if l , exists := service . Annotations [ deprecated Service Annotation ILB Backend Share ] ; exists && l == " " { klog . Warningf ( " " , deprecated Service Annotation ILB Backend Share , Service Annotation ILB Backend } 
func Get Service Network Tier ( service * v1 . Service ) ( cloud . Network Tier , error ) { l , ok := service . Annotations [ Network Tier Annotation if ! ok { return cloud . Network Tier v := cloud . Network switch v { case cloud . Network Tier case cloud . Network Tier default : return cloud . Network Tier } 
func ignore Status Not Found From v , ok := err . ( autorest . Detailed if ok && v . Status Code == http . Status Not } 
func ignore Status Forbidden From v , ok := err . ( autorest . Detailed if ok && v . Status Code == http . Status } 
func ( az * Cloud ) get Virtual Machine ( node Name types . Node Name ) ( vm compute . Virtual Machine , err error ) { vm Name := string ( node cached VM , err := az . vm Cache . Get ( vm if cached VM == nil { return vm , cloudprovider . Instance Not return * ( cached VM . ( * compute . Virtual } 
func ( az * Cloud ) Is Node Unmanaged ( node Name string ) ( bool , error ) { unmanaged Nodes , err := az . Get Unmanaged return unmanaged Nodes . Has ( node } 
func ( az * Cloud ) Is Node Unmanaged By Provider ID ( provider ID string ) bool { return ! azure Node Provider IDRE . Match ( [ ] byte ( provider } 
func convert Resource Group Name To Lower ( resource ID string ) ( string , error ) { matches := azure Resource Group Name RE . Find String Submatch ( resource if len ( matches ) != 2 { return " " , fmt . Errorf ( " " , resource ID , azure Resource Group Name resource return strings . Replace ( resource ID , resource Group , strings . To Lower ( resource } 
func is Backend Pool On Same LB ( new Backend Pool ID string , existing Backend Pools [ ] string ) ( bool , string , error ) { matches := backend Pool IDRE . Find String Submatch ( new Backend Pool if len ( matches ) != 2 { return false , " " , fmt . Errorf ( " " , new Backend Pool new LB new LB Name Trimmed := strings . Trim Right ( new LB Name , Internal Load Balancer Name for _ , backend Pool := range existing Backend Pools { matches := backend Pool IDRE . Find String Submatch ( backend if len ( matches ) != 2 { return false , " " , fmt . Errorf ( " " , backend lb if ! strings . Equal Fold ( strings . Trim Right ( lb Name , Internal Load Balancer Name Suffix ) , new LB Name Trimmed ) { return false , lb } 
func ( in * HPA Controller Configuration ) Deep Copy Into ( out * HPA Controller out . Horizontal Pod Autoscaler Sync Period = in . Horizontal Pod Autoscaler Sync out . Horizontal Pod Autoscaler Upscale Forbidden Window = in . Horizontal Pod Autoscaler Upscale Forbidden out . Horizontal Pod Autoscaler Downscale Forbidden Window = in . Horizontal Pod Autoscaler Downscale Forbidden out . Horizontal Pod Autoscaler Downscale Stabilization Window = in . Horizontal Pod Autoscaler Downscale Stabilization out . Horizontal Pod Autoscaler CPU Initialization Period = in . Horizontal Pod Autoscaler CPU Initialization out . Horizontal Pod Autoscaler Initial Readiness Delay = in . Horizontal Pod Autoscaler Initial Readiness } 
func ( in * HPA Controller Configuration ) Deep Copy ( ) * HPA Controller out := new ( HPA Controller in . Deep Copy } 
func ( c * Fake Pod Disruption Budgets ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Pod Disruption Budget , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( poddisruptionbudgets Resource , c . ns , name ) , & v1beta1 . Pod Disruption return obj . ( * v1beta1 . Pod Disruption } 
func ( c * Fake Pod Disruption Budgets ) List ( opts v1 . List Options ) ( result * v1beta1 . Pod Disruption Budget List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( poddisruptionbudgets Resource , poddisruptionbudgets Kind , c . ns , opts ) , & v1beta1 . Pod Disruption Budget label , _ , _ := testing . Extract From List list := & v1beta1 . Pod Disruption Budget List { List Meta : obj . ( * v1beta1 . Pod Disruption Budget List ) . List for _ , item := range obj . ( * v1beta1 . Pod Disruption Budget } 
func ( c * Fake Pod Disruption Budgets ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( poddisruptionbudgets } 
func ( c * Fake Pod Disruption Budgets ) Create ( pod Disruption Budget * v1beta1 . Pod Disruption Budget ) ( result * v1beta1 . Pod Disruption Budget , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( poddisruptionbudgets Resource , c . ns , pod Disruption Budget ) , & v1beta1 . Pod Disruption return obj . ( * v1beta1 . Pod Disruption } 
func ( c * Fake Pod Disruption Budgets ) Update ( pod Disruption Budget * v1beta1 . Pod Disruption Budget ) ( result * v1beta1 . Pod Disruption Budget , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( poddisruptionbudgets Resource , c . ns , pod Disruption Budget ) , & v1beta1 . Pod Disruption return obj . ( * v1beta1 . Pod Disruption } 
func ( c * Fake Pod Disruption Budgets ) Update Status ( pod Disruption Budget * v1beta1 . Pod Disruption Budget ) ( * v1beta1 . Pod Disruption Budget , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( poddisruptionbudgets Resource , " " , c . ns , pod Disruption Budget ) , & v1beta1 . Pod Disruption return obj . ( * v1beta1 . Pod Disruption } 
func ( c * Fake Pod Disruption Budgets ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( poddisruptionbudgets Resource , c . ns , name ) , & v1beta1 . Pod Disruption } 
func ( c * Fake Pod Disruption Budgets ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( poddisruptionbudgets Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Pod Disruption Budget } 
func ( c * Fake Pod Disruption Budgets ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Pod Disruption Budget , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( poddisruptionbudgets Resource , c . ns , name , pt , data , subresources ... ) , & v1beta1 . Pod Disruption return obj . ( * v1beta1 . Pod Disruption } 
func ( e * Exists ) Validate ( a admission . Attributes , o admission . Object Interfaces ) error { // if we're here, then we've already passed authentication, so we're allowed to do what we're trying to do // if we're here, then the API server has found a route, which means that if we have a non-empty namespace // its a namespaced resource. if len ( a . Get Namespace ( ) ) == 0 || a . Get Kind ( ) . Group // we need to wait for our caches to warm if ! e . Wait For Ready ( ) { return admission . New _ , err := e . namespace Lister . Get ( a . Get if ! errors . Is Not Found ( err ) { return errors . New Internal // in case of latency in our caches, make a call direct to storage to verify that it truly exists or not _ , err = e . client . Core V1 ( ) . Namespaces ( ) . Get ( a . Get Namespace ( ) , metav1 . Get if err != nil { if errors . Is Not return errors . New Internal } 
func New Exists ( ) * Exists { return & Exists { Handler : admission . New } 
func ( e * Exists ) Set External Kube Informer Factory ( f informers . Shared Informer Factory ) { namespace e . namespace Lister = namespace e . Set Ready Func ( namespace Informer . Informer ( ) . Has } 
func ( e * Exists ) Validate Initialization ( ) error { if e . namespace } 
func ( c * Fake Daemon Sets ) Get ( name string , options v1 . Get Options ) ( result * v1beta2 . Daemon Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( daemonsets Resource , c . ns , name ) , & v1beta2 . Daemon return obj . ( * v1beta2 . Daemon } 
func ( c * Fake Daemon Sets ) List ( opts v1 . List Options ) ( result * v1beta2 . Daemon Set List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( daemonsets Resource , daemonsets Kind , c . ns , opts ) , & v1beta2 . Daemon Set label , _ , _ := testing . Extract From List list := & v1beta2 . Daemon Set List { List Meta : obj . ( * v1beta2 . Daemon Set List ) . List for _ , item := range obj . ( * v1beta2 . Daemon Set } 
func ( c * Fake Daemon Sets ) Create ( daemon Set * v1beta2 . Daemon Set ) ( result * v1beta2 . Daemon Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( daemonsets Resource , c . ns , daemon Set ) , & v1beta2 . Daemon return obj . ( * v1beta2 . Daemon } 
func ( c * Fake Daemon Sets ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta2 . Daemon Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( daemonsets Resource , c . ns , name , pt , data , subresources ... ) , & v1beta2 . Daemon return obj . ( * v1beta2 . Daemon } 
func ( g * gen Proto IDL ) Filter ( c * generator . Context , t * types . Type ) bool { tag Vals := types . Extract Comment Tags ( " " , t . Comment if tag Vals != nil { if tag if tag klog . Fatalf ( `Comment tag "protobuf" must be true or false, found: %q` , tag if ! g . generate ok := is } 
func is Optional if extract Bool Tag Or Die ( " " , t . Comment } 
func ( g * gen Proto IDL ) Generate Type ( c * generator . Context , t * types . Type , w io . Writer ) error { sw := generator . New Snippet b := body Gen { locator : & protobuf Locator { namer : c . Namers [ " " ] . ( Protobuf From Go Namer ) , tracker : g . imports , universe : c . Universe , local Go Package : g . local Go Package . Package , } , local Package : g . local Package , omit Gogo : g . omit Gogo , omit Field Types : g . omit Field switch t . Kind { case types . Alias : return b . do case types . Struct : return b . do } 
func ( p protobuf Locator ) Cast Type Name ( name types . Name ) string { if name . Package == p . local Go } 
func ( p protobuf Locator ) Proto Type For ( t * types . Type ) ( * types . Type , error ) { switch { // we've already converted the type, or it's a map case t . Kind == types . Protobuf || t . Kind == types . Map : p . tracker . Add // it's a fundamental type if t , ok := is Fundamental Proto Type ( t ) ; ok { p . tracker . Add // it's a message if t . Kind == types . Struct || is Optional Alias ( t ) { t := & types . Type { Name : p . namer . Go Name To Proto Name ( t . Name ) , Kind : types . Protobuf , Comment Lines : t . Comment p . tracker . Add return nil , err Unrecognized } 
func protobuf Tag To Field ( tag string , field * proto Field , m types . Member , t * types . Type , local proto field . Tag = proto if last := strings . Last } else { name = types . Name { Name : parts [ 0 ] , Package : local Package . Package , Path : local proto for i , extra := range parts [ 3 : ] { parts := strings . Split switch parts [ 0 ] { case " " : proto proto field . Extras = proto if name , ok := proto delete ( proto } 
func ( v * version ) Runtime Classes ( ) Runtime Class Informer { return & runtime Class Informer { factory : v . factory , tweak List Options : v . tweak List } 
func Run Global Checks ( global Checks [ ] Global for _ , check := range global } 
func Run Cmd Checks ( cmd * cobra . Command , cmd Checks [ ] Cmd Check , skip Cmd [ ] string ) [ ] error { cmd Path := cmd . Command for _ , skip Cmd Path := range skip Cmd { if cmd Path == skip Cmd Path { fmt . Fprintf ( os . Stdout , " \n " , cmd if cmd . Has Sub Commands ( ) { for _ , sub Cmd := range cmd . Commands ( ) { errors = append ( errors , Run Cmd Checks ( sub Cmd , cmd Checks , skip fmt . Fprintf ( os . Stdout , " \n " , cmd for _ , check := range cmd } 
func Check Long cmd Path := cmd . Command if len ( long ) > 0 { if strings . Trim ( long , " \t \n " ) != long { return [ ] error { fmt . Errorf ( `command %q: long description is not normalized, make sure you are calling templates.Long Desc (from pkg/cmd/templates) before assigning cmd.Long` , cmd } 
func Check Flags ( cmd * cobra . Command ) [ ] error { all Flags cmd . Flags ( ) . Visit All ( func ( f * pflag . Flag ) { all Flags Slice = append ( all Flags cmd . Persistent Flags ( ) . Visit All ( func ( f * pflag . Flag ) { all Flags Slice = append ( all Flags fmt . Fprintf ( os . Stdout , " ", l n(a l l Flags if err != nil { errors = append ( errors , fmt . Errorf ( " " , cmd . Command for _ , flag := range all Flags if ! regex . Match String ( name ) { errors = append ( errors , fmt . Errorf ( " " , cmd . Command } 
func Check Global Var pflag . Command Line . Visit } 
func ( plugin * vsphere Volume Plugin ) Init ( host volume . Volume } 
func ( b * vsphere Volume Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func ( b * vsphere Volume Mounter ) Set Up At ( dir string , fs Group * int64 ) error { klog . V ( 5 ) . Infof ( " " , b . vol // TODO: handle failed mounts here. notmnt , err := b . mounter . Is Likely Not Mount if err != nil && ! os . Is Not if err := os . Mkdir // Perform a bind mount to the full path to allow duplicate mounts of the same PD. global PD Path := make Global PD Path ( b . plugin . host , b . vol mount Options := util . Join Mount Options ( options , b . mount err = b . mounter . Mount ( global PD Path , dir , " " , mount if err != nil { notmnt , mnt Err := b . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! notmnt { if mnt Err = b . mounter . Unmount ( dir ) ; mnt Err != nil { klog . Errorf ( " " , mnt notmnt , mnt Err := b . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! notmnt { klog . Errorf ( " " , b . Get volume . Set Volume Ownership ( b , fs klog . V ( 3 ) . Infof ( " " , b . vol } 
func ( v * vsphere Volume Unmounter ) Tear Down At ( dir string ) error { return mount . Cleanup Mount } 
func Get Datacenter ( ctx context . Context , connection * V Sphere Connection , datacenter Path string ) ( * Datacenter , error ) { finder := find . New datacenter , err := finder . Datacenter ( ctx , datacenter if err != nil { klog . Errorf ( " " , datacenter } 
func Get All Datacenter ( ctx context . Context , connection * V Sphere finder := find . New datacenters , err := finder . Datacenter } 
func ( dc * Datacenter ) Get VM By UUID ( ctx context . Context , vm UUID string ) ( * Virtual Machine , error ) { s := object . New Search vm UUID = strings . To Lower ( strings . Trim Space ( vm svm , err := s . Find By Uuid ( ctx , dc . Datacenter , vm if err != nil { klog . Errorf ( " " , vm if svm == nil { klog . Errorf ( " " , vm return nil , Err No VM virtual Machine := Virtual Machine { object . New Virtual return & virtual } 
func ( dc * Datacenter ) Get Host By VMUUID ( ctx context . Context , vm UUID string ) ( * types . Managed Object Reference , error ) { virtual Machine , err := dc . Get VM By UUID ( ctx , vm var vm Mo mo . Virtual pc := property . Default Collector ( virtual err = pc . Retrieve One ( ctx , virtual Machine . Reference ( ) , [ ] string { " " } , & vm host := vm klog . Infof ( " " , virtual } 
func ( dc * Datacenter ) Get VM By Path ( ctx context . Context , vm Path string ) ( * Virtual Machine , error ) { finder := get vm , err := finder . Virtual Machine ( ctx , vm if err != nil { klog . Errorf ( " " , vm virtual Machine := Virtual return & virtual } 
func ( dc * Datacenter ) Get All Datastores ( ctx context . Context ) ( map [ string ] * Datastore Info , error ) { finder := get datastores , err := finder . Datastore var ds List [ ] types . Managed Object for _ , ds := range datastores { ds List = append ( ds var ds Mo pc := property . Default properties := [ ] string { Datastore Info err = pc . Retrieve ( ctx , ds List , properties , & ds Mo if err != nil { klog . Errorf ( " " + " " , ds ds URL Info Map := make ( map [ string ] * Datastore for _ , ds Mo := range ds Mo List { ds URL Info Map [ ds Mo . Info . Get Datastore Info ( ) . Url ] = & Datastore Info { & Datastore { object . New Datastore ( dc . Client ( ) , ds Mo . Reference ( ) ) , dc } , ds Mo . Info . Get Datastore klog . V ( 9 ) . Infof ( " " , ds URL Info return ds URL Info } 
func ( dc * Datacenter ) Get All Hosts ( ctx context . Context ) ( [ ] types . Managed Object Reference , error ) { finder := get host Systems , err := finder . Host System var host Mors [ ] types . Managed Object for _ , hs := range host Systems { host Mors = append ( host return host } 
func ( dc * Datacenter ) Get Datastore By Path ( ctx context . Context , vm Disk Path string ) ( * Datastore , error ) { datastore Path Obj := new ( object . Datastore is Success := datastore Path Obj . From String ( vm Disk if ! is Success { klog . Errorf ( " " , vm Disk return dc . Get Datastore By Name ( ctx , datastore Path } 
func ( dc * Datacenter ) Get Datastore By Name ( ctx context . Context , name string ) ( * Datastore , error ) { finder := get } 
func ( dc * Datacenter ) Get Resource Pool ( ctx context . Context , resource Pool Path string ) ( * object . Resource Pool , error ) { finder := get var resource Pool * object . Resource resource Pool , err = finder . Resource Pool Or Default ( ctx , resource Pool if err != nil { klog . Errorf ( " " , resource Pool return resource } 
func ( dc * Datacenter ) Get Folder By Path ( ctx context . Context , folder Path string ) ( * Folder , error ) { finder := get vm Folder , err := finder . Folder ( ctx , folder if err != nil { klog . Errorf ( " " , folder folder := Folder { vm } 
func ( dc * Datacenter ) Get VM Mo List ( ctx context . Context , vm Obj List [ ] * Virtual Machine , properties [ ] string ) ( [ ] mo . Virtual Machine , error ) { var vm Mo List [ ] mo . Virtual var vm Refs [ ] types . Managed Object if len ( vm Obj for _ , vm Obj := range vm Obj List { vm Refs = append ( vm Refs , vm pc := property . Default err := pc . Retrieve ( ctx , vm Refs , properties , & vm Mo if err != nil { klog . Errorf ( " " , vm Obj return vm Mo } 
func ( dc * Datacenter ) Get Virtual Disk Page83Data ( ctx context . Context , disk Path string ) ( string , error ) { if len ( disk Path ) > 0 && filepath . Ext ( disk Path ) != " " { disk vdm := object . New Virtual Disk // Returns uuid of vmdk virtual disk disk UUID , err := vdm . Query Virtual Disk Uuid ( ctx , disk if err != nil { klog . Warningf ( " " , disk disk UUID = format Virtual Disk UUID ( disk return disk } 
func ( dc * Datacenter ) Get Datastore Mo List ( ctx context . Context , ds Obj List [ ] * Datastore , properties [ ] string ) ( [ ] mo . Datastore , error ) { var ds Mo var ds Refs [ ] types . Managed Object if len ( ds Obj for _ , ds Obj := range ds Obj List { ds Refs = append ( ds Refs , ds pc := property . Default err := pc . Retrieve ( ctx , ds Refs , properties , & ds Mo if err != nil { klog . Errorf ( " " , ds Obj return ds Mo } 
func ( dc * Datacenter ) Check Disks Attached ( ctx context . Context , node var vm List [ ] * Virtual for node Name , vol Paths := range node Volumes { for _ , vol Path := range vol Paths { set Node Volume Map ( attached , vol Path , node vm , err := dc . Get VM By Path ( ctx , node if err != nil { if Is Not Found ( err ) { klog . Warningf ( " " , node Name , vol vm List = append ( vm if len ( vm vm Mo List , err := dc . Get VM Mo List ( ctx , vm if err != nil { // When there is an error fetching instance information // it is safer to return nil and let volume information not be touched. klog . Errorf ( " " , vm for _ , vm Mo := range vm Mo List { if vm Mo . Config == nil { klog . Errorf ( " " , vm for node Name , vol Paths := range node Volumes { if node Name == vm Mo . Name { verify Volume Paths For VM ( vm Mo , vol } 
func verify Volume Paths For VM ( vm Mo mo . Virtual Machine , vol Paths [ ] string , node Volume Map map [ string ] map [ string ] bool ) { // Verify if the volume paths are present on the VM backing virtual disk devices for _ , vol Path := range vol Paths { vm Devices := object . Virtual Device List ( vm for _ , device := range vm Devices { if vm Devices . Type Name ( device ) == " " { virtual Device := device . Get Virtual if backing , ok := virtual Device . Backing . ( * types . Virtual Disk Flat Ver2Backing Info ) ; ok { if backing . File Name == vol Path { set Node Volume Map ( node Volume Map , vol Path , vm } 
func ( s * runtime Class Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Runtime Class , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1alpha1 . Runtime } 
func ( s * runtime Class Lister ) Get ( name string ) ( * v1alpha1 . Runtime Class , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1alpha1 . Runtime } 
func New Audit Sink Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Audit Sink Informer ( client , resync } 
func New Filtered Audit Sink Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Auditregistration V1alpha1 ( ) . Audit } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Auditregistration V1alpha1 ( ) . Audit } , } , & auditregistrationv1alpha1 . Audit Sink { } , resync } 
func monitor Resize Events ( fd uintptr , resize Events chan <- remotecommand . Terminal Size , stop chan struct { } ) { go func ( ) { defer runtime . Handle for { select { case <- winch : size := Get // try to send size select { case resize } 
func New Fake AWS Services ( cluster ID string ) * Fake AWS Services { s := & Fake AWS s . ec2 = & Fake s . elb = & Fake s . elbv2 = & Fake s . asg = & Fake s . metadata = & Fake s . kms = & Fake s . network Interfaces s . network Interfaces Vpc I self self Instance . Instance self Instance . Placement = & ec2 . Placement { Availability self Instance . Private Dns self Instance . Private Ip self Instance . Public Ip s . self Instance = self s . instances = [ ] * ec2 . Instance { self tag . Key = aws . String ( Tag Name Kubernetes Cluster tag . Value = aws . String ( cluster self } 
func ( s * Fake AWS Services ) With Az ( az string ) * Fake AWS Services { if s . self Instance . Placement == nil { s . self s . self Instance . Placement . Availability } 
func ( s * Fake AWS } 
func ( s * Fake AWS Services ) Load } 
func ( s * Fake AWS Services ) Load Balancing } 
func ( s * Fake AWS } 
func ( s * Fake AWS Services ) Key } 
func ( ec2i * Fake EC2Impl ) Describe Instances ( request * ec2 . Describe Instances for _ , instance := range ec2i . aws . instances { if request . Instance Ids != nil { if instance . Instance for _ , instance ID := range request . Instance Ids { if * instance ID == * instance . Instance if request . Filters != nil { all for _ , filter := range request . Filters { if ! instance Matches Filter ( instance , filter ) { all if ! all } 
func ( ec2i * Fake EC2Impl ) Attach Volume ( request * ec2 . Attach Volume Input ) ( resp * ec2 . Volume } 
func ( ec2i * Fake EC2Impl ) Detach Volume ( request * ec2 . Detach Volume Input ) ( resp * ec2 . Volume } 
func ( ec2i * Fake EC2Impl ) Describe Volumes ( request * ec2 . Describe Volumes } 
func ( ec2i * Fake EC2Impl ) Create Volume ( request * ec2 . Create Volume } 
func ( ec2i * Fake EC2Impl ) Delete Volume ( request * ec2 . Delete Volume Input ) ( resp * ec2 . Delete Volume } 
func ( ec2i * Fake EC2Impl ) Describe Security Groups ( request * ec2 . Describe Security Groups Input ) ( [ ] * ec2 . Security } 
func ( ec2i * Fake EC2Impl ) Create Subnet ( request * ec2 . Subnet ) ( * ec2 . Create Subnet response := & ec2 . Create Subnet } 
func ( ec2i * Fake EC2Impl ) Describe Subnets ( request * ec2 . Describe Subnets Input ) ( [ ] * ec2 . Subnet , error ) { ec2i . Describe Subnets } 
func ( ec2i * Fake EC2Impl ) Describe Route Tables ( request * ec2 . Describe Route Tables Input ) ( [ ] * ec2 . Route Table , error ) { ec2i . Describe Route Tables return ec2i . Route } 
func ( ec2i * Fake EC2Impl ) Create Route Table ( request * ec2 . Route Table ) ( * ec2 . Create Route Table Output , error ) { ec2i . Route Tables = append ( ec2i . Route response := & ec2 . Create Route Table Output { Route } 
func ( ec2i * Fake EC2Impl ) Create Route ( request * ec2 . Create Route Input ) ( * ec2 . Create Route } 
func ( ec2i * Fake EC2Impl ) Delete Route ( request * ec2 . Delete Route Input ) ( * ec2 . Delete Route } 
func ( ec2i * Fake EC2Impl ) Modify Instance Attribute ( request * ec2 . Modify Instance Attribute Input ) ( * ec2 . Modify Instance Attribute } 
func ( ec2i * Fake EC2Impl ) Describe Vpcs ( request * ec2 . Describe Vpcs Input ) ( * ec2 . Describe Vpcs Output , error ) { return & ec2 . Describe Vpcs Output { Vpcs : [ ] * ec2 . Vpc { { Cidr } 
func ( m * Fake Metadata ) Get Metadata ( key string ) ( string , error ) { network Interfaces i := m . aws . self if i . Placement != nil { az = aws . String Value ( i . Placement . Availability } else if key == " " { return aws . String Value ( i . Instance } else if key == " " { return aws . String Value ( i . Private Dns } else if key == " " { return aws . String Value ( i . Public Dns } else if key == " " { return aws . String Value ( i . Private Ip } else if key == " " { return aws . String Value ( i . Public Ip } else if strings . Has Prefix ( key , network Interfaces Prefix ) { if key == network Interfaces Prefix { return strings . Join ( m . aws . network Interfaces key mac Param := key if len ( key Split ) == 5 && key Split [ 4 ] == " " { for i , mac Elem := range m . aws . network Interfaces Macs { if mac Param == mac Elem { return m . aws . network Interfaces Vpc I if len ( key Split ) == 5 && key Split [ 4 ] == " " { for i , mac Elem := range m . aws . network Interfaces Macs { if mac Param == mac Elem { return strings . Join ( m . aws . network Interfaces Private I } 
func ( elb * Fake ELB ) Delete Load Balancer ( input * elb . Delete Load Balancer Input ) ( * elb . Delete Load Balancer } 
func ( elb * Fake ELB ) Describe Load Balancers ( input * elb . Describe Load Balancers Input ) ( * elb . Describe Load Balancers } 
func ( elb * Fake ELB ) Add Tags ( input * elb . Add Tags Input ) ( * elb . Add Tags } 
func ( elb * Fake ELB ) Set Load Balancer Policies Of Listener ( input * elb . Set Load Balancer Policies Of Listener Input ) ( * elb . Set Load Balancer Policies Of Listener } 
func ( elb * Fake ELB ) Describe Load Balancer Policies ( input * elb . Describe Load Balancer Policies Input ) ( * elb . Describe Load Balancer Policies } 
func ( elb * Fake ELBV2 ) Add Tags ( input * elbv2 . Add Tags Input ) ( * elbv2 . Add Tags } 
func ( elb * Fake ELBV2 ) Describe Target Health ( input * elbv2 . Describe Target Health Input ) ( * elbv2 . Describe Target Health } 
func Fs Info ( path string ) ( int64 , int64 , int64 , int64 , int64 , int64 , error ) { var free Bytes Available , total Number Of Bytes , total Number Of Free ret , _ , err := syscall . Syscall6 ( proc Get Disk Free Space Ex . Addr ( ) , 4 , uintptr ( unsafe . Pointer ( syscall . String To UTF16Ptr ( path ) ) ) , uintptr ( unsafe . Pointer ( & free Bytes Available ) ) , uintptr ( unsafe . Pointer ( & total Number Of Bytes ) ) , uintptr ( unsafe . Pointer ( & total Number Of Free return free Bytes Available , total Number Of Bytes , total Number Of Bytes - free Bytes } 
func Disk Usage ( path string ) ( * resource . Quantity , error ) { _ , _ , usage , _ , _ , _ , err := Fs used , err := resource . Parse used . Format = resource . Binary } 
func history Viewer ( rest Client Getter genericclioptions . REST Client Getter , mapping * meta . REST Mapping ) ( kubectl . History Viewer , error ) { client Config , err := rest Client Getter . To REST external , err := kubernetes . New For Config ( client return kubectl . History Viewer For ( mapping . Group Version Kind . Group } 
func Encode Kubelet Config ( internal * kubeletconfig . Kubelet Configuration , target Version schema . Group Version ) ( [ ] byte , error ) { encoder , err := New Kubeletconfig YAML Encoder ( target } 
func New Kubeletconfig YAML Encoder ( target Version schema . Group Version ) ( runtime . Encoder , error ) { _ , codecs , err := scheme . New Scheme And media info , ok := runtime . Serializer Info For Media Type ( codecs . Supported Media Types ( ) , media if ! ok { return nil , fmt . Errorf ( " " , media return codecs . Encoder For Version ( info . Serializer , target } 
func New YAML Encoder ( group Name string ) ( runtime . Encoder , error ) { // encode to YAML media info , ok := runtime . Serializer Info For Media Type ( legacyscheme . Codecs . Supported Media Types ( ) , media if ! ok { return nil , fmt . Errorf ( " " , media versions := legacyscheme . Scheme . Prioritized Versions For Group ( group if len ( versions ) == 0 { return nil , fmt . Errorf ( " " , group // the "best" version supposedly comes first in the list returned from legacyscheme.Registry.Enabled Versions For Group return legacyscheme . Codecs . Encoder For } 
func Decode Kubelet Configuration ( kubelet Codecs * serializer . Codec Factory , data [ ] byte ) ( * kubeletconfig . Kubelet Configuration , error ) { // the Universal Decoder runs defaulting and returns the internal type by default obj , gvk , err := kubelet Codecs . Universal internal KC , ok := obj . ( * kubeletconfig . Kubelet return internal } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Custom Resource Definition { } , & Custom Resource Definition } 
func Name Is DNS Subdomain ( name string , prefix bool ) [ ] string { if prefix { name = mask Trailing return validation . Is } 
func Name Is DNS Label ( name string , prefix bool ) [ ] string { if prefix { name = mask Trailing return validation . Is } 
func Name Is DNS1035Label ( name string , prefix bool ) [ ] string { if prefix { name = mask Trailing return validation . Is } 
func mask Trailing Dash ( name string ) string { if strings . Has } 
func Validate Nonnegative Field ( value int64 , fld Path * field . Path ) field . Error List { all Errs := field . Error if value < 0 { all Errs = append ( all Errs , field . Invalid ( fld Path , value , Is Negative Error return all } 
func New Cmd Reset ( in io . Reader , out io . Writer ) * cobra . Command { var certs var cri Socket var ignore Preflight var force kube Config File := kubeadmconstants . Get Admin Kube Config cmd := & cobra . Command { Use : " " , Short : " " , Run : func ( cmd * cobra . Command , args [ ] string ) { ignore Preflight Errors Set , err := validation . Validate Ignore Preflight Errors ( ignore Preflight kubeadmutil . Check var cfg * kubeadmapi . Init client , err = get Clientset ( kube Config if err == nil { klog . V ( 1 ) . Infof ( " " , kube Config cfg , err = configutil . Fetch Init Configuration From } else { klog . V ( 1 ) . Infof ( " " , kube Config if cri Socket Path == " " { cri Socket Path , err = reset Detect CRI kubeadmutil . Check klog . V ( 1 ) . Infof ( " " , cri Socket r , err := New Reset ( in , ignore Preflight Errors Set , force Reset , certs Dir , cri Socket kubeadmutil . Check kubeadmutil . Check options . Add Ignore Preflight Errors Flag ( cmd . Persistent Flags ( ) , & ignore Preflight options . Add Kube Config Flag ( cmd . Persistent Flags ( ) , & kube Config cmd . Persistent Flags ( ) . String Var ( & certs Dir , " " , kubeadmapiv1beta2 . Default Certificates cmdutil . Add CRI Socket Flag ( cmd . Persistent Flags ( ) , & cri Socket cmd . Persistent Flags ( ) . Bool Var P ( & force } 
func New Reset ( in io . Reader , ignore Preflight Errors sets . String , force Reset bool , certs Dir , cri Socket Path string ) ( * Reset , error ) { if ! force s := bufio . New if strings . To if err := preflight . Run Root Check Only ( ignore Preflight return & Reset { certs Dir : certs Dir , cri Socket Path : cri Socket } 
func ( r * Reset ) Run ( out io . Writer , client clientset . Interface , cfg * kubeadmapi . Init Configuration ) error { var dirs To // Reset the Cluster Status for a given control-plane node. if is Control Plane ( ) && cfg != nil { uploadconfig . Reset Cluster Status For Node ( cfg . Node etcd Manifest Path := filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Manifests Sub Dir etcd Data Dir , err := get Etcd Data Dir ( etcd Manifest if err == nil { dirs To Clean = append ( dirs To Clean , etcd Data if cfg != nil { if err := etcdphase . Remove Stacked Etcd Member From init System , err := initsystem . Get Init if err := init System . Service // Try to unmount mounted directories under kubeadmconstants.Kubelet Run Directory in order to be able to remove the kubeadmconstants.Kubelet Run Directory directory later fmt . Printf ( " \n " , kubeadmconstants . Kubelet Run umount Dirs Cmd := fmt . Sprintf ( " " , kubeadmconstants . Kubelet Run klog . V ( 1 ) . Infof ( " " , umount Dirs umount Output Bytes , err := exec . Command ( " " , " " , umount Dirs if err != nil { klog . Errorf ( " \n " , kubeadmconstants . Kubelet Run Directory , string ( umount Output if err := remove Containers ( utilsexec . New ( ) , r . cri Socket dirs To Clean = append ( dirs To Clean , [ ] string { kubeadmconstants . Kubelet Run // Then clean contents from the stateful kubelet, etcd and cni directories fmt . Printf ( " \n " , dirs To for _ , dir := range dirs To clean if r . certs Dir != kubeadmapiv1beta2 . Default Certificates Dir { klog . Warningf ( " \n " , r . certs reset Config Dir ( kubeadmconstants . Kubernetes Dir , r . certs } 
func clean Dir ( file Path string ) error { // If the directory doesn't even exist there's nothing to do, and we do // not consider this an error if _ , err := os . Stat ( file Path ) ; os . Is Not d , err := os . Open ( file for _ , name := range names { if err = os . Remove All ( filepath . Join ( file } 
func reset Config Dir ( config Path Dir , pki Path Dir string ) { dirs To Clean := [ ] string { filepath . Join ( config Path Dir , kubeadmconstants . Manifests Sub Dir Name ) , pki Path fmt . Printf ( " \n " , dirs To for _ , dir := range dirs To Clean { if err := clean files To Clean := [ ] string { filepath . Join ( config Path Dir , kubeadmconstants . Admin Kube Config File Name ) , filepath . Join ( config Path Dir , kubeadmconstants . Kubelet Kube Config File Name ) , filepath . Join ( config Path Dir , kubeadmconstants . Kubelet Bootstrap Kube Config File Name ) , filepath . Join ( config Path Dir , kubeadmconstants . Controller Manager Kube Config File Name ) , filepath . Join ( config Path Dir , kubeadmconstants . Scheduler Kube Config File fmt . Printf ( " \n " , files To for _ , path := range files To Clean { if err := os . Remove } 
func is Control Plane ( ) bool { filepath := kubeadmconstants . Get Static Pod Filepath ( kubeadmconstants . Kube API Server , kubeadmconstants . Get Static Pod if _ , err := os . Stat ( filepath ) ; os . Is Not } 
func New Controller ( dynamic Config Dir string , transform Transform Func ) * Controller { return & Controller { transform : transform , // channels must have capacity at least 1, since we signal with non-blocking writes pending Config Source : make ( chan bool , 1 ) , config Status : status . New Node Config Status ( ) , checkpoint Store : store . New Fs Store ( utilfs . Default Fs { } , filepath . Join ( dynamic Config Dir , store } 
func ( cc * Controller ) Bootstrap ( ) ( * kubeletconfig . Kubelet // ensure the filesystem is initialized if err := cc . initialize Dynamic Config // determine assigned source and set status assigned Source , err := cc . checkpoint if assigned Source != nil { cc . config Status . Set Assigned ( assigned Source . Node Config // determine last-known-good source and set status last Known Good Source , err := cc . checkpoint Store . Last Known if last Known Good Source != nil { cc . config Status . Set Last Known Good ( last Known Good Source . Node Config // if the assigned source is nil, return nil to indicate local config if assigned // attempt to load assigned config assigned Config , reason , err := cc . load Config ( assigned if err == nil { // update the active source to the non-nil assigned source cc . config Status . Set Active ( assigned Source . Node Config // update the last-known-good config if necessary, and start a timer that // periodically checks whether the last-known good needs to be updated // we only do this when the assigned config loads and passes validation // wait.Forever will call the func once before starting the timer go wait . Forever ( func ( ) { cc . check Trial ( config Trial return assigned // set status to indicate the failure with the assigned config cc . config Status . Set // if the last-known-good source is nil, return nil to indicate local config if last Known Good // attempt to load the last-known-good config last Known Good Config , _ , err := cc . load Config ( last Known Good // set status to indicate the active source is the non-nil last-known-good source cc . config Status . Set Active ( last Known Good Source . Node Config return last Known Good } 
func ( cc * Controller ) Start Sync ( client clientset . Interface , event Client v1core . Events Getter , node Name string ) error { const err if client == nil { return fmt . Errorf ( err if event Client == nil { return fmt . Errorf ( err if node Name == " " { return fmt . Errorf ( err // Rather than use utilruntime.Handle Crash, which doesn't actually crash in the Kubelet, // we use Handle Panic to manually call the panic handlers and then crash. // We have a better chance of recovering normal operation if we just restart the Kubelet in the event // of a Go runtime error. // NOTE(mtaufen): utilpanic.Handle Panic returns a function and you have to call it for your thing to run! // This was EVIL to debug (difficult to see missing `()`). // The code now uses `go name()` instead of `go utilpanic.Handle Panic(func(){...})()` to avoid confusion. // status sync worker status Sync Loop Func := utilpanic . Handle wait . Jitter Until ( func ( ) { cc . config Status . Sync ( client , node } , 10 * time . Second , 0.2 , true , wait . Never // remote config source informer, if we have a remote source to watch assigned Source , err := cc . checkpoint if err != nil { return fmt . Errorf ( err } else if assigned } else { cc . remote Config Source Informer = assigned Source . Informer ( client , cache . Resource Event Handler Funcs { Add Func : cc . on Add Remote Config Source Event , Update Func : cc . on Update Remote Config Source Event , Delete Func : cc . on Delete Remote Config Source remote Config Source Informer Func := utilpanic . Handle Panic ( func ( ) { if cc . remote Config Source cc . remote Config Source Informer . Run ( wait . Never // node informer cc . node Informer = new Shared Node Informer ( client , node Name , cc . on Add Node Event , cc . on Update Node Event , cc . on Delete Node node Informer Func := utilpanic . Handle cc . node Informer . Run ( wait . Never // config sync worker config Sync Loop Func := utilpanic . Handle wait . Jitter Until ( func ( ) { cc . sync Config Source ( client , event Client , node } , 10 * time . Second , 0.2 , true , wait . Never go status Sync Loop go remote Config Source Informer go node Informer go config Sync Loop } 
func ( cc * Controller ) load Config ( source checkpoint . Remote Config Source ) ( * kubeletconfig . Kubelet Configuration , string , error ) { // load Kubelet Configuration from checkpoint kc , err := cc . checkpoint if err != nil { return nil , status . Load // apply any required transformations to the Kubelet Configuration if cc . transform != nil { if err := cc . transform ( kc ) ; err != nil { return nil , status . Internal // validate the result if err := validation . Validate Kubelet Configuration ( kc ) ; err != nil { return nil , status . Validate } 
func ( cc * Controller ) check Trial ( duration time . Duration ) { // when the trial period is over, the assigned config becomes the last-known-good if trial , err := cc . in } else if ! trial { if err := cc . graduate Assigned To Last Known } 
func ( cc * Controller ) in Trial ( trial t , err := cc . checkpoint Store . Assigned if now . Sub ( t ) <= trial } 
func ( cc * Controller ) graduate Assigned To Last Known Good ( ) error { // get assigned assigned , err := cc . checkpoint // get last-known-good last Known Good , err := cc . checkpoint Store . Last Known // if the sources are equal, no need to change if assigned == last Known Good || assigned != nil && last Known Good != nil && apiequality . Semantic . Deep Equal ( assigned . Node Config Source ( ) , last Known Good . Node Config // update last-known-good err = cc . checkpoint Store . Set Last Known // update the status to reflect the new last-known-good config cc . config Status . Set Last Known Good ( assigned . Node Config utillog . Infof ( " " , assigned . API Path ( ) , assigned . UID ( ) , assigned . Resource } 
func ( s * run As Any ) Validate ( _ * field . Path , _ * api . Pod , _ * api . Container , run As Non Root * bool , run As User * int64 ) field . Error List { return field . Error } 
func ( b * gce Persistent Disk Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Deployment { } , & Deployment List { } , & Deployment Rollback { } , & Scale { } , & Stateful Set { } , & Stateful Set List { } , & Controller Revision { } , & Controller Revision metav1 . Add To Group Version ( scheme , Scheme Group } 
for i := range r . loop Funcs { go r . loop } 
} 
func New Self Signed CA tmpl := x509 . Certificate { Serial Number : new ( big . Int ) . Set Int64 ( 0 ) , Subject : pkix . Name { Common Name : cfg . Common Name , Organization : cfg . Organization , } , Not Before : now . UTC ( ) , Not After : now . Add ( duration365d * 10 ) . UTC ( ) , Key Usage : x509 . Key Usage Key Encipherment | x509 . Key Usage Digital Signature | x509 . Key Usage Cert Sign , Basic Constraints Valid : true , Is cert DER Bytes , err := x509 . Create return x509 . Parse Certificate ( cert DER } 
func Generate Self Signed Cert Key ( host string , alternate I Ps [ ] net . IP , alternate DNS [ ] string ) ( [ ] byte , [ ] byte , error ) { return Generate Self Signed Cert Key With Fixtures ( host , alternate I Ps , alternate } 
func Generate Self Signed Cert Key With Fixtures ( host string , alternate I Ps [ ] net . IP , alternate DNS [ ] string , fixture Directory string ) ( [ ] byte , [ ] byte , error ) { valid max base Name := fmt . Sprintf ( " " , host , strings . Join ( ips To Strings ( alternate I Ps ) , " " ) , strings . Join ( alternate cert Fixture Path := path . Join ( fixture Directory , base key Fixture Path := path . Join ( fixture Directory , base if len ( fixture Directory ) > 0 { cert , err := ioutil . Read File ( cert Fixture if err == nil { key , err := ioutil . Read File ( key Fixture return nil , nil , fmt . Errorf ( " " , cert Fixture Path , key Fixture max ca Key , err := rsa . Generate ca Template := x509 . Certificate { Serial Number : big . New Int ( 1 ) , Subject : pkix . Name { Common Name : fmt . Sprintf ( " " , host , time . Now ( ) . Unix ( ) ) , } , Not Before : valid From , Not After : valid From . Add ( max Age ) , Key Usage : x509 . Key Usage Key Encipherment | x509 . Key Usage Digital Signature | x509 . Key Usage Cert Sign , Basic Constraints Valid : true , Is ca DER Bytes , err := x509 . Create Certificate ( cryptorand . Reader , & ca Template , & ca Template , & ca Key . Public Key , ca ca Certificate , err := x509 . Parse Certificate ( ca DER priv , err := rsa . Generate template := x509 . Certificate { Serial Number : big . New Int ( 2 ) , Subject : pkix . Name { Common Name : fmt . Sprintf ( " " , host , time . Now ( ) . Unix ( ) ) , } , Not Before : valid From , Not After : valid From . Add ( max Age ) , Key Usage : x509 . Key Usage Key Encipherment | x509 . Key Usage Digital Signature , Ext Key Usage : [ ] x509 . Ext Key Usage { x509 . Ext Key Usage Server Auth } , Basic Constraints if ip := net . Parse IP ( host ) ; ip != nil { template . IP Addresses = append ( template . IP } else { template . DNS Names = append ( template . DNS template . IP Addresses = append ( template . IP Addresses , alternate I template . DNS Names = append ( template . DNS Names , alternate der Bytes , err := x509 . Create Certificate ( cryptorand . Reader , & template , ca Certificate , & priv . Public Key , ca // Generate cert, followed by ca cert if err := pem . Encode ( & cert Buffer , & pem . Block { Type : Certificate Block Type , Bytes : der if err := pem . Encode ( & cert Buffer , & pem . Block { Type : Certificate Block Type , Bytes : ca DER // Generate key key if err := pem . Encode ( & key Buffer , & pem . Block { Type : keyutil . RSA Private Key Block Type , Bytes : x509 . Marshal PKCS1Private if len ( fixture Directory ) > 0 { if err := ioutil . Write File ( cert Fixture Path , cert Buffer . Bytes ( ) , 0644 ) ; err != nil { return nil , nil , fmt . Errorf ( " " , cert Fixture if err := ioutil . Write File ( key Fixture Path , key Buffer . Bytes ( ) , 0644 ) ; err != nil { return nil , nil , fmt . Errorf ( " " , cert Fixture return cert Buffer . Bytes ( ) , key } 
func Calculate Node Affinity Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { node := node if node == nil { return schedulerapi . Host if priority Meta , ok := meta . ( * priority Metadata ) ; ok { // We were able to parse metadata, use affinity from there. affinity = priority // A nil element of Preferred During Scheduling Ignored During Execution matches no objects. // An element of Preferred During Scheduling Ignored During Execution that refers to an // empty Preferred Scheduling Term matches all objects. if affinity != nil && affinity . Node Affinity != nil && affinity . Node Affinity . Preferred During Scheduling Ignored During Execution != nil { // Match Preferred During Scheduling Ignored During Execution term by term. for i := range affinity . Node Affinity . Preferred During Scheduling Ignored During Execution { preferred Scheduling Term := & affinity . Node Affinity . Preferred During Scheduling Ignored During if preferred Scheduling // TODO: Avoid computing it for all nodes if this becomes a performance problem. node Selector , err := v1helper . Node Selector Requirements As Selector ( preferred Scheduling Term . Preference . Match if err != nil { return schedulerapi . Host if node Selector . Matches ( labels . Set ( node . Labels ) ) { count += preferred Scheduling return schedulerapi . Host } 
func New Controller ( p Controller Parameters ) ( * Persistent Volume Controller , error ) { event Recorder := p . Event if event Recorder == nil { broadcaster := record . New broadcaster . Start broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : p . Kube Client . Core event Recorder = broadcaster . New Recorder ( scheme . Scheme , v1 . Event controller := & Persistent Volume Controller { volumes : new Persistent Volume Ordered Index ( ) , claims : cache . New Store ( cache . Deletion Handling Meta Namespace Key Func ) , kube Client : p . Kube Client , event Recorder : event Recorder , running Operations : goroutinemap . New Go Routine Map ( true /* exponential Back Off On Error */ ) , cloud : p . Cloud , enable Dynamic Provisioning : p . Enable Dynamic Provisioning , cluster Name : p . Cluster Name , create Provisioned PV Retry Count : create Provisioned PV Retry Count , create Provisioned PV Interval : create Provisioned PV Interval , claim Queue : workqueue . New Named ( " " ) , volume Queue : workqueue . New Named ( " " ) , resync Period : p . Sync // Prober is nil because PV is not aware of Flexvolume. if err := controller . volume Plugin Mgr . Init Plugins ( p . Volume p . Volume Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { controller . enqueue Work ( controller . volume Queue , obj ) } , Update Func : func ( old Obj , new Obj interface { } ) { controller . enqueue Work ( controller . volume Queue , new Obj ) } , Delete Func : func ( obj interface { } ) { controller . enqueue Work ( controller . volume controller . volume Lister = p . Volume controller . volume Lister Synced = p . Volume Informer . Informer ( ) . Has p . Claim Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { controller . enqueue Work ( controller . claim Queue , obj ) } , Update Func : func ( old Obj , new Obj interface { } ) { controller . enqueue Work ( controller . claim Queue , new Obj ) } , Delete Func : func ( obj interface { } ) { controller . enqueue Work ( controller . claim controller . claim Lister = p . Claim controller . claim Lister Synced = p . Claim Informer . Informer ( ) . Has controller . class Lister = p . Class controller . class Lister Synced = p . Class Informer . Informer ( ) . Has controller . pod Lister = p . Pod controller . pod Lister Synced = p . Pod Informer . Informer ( ) . Has controller . Node Lister = p . Node controller . Node Lister Synced = p . Node Informer . Informer ( ) . Has } 
func ( ctrl * Persistent Volume Controller ) initialize Caches ( volume Lister corelisters . Persistent Volume Lister , claim Lister corelisters . Persistent Volume Claim Lister ) { volume List , err := volume for _ , volume := range volume List { volume Clone := volume . Deep if _ , err = ctrl . store Volume Update ( volume claim List , err := claim for _ , claim := range claim List { if _ , err = ctrl . store Claim Update ( claim . Deep } 
func ( ctrl * Persistent Volume Controller ) enqueue Work ( queue workqueue . Interface , obj interface { } ) { // Beware of "xxx deleted" events if unknown , ok := obj . ( cache . Deleted Final State obj Name , err := controller . Key klog . V ( 5 ) . Infof ( " " , obj queue . Add ( obj } 
func ( ctrl * Persistent Volume Controller ) update Volume ( volume * v1 . Persistent Volume ) { // Store the new volume version in the cache and do not process it if this // is an old version. new , err := ctrl . store Volume err = ctrl . sync if err != nil { if errors . Is } 
func ( ctrl * Persistent Volume Controller ) delete Volume ( volume * v1 . Persistent if volume . Spec . Claim // sync the claim when its volume is deleted. Explicitly syncing the // claim here in response to volume deletion prevents the claim from // waiting until the next sync period for its Lost status. claim Key := claimref To Claim Key ( volume . Spec . Claim klog . V ( 5 ) . Infof ( " " , volume . Name , claim ctrl . claim Queue . Add ( claim } 
func ( ctrl * Persistent Volume Controller ) update Claim ( claim * v1 . Persistent Volume Claim ) { // Store the new claim version in the cache and do not process it if this is // an old version. new , err := ctrl . store Claim err = ctrl . sync if err != nil { if errors . Is Conflict ( err ) { // Version conflict error happens quite often and the controller // recovers from it easily. klog . V ( 3 ) . Infof ( " " , claim To Claim } else { klog . Errorf ( " " , claim To Claim } 
func ( ctrl * Persistent Volume Controller ) delete Claim ( claim * v1 . Persistent Volume klog . V ( 4 ) . Infof ( " " , claim To Claim volume Name := claim . Spec . Volume if volume Name == " " { klog . V ( 5 ) . Infof ( " " , claim To Claim // sync the volume when its claim is deleted. Explicitly sync'ing the // volume here in response to claim deletion prevents the volume from // waiting until the next sync period for its Release. klog . V ( 5 ) . Infof ( " " , claim To Claim Key ( claim ) , volume ctrl . volume Queue . Add ( volume } 
func ( ctrl * Persistent Volume Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle defer ctrl . claim Queue . Shut defer ctrl . volume Queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , ctrl . volume Lister Synced , ctrl . claim Lister Synced , ctrl . class Lister Synced , ctrl . pod Lister Synced , ctrl . Node Lister ctrl . initialize Caches ( ctrl . volume Lister , ctrl . claim go wait . Until ( ctrl . resync , ctrl . resync Period , stop go wait . Until ( ctrl . volume Worker , time . Second , stop go wait . Until ( ctrl . claim Worker , time . Second , stop <- stop } 
func ( ctrl * Persistent Volume Controller ) volume Worker ( ) { work Func := func ( ) bool { key Obj , quit := ctrl . volume defer ctrl . volume Queue . Done ( key key := key _ , name , err := cache . Split Meta Namespace volume , err := ctrl . volume if err == nil { // The volume still exists in informer cache, the event must have // been add/update/sync ctrl . update if ! errors . Is Not // The volume is not in informer cache, the event must have been // "delete" volume Obj , found , err := ctrl . volumes . store . Get By volume , ok := volume Obj . ( * v1 . Persistent if ! ok { klog . Errorf ( " " , volume ctrl . delete for { if quit := work } 
func ( ctrl * Persistent Volume Controller ) claim Worker ( ) { work Func := func ( ) bool { key Obj , quit := ctrl . claim defer ctrl . claim Queue . Done ( key key := key namespace , name , err := cache . Split Meta Namespace claim , err := ctrl . claim Lister . Persistent Volume if err == nil { // The claim still exists in informer cache, the event must have // been add/update/sync ctrl . update if ! errors . Is Not // The claim is not in informer cache, the event must have been "delete" claim Obj , found , err := ctrl . claims . Get By claim , ok := claim Obj . ( * v1 . Persistent Volume if ! ok { klog . Errorf ( " " , claim ctrl . delete for { if quit := work } 
func ( ctrl * Persistent Volume pvcs , err := ctrl . claim Lister . List ( labels . New for _ , pvc := range pvcs { ctrl . enqueue Work ( ctrl . claim pvs , err := ctrl . volume Lister . List ( labels . New for _ , pv := range pvs { ctrl . enqueue Work ( ctrl . volume } 
func ( ctrl * Persistent Volume Controller ) set Claim Provisioner ( claim * v1 . Persistent Volume Claim , provisioner Name string ) ( * v1 . Persistent Volume Claim , error ) { if val , ok := claim . Annotations [ ann Storage Provisioner ] ; ok && val == provisioner // The volume from method args can be pointing to watcher cache. We must not // modify these, therefore create a copy. claim Clone := claim . Deep metav1 . Set Meta Data Annotation ( & claim Clone . Object Meta , ann Storage Provisioner , provisioner new Claim , err := ctrl . kube Client . Core V1 ( ) . Persistent Volume Claims ( claim . Namespace ) . Update ( claim if err != nil { return new _ , err = ctrl . store Claim Update ( new if err != nil { return new return new } 
func get Claim Status For Logging ( claim * v1 . Persistent Volume Claim ) string { bound := metav1 . Has Annotation ( claim . Object Meta , ann Bind bound By Controller := metav1 . Has Annotation ( claim . Object Meta , ann Bound By return fmt . Sprintf ( " " , claim . Status . Phase , claim . Spec . Volume Name , bound , bound By } 
func store Object Update ( store cache . Store , obj interface { } , class Name string ) ( bool , error ) { obj Name , err := controller . Key old if err != nil { return false , fmt . Errorf ( " " , class Name , obj obj if ! found { // This is a new object klog . V ( 4 ) . Infof ( " " , class Name , obj Name , obj Accessor . Get Resource if err = store . Add ( obj ) ; err != nil { return false , fmt . Errorf ( " " , class Name , obj old Obj Accessor , err := meta . Accessor ( old obj Resource Version , err := strconv . Parse Int ( obj Accessor . Get Resource if err != nil { return false , fmt . Errorf ( " " , obj Accessor . Get Resource Version ( ) , class Name , obj old Obj Resource Version , err := strconv . Parse Int ( old Obj Accessor . Get Resource if err != nil { return false , fmt . Errorf ( " " , old Obj Accessor . Get Resource Version ( ) , class Name , obj // Throw away only older version, let the same version pass - we do want to // get periodic sync events. if old Obj Resource Version > obj Resource Version { klog . V ( 4 ) . Infof ( " " , class Name , obj Name , obj Accessor . Get Resource klog . V ( 4 ) . Infof ( " " , class Name , obj Name , obj Accessor . Get Resource if err = store . Update ( obj ) ; err != nil { return false , fmt . Errorf ( " " , class Name , obj } 
func New Dynamic Backend ( rc * rest . REST Client , initial Backoff time . Duration ) audit . Backend { return & backend { w : & webhook . Generic Webhook { Rest Client : rc , Initial Backoff : initial Backoff , } , name : fmt . Sprintf ( " " , Plugin } 
func New Backend ( kube Config File string , group Version schema . Group Version , initial Backoff time . Duration ) ( audit . Backend , error ) { w , err := load Webhook ( kube Config File , group Version , initial return & backend { w : w , name : Plugin } 
func ( r * Role Binding Builder ) Groups ( groups ... string ) * Role Binding Builder { for _ , group := range groups { r . Role Binding . Subjects = append ( r . Role Binding . Subjects , rbacv1 . Subject { Kind : rbacv1 . Group Kind , API Group : Group } 
func ( util * AWS Disk Util ) Delete Volume ( d * aws Elastic Block Store Deleter ) error { cloud , err := get Cloud Provider ( d . aws Elastic Block Store . plugin . host . Get Cloud deleted , err := cloud . Delete Disk ( d . volume if err != nil { // AWS cloud provider returns volume.deleted Volume In Use Error when // necessary, no handling needed here. klog . V ( 2 ) . Infof ( " " , d . volume if deleted { klog . V ( 2 ) . Infof ( " " , d . volume } else { klog . V ( 2 ) . Infof ( " " , d . volume } 
func ( util * AWS Disk Util ) Create Volume ( c * aws Elastic Block Store Provisioner , node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term ) ( aws . Kubernetes Volume ID , int , map [ string ] string , string , error ) { cloud , err := get Cloud Provider ( c . aws Elastic Block Store . plugin . host . Get Cloud if c . options . Cloud } else { tags = * c . options . Cloud tags [ " " ] = volumeutil . Generate Volume Name ( c . options . Cluster Name , c . options . PV capacity := c . options . PVC . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource zones With Nodes , err := cloud . Get Candidate Zones For Dynamic volume Options , err := populate Volume Options ( c . plugin . Get Plugin Name ( ) , c . options . PVC . Name , capacity , tags , c . options . Parameters , node , allowed Topologies , zones With name , err := cloud . Create Disk ( volume labels , err := cloud . Get Volume for k , v := range c . options . Parameters { if strings . To Lower ( k ) == volume . Volume Parameter FS return name , volume Options . Capacity } 
func populate Volume Options ( plugin Name , pvc Name string , capacity GB resource . Quantity , tags map [ string ] string , storage Params map [ string ] string , node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term , zones With Nodes sets . String ) ( * aws . Volume Options , error ) { request Gi B , err := volumehelpers . Round Up To Gi B Int ( capacity volume Options := & aws . Volume Options { Capacity GB : request Gi // Apply Parameters (case-insensitive). We leave validation of // the values to the cloud provider. zone zones for k , v := range storage Params { switch strings . To Lower ( k ) { case " " : volume Options . Volume case " " : zone case " " : zones zones , err = volumehelpers . Zones To case " " : volume Options . IOPS Per case " " : volume Options . Encrypted , err = strconv . Parse case " " : volume Options . Kms Key case volume . Volume Parameter FS Type : // Do nothing but don't make this fail default : return nil , fmt . Errorf ( " " , k , plugin volume Options . Availability Zone , err = volumehelpers . Select Zone For Volume ( zone Present , zones Present , zone , zones , zones With Nodes , node , allowed Topologies , pvc return volume } 
func verify Device Path ( device Paths [ ] string ) ( string , error ) { for _ , path := range device Paths { if path Exists , err := mount . Path } else if path } 
func get Disk By ID Paths ( volume ID aws . Kubernetes Volume ID , partition string , device Path string ) [ ] string { device if device Path != " " { device Paths = append ( device Paths , device if partition != " " { for i , path := range device Paths { device Paths [ i ] = path + disk Partition // We need to find NVME volumes, which are mounted on a "random" nvme path ("/dev/nvme0n1"), // and we have to get the volume id from the nvme interface aws Volume ID , err := volume ID . Map To AWS Volume if err != nil { klog . Warningf ( " " , volume } else { // This is the magic name on which AWS presents NVME devices under /dev/disk/by-id/ // For example, vol-0fab1d5e3f72a5e23 creates a symlink at /dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_vol0fab1d5e3f72a5e23 nvme Name := " " + strings . Replace ( string ( aws Volume nvme Path , err := find Nvme Volume ( nvme if err != nil { klog . Warningf ( " " , volume } else if nvme Path != " " { device Paths = append ( device Paths , nvme return device } 
func get Cloud Provider ( cloud Provider cloudprovider . Interface ) ( * aws . Cloud , error ) { aws Cloud Provider , ok := cloud if ! ok || aws Cloud Provider == nil { return nil , fmt . Errorf ( " " , cloud return aws Cloud } 
func find Nvme Volume ( find Name string ) ( device string , err error ) { p := filepath . Join ( " " , find if err != nil { if os . Is Not if stat . Mode ( ) & os . Mode Symlink != os . Mode // Find the target, resolving to an absolute path // For example, /dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_vol0fab1d5e3f72a5e23 -> ../../nvme2n1 resolved , err := filepath . Eval if ! strings . Has } 
func Key Values From scanner := bufio . New Scanner ( bytes . New current for scanner . Scan ( ) { // Process the current line, retrieving a key/value pair if // possible. scanned kv , err := Key Values From Line ( scanned Bytes , current current } 
func Key Values From Line ( line [ ] byte , current // We trim UTF8 BOM from the first line of the file but no others if current Line == 0 { line = bytes . Trim // trim the line from all leading whitespace first line = bytes . Trim Left Func ( line , unicode . Is data := strings . Split if errs := validation . Is Env Var } 
func parse Zoned ( zoned String string , kind v1 . Azure Data Disk Kind ) ( bool , error ) { if zoned String == " " { return kind == v1 . Azure Managed zoned , err := strconv . Parse Bool ( zoned if zoned && kind != v1 . Azure Managed } 
func Add To Scheme ( scheme * runtime . Scheme ) { utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( apiregistration . Add To } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Event List ) ( nil ) , ( * audit . Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Event List_To_audit_Event List ( a . ( * Event List ) , b . ( * audit . Event if err := s . Add Generated Conversion Func ( ( * audit . Event List ) ( nil ) , ( * Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Event List_To_v1_Event List ( a . ( * audit . Event List ) , b . ( * Event if err := s . Add Generated Conversion Func ( ( * Group Resources ) ( nil ) , ( * audit . Group Resources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resources_To_audit_Group Resources ( a . ( * Group Resources ) , b . ( * audit . Group if err := s . Add Generated Conversion Func ( ( * audit . Group Resources ) ( nil ) , ( * Group Resources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Group Resources_To_v1_Group Resources ( a . ( * audit . Group Resources ) , b . ( * Group if err := s . Add Generated Conversion Func ( ( * Object Reference ) ( nil ) , ( * audit . Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Reference_To_audit_Object Reference ( a . ( * Object Reference ) , b . ( * audit . Object if err := s . Add Generated Conversion Func ( ( * audit . Object Reference ) ( nil ) , ( * Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Object Reference_To_v1_Object Reference ( a . ( * audit . Object Reference ) , b . ( * Object if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Policy List ) ( nil ) , ( * audit . Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Policy List_To_audit_Policy List ( a . ( * Policy List ) , b . ( * audit . Policy if err := s . Add Generated Conversion Func ( ( * audit . Policy List ) ( nil ) , ( * Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Policy List_To_v1_Policy List ( a . ( * audit . Policy List ) , b . ( * Policy if err := s . Add Generated Conversion Func ( ( * Policy Rule ) ( nil ) , ( * audit . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Policy Rule_To_audit_Policy Rule ( a . ( * Policy Rule ) , b . ( * audit . Policy if err := s . Add Generated Conversion Func ( ( * audit . Policy Rule ) ( nil ) , ( * Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_audit_Policy Rule_To_v1_Policy Rule ( a . ( * audit . Policy Rule ) , b . ( * Policy } 
func Convert_v1_Event_To_audit_Event ( in * Event , out * audit . Event , s conversion . Scope ) error { return auto } 
func Convert_audit_Event_To_v1_Event ( in * audit . Event , out * Event , s conversion . Scope ) error { return auto } 
func Convert_v1_Event List_To_audit_Event List ( in * Event List , out * audit . Event List , s conversion . Scope ) error { return auto Convert_v1_Event List_To_audit_Event } 
func Convert_audit_Event List_To_v1_Event List ( in * audit . Event List , out * Event List , s conversion . Scope ) error { return auto Convert_audit_Event List_To_v1_Event } 
func Convert_v1_Group Resources_To_audit_Group Resources ( in * Group Resources , out * audit . Group Resources , s conversion . Scope ) error { return auto Convert_v1_Group Resources_To_audit_Group } 
func Convert_audit_Group Resources_To_v1_Group Resources ( in * audit . Group Resources , out * Group Resources , s conversion . Scope ) error { return auto Convert_audit_Group Resources_To_v1_Group } 
func Convert_v1_Object Reference_To_audit_Object Reference ( in * Object Reference , out * audit . Object Reference , s conversion . Scope ) error { return auto Convert_v1_Object Reference_To_audit_Object } 
func Convert_audit_Object Reference_To_v1_Object Reference ( in * audit . Object Reference , out * Object Reference , s conversion . Scope ) error { return auto Convert_audit_Object Reference_To_v1_Object } 
func Convert_v1_Policy_To_audit_Policy ( in * Policy , out * audit . Policy , s conversion . Scope ) error { return auto } 
func Convert_audit_Policy_To_v1_Policy ( in * audit . Policy , out * Policy , s conversion . Scope ) error { return auto } 
func Convert_v1_Policy List_To_audit_Policy List ( in * Policy List , out * audit . Policy List , s conversion . Scope ) error { return auto Convert_v1_Policy List_To_audit_Policy } 
func Convert_audit_Policy List_To_v1_Policy List ( in * audit . Policy List , out * Policy List , s conversion . Scope ) error { return auto Convert_audit_Policy List_To_v1_Policy } 
func Convert_v1_Policy Rule_To_audit_Policy Rule ( in * Policy Rule , out * audit . Policy Rule , s conversion . Scope ) error { return auto Convert_v1_Policy Rule_To_audit_Policy } 
func Convert_audit_Policy Rule_To_v1_Policy Rule ( in * audit . Policy Rule , out * Policy Rule , s conversion . Scope ) error { return auto Convert_audit_Policy Rule_To_v1_Policy } 
func New Storage ( opts Getter generic . REST Options Getter ) * CSI Driver Storage { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & storageapi . CSI Driver { } } , New List Func : func ( ) runtime . Object { return & storageapi . CSI Driver List { } } , Default Qualified Resource : storageapi . Resource ( " " ) , Create Strategy : csidriver . Strategy , Update Strategy : csidriver . Strategy , Delete Strategy : csidriver . Strategy , Return Deleted options := & generic . Store Options { REST Options : opts if err := store . Complete With return & CSI Driver Storage { CSI } 
func ( kf * Kunstructured Factory Impl ) Slice From Bytes ( in [ ] byte ) ( [ ] ifc . Kunstructured , error ) { decoder := yaml . New YAML Or JSON Decoder ( bytes . New for err == nil || is Empty Yaml result = append ( result , & Unstruct } 
func ( kf * Kunstructured Factory Impl ) From Map ( m map [ string ] interface { } ) ifc . Kunstructured { return & Unstruct } 
func ( kf * Kunstructured Factory Impl ) Make Config Map ( args * types . Config Map Args , options * types . Generator Options ) ( ifc . Kunstructured , error ) { cm , err := kf . cm Factory . Make Config return New Kunstructured From } 
func ( kf * Kunstructured Factory Impl ) Make Secret ( args * types . Secret Args , options * types . Generator Options ) ( ifc . Kunstructured , error ) { sec , err := kf . secret Factory . Make return New Kunstructured From } 
func ( kf * Kunstructured Factory Impl ) Set ( ldr ifc . Loader ) { kf . cm Factory = configmapandsecret . New Config Map kf . secret Factory = configmapandsecret . New Secret } 
func ( kf * Kunstructured Factory Impl ) validate ( u unstructured . Unstructured ) error { kind := u . Get } else if strings . Has if u . Get } 
func ( strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new old _ , _ = new Role , old } 
func ( strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error return validation . Validate } 
func ( strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new error List := validation . Validate Role ( new return append ( error List , validation . Validate Role Update ( new } 
func ( o * Drain Cmd if len ( args ) == 0 && ! cmd . Flags ( ) . Changed ( " " ) { return cmdutil . Usage if len ( args ) > 0 && len ( o . drainer . Selector ) > 0 { return cmdutil . Usage o . drainer . Dry Run = cmdutil . Get Dry Run if o . drainer . Client , err = f . Kubernetes Client if len ( o . drainer . Pod Selector ) > 0 { if _ , err := labels . Parse ( o . drainer . Pod o . node o . Namespace , _ , err = f . To Raw Kube Config o . To Printer = func ( operation string ) ( printers . Resource Printer Func , error ) { o . Print Flags . Name Print if o . drainer . Dry Run { o . Print printer , err := o . Print Flags . To return printer . Print builder := f . New Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . Resource Names ( " " , args ... ) . Single Resource if len ( o . drainer . Selector ) > 0 { builder = builder . Label Selector Param ( o . drainer . Selector ) . Resource if info . Mapping . Resource . Group Resource ( ) != ( schema . Group o . node Infos = append ( o . node } 
func ( o * Drain Cmd Options ) Run Drain ( ) error { if err := o . Run Cordon Or print Obj , err := o . To drained Nodes := sets . New for _ , info := range o . node if ! o . drainer . Dry Run { err = o . delete Or Evict Pods if err == nil || o . drainer . Dry Run { drained print } else { fmt . Fprintf ( o . Err remaining for _ , remaining Info := range o . node Infos { if drained Nodes . Has ( remaining remaining Nodes = append ( remaining Nodes , remaining if len ( remaining Nodes ) > 0 { fmt . Fprintf ( o . Err for _ , node Name := range remaining Nodes { fmt . Fprintf ( o . Err Out , " \n " , node } 
func ( o * Drain Cmd Options ) delete Or Evict policy Group Version , err := drain . Check Eviction get Pod Fn := func ( namespace , name string ) ( * corev1 . Pod , error ) { return o . drainer . Client . Core V1 ( ) . Pods ( namespace ) . Get ( name , metav1 . Get if len ( policy Group Version ) > 0 { return o . evict Pods ( pods , policy Group Version , get Pod } else { return o . delete Pods ( pods , get Pod } 
func ( o * Drain Cmd Options ) Run Cordon Or Uncordon ( desired bool ) error { cordon Or if ! desired { cordon Or Uncordon = " " + cordon Or for _ , node Info := range o . node Infos { print Error := func ( err error ) { fmt . Fprintf ( o . Err Out , " \n " , cordon Or Uncordon , node gvk := node Info . Resource Mapping ( ) . Group Version if gvk . Kind == " " { c , err := drain . New Cordon Helper From Runtime Object ( node if err != nil { print if update Required := c . Update If Required ( desired ) ; ! update Required { print Obj , err := o . To if err != nil { fmt . Fprintf ( o . Err print Obj ( node } else { if ! o . drainer . Dry Run { err , patch Err := c . Patch Or if patch Err != nil { print Error ( patch if err != nil { print print Obj , err := o . To if err != nil { fmt . Fprintf ( o . Err print Obj ( node } else { print Obj , err := o . To if err != nil { fmt . Fprintf ( o . Err print Obj ( node } 
func ( in * Example ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object } 
func ( in * Example ) Deep in . Deep Copy } 
func ( in * Example ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Example List ) Deep Copy Into ( out * Example out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Example List ) Deep Copy ( ) * Example out := new ( Example in . Deep Copy } 
func ( in * Example List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Example Spec ) Deep Copy ( ) * Example out := new ( Example in . Deep Copy } 
func ( in * Example Status ) Deep Copy ( ) * Example out := new ( Example in . Deep Copy } 
func ( s * Status ) Is } 
func ( s * Status ) As Error ( ) error { if s . Is } 
func New } 
func New Volume Manager ( controller Attach Detach Enabled bool , node Name k8stypes . Node Name , pod Manager pod . Manager , pod Status Provider status . Pod Status Provider , kube Client clientset . Interface , volume Plugin Mgr * volume . Volume Plugin Mgr , kube Container Runtime container . Runtime , mounter mount . Interface , kubelet Pods Dir string , recorder record . Event Recorder , check Node Capabilities Before Mount bool , keep Terminated Pod Volumes bool ) Volume Manager { vm := & volume Manager { kube Client : kube Client , volume Plugin Mgr : volume Plugin Mgr , desired State Of World : cache . New Desired State Of World ( volume Plugin Mgr ) , actual State Of World : cache . New Actual State Of World ( node Name , volume Plugin Mgr ) , operation Executor : operationexecutor . New Operation Executor ( operationexecutor . New Operation Generator ( kube Client , volume Plugin Mgr , recorder , check Node Capabilities Before Mount , volumepathhandler . New Block Volume Path vm . desired State Of World Populator = populator . New Desired State Of World Populator ( kube Client , desired State Of World Populator Loop Sleep Period , desired State Of World Populator Get Pod Status Retry Duration , pod Manager , pod Status Provider , vm . desired State Of World , vm . actual State Of World , kube Container Runtime , keep Terminated Pod vm . reconciler = reconciler . New Reconciler ( kube Client , controller Attach Detach Enabled , reconciler Loop Sleep Period , wait For Attach Timeout , node Name , vm . desired State Of World , vm . actual State Of World , vm . desired State Of World Populator . Has Added Pods , vm . operation Executor , mounter , volume Plugin Mgr , kubelet Pods } 
func ( vm * volume Manager ) get Unattached Volumes ( expected Volumes [ ] string ) [ ] string { unattached for _ , volume := range expected Volumes { if ! vm . actual State Of World . Volume Exists ( v1 . Unique Volume Name ( volume ) ) { unattached Volumes = append ( unattached return unattached } 
func ( vm * volume Manager ) verify Volumes Mounted Func ( pod Name types . Unique Pod Name , expected Volumes [ ] string ) wait . Condition Func { return func ( ) ( done bool , err error ) { return len ( vm . get Unmounted Volumes ( pod Name , expected } 
func ( vm * volume Manager ) get Unmounted Volumes ( pod Name types . Unique Pod Name , expected Volumes [ ] string ) [ ] string { mounted Volumes := sets . New for _ , mounted Volume := range vm . actual State Of World . Get Mounted Volumes For Pod ( pod Name ) { mounted Volumes . Insert ( mounted Volume . Outer Volume Spec return filter Unmounted Volumes ( mounted Volumes , expected } 
func filter Unmounted Volumes ( mounted Volumes sets . String , expected Volumes [ ] string ) [ ] string { unmounted for _ , expected Volume := range expected Volumes { if ! mounted Volumes . Has ( expected Volume ) { unmounted Volumes = append ( unmounted Volumes , expected return unmounted } 
func get Expected Volumes ( pod * v1 . Pod ) [ ] string { expected for _ , pod Volume := range pod . Spec . Volumes { expected Volumes = append ( expected Volumes , pod return expected } 
func get Extra Supplemental Gid ( volume Gid Value string , pod * v1 . Pod ) ( int64 , bool ) { if volume Gid gid , err := strconv . Parse Int ( volume Gid if pod . Spec . Security Context != nil { for _ , existing Gid := range pod . Spec . Security Context . Supplemental Groups { if gid == int64 ( existing } 
func ( n Namespaced } 
func ( a API Object Versioner ) Update Object ( obj runtime . Object , resource version if resource Version != 0 { version String = strconv . Format Uint ( resource accessor . Set Resource Version ( version } 
func ( a API Object Versioner ) Update List ( obj runtime . Object , resource Version uint64 , next Key string ) error { list Accessor , err := meta . List if err != nil || list version if resource Version != 0 { version String = strconv . Format Uint ( resource list Accessor . Set Resource Version ( version list Accessor . Set Continue ( next } 
func ( a API Object Versioner ) Prepare Object For accessor . Set Resource accessor . Set Self } 
func ( a API Object Versioner ) Object Resource version := accessor . Get Resource return strconv . Parse } 
func ( a API Object Versioner ) Parse Resource Version ( resource Version string ) ( uint64 , error ) { if resource Version == " " || resource version , err := strconv . Parse Uint ( resource if err != nil { return 0 , storage . New Invalid Error ( field . Error List { // Validation errors are supposed to return version-specific field // paths, but this is probably close enough. field . Invalid ( field . New Path ( " " ) , resource } 
func ( a API Object Versioner ) Compare Resource Version ( lhs , rhs runtime . Object ) int { lhs Version , err := Versioner . Object Resource rhs Version , err := Versioner . Object Resource if lhs Version == rhs if lhs Version < rhs } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Metric Identifier ) ( nil ) , ( * custommetrics . Metric Identifier ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Metric Identifier_To_custom_metrics_Metric Identifier ( a . ( * Metric Identifier ) , b . ( * custommetrics . Metric if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric Identifier ) ( nil ) , ( * Metric Identifier ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric Identifier_To_v1beta2_Metric Identifier ( a . ( * custommetrics . Metric Identifier ) , b . ( * Metric if err := s . Add Generated Conversion Func ( ( * Metric List Options ) ( nil ) , ( * custommetrics . Metric List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Metric List Options_To_custom_metrics_Metric List Options ( a . ( * Metric List Options ) , b . ( * custommetrics . Metric List if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric List Options ) ( nil ) , ( * Metric List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric List Options_To_v1beta2_Metric List Options ( a . ( * custommetrics . Metric List Options ) , b . ( * Metric List if err := s . Add Generated Conversion Func ( ( * Metric Value ) ( nil ) , ( * custommetrics . Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Metric Value_To_custom_metrics_Metric Value ( a . ( * Metric Value ) , b . ( * custommetrics . Metric if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric Value ) ( nil ) , ( * Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric Value_To_v1beta2_Metric Value ( a . ( * custommetrics . Metric Value ) , b . ( * Metric if err := s . Add Generated Conversion Func ( ( * Metric Value List ) ( nil ) , ( * custommetrics . Metric Value List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Metric Value List_To_custom_metrics_Metric Value List ( a . ( * Metric Value List ) , b . ( * custommetrics . Metric Value if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric Value List ) ( nil ) , ( * Metric Value List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric Value List_To_v1beta2_Metric Value List ( a . ( * custommetrics . Metric Value List ) , b . ( * Metric Value } 
func Convert_v1beta2_Metric Identifier_To_custom_metrics_Metric Identifier ( in * Metric Identifier , out * custommetrics . Metric Identifier , s conversion . Scope ) error { return auto Convert_v1beta2_Metric Identifier_To_custom_metrics_Metric } 
func Convert_custom_metrics_Metric Identifier_To_v1beta2_Metric Identifier ( in * custommetrics . Metric Identifier , out * Metric Identifier , s conversion . Scope ) error { return auto Convert_custom_metrics_Metric Identifier_To_v1beta2_Metric } 
func Convert_v1beta2_Metric List Options_To_custom_metrics_Metric List Options ( in * Metric List Options , out * custommetrics . Metric List Options , s conversion . Scope ) error { return auto Convert_v1beta2_Metric List Options_To_custom_metrics_Metric List } 
func Convert_custom_metrics_Metric List Options_To_v1beta2_Metric List Options ( in * custommetrics . Metric List Options , out * Metric List Options , s conversion . Scope ) error { return auto Convert_custom_metrics_Metric List Options_To_v1beta2_Metric List } 
func Convert_v1beta2_Metric Value_To_custom_metrics_Metric Value ( in * Metric Value , out * custommetrics . Metric Value , s conversion . Scope ) error { return auto Convert_v1beta2_Metric Value_To_custom_metrics_Metric } 
func Convert_custom_metrics_Metric Value_To_v1beta2_Metric Value ( in * custommetrics . Metric Value , out * Metric Value , s conversion . Scope ) error { return auto Convert_custom_metrics_Metric Value_To_v1beta2_Metric } 
func Convert_v1beta2_Metric Value List_To_custom_metrics_Metric Value List ( in * Metric Value List , out * custommetrics . Metric Value List , s conversion . Scope ) error { return auto Convert_v1beta2_Metric Value List_To_custom_metrics_Metric Value } 
func Convert_custom_metrics_Metric Value List_To_v1beta2_Metric Value List ( in * custommetrics . Metric Value List , out * Metric Value List , s conversion . Scope ) error { return auto Convert_custom_metrics_Metric Value List_To_v1beta2_Metric Value } 
func ( o * OS Validator ) Validate ( spec Sys Spec ) ( error , error ) { os , err := exec . Command ( " " ) . Combined return nil , o . validate OS ( strings . Trim } 
func add Conversion Funcs ( scheme * runtime . Scheme ) error { err := scheme . Add Conversion Funcs ( Convert_scheme_Scale Status_To_v1_Scale Status , Convert_v1_Scale Status_To_scheme_Scale } 
func ( in * Replica Set Spec ) Deep Copy Into ( out * Replica Set } 
func ( f * Config Map Factory ) Make Config Map ( args * types . Config Map Args , options * types . Generator Options ) ( * corev1 . Config cm := f . make Fresh Config pairs , err := key Values From Env File ( f . ldr , args . Env if err != nil { return nil , errors . Wrap ( err , fmt . Sprintf ( " " , args . Env pairs , err = key Values From Literal Sources ( args . Literal if err != nil { return nil , errors . Wrap ( err , fmt . Sprintf ( " " , args . Literal pairs , err = key Values From File Sources ( f . ldr , args . File if err != nil { return nil , errors . Wrap ( err , fmt . Sprintf ( " " , args . File for _ , p := range all { err = add Kv To Config if options != nil { cm . Set cm . Set } 
func add Kv To Config Map ( config Map * v1 . Config Map , key Name , data string ) error { // Note, the rules for Config Map keys are the exact same as the ones for Secret Keys. if errs := validation . Is Config Map Key ( key Name ) ; len ( errs ) != 0 { return fmt . Errorf ( " " , key key Exists Error // If the configmap data contains byte sequences that are all in the UTF-8 // range, we will write it to .Data if utf8 . Valid ( [ ] byte ( data ) ) { if _ , entry Exists := config Map . Data [ key Name ] ; entry Exists { return fmt . Errorf ( key Exists Error Msg , key Name , config config Map . Data [ key // otherwise, it's Binary Data if config Map . Binary Data == nil { config Map . Binary if _ , entry Exists := config Map . Binary Data [ key Name ] ; entry Exists { return fmt . Errorf ( key Exists Error Msg , key Name , config Map . Binary config Map . Binary Data [ key } 
func new Quota Accessor ( ) ( * quota Accessor , error ) { live Lookup updated // client and lister will be set when Set Internal Kube Client Set and Set Internal Kube Informer Factory are invoked return & quota Accessor { live Lookup Cache : live Lookup Cache , live TTL : time . Duration ( 30 * time . Second ) , updated Quotas : updated } 
func ( e * quota Accessor ) check Cache ( quota * corev1 . Resource Quota ) * corev1 . Resource uncast Cached Quota , ok := e . updated cached Quota := uncast Cached Quota . ( * corev1 . Resource if etcd Versioner . Compare Resource Version ( quota , cached Quota ) >= 0 { e . updated return cached } 
func New Controller ( kube Client clientset . Interface ) ( * Controller , error ) { event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Client . Core if kube Client != nil && kube Client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { if err := metrics . Register Metric And Track Rate Limiter Usage ( " " , kube Client . Core V1 ( ) . REST Client ( ) . Get Rate jm := & Controller { kube Client : kube Client , job Control : real Job Control { Kube Client : kube Client } , sj Control : & real SJ Control { Kube Client : kube Client } , pod Control : & real Pod Control { Kube Client : kube Client } , recorder : event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event } 
func ( jm * Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle // Check things every 10 second. go wait . Until ( jm . sync All , 10 * time . Second , stop <- stop } 
func ( jm * Controller ) sync All ( ) { // List children (Jobs) before parents (Cron Job). // This guarantees that if we see any Job that got orphaned by the GC orphan finalizer, // we must also see that the parent Cron Job has non-nil Deletion Timestamp (see #42639). // Note that this only works because we are NOT using any caches here. job List Func := func ( opts metav1 . List Options ) ( runtime . Object , error ) { return jm . kube Client . Batch V1 ( ) . Jobs ( metav1 . Namespace jl Tmp , err := pager . New ( pager . Simple Page Func ( job List Func ) ) . List ( context . Background ( ) , metav1 . List if err != nil { utilruntime . Handle jl , ok := jl Tmp . ( * batchv1 . Job if ! ok { utilruntime . Handle Error ( fmt . Errorf ( " " , jl cron Job List Func := func ( opts metav1 . List Options ) ( runtime . Object , error ) { return jm . kube Client . Batch V1beta1 ( ) . Cron Jobs ( metav1 . Namespace sjl Tmp , err := pager . New ( pager . Simple Page Func ( cron Job List Func ) ) . List ( context . Background ( ) , metav1 . List if err != nil { utilruntime . Handle sjl , ok := sjl Tmp . ( * batchv1beta1 . Cron Job if ! ok { utilruntime . Handle Error ( fmt . Errorf ( " " , sjl jobs By Sj := group Jobs By klog . V ( 4 ) . Infof ( " " , len ( jobs By for _ , sj := range sjs { sync One ( & sj , jobs By Sj [ sj . UID ] , time . Now ( ) , jm . job Control , jm . sj cleanup Finished Jobs ( & sj , jobs By Sj [ sj . UID ] , jm . job Control , jm . sj } 
func cleanup Finished Jobs ( sj * batchv1beta1 . Cron Job , js [ ] batchv1 . Job , jc job Control Interface , sjc sj Control Interface , recorder record . Event Recorder ) { // If neither limits are active, there is no need to do anything. if sj . Spec . Failed Jobs History Limit == nil && sj . Spec . Successful Jobs History failed succesful for _ , job := range js { is Finished , finished Status := get Finished if is Finished && finished Status == batchv1 . Job Complete { succesful Jobs = append ( succesful } else if is Finished && finished Status == batchv1 . Job Failed { failed Jobs = append ( failed if sj . Spec . Successful Jobs History Limit != nil { remove Oldest Jobs ( sj , succesful Jobs , jc , * sj . Spec . Successful Jobs History if sj . Spec . Failed Jobs History Limit != nil { remove Oldest Jobs ( sj , failed Jobs , jc , * sj . Spec . Failed Jobs History // Update the Cron Job, in case jobs were removed from the list. if _ , err := sjc . Update Status ( sj ) ; err != nil { name For klog . Infof ( " " , name For Log , sj . Resource } 
func remove Oldest Jobs ( sj * batchv1beta1 . Cron Job , js [ ] batchv1 . Job , jc job Control Interface , max Jobs int32 , recorder record . Event Recorder ) { num To Delete := len ( js ) - int ( max if num To name For klog . V ( 4 ) . Infof ( " " , num To Delete , len ( js ) , name For sort . Sort ( by Job Start for i := 0 ; i < num To Delete ; i ++ { klog . V ( 4 ) . Infof ( " " , js [ i ] . Name , name For delete } 
func sync One ( sj * batchv1beta1 . Cron Job , js [ ] batchv1 . Job , now time . Time , jc job Control Interface , sjc sj Control Interface , recorder record . Event Recorder ) { name For children for _ , j := range js { children Jobs [ j . Object found := in Active List ( * sj , j . Object if ! found && ! Is Job Finished ( & j ) { recorder . Eventf ( sj , v1 . Event Type // We found an unfinished job that has us as the parent, but it is not in our Active list. // This could happen if we crashed right after creating the Job and before updating the status, // or if our jobs list is newer than our sj status after a relist, or if someone intentionally created // a job that they wanted us to adopt. // TODO: maybe handle the adoption case? Concurrency/suspend rules will not apply in that case, obviously, since we can't // stop users from creating jobs if they have permission. It is assumed that if a // user has permission to create a job within a namespace, then they have permission to make any scheduled Job // in the same namespace "adopt" that job. Replica Sets and their Pods work the same way. // TBS: how to update sj.Status.Last Schedule Time if the adopted job is newer than any we knew about? } else if found && Is Job Finished ( & j ) { _ , status := get Finished delete From Active List ( sj , j . Object recorder . Eventf ( sj , v1 . Event Type // Remove any job reference from the active list if the corresponding job does not exist any more. // Otherwise, the cronjob may be stuck in active mode forever even though there is no matching // job running. for _ , j := range sj . Status . Active { if found := children Jobs [ j . UID ] ; ! found { recorder . Eventf ( sj , v1 . Event Type delete From Active updated SJ , err := sjc . Update if err != nil { klog . Errorf ( " " , name For Log , sj . Resource * sj = * updated if sj . Deletion Timestamp != nil { // The Cron if sj . Spec . Suspend != nil && * sj . Spec . Suspend { klog . V ( 4 ) . Infof ( " " , name For times , err := get Recent Unmet Schedule if err != nil { recorder . Eventf ( sj , v1 . Event Type klog . Errorf ( " " , name For // TODO: handle multiple unmet start times, from oldest to newest, updating status as needed. if len ( times ) == 0 { klog . V ( 4 ) . Infof ( " " , name For if len ( times ) > 1 { klog . V ( 4 ) . Infof ( " " , name For scheduled too if sj . Spec . Starting Deadline Seconds != nil { too Late = scheduled Time . Add ( time . Second * time . Duration ( * sj . Spec . Starting Deadline if too Late { klog . V ( 4 ) . Infof ( " " , name For recorder . Eventf ( sj , v1 . Event Type Warning , " " , " " , scheduled // TODO: Since we don't set Last Schedule Time when not scheduling, we are going to keep noticing // the miss every cycle. In order to avoid sending multiple events, and to avoid processing // the sj again and again, we could set a Status.Last Missed Time when we notice a miss. // Then, when we call get Recent Unmet Schedule Times, we can take max(creation Timestamp, // Status.Last Schedule Time, Status.Last Missed if sj . Spec . Concurrency Policy == batchv1beta1 . Forbid Concurrent && len ( sj . Status . Active ) > 0 { // Regardless which source of information we use for the set of active jobs, // there is some risk that we won't see an active job when there is one. // (because we haven't seen the status update to the SJ or the created pod). // So it is theoretically possible to have concurrency with Forbid. // As long the as the invocations are "far enough apart in time", this usually won't happen. // // TODO: for Forbid, we could use the same name for every execution, as a lock. // With replace, we could use a name that is deterministic per execution time. // But that would mean that you could not inspect prior successes or failures of Forbid jobs. klog . V ( 4 ) . Infof ( " " , name For if sj . Spec . Concurrency Policy == batchv1beta1 . Replace Concurrent { for _ , j := range sj . Status . Active { klog . V ( 4 ) . Infof ( " " , j . Name , name For job , err := jc . Get if err != nil { recorder . Eventf ( sj , v1 . Event Type if ! delete job Req , err := get Job From Template ( sj , scheduled if err != nil { klog . Errorf ( " " , name For job Resp , err := jc . Create Job ( sj . Namespace , job if err != nil { recorder . Eventf ( sj , v1 . Event Type klog . V ( 4 ) . Infof ( " " , job Resp . Name , name For recorder . Eventf ( sj , v1 . Event Type Normal , " " , " " , job // ------------------------------------------------------------------ // // If this process restarts at this point (after posting a job, but // before updating the status), then we might try to start the job on // the next time. Actually, if we re-list the S Js and Jobs on the next // iteration of sync All, we might not see our own status update, and // then post one again. So, we need to use the job name as a lock to // prevent us from making the job twice (name the job with hash of its // scheduled time). // Add the just-started job to the status list. ref , err := get Ref ( job if err != nil { klog . V ( 2 ) . Infof ( " " , name For sj . Status . Last Schedule Time = & metav1 . Time { Time : scheduled if _ , err := sjc . Update Status ( sj ) ; err != nil { klog . Infof ( " " , name For Log , sj . Resource } 
func delete Job ( sj * batchv1beta1 . Cron Job , job * batchv1 . Job , jc job Control Interface , recorder record . Event Recorder ) bool { name For // delete the job itself... if err := jc . Delete Job ( job . Namespace , job . Name ) ; err != nil { recorder . Eventf ( sj , v1 . Event Type klog . Errorf ( " " , job . Name , name For // ... and its reference from active list delete From Active List ( sj , job . Object recorder . Eventf ( sj , v1 . Event Type } 
func New ( image Fs Info Provider Image Fs Info Provider , root Path string , using Legacy Stats bool ) ( Interface , error ) { client , err := winstats . New Perf Counter return & cadvisor Client { root Path : root Path , win Stats } 
func New Resource Analyzer ( stats Provider Provider , cal Volume Frequency time . Duration ) Resource Analyzer { fs Analyzer := new Fs Resource Analyzer ( stats Provider , cal Volume summary Provider := New Summary Provider ( stats return & resource Analyzer { fs Analyzer , summary } 
func Convert_v1alpha1_Job Controller Configuration_To_config_Job Controller Configuration ( in * v1alpha1 . Job Controller Configuration , out * config . Job Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Job Controller Configuration_To_config_Job Controller } 
func Convert_config_Job Controller Configuration_To_v1alpha1_Job Controller Configuration ( in * config . Job Controller Configuration , out * v1alpha1 . Job Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Job Controller Configuration_To_v1alpha1_Job Controller } 
func Parallelize Until ( ctx context . Context , workers , pieces int , do Work Piece Do Work Piece to for i := 0 ; i < pieces ; i ++ { to close ( to wg := sync . Wait for i := 0 ; i < workers ; i ++ { go func ( ) { defer utilruntime . Handle for piece := range to default : do Work } 
func New Cluster Role Binding Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Cluster Role Binding Informer ( client , resync } 
func Get C Advisor Custom Metrics Definition Path ( container * v1 . Container ) ( * string , error ) { // Assumes that the container has Custom Metrics enabled if it has "/etc/custom-metrics" directory // mounted as a volume. Custom Metrics definition is expected to be in "definition.json". if container . Volume Mounts != nil { for _ , volume Mount := range container . Volume Mounts { if path . Clean ( volume Mount . Mount Path ) == path . Clean ( Custom Metrics Definition Dir ) { // TODO: add definition file validation. definition Path := path . Clean ( path . Join ( volume Mount . Mount Path , Custom Metrics Definition Container return & definition } 
func Parse CSR ( obj * Certificate Signing Request ) ( * x509 . Certificate Request , error ) { // extract PEM from request object pem block , _ := pem . Decode ( pem csr , err := x509 . Parse Certificate } 
func New Broadcaster ( queue Length int , full Channel Behavior Full Channel Behavior ) * Broadcaster { m := & Broadcaster { watchers : map [ int64 ] * broadcaster Watcher { } , incoming : make ( chan Event , incoming Queue Length ) , watch Queue Length : queue Length , full Channel Behavior : full Channel } 
func ( b * Broadcaster ) block Queue ( f func ( ) ) { var wg sync . Wait b . incoming <- Event { Type : internal Run Function Marker , Object : function Fake Runtime } 
func ( m * Broadcaster ) Watch ( ) Interface { var w * broadcaster m . block id := m . next m . next w = & broadcaster Watcher { result : make ( chan Event , m . watch Queue } 
func ( m * Broadcaster ) Watch With Prefix ( queued Events [ ] Event ) Interface { var w * broadcaster m . block id := m . next m . next length := m . watch Queue if n := len ( queued w = & broadcaster for _ , e := range queued } 
func ( m * Broadcaster ) stop } 
func ( m * Broadcaster ) close // Delete everything from the map, since presence/absence in the map is used // by stop Watching to avoid double-closing the channel. m . watchers = map [ int64 ] * broadcaster } 
func ( m * Broadcaster ) Action ( action Event } 
if m . full Channel Behavior == Drop If Channel } 
func ( mw * broadcaster mw . m . stop } 
func ( c * Fake Resource Quotas ) Get ( name string , options v1 . Get Options ) ( result * corev1 . Resource Quota , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( resourcequotas Resource , c . ns , name ) , & corev1 . Resource return obj . ( * corev1 . Resource } 
func ( c * Fake Resource Quotas ) List ( opts v1 . List Options ) ( result * corev1 . Resource Quota List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( resourcequotas Resource , resourcequotas Kind , c . ns , opts ) , & corev1 . Resource Quota label , _ , _ := testing . Extract From List list := & corev1 . Resource Quota List { List Meta : obj . ( * corev1 . Resource Quota List ) . List for _ , item := range obj . ( * corev1 . Resource Quota } 
func ( c * Fake Resource Quotas ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( resourcequotas } 
func ( c * Fake Resource Quotas ) Create ( resource Quota * corev1 . Resource Quota ) ( result * corev1 . Resource Quota , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( resourcequotas Resource , c . ns , resource Quota ) , & corev1 . Resource return obj . ( * corev1 . Resource } 
func ( c * Fake Resource Quotas ) Update ( resource Quota * corev1 . Resource Quota ) ( result * corev1 . Resource Quota , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( resourcequotas Resource , c . ns , resource Quota ) , & corev1 . Resource return obj . ( * corev1 . Resource } 
func ( c * Fake Resource Quotas ) Update Status ( resource Quota * corev1 . Resource Quota ) ( * corev1 . Resource Quota , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( resourcequotas Resource , " " , c . ns , resource Quota ) , & corev1 . Resource return obj . ( * corev1 . Resource } 
func ( c * Fake Resource Quotas ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( resourcequotas Resource , c . ns , name ) , & corev1 . Resource } 
func ( c * Fake Resource Quotas ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( resourcequotas Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & corev1 . Resource Quota } 
func ( c * Fake Resource Quotas ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * corev1 . Resource Quota , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( resourcequotas Resource , c . ns , name , pt , data , subresources ... ) , & corev1 . Resource return obj . ( * corev1 . Resource } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * List Options ) ( nil ) , ( * v1 . List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_internalversion_List Options_To_v1_List Options ( a . ( * List Options ) , b . ( * v1 . List if err := s . Add Generated Conversion Func ( ( * v1 . List Options ) ( nil ) , ( * List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_List Options_To_internalversion_List Options ( a . ( * v1 . List Options ) , b . ( * List if err := s . Add Conversion Func ( ( * List Options ) ( nil ) , ( * v1 . List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_internalversion_List Options_To_v1_List Options ( a . ( * List Options ) , b . ( * v1 . List if err := s . Add Conversion Func ( ( * v1 . List Options ) ( nil ) , ( * List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_List Options_To_internalversion_List Options ( a . ( * v1 . List Options ) , b . ( * List } 
func Convert_internalversion_List_To_v1_List ( in * List , out * v1 . List , s conversion . Scope ) error { return auto } 
func Convert_v1_List_To_internalversion_List ( in * v1 . List , out * List , s conversion . Scope ) error { return auto } 
func ( e * events ) Patch With Event Namespace ( incomplete Event * v1 . Event , data [ ] byte ) ( * v1 . Event , error ) { if e . ns != " " && incomplete Event . Namespace != e . ns { return nil , fmt . Errorf ( " " , incomplete err := e . client . Patch ( types . Strategic Merge Patch Type ) . Namespace If Scoped ( incomplete Event . Namespace , len ( incomplete Event . Namespace ) > 0 ) . Resource ( " " ) . Name ( incomplete } 
func get VMI Dby Nodename ( pc * PC Cloud , node Name string ) ( string , error ) { photon Client , err := get Photon vm List , err := photon Client . Projects . Get V Ms ( pc . proj if err != nil { klog . Errorf ( " " , pc . proj ID , node for _ , vm := range vm List . Items { if vm . Name == node return " " , fmt . Errorf ( " " , node } 
func get VMI Dby IP ( pc * PC Cloud , IP Address string ) ( string , error ) { photon Client , err := get Photon vm List , err := photon Client . Projects . Get V Ms ( pc . proj if err != nil { klog . Errorf ( " " , pc . proj for _ , vm := range vm List . Items { task , err := photon Client . V Ms . Get } else { task , err = photon } else { network Connections := task . Resource networks := network if val , ok := network [ " " ] ; ok && val != nil { ip if ip Addr == IP return " " , fmt . Errorf ( " " , IP } 
func ( pc * PC Cloud ) List ( filter string ) ( [ ] k8stypes . Node } 
func ( pc * PC Cloud ) Node Addresses ( ctx context . Context , node Name k8stypes . Node Name ) ( [ ] v1 . Node Address , error ) { node Addrs := [ ] v1 . Node name := string ( node if name == pc . local K8s return node } else { for _ , addr := range addrs { if ipnet , ok := addr . ( * net . IP Net ) ; ok && ! ipnet . IP . Is Loopback ( ) { if ipnet . IP . To4 ( ) != nil { // Filter external IP by MAC address OU Is from v Center and from ESX if strings . Has Prefix ( i . Hardware Addr . String ( ) , MAC_OUI_VC ) || strings . Has Prefix ( i . Hardware Addr . String ( ) , MAC_OUI_ESX ) { nodehelpers . Add To Node Addresses ( & node Addrs , v1 . Node Address { Type : v1 . Node External } else { nodehelpers . Add To Node Addresses ( & node Addrs , v1 . Node Address { Type : v1 . Node Internal return node // Inquiring IP addresses from photon controller endpoint only for a node other than this node. // This is assumed to be done by master only. vm ID , err := get Instance return node photon Client , err := get Photon return node // Retrieve the Photon VM's IP addresses from the Photon Controller endpoint based on the VM ID vm List , err := photon Client . Projects . Get V Ms ( pc . proj if err != nil { klog . Errorf ( " " , pc . proj return node for _ , vm := range vm List . Items { if vm . ID == vm ID { task , err := photon Client . V Ms . Get return node } else { task , err = photon return node } else { network Connections := task . Resource networks := network for _ , nt := range networks { ip mac if val , ok := network [ " " ] ; ok && val != nil { ip if val , ok := network [ " " ] ; ok && val != nil { mac if ip Addr != " " { if strings . Has Prefix ( mac Addr , MAC_OUI_VC ) || strings . Has Prefix ( mac Addr , MAC_OUI_ESX ) { nodehelpers . Add To Node Addresses ( & node Addrs , v1 . Node Address { Type : v1 . Node External IP , Address : ip } else { nodehelpers . Add To Node Addresses ( & node Addrs , v1 . Node Address { Type : v1 . Node Internal IP , Address : ip return node return node } 
func ( pc * PC Cloud ) Node Addresses By Provider ID ( ctx context . Context , provider ID string ) ( [ ] v1 . Node Address , error ) { return [ ] v1 . Node Address { } , cloudprovider . Not } 
func ( pc * PC Cloud ) Instance Exists By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { return false , cloudprovider . Not } 
func ( pc * PC Cloud ) Instance ID ( ctx context . Context , node Name k8stypes . Node Name ) ( string , error ) { name := string ( node if name == pc . local K8s Hostname { return pc . local Instance // We assume only master need to get Instance ID of a node other than itself id , err := get Instance } 
func ( pc * PC Cloud ) Instance Type By Provider ID ( ctx context . Context , provider ID string ) ( string , error ) { return " " , cloudprovider . Not } 
func ( pc * PC Cloud ) Get Zone By Provider ID ( ctx context . Context , provider } 
func ( pc * PC Cloud ) Get Zone By Node Name ( ctx context . Context , node Name k8stypes . Node } 
func ( pc * PC Cloud ) Detach Disk ( ctx context . Context , pd ID string , node Name k8stypes . Node Name ) error { photon Client , err := get Photon operation := & photon . Vm Disk Operation { Disk ID : pd vm ID , err := pc . Instance ID ( ctx , node task , err := photon Client . V Ms . Detach Disk ( vm if err != nil { klog . Errorf ( " " , pd _ , err = photon if err != nil { klog . Errorf ( " " , pd } 
func ( pc * PC Cloud ) Disk Is Attached ( ctx context . Context , pd ID string , node Name k8stypes . Node Name ) ( bool , error ) { photon Client , err := get Photon disk , err := photon Client . Disks . Get ( pd if err != nil { klog . Errorf ( " " , pd vm ID , err := pc . Instance ID ( ctx , node if err == cloudprovider . Instance Not Found { klog . Infof ( " " , node Name , pd for _ , vm := range disk . V Ms { if vm == vm } 
func ( pc * PC Cloud ) Disks Are Attached ( ctx context . Context , pd I Ds [ ] string , node Name k8stypes . Node photon Client , err := get Photon for _ , pd ID := range pd I Ds { attached [ pd vm ID , err := pc . Instance ID ( ctx , node if err == cloudprovider . Instance Not Found { klog . Infof ( " " , node for _ , pd ID := range pd I Ds { disk , err := photon Client . Disks . Get ( pd if err != nil { klog . Warningf ( " " , pd } else { for _ , vm := range disk . V Ms { if vm == vm ID { attached [ pd } 
func ( pc * PC Cloud ) Create Disk ( volume Options * Volume Options ) ( pd ID string , err error ) { photon Client , err := get Photon disk Spec := photon . Disk Create disk Spec . Name = volume disk Spec . Flavor = volume disk Spec . Capacity GB = volume Options . Capacity disk Spec . Kind = Disk Spec task , err := photon Client . Projects . Create Disk ( pc . proj ID , & disk wait Task , err := photon return wait } 
func ( pc * PC Cloud ) Delete Disk ( pd ID string ) error { photon Client , err := get Photon task , err := photon Client . Disks . Delete ( pd _ , err = photon } 
func Create Rand } 
func Encrypt Bytes ( data , key [ ] byte ) ( [ ] byte , error ) { block , err := aes . New gcm , err := cipher . New nonce , err := Create Rand Bytes ( uint32 ( gcm . Nonce } 
func Decrypt Bytes ( data , key [ ] byte ) ( [ ] byte , error ) { block , err := aes . New gcm , err := cipher . New nonce Size := gcm . Nonce if len ( data ) < nonce nonce , out := data [ : nonce Size ] , data [ nonce } 
func Generate Endpoint ( ) ( string , error ) { addr , err := net . Resolve TCP l , err := net . Listen return fmt . Sprintf ( " " , l . Addr ( ) . ( * net . TCP } 
func With Authorization ( handler http . Handler , a authorizer . Authorizer , s runtime . Negotiated return http . Handler Func ( func ( w http . Response ae := request . Audit Event attributes , err := Get Authorizer if err != nil { responsewriters . Internal // an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here. if authorized == authorizer . Decision Allow { audit . Log Annotation ( ae , decision Annotation Key , decision audit . Log Annotation ( ae , reason Annotation handler . Serve if err != nil { audit . Log Annotation ( ae , reason Annotation Key , reason responsewriters . Internal klog . V ( 4 ) . Infof ( " " , req . Request audit . Log Annotation ( ae , decision Annotation Key , decision audit . Log Annotation ( ae , reason Annotation } 
func Register Cloud Provider ( name string , cloud Factory ) { providers defer providers } 
func Is Cloud Provider ( name string ) bool { providers defer providers } 
func Get Cloud Provider ( name string , config io . Reader ) ( Interface , error ) { providers defer providers } 
func Init Cloud Provider ( name string , config File if Is for _ , provider := range deprecated Cloud if config File config , err = os . Open ( config File if err != nil { klog . Fatalf ( " " , config File cloud , err = Get Cloud } else { // Pass explicit nil so plugins can actually check for nil. See // "Why is my nil error value not equal to nil?" in golang.org/doc/faq. cloud , err = Get Cloud } 
func Can Use IP Tables Proxier ( iptver IP Tables Versioner , kcompat Kernel Compat Tester ) ( bool , error ) { min Version , err := utilversion . Parse Generic ( iptables Min version String , err := iptver . Get version , err := utilversion . Parse Generic ( version if version . Less Than ( min // Check that the kernel supports what we need. if err := kcompat . Is } 
func new Service Info ( port * v1 . Service Port , service * v1 . Service , base Info * proxy . Base Service Info ) proxy . Service Port { info := & service Info { Base Service Info : base // Store the following for performance reasons. svc Name := types . Namespaced svc Port Name := proxy . Service Port Name { Namespaced Name : svc protocol := strings . To info . service Name String = svc Port info . service Port Chain Name = service Port Chain Name ( info . service Name info . service Firewall Chain Name = service Firewall Chain Name ( info . service Name info . service LB Chain Name = service LB Chain Name ( info . service Name } 
func ( e * endpoints Info ) Equal ( other proxy . Endpoint ) bool { o , ok := other . ( * endpoints return e . Endpoint == o . Endpoint && e . Is Local == o . Is Local && e . protocol == o . protocol && e . chain Name == o . chain } 
func ( e * endpoints Info ) endpoint Chain ( svc Name e . chain Name = service Port Endpoint Chain Name ( svc Name return e . chain } 
func New Proxier ( ipt utiliptables . Interface , sysctl utilsysctl . Interface , exec utilexec . Interface , sync Period time . Duration , min Sync Period time . Duration , masquerade All bool , masquerade Bit int , cluster CIDR string , hostname string , node IP net . IP , recorder record . Event Recorder , healthz Server healthcheck . Healthz Updater , node Port Addresses [ ] string , ) ( * Proxier , error ) { // Set the route_localnet sysctl we need for if val , _ := sysctl . Get Sysctl ( sysctl Route Localnet ) ; val != 1 { if err := sysctl . Set Sysctl ( sysctl Route Localnet , 1 ) ; err != nil { return nil , fmt . Errorf ( " " , sysctl Route // Proxy needs br_netfilter and bridge-nf-call-iptables=1 when containers // are connected to a Linux bridge (but not SDN bridges). Until most // plugins handle this, log when config is missing if val , err := sysctl . Get Sysctl ( sysctl Bridge Call IP // Generate the masquerade mark to use for SNAT rules. masquerade Value := 1 << uint ( masquerade masquerade Mark := fmt . Sprintf ( " " , masquerade Value , masquerade if node node IP = net . Parse if len ( cluster } else if utilnet . Is I Pv6CIDR String ( cluster CIDR ) != ipt . Is Ipv6 ( ) { return nil , fmt . Errorf ( " " , cluster CIDR , ipt . Is health Checker := healthcheck . New is I Pv6 := ipt . Is proxier := & Proxier { ports Map : make ( map [ utilproxy . Local Port ] utilproxy . Closeable ) , service Map : make ( proxy . Service Map ) , service Changes : proxy . New Service Change Tracker ( new Service Info , & is I Pv6 , recorder ) , endpoints Map : make ( proxy . Endpoints Map ) , endpoints Changes : proxy . New Endpoint Change Tracker ( hostname , new Endpoint Info , & is I Pv6 , recorder ) , iptables : ipt , masquerade All : masquerade All , masquerade Mark : masquerade Mark , exec : exec , cluster CIDR : cluster CIDR , hostname : hostname , node IP : node IP , port Mapper : & listen Port Opener { } , recorder : recorder , health Checker : health Checker , healthz Server : healthz Server , precomputed Probabilities : make ( [ ] string , 0 , 1001 ) , iptables Data : bytes . New Buffer ( nil ) , existing Filter Chains Data : bytes . New Buffer ( nil ) , filter Chains : bytes . New Buffer ( nil ) , filter Rules : bytes . New Buffer ( nil ) , nat Chains : bytes . New Buffer ( nil ) , nat Rules : bytes . New Buffer ( nil ) , node Port Addresses : node Port Addresses , network Interfacer : utilproxy . Real burst klog . V ( 3 ) . Infof ( " " , min Sync Period , sync Period , burst proxier . sync Runner = async . New Bounded Frequency Runner ( " " , proxier . sync Proxy Rules , min Sync Period , sync Period , burst } 
func Cleanup Leftovers ( ipt utiliptables . Interface ) ( encountered Error bool ) { // Unlink our chains for _ , jump := range append ( iptables Jump Chains , iptables Cleanup Only Chains ... ) { args := append ( jump . extra Args , " " , " " , " " , jump . comment , " " , string ( jump . dst if err := ipt . Delete Rule ( jump . table , jump . src Chain , args ... ) ; err != nil { if ! utiliptables . Is Not Found encountered // Flush and remove all of our "-t nat" chains. iptables Data := bytes . New if err := ipt . Save Into ( utiliptables . Table NAT , iptables Data ) ; err != nil { klog . Errorf ( " " , utiliptables . Table encountered } else { existing NAT Chains := utiliptables . Get Chain Lines ( utiliptables . Table NAT , iptables nat Chains := bytes . New nat Rules := bytes . New write Line ( nat // Start with chains we know we need to remove. for _ , chain := range [ ] utiliptables . Chain { kube Services Chain , kube Node Ports Chain , kube Postrouting Chain , Kube Mark Masq Chain } { if _ , found := existing NAT Chains [ chain ] ; found { chain write Bytes Line ( nat Chains , existing NAT write Line ( nat Rules , " " , chain // Hunt for service and endpoint chains. for chain := range existing NAT Chains { chain if strings . Has Prefix ( chain String , " " ) || strings . Has Prefix ( chain String , " " ) || strings . Has Prefix ( chain String , " " ) || strings . Has Prefix ( chain String , " " ) { write Bytes Line ( nat Chains , existing NAT write Line ( nat Rules , " " , chain write Line ( nat nat Lines := append ( nat Chains . Bytes ( ) , nat // Write it. err = ipt . Restore ( utiliptables . Table NAT , nat Lines , utiliptables . No Flush Tables , utiliptables . Restore if err != nil { klog . Errorf ( " \n \n " , utiliptables . Table NAT , err , string ( nat encountered // Flush and remove all of our "-t filter" chains. iptables if err := ipt . Save Into ( utiliptables . Table Filter , iptables Data ) ; err != nil { klog . Errorf ( " " , utiliptables . Table encountered } else { existing Filter Chains := utiliptables . Get Chain Lines ( utiliptables . Table Filter , iptables filter Chains := bytes . New filter Rules := bytes . New write Line ( filter for _ , chain := range [ ] utiliptables . Chain { kube Services Chain , kube External Services Chain , kube Forward Chain } { if _ , found := existing Filter Chains [ chain ] ; found { chain write Bytes Line ( filter Chains , existing Filter write Line ( filter Rules , " " , chain write Line ( filter filter Lines := append ( filter Chains . Bytes ( ) , filter // Write it. if err := ipt . Restore ( utiliptables . Table Filter , filter Lines , utiliptables . No Flush Tables , utiliptables . Restore Counters ) ; err != nil { klog . Errorf ( " \n \n " , utiliptables . Table Filter , err , string ( filter encountered return encountered } 
func ( proxier * Proxier ) precompute Probabilities ( number Of Precomputed int ) { if len ( proxier . precomputed Probabilities ) == 0 { proxier . precomputed Probabilities = append ( proxier . precomputed for i := len ( proxier . precomputed Probabilities ) ; i <= number Of Precomputed ; i ++ { proxier . precomputed Probabilities = append ( proxier . precomputed Probabilities , compute } 
func ( proxier * Proxier ) probability ( n int ) string { if n >= len ( proxier . precomputed Probabilities ) { proxier . precompute return proxier . precomputed } 
func port Proto Hash ( service Port Name string , protocol string ) string { hash := sha256 . Sum256 ( [ ] byte ( service Port encoded := base32 . Std Encoding . Encode To } 
func service Port Chain Name ( service Port Name string , protocol string ) utiliptables . Chain { return utiliptables . Chain ( " " + port Proto Hash ( service Port } 
func service Firewall Chain Name ( service Port Name string , protocol string ) utiliptables . Chain { return utiliptables . Chain ( " " + port Proto Hash ( service Port } 
func service LB Chain Name ( service Port Name string , protocol string ) utiliptables . Chain { return utiliptables . Chain ( " " + port Proto Hash ( service Port } 
func service Port Endpoint Chain Name ( service Port Name string , protocol string , endpoint string ) utiliptables . Chain { hash := sha256 . Sum256 ( [ ] byte ( service Port encoded := base32 . Std Encoding . Encode To } 
func ( proxier * Proxier ) delete Endpoint Connections ( connection Map [ ] proxy . Service Endpoint ) { for _ , ep Svc Pair := range connection Map { if svc Info , ok := proxier . service Map [ ep Svc Pair . Service Port Name ] ; ok && svc Info . Get Protocol ( ) == v1 . Protocol UDP { endpoint IP := utilproxy . IP Part ( ep Svc node Port := svc Info . Get Node if node Port != 0 { err = conntrack . Clear Entries For Port NAT ( proxier . exec , endpoint IP , node Port , v1 . Protocol } else { err = conntrack . Clear Entries For NAT ( proxier . exec , svc Info . Cluster IP String ( ) , endpoint IP , v1 . Protocol if err != nil { klog . Errorf ( " " , ep Svc Pair . Service Port for _ , ext IP := range svc Info . External IP Strings ( ) { err := conntrack . Clear Entries For NAT ( proxier . exec , ext IP , endpoint IP , v1 . Protocol if err != nil { klog . Errorf ( " " , ep Svc Pair . Service Port Name . String ( ) , ext for _ , lb IP := range svc Info . Load Balancer IP Strings ( ) { err := conntrack . Clear Entries For NAT ( proxier . exec , lb IP , endpoint IP , v1 . Protocol if err != nil { klog . Errorf ( " " , ep Svc Pair . Service Port Name . String ( ) , lb } 
func ( proxier * Proxier ) append Service Comment Locked ( args [ ] string , svc Name string ) { // Not printing these comments, can reduce size of iptables (in case of large // number of endpoints) even by 40%+. So if total number of endpoint chains // is large enough, we simply drop those comments. if proxier . endpoint Chains Number > endpoint Chains Number args = append ( args , " " , " " , " " , svc } 
func ( proxier * Proxier ) sync Proxy defer func ( ) { metrics . Sync Proxy Rules Latency . Observe ( metrics . Since In metrics . Deprecated Sync Proxy Rules Latency . Observe ( metrics . Since In // don't sync rules till we've received services and endpoints if ! proxier . endpoints Synced || ! proxier . services // We assume that if this was called, we really want to sync them, // even if nothing changed in the meantime. In other words, callers are // responsible for detecting no-op changes and not calling this function. service Update Result := proxy . Update Service Map ( proxier . service Map , proxier . service endpoint Update Result := proxy . Update Endpoints Map ( proxier . endpoints Map , proxier . endpoints stale Services := service Update Result . UDP Stale Cluster // merge stale services gathered from update Endpoints Map for _ , svc Port Name := range endpoint Update Result . Stale Service Names { if svc Info , ok := proxier . service Map [ svc Port Name ] ; ok && svc Info != nil && svc Info . Get Protocol ( ) == v1 . Protocol UDP { klog . V ( 2 ) . Infof ( " " , svc Port Name , svc Info . Cluster IP stale Services . Insert ( svc Info . Cluster IP for _ , ext IP := range svc Info . External IP Strings ( ) { stale Services . Insert ( ext // Create and link the kube chains. for _ , jump := range iptables Jump Chains { if _ , err := proxier . iptables . Ensure Chain ( jump . table , jump . dst Chain ) ; err != nil { klog . Errorf ( " " , jump . table , jump . dst args := append ( jump . extra Args , " " , " " , " " , jump . comment , " " , string ( jump . dst if _ , err := proxier . iptables . Ensure Rule ( utiliptables . Prepend , jump . table , jump . src Chain , args ... ) ; err != nil { klog . Errorf ( " " , jump . table , jump . src Chain , jump . dst // // Below this point we will not return until we try to write the iptables rules. // // Get iptables-save output so we can check for existing chains and rules. // This will be a map of chain name to chain with rules as stored in iptables-save/iptables-restore existing Filter proxier . existing Filter Chains err := proxier . iptables . Save Into ( utiliptables . Table Filter , proxier . existing Filter Chains } else { // otherwise parse the output existing Filter Chains = utiliptables . Get Chain Lines ( utiliptables . Table Filter , proxier . existing Filter Chains // IMPORTANT: existing NAT Chains may share memory with proxier.iptables Data. existing NAT proxier . iptables err = proxier . iptables . Save Into ( utiliptables . Table NAT , proxier . iptables } else { // otherwise parse the output existing NAT Chains = utiliptables . Get Chain Lines ( utiliptables . Table NAT , proxier . iptables // Reset all buffers used later. // This is to avoid memory reallocations and thus improve performance. proxier . filter proxier . filter proxier . nat proxier . nat // Write table headers. write Line ( proxier . filter write Line ( proxier . nat // Make sure we keep stats for the top-level chains, if they existed // (which most should have because we created them above). for _ , chain Name := range [ ] utiliptables . Chain { kube Services Chain , kube External Services Chain , kube Forward Chain } { if chain , ok := existing Filter Chains [ chain Name ] ; ok { write Bytes Line ( proxier . filter } else { write Line ( proxier . filter Chains , utiliptables . Make Chain Line ( chain for _ , chain Name := range [ ] utiliptables . Chain { kube Services Chain , kube Node Ports Chain , kube Postrouting Chain , Kube Mark Masq Chain } { if chain , ok := existing NAT Chains [ chain Name ] ; ok { write Bytes Line ( proxier . nat } else { write Line ( proxier . nat Chains , utiliptables . Make Chain Line ( chain // Install the kubernetes-specific postrouting rules. We use a whole chain for // this so that it is easier to flush and change, for example if the mark // value should ever change. write Line ( proxier . nat Rules , [ ] string { " " , string ( kube Postrouting Chain ) , " " , " " , " " , `"kubernetes service traffic requiring SNAT"` , " " , " " , " " , proxier . masquerade // Install the kubernetes-specific masquerade mark rule. We use a whole chain for // this so that it is easier to flush and change, for example if the mark // value should ever change. write Line ( proxier . nat Rules , [ ] string { " " , string ( Kube Mark Masq Chain ) , " " , " " , " " , proxier . masquerade // Accumulate NAT chains to keep. active NAT // Accumulate the set of local ports that we will be holding open once this update is complete replacement Ports Map := map [ utilproxy . Local // We are creating those slices ones here to avoid memory reallocations // in every loop. Note that reuse the memory, instead of doing: // slice = <some new slice> // you should always do one of the below: // slice = slice[:0] // and then append to it // slice = append(slice[:0], ...) endpoints := make ( [ ] * endpoints endpoint // Compute total number of endpoint chains across all services. proxier . endpoint Chains for svc Name := range proxier . service Map { proxier . endpoint Chains Number += len ( proxier . endpoints Map [ svc // Build rules for each service. for svc Name , svc := range proxier . service Map { svc Info , ok := svc . ( * service if ! ok { klog . Errorf ( " " , svc is I Pv6 := utilnet . Is I Pv6 ( svc Info . Cluster protocol := strings . To Lower ( string ( svc svc Name String := svc Info . service Name has Endpoints := len ( proxier . endpoints Map [ svc svc Chain := svc Info . service Port Chain if has Endpoints { // Create the per-service chain, retaining counters if possible. if chain , ok := existing NAT Chains [ svc Chain ] ; ok { write Bytes Line ( proxier . nat } else { write Line ( proxier . nat Chains , utiliptables . Make Chain Line ( svc active NAT Chains [ svc svc Xlb Chain := svc Info . service LB Chain if svc Info . Only Node Local Endpoints { // Only for services request Only Local traffic // create the per-service LB chain, retaining counters if possible. if lb Chain , ok := existing NAT Chains [ svc Xlb Chain ] ; ok { write Bytes Line ( proxier . nat Chains , lb } else { write Line ( proxier . nat Chains , utiliptables . Make Chain Line ( svc Xlb active NAT Chains [ svc Xlb // Capture the cluster IP. if has Endpoints { args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s cluster IP"` , svc Name String ) , " " , protocol , " " , protocol , " " , utilproxy . To CIDR ( svc Info . Cluster IP ) , " " , strconv . Itoa ( svc if proxier . masquerade All { write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Mark Masq } else if len ( proxier . cluster CIDR ) > 0 { // This masquerades off-cluster traffic to a service VIP. The idea // is that you can establish a static route for your Service range, // routing to any node, and that node will bridge into the Service // for you. Since that might bounce off-node, we masquerade here. // If/when we support "Local" policy for VI Ps, we should update this. write Line ( proxier . nat Rules , append ( args , " " , proxier . cluster CIDR , " " , string ( Kube Mark Masq write Line ( proxier . nat Rules , append ( args , " " , string ( svc } else { // No endpoints. write Line ( proxier . filter Rules , " " , string ( kube Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s has no endpoints"` , svc Name String ) , " " , protocol , " " , protocol , " " , utilproxy . To CIDR ( svc Info . Cluster IP ) , " " , strconv . Itoa ( svc // Capture external I Ps. for _ , external IP := range svc Info . External I Ps { // If the "external" IP happens to be an IP that is local to this // machine, hold the local port open so no other process can open it // (because the socket might open but it would never work). if local , err := utilproxy . Is Local IP ( external } else if local && ( svc Info . Get Protocol ( ) != v1 . Protocol SCTP ) { lp := utilproxy . Local Port { Description : " " + svc Name String , IP : external IP , Port : svc if proxier . ports replacement Ports Map [ lp ] = proxier . ports } else { socket , err := proxier . port Mapper . Open Local proxier . recorder . Eventf ( & v1 . Object Reference { Kind : " " , Name : proxier . hostname , UID : types . UID ( proxier . hostname ) , Namespace : " " , } , v1 . Event Type replacement Ports if has Endpoints { args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s external IP"` , svc Name String ) , " " , protocol , " " , protocol , " " , utilproxy . To CIDR ( net . Parse IP ( external IP ) ) , " " , strconv . Itoa ( svc // We have to SNAT packets to external I Ps. write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Mark Masq // Allow traffic for external I Ps that does not come from a bridge (i.e. not from a container) // nor from a local process to be forwarded to the service. // This rule roughly translates to "all traffic from off-machine". // This is imperfect in the face of network plugins that might not use a bridge, but we can revisit that later. external Traffic Only write Line ( proxier . nat Rules , append ( external Traffic Only Args , " " , string ( svc dst Local Only // Allow traffic bound for external I Ps that happen to be recognized as local I Ps to stay local. // This covers cases like GCE load-balancers which get added to the local routing table. write Line ( proxier . nat Rules , append ( dst Local Only Args , " " , string ( svc } else { // No endpoints. write Line ( proxier . filter Rules , " " , string ( kube External Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s has no endpoints"` , svc Name String ) , " " , protocol , " " , protocol , " " , utilproxy . To CIDR ( net . Parse IP ( external IP ) ) , " " , strconv . Itoa ( svc // Capture load-balancer ingress. fw Chain := svc Info . service Firewall Chain for _ , ingress := range svc Info . Load Balancer Status . Ingress { if ingress . IP != " " { if has Endpoints { // create service firewall chain if chain , ok := existing NAT Chains [ fw Chain ] ; ok { write Bytes Line ( proxier . nat } else { write Line ( proxier . nat Chains , utiliptables . Make Chain Line ( fw active NAT Chains [ fw // The service firewall rules are created based on Service Spec.load Balancer Source Ranges field. // This currently works for loadbalancers that preserves source ips. // For loadbalancers which direct traffic to service Node Port, the firewall rules will not apply. args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s loadbalancer IP"` , svc Name String ) , " " , protocol , " " , protocol , " " , utilproxy . To CIDR ( net . Parse IP ( ingress . IP ) ) , " " , strconv . Itoa ( svc // jump to service firewall chain write Line ( proxier . nat Rules , append ( args , " " , string ( fw args = append ( args [ : 0 ] , " " , string ( fw Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s loadbalancer IP"` , svc Name // Each source match rule in the FW chain may jump to either the SVC or the XLB chain chosen Chain := svc Xlb // If we are proxying globally, we need to masquerade in case we cross nodes. // If we are proxying only locally, we can retain the source IP. if ! svc Info . Only Node Local Endpoints { write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Mark Masq chosen Chain = svc if len ( svc Info . Load Balancer Source Ranges ) == 0 { // allow all sources, so jump directly to the KUBE-SVC or KUBE-XLB chain write Line ( proxier . nat Rules , append ( args , " " , string ( chosen } else { // firewall filter based on each source range allow From for _ , src := range svc Info . Load Balancer Source Ranges { write Line ( proxier . nat Rules , append ( args , " " , src , " " , string ( chosen // ignore error because it has been validated _ , cidr , _ := net . Parse if cidr . Contains ( proxier . node IP ) { allow From // generally, ip route rule was added to intercept request to loadbalancer vip from the // loadbalancer's backend hosts. In this case, request will not hit the loadbalancer but loop back directly. // Need to add the following rule to allow request on host. if allow From Node { write Line ( proxier . nat Rules , append ( args , " " , utilproxy . To CIDR ( net . Parse IP ( ingress . IP ) ) , " " , string ( chosen // If the packet was able to reach the end of firewall chain, then it did not get DNA Ted. // It means the packet cannot go thru the firewall, then mark it for DROP write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Mark Drop } else { // No endpoints. write Line ( proxier . filter Rules , " " , string ( kube Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s has no endpoints"` , svc Name String ) , " " , protocol , " " , protocol , " " , utilproxy . To CIDR ( net . Parse IP ( ingress . IP ) ) , " " , strconv . Itoa ( svc // Capture nodeports. If we had more than 2 rules it might be // worthwhile to make a new per-service chain for nodeport rules, but // with just 2 rules it ends up being a waste and a cognitive burden. if svc Info . Node Port != 0 { // Hold the local port open so no other process can open it // (because the socket might open but it would never work). addresses , err := utilproxy . Get Node Addresses ( proxier . node Port Addresses , proxier . network lps := make ( [ ] utilproxy . Local for address := range addresses { lp := utilproxy . Local Port { Description : " " + svc Name String , IP : address , Port : svc Info . Node if utilproxy . Is Zero // For ports on node I Ps, open the actual port and hold it. for _ , lp := range lps { if proxier . ports replacement Ports Map [ lp ] = proxier . ports } else if svc Info . Get Protocol ( ) != v1 . Protocol SCTP { socket , err := proxier . port Mapper . Open Local if lp . Protocol == " " { // TODO: We might have multiple services using the same port, and this will clear conntrack for all of them. // This is very low impact. The Node Port range is intentionally obscure, and unlikely to actually collide with real Services. // This only affects UDP connections, which are not common. // See issue: https://github.com/kubernetes/kubernetes/issues/49881 err := conntrack . Clear Entries For Port ( proxier . exec , lp . Port , is I Pv6 , v1 . Protocol replacement Ports if has Endpoints { args = append ( args [ : 0 ] , " " , string ( kube Node Ports Chain ) , " " , " " , " " , svc Name String , " " , protocol , " " , protocol , " " , strconv . Itoa ( svc Info . Node if ! svc Info . Only Node Local Endpoints { // Nodeports need SNAT, unless they're local. write Line ( proxier . nat Rules , append ( args , " " , string ( Kube Mark Masq // Jump to the service chain. write Line ( proxier . nat Rules , append ( args , " " , string ( svc } else { // TODO: Make all node if is I write Line ( proxier . nat Rules , append ( args , " " , loopback , " " , string ( Kube Mark Masq write Line ( proxier . nat Rules , append ( args , " " , string ( svc Xlb } else { // No endpoints. write Line ( proxier . filter Rules , " " , string ( kube External Services Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s has no endpoints"` , svc Name String ) , " " , " " , " " , " " , " " , protocol , " " , protocol , " " , strconv . Itoa ( svc Info . Node if ! has endpoint Chains = endpoint var endpoint for _ , ep := range proxier . endpoints Map [ svc Name ] { ep Info , ok := ep . ( * endpoints endpoints = append ( endpoints , ep endpoint Chain = ep Info . endpoint Chain ( svc Name endpoint Chains = append ( endpoint Chains , endpoint // Create the endpoint chain, retaining counters if possible. if chain , ok := existing NAT Chains [ utiliptables . Chain ( endpoint Chain ) ] ; ok { write Bytes Line ( proxier . nat } else { write Line ( proxier . nat Chains , utiliptables . Make Chain Line ( endpoint active NAT Chains [ endpoint // First write session affinity rules, if applicable. if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { for _ , endpoint Chain := range endpoint Chains { args = append ( args [ : 0 ] , " " , string ( svc proxier . append Service Comment Locked ( args , svc Name args = append ( args , " " , " " , " " , string ( endpoint Chain ) , " " , " " , strconv . Itoa ( svc Info . Sticky Max Age Seconds ) , " " , " " , string ( endpoint write Line ( proxier . nat // Now write loadbalancing & DNAT rules. n := len ( endpoint local Endpoints := make ( [ ] * endpoints local Endpoint for i , endpoint Chain := range endpoint Chains { // Write ingress loadbalancing & DNAT rules only for services that request Only Local traffic. if svc Info . Only Node Local Endpoints && endpoints [ i ] . Is Local { // These slices parallel each other; must be kept in sync local Endpoints = append ( local local Endpoint Chains = append ( local Endpoint Chains , endpoint ep if ep // Balancing rules in the per-service chain. args = append ( args [ : 0 ] , " " , string ( svc proxier . append Service Comment Locked ( args , svc Name // The final (or only if n == 1) rule is a guaranteed match. args = append ( args , " " , string ( endpoint write Line ( proxier . nat // Rules in the per-endpoint chain. args = append ( args [ : 0 ] , " " , string ( endpoint proxier . append Service Comment Locked ( args , svc Name // Handle traffic that loops back to the originator with SNAT. write Line ( proxier . nat Rules , append ( args , " " , utilproxy . To CIDR ( net . Parse IP ( ep IP ) ) , " " , string ( Kube Mark Masq // Update client-affinity lists. if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { args = append ( args , " " , " " , " " , string ( endpoint write Line ( proxier . nat // The logic below this applies only if this service is marked as Only Local if ! svc Info . Only Node Local // First rule in the chain redirects all pod -> external VIP traffic to the // Service's Cluster IP instead. This happens whether or not we have local // endpoints; only if cluster CIDR is specified if len ( proxier . cluster CIDR ) > 0 { args = append ( args [ : 0 ] , " " , string ( svc Xlb Chain ) , " " , " " , " " , `"Redirect pods trying to reach external loadbalancer VIP to cluster IP"` , " " , proxier . cluster CIDR , " " , string ( svc write Line ( proxier . nat num Local Endpoints := len ( local Endpoint if num Local Endpoints == 0 { // Blackhole all traffic since there are no local endpoints args = append ( args [ : 0 ] , " " , string ( svc Xlb Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s has no local endpoints"` , svc Name String ) , " " , string ( Kube Mark Drop write Line ( proxier . nat } else { // First write session affinity rules only over local endpoints, if applicable. if svc Info . Session Affinity Type == v1 . Service Affinity Client IP { for _ , endpoint Chain := range local Endpoint Chains { write Line ( proxier . nat Rules , " " , string ( svc Xlb Chain ) , " " , " " , " " , svc Name String , " " , " " , " " , string ( endpoint Chain ) , " " , " " , strconv . Itoa ( svc Info . Sticky Max Age Seconds ) , " " , " " , string ( endpoint // Setup probability filter rules only over local endpoints for i , endpoint Chain := range local Endpoint Chains { // Balancing rules in the per-service chain. args = append ( args [ : 0 ] , " " , string ( svc Xlb Chain ) , " " , " " , " " , fmt . Sprintf ( `"Balancing rule %d for %s"` , i , svc Name if i < ( num Local Endpoints - 1 ) { // Each rule is a probabilistic match. args = append ( args , " " , " " , " " , " " , " " , proxier . probability ( num Local // The final (or only if n == 1) rule is a guaranteed match. args = append ( args , " " , string ( endpoint write Line ( proxier . nat // Delete chains no longer in use. for chain := range existing NAT Chains { if ! active NAT Chains [ chain ] { chain if ! strings . Has Prefix ( chain String , " " ) && ! strings . Has Prefix ( chain String , " " ) && ! strings . Has Prefix ( chain String , " " ) && ! strings . Has Prefix ( chain // We must (as per iptables) write a chain-line for it, which has // the nice effect of flushing the chain. Then we can remove the // chain. write Bytes Line ( proxier . nat Chains , existing NAT write Line ( proxier . nat Rules , " " , chain // Finally, tail-call to the nodeports chain. This needs to be after all // other service portal rules. addresses , err := utilproxy . Get Node Addresses ( proxier . node Port Addresses , proxier . network } else { is I Pv6 := proxier . iptables . Is for address := range addresses { // TODO(thockin, m1093782566): If/when we have dual-stack support we will want to distinguish v4 from v6 zero-CID Rs. if utilproxy . Is Zero CIDR ( address ) { args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , `"kubernetes service nodeports; NOTE: this must be the last rule in this chain"` , " " , " " , " " , " " , " " , string ( kube Node Ports write Line ( proxier . nat // Ignore IP addresses with incorrect version if is I Pv6 && ! utilnet . Is I Pv6String ( address ) || ! is I Pv6 && utilnet . Is I // create nodeport rules for each IP one by one args = append ( args [ : 0 ] , " " , string ( kube Services Chain ) , " " , " " , " " , `"kubernetes service nodeports; NOTE: this must be the last rule in this chain"` , " " , address , " " , string ( kube Node Ports write Line ( proxier . nat // Drop the packets in INVALID state, which would potentially cause // unexpected connection reset. // https://github.com/kubernetes/kubernetes/issues/74839 write Line ( proxier . filter Rules , " " , string ( kube Forward // If the masquerade Mark has been added then we want to forward that same // traffic, this allows Node Port traffic to be forwarded even if the default // FORWARD policy is not accept. write Line ( proxier . filter Rules , " " , string ( kube Forward Chain ) , " " , " " , " " , `"kubernetes forwarding rules"` , " " , " " , " " , proxier . masquerade // The following rules can only be set if cluster CIDR has been defined. if len ( proxier . cluster CIDR ) != 0 { // The following two rules ensure the traffic after the initial packet // accepted by the "kubernetes forwarding rules" rule above will be // accepted, to be as specific as possible the traffic must be sourced // or destined to the cluster CIDR (to/from a pod). write Line ( proxier . filter Rules , " " , string ( kube Forward Chain ) , " " , proxier . cluster write Line ( proxier . filter Rules , " " , string ( kube Forward Chain ) , " " , " " , " " , `"kubernetes forwarding conntrack pod destination rule"` , " " , proxier . cluster // Write the end-of-table markers. write Line ( proxier . filter write Line ( proxier . nat // Sync rules. // NOTE: No Flush Tables is used so we don't flush non-kubernetes chains in the table proxier . iptables proxier . iptables Data . Write ( proxier . filter proxier . iptables Data . Write ( proxier . filter proxier . iptables Data . Write ( proxier . nat proxier . iptables Data . Write ( proxier . nat klog . V ( 5 ) . Infof ( " " , proxier . iptables err = proxier . iptables . Restore All ( proxier . iptables Data . Bytes ( ) , utiliptables . No Flush Tables , utiliptables . Restore if err != nil { klog . Errorf ( " \n \n " , err , proxier . iptables utilproxy . Revert Ports ( replacement Ports Map , proxier . ports for _ , last Change Trigger Time := range endpoint Update Result . Last Change Trigger Times { latency := metrics . Since In Seconds ( last Change Trigger metrics . Network Programming // Close old local ports and save new ones. for k , v := range proxier . ports Map { if replacement Ports proxier . ports Map = replacement Ports // Update healthz timestamp. if proxier . healthz Server != nil { proxier . healthz Server . Update // Update healthchecks. The endpoints list might include services that are // not "Only Local", but the services list will not, and the health Checker // will just drop those endpoints. if err := proxier . health Checker . Sync Services ( service Update Result . HC Service Node if err := proxier . health Checker . Sync Endpoints ( endpoint Update Result . HC Endpoints Local IP // Finish housekeeping. // TODO: these could be made more consistent. for _ , svc IP := range stale Services . Unsorted List ( ) { if err := conntrack . Clear Entries For IP ( proxier . exec , svc IP , v1 . Protocol UDP ) ; err != nil { klog . Errorf ( " " , svc proxier . delete Endpoint Connections ( endpoint Update Result . Stale } 
func ( in * API Service ) Deep Copy Into ( out * API out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * API Service ) Deep Copy ( ) * API out := new ( API in . Deep Copy } 
func ( in * API Service ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * API Service Condition ) Deep Copy Into ( out * API Service in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * API Service Condition ) Deep Copy ( ) * API Service out := new ( API Service in . Deep Copy } 
func ( in * API Service List ) Deep Copy Into ( out * API Service out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] API for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * API Service List ) Deep Copy ( ) * API Service out := new ( API Service in . Deep Copy } 
func ( in * API Service List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * API Service Spec ) Deep Copy ( ) * API Service out := new ( API Service in . Deep Copy } 
func ( in * API Service Status ) Deep Copy Into ( out * API Service * out = make ( [ ] API Service for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * API Service Status ) Deep Copy ( ) * API Service out := new ( API Service in . Deep Copy } 
func ( in * Service Reference ) Deep Copy Into ( out * Service } 
func Periodically Invalidate ( cache Available AP Is Getter , interval time . Duration , stop Ch <- chan struct { } ) { ticker := time . New case <- stop } 
func New For Config ( base Config * rest . Config , mapper meta . REST Mapper , available AP Is Available AP Is Getter ) Custom Metrics Client { return & multi Client { clients : make ( map [ schema . Group Version ] Custom Metrics Client ) , available AP Is : available AP Is , new Client : func ( ver schema . Group Version ) ( Custom Metrics Client , error ) { return New For Version For Config ( rest . Copy Config ( base } 
func ( c * multi Client ) get Preferred Client ( ) ( Custom Metrics Client , error ) { pref , err := c . available AP Is . Preferred c . mu . R c . mu . R client , err = c . new } 
func Resource List ( resources v1 . Resource List ) string { resource for key , value := range resources { resource Strings = append ( resource // sort the results for consistent log output sort . Strings ( resource return strings . Join ( resource } 
func ( v Element Building Visitor ) map Element ( meta apply . Field Meta Impl , item * map Item ) ( * apply . Map Element , error ) { // Function to return schema type of the map values var fn schema Fn = func ( string ) proto . Schema { // All map values share the same schema if item . Map != nil && item . Map . Sub Type != nil { return item . Map . Sub // Collect same fields from multiple maps into a map of elements values , err := v . create Map Values ( fn , meta , item . Map Element // Return the result return & apply . Map Element { Field Meta Impl : meta , Map Element Data : item . Map Element } 
func ( v Element Building Visitor ) create Map Values ( schema Fn schema Fn , meta apply . Field Meta Impl , data apply . Map Element for _ , key := range keys Union ( data . Get Recorded Map ( ) , data . Get Local Map ( ) , data . Get Remote Map ( ) ) { combined := apply . Raw Element if recorded , recorded Set := nil Safe Lookup ( key , data . Get Recorded Map ( ) ) ; recorded Set { combined . Set if local , local Set := nil Safe Lookup ( key , data . Get Local Map ( ) ) ; local Set { combined . Set if remote , remote Set := nil Safe Lookup ( key , data . Get Remote Map ( ) ) ; remote Set { combined . Set // Create an item for the field field , err := v . get Item ( schema // Build the element for this field element , err := field . Create } 
func New Patch Transformer ( slice [ ] * resource . Resource , rf * resource . Factory ) ( transformers . Transformer , error ) { if len ( slice ) == 0 { return transformers . New No Op return & patch } 
func ( pt * patch Transformer ) Transform ( base Resource Map resmap . Res Map ) error { // Merge and then index the patches by Id. patches , err := pt . merge matched Ids := base Resource Map . Get Matching Ids ( id . Gvkn if len ( matched Ids ) == 0 { return fmt . Errorf ( " " , id . Gvkn if len ( matched Ids ) > 1 { return fmt . Errorf ( " " , matched id = matched base := base Resource versioned Obj , err := scheme . Scheme . New ( to Schema base Name := base . Get switch { case runtime . Is Not Registered Error ( err ) : // Use JSON merge patch to handle types w/o schema base patch merged Bytes , err := jsonpatch . Merge Patch ( base Bytes , patch err = json . Unmarshal ( merged default : // Use Strategic-Merge-Patch to handle types w/ schema // TODO: Change this to use the new Merge package. // Store the name of the base object, because this name may have been munged. // Apply this name to the patched object. lookup Patch Meta , err := strategicpatch . New Patch Meta From Struct ( versioned merged , err = strategicpatch . Strategic Merge Map Patch Using Lookup Patch Meta ( base . Map ( ) , patch . Map ( ) , lookup Patch base . Set Name ( base base Resource Map [ id ] . Set } 
func ( pt * patch Transformer ) merge Patches ( ) ( resmap . Res Map , error ) { rc := resmap . Res versioned Obj , err := scheme . Scheme . New ( to Schema if err != nil && ! runtime . Is Not Registered var cd conflict if err != nil { cd = new JMP Conflict } else { cd , err = new SMP Conflict Detector ( versioned conflict , err := cd . has if conflict { conflicting Patch , err := cd . find return nil , fmt . Errorf ( " " , conflicting merged , err := cd . merge } 
func to Schema Gvk ( x gvk . Gvk ) schema . Group Version Kind { return schema . Group Version } 
func ( c * Fake Controller Revisions ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( controllerrevisions } 
func ( c * Fake Controller Revisions ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( controllerrevisions Resource , c . ns , name ) , & v1beta2 . Controller } 
func ( c * Fake Controller Revisions ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( controllerrevisions Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta2 . Controller Revision } 
func ( c * Fake Controller Revisions ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta2 . Controller Revision , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( controllerrevisions Resource , c . ns , name , pt , data , subresources ... ) , & v1beta2 . Controller return obj . ( * v1beta2 . Controller } 
func ( exp Backoff * Exponential Backoff ) Safe To Retry ( operation Name string ) error { if time . Since ( exp Backoff . last Error Time ) <= exp Backoff . duration Before Retry { return New Exponential Backoff Error ( operation Name , * exp } 
func New Exponential Backoff Error ( operation Name string , exp Backoff Exponential Backoff ) error { return exponential Backoff Error { operation Name : operation Name , exp Backoff : exp } 
func ( c * Fake Role Bindings ) Get ( name string , options v1 . Get Options ) ( result * v1alpha1 . Role Binding , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( rolebindings Resource , c . ns , name ) , & v1alpha1 . Role return obj . ( * v1alpha1 . Role } 
func ( c * Fake Role Bindings ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( rolebindings } 
func ( c * Fake Role Bindings ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1alpha1 . Role Binding , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( rolebindings Resource , c . ns , name , pt , data , subresources ... ) , & v1alpha1 . Role return obj . ( * v1alpha1 . Role } 
func ( s * image Review Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Image Review , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1alpha1 . Image } 
func ( s * image Review Lister ) Get ( name string ) ( * v1alpha1 . Image Review , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1alpha1 . Image } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Policy Spec ) ( nil ) , ( * abac . Policy Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Policy Spec_To_abac_Policy Spec ( a . ( * Policy Spec ) , b . ( * abac . Policy if err := s . Add Generated Conversion Func ( ( * abac . Policy Spec ) ( nil ) , ( * Policy Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_abac_Policy Spec_To_v1beta1_Policy Spec ( a . ( * abac . Policy Spec ) , b . ( * Policy } 
func Convert_v1beta1_Policy_To_abac_Policy ( in * Policy , out * abac . Policy , s conversion . Scope ) error { return auto } 
func Convert_abac_Policy_To_v1beta1_Policy ( in * abac . Policy , out * Policy , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Policy Spec_To_abac_Policy Spec ( in * Policy Spec , out * abac . Policy Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Policy Spec_To_abac_Policy } 
func Convert_abac_Policy Spec_To_v1beta1_Policy Spec ( in * abac . Policy Spec , out * Policy Spec , s conversion . Scope ) error { return auto Convert_abac_Policy Spec_To_v1beta1_Policy } 
func scale Condition ( r * Job Psuedo Scaler , precondition * Scale Precondition , namespace , name string , count uint , updated Resource Version * string ) wait . Condition Func { return func ( ) ( bool , error ) { rv , err := r . Scale if updated Resource Version != nil { * updated Resource // Retry only on update conflicts. if errors . Is } 
func ( scaler * Job Psuedo Scaler ) Scale Simple ( namespace , name string , preconditions * Scale Precondition , new Size uint ) ( string , error ) { job , err := scaler . Jobs Client . Jobs ( namespace ) . Get ( name , metav1 . Get if preconditions != nil { if err := validate parallelism := int32 ( new updated Job , err := scaler . Jobs return updated Job . Object Meta . Resource } 
func ( scaler * Job Psuedo Scaler ) Scale ( namespace , name string , new Size uint , preconditions * Scale Precondition , retry , wait For Replicas * Retry Params ) error { if preconditions == nil { preconditions = & Scale if retry == nil { // Make it try only once, immediately retry = & Retry cond := scale Condition ( scaler , preconditions , namespace , name , new if err := wait . Poll if wait For Replicas != nil { job , err := scaler . Jobs Client . Jobs ( namespace ) . Get ( name , metav1 . Get err = wait . Poll Immediate ( wait For Replicas . Interval , wait For Replicas . Timeout , job Has Desired Parallelism ( scaler . Jobs if err == wait . Err Wait } 
func job Has Desired Parallelism ( job Client batchclient . Jobs Getter , job * batch . Job ) wait . Condition Func { return func ( ) ( bool , error ) { job , err := job Client . Jobs ( job . Namespace ) . Get ( job . Name , metav1 . Get } 
func ( v merge Strategy ) Merge List ( e apply . List Element ) ( apply . Result , error ) { // No merge logic if adding or deleting a field if result , done := v . do Add Or // Detect conflict in List Element if err := v . do Conflict switch m . Operation { case apply . SET : // Keep the list item value merged = append ( merged , m . Merged if len ( merged ) == 0 { // If the list is empty, return a nil entry return apply . Result { Operation : apply . SET , Merged // Return the merged list, and tell the caller to keep it return apply . Result { Operation : apply . SET , Merged } 
func ( v merge Strategy ) Merge Map ( e apply . Map Element ) ( apply . Result , error ) { // No merge logic if adding or deleting a field if result , done := v . do Add Or // Detect conflict in Map Element if err := v . do Conflict return v . do Merge Map ( e . Get } 
func ( v merge Strategy ) Merge Type ( e apply . Type Element ) ( apply . Result , error ) { // No merge logic if adding or deleting a field if result , done := v . do Add Or // Detect conflict in Type Element if err := v . do Conflict return v . do Merge Map ( e . Get } 
func ( v merge Strategy ) do Merge switch result . Operation { case apply . SET : // Keep the map item value merged [ key ] = result . Merged // Return the merged map, and tell the caller to keep it if len ( merged ) == 0 { // Special case the empty map to set the field value to nil, but keep the field key // This is how the tests expect the structures to look when parsed from yaml return apply . Result { Operation : apply . SET , Merged return apply . Result { Operation : apply . SET , Merged } 
func ( v merge Strategy ) Merge Primitive ( diff apply . Primitive } 
func ( v merge Strategy ) Merge Empty ( diff apply . Empty } 
func ( v merge Strategy ) do Conflict Detect ( e apply . Element ) error { return v . strategic . do Conflict } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . AWS Elastic Block Store Volume Source ) ( nil ) , ( * core . AWS Elastic Block Store Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_AWS Elastic Block Store Volume Source_To_core_AWS Elastic Block Store Volume Source ( a . ( * v1 . AWS Elastic Block Store Volume Source ) , b . ( * core . AWS Elastic Block Store Volume if err := s . Add Generated Conversion Func ( ( * core . AWS Elastic Block Store Volume Source ) ( nil ) , ( * v1 . AWS Elastic Block Store Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_AWS Elastic Block Store Volume Source_To_v1_AWS Elastic Block Store Volume Source ( a . ( * core . AWS Elastic Block Store Volume Source ) , b . ( * v1 . AWS Elastic Block Store Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Attached Volume ) ( nil ) , ( * core . Attached Volume ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Attached Volume_To_core_Attached Volume ( a . ( * v1 . Attached Volume ) , b . ( * core . Attached if err := s . Add Generated Conversion Func ( ( * core . Attached Volume ) ( nil ) , ( * v1 . Attached Volume ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Attached Volume_To_v1_Attached Volume ( a . ( * core . Attached Volume ) , b . ( * v1 . Attached if err := s . Add Generated Conversion Func ( ( * v1 . Avoid Pods ) ( nil ) , ( * core . Avoid Pods ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Avoid Pods_To_core_Avoid Pods ( a . ( * v1 . Avoid Pods ) , b . ( * core . Avoid if err := s . Add Generated Conversion Func ( ( * core . Avoid Pods ) ( nil ) , ( * v1 . Avoid Pods ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Avoid Pods_To_v1_Avoid Pods ( a . ( * core . Avoid Pods ) , b . ( * v1 . Avoid if err := s . Add Generated Conversion Func ( ( * v1 . Azure Disk Volume Source ) ( nil ) , ( * core . Azure Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Azure Disk Volume Source_To_core_Azure Disk Volume Source ( a . ( * v1 . Azure Disk Volume Source ) , b . ( * core . Azure Disk Volume if err := s . Add Generated Conversion Func ( ( * core . Azure Disk Volume Source ) ( nil ) , ( * v1 . Azure Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Azure Disk Volume Source_To_v1_Azure Disk Volume Source ( a . ( * core . Azure Disk Volume Source ) , b . ( * v1 . Azure Disk Volume if err := s . Add Generated Conversion Func ( ( * v1 . Azure File Persistent Volume Source ) ( nil ) , ( * core . Azure File Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Azure File Persistent Volume Source_To_core_Azure File Persistent Volume Source ( a . ( * v1 . Azure File Persistent Volume Source ) , b . ( * core . Azure File Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Azure File Persistent Volume Source ) ( nil ) , ( * v1 . Azure File Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Azure File Persistent Volume Source_To_v1_Azure File Persistent Volume Source ( a . ( * core . Azure File Persistent Volume Source ) , b . ( * v1 . Azure File Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Azure File Volume Source ) ( nil ) , ( * core . Azure File Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Azure File Volume Source_To_core_Azure File Volume Source ( a . ( * v1 . Azure File Volume Source ) , b . ( * core . Azure File Volume if err := s . Add Generated Conversion Func ( ( * core . Azure File Volume Source ) ( nil ) , ( * v1 . Azure File Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Azure File Volume Source_To_v1_Azure File Volume Source ( a . ( * core . Azure File Volume Source ) , b . ( * v1 . Azure File Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . CSI Persistent Volume Source ) ( nil ) , ( * core . CSI Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_CSI Persistent Volume Source_To_core_CSI Persistent Volume Source ( a . ( * v1 . CSI Persistent Volume Source ) , b . ( * core . CSI Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . CSI Persistent Volume Source ) ( nil ) , ( * v1 . CSI Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_CSI Persistent Volume Source_To_v1_CSI Persistent Volume Source ( a . ( * core . CSI Persistent Volume Source ) , b . ( * v1 . CSI Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . CSI Volume Source ) ( nil ) , ( * core . CSI Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_CSI Volume Source_To_core_CSI Volume Source ( a . ( * v1 . CSI Volume Source ) , b . ( * core . CSI Volume if err := s . Add Generated Conversion Func ( ( * core . CSI Volume Source ) ( nil ) , ( * v1 . CSI Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_CSI Volume Source_To_v1_CSI Volume Source ( a . ( * core . CSI Volume Source ) , b . ( * v1 . CSI Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Ceph FS Persistent Volume Source ) ( nil ) , ( * core . Ceph FS Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Ceph FS Persistent Volume Source_To_core_Ceph FS Persistent Volume Source ( a . ( * v1 . Ceph FS Persistent Volume Source ) , b . ( * core . Ceph FS Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Ceph FS Persistent Volume Source ) ( nil ) , ( * v1 . Ceph FS Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Ceph FS Persistent Volume Source_To_v1_Ceph FS Persistent Volume Source ( a . ( * core . Ceph FS Persistent Volume Source ) , b . ( * v1 . Ceph FS Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Ceph FS Volume Source ) ( nil ) , ( * core . Ceph FS Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Ceph FS Volume Source_To_core_Ceph FS Volume Source ( a . ( * v1 . Ceph FS Volume Source ) , b . ( * core . Ceph FS Volume if err := s . Add Generated Conversion Func ( ( * core . Ceph FS Volume Source ) ( nil ) , ( * v1 . Ceph FS Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Ceph FS Volume Source_To_v1_Ceph FS Volume Source ( a . ( * core . Ceph FS Volume Source ) , b . ( * v1 . Ceph FS Volume if err := s . Add Generated Conversion Func ( ( * v1 . Cinder Persistent Volume Source ) ( nil ) , ( * core . Cinder Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cinder Persistent Volume Source_To_core_Cinder Persistent Volume Source ( a . ( * v1 . Cinder Persistent Volume Source ) , b . ( * core . Cinder Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Cinder Persistent Volume Source ) ( nil ) , ( * v1 . Cinder Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Cinder Persistent Volume Source_To_v1_Cinder Persistent Volume Source ( a . ( * core . Cinder Persistent Volume Source ) , b . ( * v1 . Cinder Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Cinder Volume Source ) ( nil ) , ( * core . Cinder Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cinder Volume Source_To_core_Cinder Volume Source ( a . ( * v1 . Cinder Volume Source ) , b . ( * core . Cinder Volume if err := s . Add Generated Conversion Func ( ( * core . Cinder Volume Source ) ( nil ) , ( * v1 . Cinder Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Cinder Volume Source_To_v1_Cinder Volume Source ( a . ( * core . Cinder Volume Source ) , b . ( * v1 . Cinder Volume if err := s . Add Generated Conversion Func ( ( * v1 . Client IP Config ) ( nil ) , ( * core . Client IP Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Client IP Config_To_core_Client IP Config ( a . ( * v1 . Client IP Config ) , b . ( * core . Client IP if err := s . Add Generated Conversion Func ( ( * core . Client IP Config ) ( nil ) , ( * v1 . Client IP Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Client IP Config_To_v1_Client IP Config ( a . ( * core . Client IP Config ) , b . ( * v1 . Client IP if err := s . Add Generated Conversion Func ( ( * v1 . Component Condition ) ( nil ) , ( * core . Component Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Component Condition_To_core_Component Condition ( a . ( * v1 . Component Condition ) , b . ( * core . Component if err := s . Add Generated Conversion Func ( ( * core . Component Condition ) ( nil ) , ( * v1 . Component Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Component Condition_To_v1_Component Condition ( a . ( * core . Component Condition ) , b . ( * v1 . Component if err := s . Add Generated Conversion Func ( ( * v1 . Component Status ) ( nil ) , ( * core . Component Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Component Status_To_core_Component Status ( a . ( * v1 . Component Status ) , b . ( * core . Component if err := s . Add Generated Conversion Func ( ( * core . Component Status ) ( nil ) , ( * v1 . Component Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Component Status_To_v1_Component Status ( a . ( * core . Component Status ) , b . ( * v1 . Component if err := s . Add Generated Conversion Func ( ( * v1 . Component Status List ) ( nil ) , ( * core . Component Status List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Component Status List_To_core_Component Status List ( a . ( * v1 . Component Status List ) , b . ( * core . Component Status if err := s . Add Generated Conversion Func ( ( * core . Component Status List ) ( nil ) , ( * v1 . Component Status List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Component Status List_To_v1_Component Status List ( a . ( * core . Component Status List ) , b . ( * v1 . Component Status if err := s . Add Generated Conversion Func ( ( * v1 . Config Map ) ( nil ) , ( * core . Config Map ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map_To_core_Config Map ( a . ( * v1 . Config Map ) , b . ( * core . Config if err := s . Add Generated Conversion Func ( ( * core . Config Map ) ( nil ) , ( * v1 . Config Map ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map_To_v1_Config Map ( a . ( * core . Config Map ) , b . ( * v1 . Config if err := s . Add Generated Conversion Func ( ( * v1 . Config Map Env Source ) ( nil ) , ( * core . Config Map Env Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map Env Source_To_core_Config Map Env Source ( a . ( * v1 . Config Map Env Source ) , b . ( * core . Config Map Env if err := s . Add Generated Conversion Func ( ( * core . Config Map Env Source ) ( nil ) , ( * v1 . Config Map Env Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map Env Source_To_v1_Config Map Env Source ( a . ( * core . Config Map Env Source ) , b . ( * v1 . Config Map Env if err := s . Add Generated Conversion Func ( ( * v1 . Config Map Key Selector ) ( nil ) , ( * core . Config Map Key Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map Key Selector_To_core_Config Map Key Selector ( a . ( * v1 . Config Map Key Selector ) , b . ( * core . Config Map Key if err := s . Add Generated Conversion Func ( ( * core . Config Map Key Selector ) ( nil ) , ( * v1 . Config Map Key Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map Key Selector_To_v1_Config Map Key Selector ( a . ( * core . Config Map Key Selector ) , b . ( * v1 . Config Map Key if err := s . Add Generated Conversion Func ( ( * v1 . Config Map List ) ( nil ) , ( * core . Config Map List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map List_To_core_Config Map List ( a . ( * v1 . Config Map List ) , b . ( * core . Config Map if err := s . Add Generated Conversion Func ( ( * core . Config Map List ) ( nil ) , ( * v1 . Config Map List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map List_To_v1_Config Map List ( a . ( * core . Config Map List ) , b . ( * v1 . Config Map if err := s . Add Generated Conversion Func ( ( * v1 . Config Map Node Config Source ) ( nil ) , ( * core . Config Map Node Config Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map Node Config Source_To_core_Config Map Node Config Source ( a . ( * v1 . Config Map Node Config Source ) , b . ( * core . Config Map Node Config if err := s . Add Generated Conversion Func ( ( * core . Config Map Node Config Source ) ( nil ) , ( * v1 . Config Map Node Config Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map Node Config Source_To_v1_Config Map Node Config Source ( a . ( * core . Config Map Node Config Source ) , b . ( * v1 . Config Map Node Config if err := s . Add Generated Conversion Func ( ( * v1 . Config Map Projection ) ( nil ) , ( * core . Config Map Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map Projection_To_core_Config Map Projection ( a . ( * v1 . Config Map Projection ) , b . ( * core . Config Map if err := s . Add Generated Conversion Func ( ( * core . Config Map Projection ) ( nil ) , ( * v1 . Config Map Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map Projection_To_v1_Config Map Projection ( a . ( * core . Config Map Projection ) , b . ( * v1 . Config Map if err := s . Add Generated Conversion Func ( ( * v1 . Config Map Volume Source ) ( nil ) , ( * core . Config Map Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Config Map Volume Source_To_core_Config Map Volume Source ( a . ( * v1 . Config Map Volume Source ) , b . ( * core . Config Map Volume if err := s . Add Generated Conversion Func ( ( * core . Config Map Volume Source ) ( nil ) , ( * v1 . Config Map Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Config Map Volume Source_To_v1_Config Map Volume Source ( a . ( * core . Config Map Volume Source ) , b . ( * v1 . Config Map Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Container Image ) ( nil ) , ( * core . Container Image ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container Image_To_core_Container Image ( a . ( * v1 . Container Image ) , b . ( * core . Container if err := s . Add Generated Conversion Func ( ( * core . Container Image ) ( nil ) , ( * v1 . Container Image ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container Image_To_v1_Container Image ( a . ( * core . Container Image ) , b . ( * v1 . Container if err := s . Add Generated Conversion Func ( ( * v1 . Container Port ) ( nil ) , ( * core . Container Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container Port_To_core_Container Port ( a . ( * v1 . Container Port ) , b . ( * core . Container if err := s . Add Generated Conversion Func ( ( * core . Container Port ) ( nil ) , ( * v1 . Container Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container Port_To_v1_Container Port ( a . ( * core . Container Port ) , b . ( * v1 . Container if err := s . Add Generated Conversion Func ( ( * v1 . Container State ) ( nil ) , ( * core . Container State ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container State_To_core_Container State ( a . ( * v1 . Container State ) , b . ( * core . Container if err := s . Add Generated Conversion Func ( ( * core . Container State ) ( nil ) , ( * v1 . Container State ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container State_To_v1_Container State ( a . ( * core . Container State ) , b . ( * v1 . Container if err := s . Add Generated Conversion Func ( ( * v1 . Container State Running ) ( nil ) , ( * core . Container State Running ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container State Running_To_core_Container State Running ( a . ( * v1 . Container State Running ) , b . ( * core . Container State if err := s . Add Generated Conversion Func ( ( * core . Container State Running ) ( nil ) , ( * v1 . Container State Running ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container State Running_To_v1_Container State Running ( a . ( * core . Container State Running ) , b . ( * v1 . Container State if err := s . Add Generated Conversion Func ( ( * v1 . Container State Terminated ) ( nil ) , ( * core . Container State Terminated ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container State Terminated_To_core_Container State Terminated ( a . ( * v1 . Container State Terminated ) , b . ( * core . Container State if err := s . Add Generated Conversion Func ( ( * core . Container State Terminated ) ( nil ) , ( * v1 . Container State Terminated ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container State Terminated_To_v1_Container State Terminated ( a . ( * core . Container State Terminated ) , b . ( * v1 . Container State if err := s . Add Generated Conversion Func ( ( * v1 . Container State Waiting ) ( nil ) , ( * core . Container State Waiting ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container State Waiting_To_core_Container State Waiting ( a . ( * v1 . Container State Waiting ) , b . ( * core . Container State if err := s . Add Generated Conversion Func ( ( * core . Container State Waiting ) ( nil ) , ( * v1 . Container State Waiting ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container State Waiting_To_v1_Container State Waiting ( a . ( * core . Container State Waiting ) , b . ( * v1 . Container State if err := s . Add Generated Conversion Func ( ( * v1 . Container Status ) ( nil ) , ( * core . Container Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Container Status_To_core_Container Status ( a . ( * v1 . Container Status ) , b . ( * core . Container if err := s . Add Generated Conversion Func ( ( * core . Container Status ) ( nil ) , ( * v1 . Container Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Container Status_To_v1_Container Status ( a . ( * core . Container Status ) , b . ( * v1 . Container if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Endpoint ) ( nil ) , ( * core . Daemon Endpoint ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Endpoint_To_core_Daemon Endpoint ( a . ( * v1 . Daemon Endpoint ) , b . ( * core . Daemon if err := s . Add Generated Conversion Func ( ( * core . Daemon Endpoint ) ( nil ) , ( * v1 . Daemon Endpoint ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Daemon Endpoint_To_v1_Daemon Endpoint ( a . ( * core . Daemon Endpoint ) , b . ( * v1 . Daemon if err := s . Add Generated Conversion Func ( ( * v1 . Downward API Projection ) ( nil ) , ( * core . Downward API Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Downward API Projection_To_core_Downward API Projection ( a . ( * v1 . Downward API Projection ) , b . ( * core . Downward API if err := s . Add Generated Conversion Func ( ( * core . Downward API Projection ) ( nil ) , ( * v1 . Downward API Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Downward API Projection_To_v1_Downward API Projection ( a . ( * core . Downward API Projection ) , b . ( * v1 . Downward API if err := s . Add Generated Conversion Func ( ( * v1 . Downward API Volume File ) ( nil ) , ( * core . Downward API Volume File ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Downward API Volume File_To_core_Downward API Volume File ( a . ( * v1 . Downward API Volume File ) , b . ( * core . Downward API Volume if err := s . Add Generated Conversion Func ( ( * core . Downward API Volume File ) ( nil ) , ( * v1 . Downward API Volume File ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Downward API Volume File_To_v1_Downward API Volume File ( a . ( * core . Downward API Volume File ) , b . ( * v1 . Downward API Volume if err := s . Add Generated Conversion Func ( ( * v1 . Downward API Volume Source ) ( nil ) , ( * core . Downward API Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Downward API Volume Source_To_core_Downward API Volume Source ( a . ( * v1 . Downward API Volume Source ) , b . ( * core . Downward API Volume if err := s . Add Generated Conversion Func ( ( * core . Downward API Volume Source ) ( nil ) , ( * v1 . Downward API Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Downward API Volume Source_To_v1_Downward API Volume Source ( a . ( * core . Downward API Volume Source ) , b . ( * v1 . Downward API Volume if err := s . Add Generated Conversion Func ( ( * v1 . Empty Dir Volume Source ) ( nil ) , ( * core . Empty Dir Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Empty Dir Volume Source_To_core_Empty Dir Volume Source ( a . ( * v1 . Empty Dir Volume Source ) , b . ( * core . Empty Dir Volume if err := s . Add Generated Conversion Func ( ( * core . Empty Dir Volume Source ) ( nil ) , ( * v1 . Empty Dir Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Empty Dir Volume Source_To_v1_Empty Dir Volume Source ( a . ( * core . Empty Dir Volume Source ) , b . ( * v1 . Empty Dir Volume if err := s . Add Generated Conversion Func ( ( * v1 . Endpoint Address ) ( nil ) , ( * core . Endpoint Address ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Endpoint Address_To_core_Endpoint Address ( a . ( * v1 . Endpoint Address ) , b . ( * core . Endpoint if err := s . Add Generated Conversion Func ( ( * core . Endpoint Address ) ( nil ) , ( * v1 . Endpoint Address ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Endpoint Address_To_v1_Endpoint Address ( a . ( * core . Endpoint Address ) , b . ( * v1 . Endpoint if err := s . Add Generated Conversion Func ( ( * v1 . Endpoint Port ) ( nil ) , ( * core . Endpoint Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Endpoint Port_To_core_Endpoint Port ( a . ( * v1 . Endpoint Port ) , b . ( * core . Endpoint if err := s . Add Generated Conversion Func ( ( * core . Endpoint Port ) ( nil ) , ( * v1 . Endpoint Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Endpoint Port_To_v1_Endpoint Port ( a . ( * core . Endpoint Port ) , b . ( * v1 . Endpoint if err := s . Add Generated Conversion Func ( ( * v1 . Endpoint Subset ) ( nil ) , ( * core . Endpoint Subset ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Endpoint Subset_To_core_Endpoint Subset ( a . ( * v1 . Endpoint Subset ) , b . ( * core . Endpoint if err := s . Add Generated Conversion Func ( ( * core . Endpoint Subset ) ( nil ) , ( * v1 . Endpoint Subset ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Endpoint Subset_To_v1_Endpoint Subset ( a . ( * core . Endpoint Subset ) , b . ( * v1 . Endpoint if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Endpoints List ) ( nil ) , ( * core . Endpoints List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Endpoints List_To_core_Endpoints List ( a . ( * v1 . Endpoints List ) , b . ( * core . Endpoints if err := s . Add Generated Conversion Func ( ( * core . Endpoints List ) ( nil ) , ( * v1 . Endpoints List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Endpoints List_To_v1_Endpoints List ( a . ( * core . Endpoints List ) , b . ( * v1 . Endpoints if err := s . Add Generated Conversion Func ( ( * v1 . Env From Source ) ( nil ) , ( * core . Env From Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Env From Source_To_core_Env From Source ( a . ( * v1 . Env From Source ) , b . ( * core . Env From if err := s . Add Generated Conversion Func ( ( * core . Env From Source ) ( nil ) , ( * v1 . Env From Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Env From Source_To_v1_Env From Source ( a . ( * core . Env From Source ) , b . ( * v1 . Env From if err := s . Add Generated Conversion Func ( ( * v1 . Env Var ) ( nil ) , ( * core . Env Var ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Env Var_To_core_Env Var ( a . ( * v1 . Env Var ) , b . ( * core . Env if err := s . Add Generated Conversion Func ( ( * core . Env Var ) ( nil ) , ( * v1 . Env Var ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Env Var_To_v1_Env Var ( a . ( * core . Env Var ) , b . ( * v1 . Env if err := s . Add Generated Conversion Func ( ( * v1 . Env Var Source ) ( nil ) , ( * core . Env Var Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Env Var Source_To_core_Env Var Source ( a . ( * v1 . Env Var Source ) , b . ( * core . Env Var if err := s . Add Generated Conversion Func ( ( * core . Env Var Source ) ( nil ) , ( * v1 . Env Var Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Env Var Source_To_v1_Env Var Source ( a . ( * core . Env Var Source ) , b . ( * v1 . Env Var if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Event List ) ( nil ) , ( * core . Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Event List_To_core_Event List ( a . ( * v1 . Event List ) , b . ( * core . Event if err := s . Add Generated Conversion Func ( ( * core . Event List ) ( nil ) , ( * v1 . Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Event List_To_v1_Event List ( a . ( * core . Event List ) , b . ( * v1 . Event if err := s . Add Generated Conversion Func ( ( * v1 . Event Series ) ( nil ) , ( * core . Event Series ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Event Series_To_core_Event Series ( a . ( * v1 . Event Series ) , b . ( * core . Event if err := s . Add Generated Conversion Func ( ( * core . Event Series ) ( nil ) , ( * v1 . Event Series ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Event Series_To_v1_Event Series ( a . ( * core . Event Series ) , b . ( * v1 . Event if err := s . Add Generated Conversion Func ( ( * v1 . Event Source ) ( nil ) , ( * core . Event Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Event Source_To_core_Event Source ( a . ( * v1 . Event Source ) , b . ( * core . Event if err := s . Add Generated Conversion Func ( ( * core . Event Source ) ( nil ) , ( * v1 . Event Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Event Source_To_v1_Event Source ( a . ( * core . Event Source ) , b . ( * v1 . Event if err := s . Add Generated Conversion Func ( ( * v1 . Exec Action ) ( nil ) , ( * core . Exec Action ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Exec Action_To_core_Exec Action ( a . ( * v1 . Exec Action ) , b . ( * core . Exec if err := s . Add Generated Conversion Func ( ( * core . Exec Action ) ( nil ) , ( * v1 . Exec Action ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Exec Action_To_v1_Exec Action ( a . ( * core . Exec Action ) , b . ( * v1 . Exec if err := s . Add Generated Conversion Func ( ( * v1 . FC Volume Source ) ( nil ) , ( * core . FC Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_FC Volume Source_To_core_FC Volume Source ( a . ( * v1 . FC Volume Source ) , b . ( * core . FC Volume if err := s . Add Generated Conversion Func ( ( * core . FC Volume Source ) ( nil ) , ( * v1 . FC Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_FC Volume Source_To_v1_FC Volume Source ( a . ( * core . FC Volume Source ) , b . ( * v1 . FC Volume if err := s . Add Generated Conversion Func ( ( * v1 . Flex Persistent Volume Source ) ( nil ) , ( * core . Flex Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Flex Persistent Volume Source_To_core_Flex Persistent Volume Source ( a . ( * v1 . Flex Persistent Volume Source ) , b . ( * core . Flex Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Flex Persistent Volume Source ) ( nil ) , ( * v1 . Flex Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Flex Persistent Volume Source_To_v1_Flex Persistent Volume Source ( a . ( * core . Flex Persistent Volume Source ) , b . ( * v1 . Flex Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Flex Volume Source ) ( nil ) , ( * core . Flex Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Flex Volume Source_To_core_Flex Volume Source ( a . ( * v1 . Flex Volume Source ) , b . ( * core . Flex Volume if err := s . Add Generated Conversion Func ( ( * core . Flex Volume Source ) ( nil ) , ( * v1 . Flex Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Flex Volume Source_To_v1_Flex Volume Source ( a . ( * core . Flex Volume Source ) , b . ( * v1 . Flex Volume if err := s . Add Generated Conversion Func ( ( * v1 . Flocker Volume Source ) ( nil ) , ( * core . Flocker Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Flocker Volume Source_To_core_Flocker Volume Source ( a . ( * v1 . Flocker Volume Source ) , b . ( * core . Flocker Volume if err := s . Add Generated Conversion Func ( ( * core . Flocker Volume Source ) ( nil ) , ( * v1 . Flocker Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Flocker Volume Source_To_v1_Flocker Volume Source ( a . ( * core . Flocker Volume Source ) , b . ( * v1 . Flocker Volume if err := s . Add Generated Conversion Func ( ( * v1 . GCE Persistent Disk Volume Source ) ( nil ) , ( * core . GCE Persistent Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_GCE Persistent Disk Volume Source_To_core_GCE Persistent Disk Volume Source ( a . ( * v1 . GCE Persistent Disk Volume Source ) , b . ( * core . GCE Persistent Disk Volume if err := s . Add Generated Conversion Func ( ( * core . GCE Persistent Disk Volume Source ) ( nil ) , ( * v1 . GCE Persistent Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_GCE Persistent Disk Volume Source_To_v1_GCE Persistent Disk Volume Source ( a . ( * core . GCE Persistent Disk Volume Source ) , b . ( * v1 . GCE Persistent Disk Volume if err := s . Add Generated Conversion Func ( ( * v1 . Git Repo Volume Source ) ( nil ) , ( * core . Git Repo Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Git Repo Volume Source_To_core_Git Repo Volume Source ( a . ( * v1 . Git Repo Volume Source ) , b . ( * core . Git Repo Volume if err := s . Add Generated Conversion Func ( ( * core . Git Repo Volume Source ) ( nil ) , ( * v1 . Git Repo Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Git Repo Volume Source_To_v1_Git Repo Volume Source ( a . ( * core . Git Repo Volume Source ) , b . ( * v1 . Git Repo Volume if err := s . Add Generated Conversion Func ( ( * v1 . Glusterfs Persistent Volume Source ) ( nil ) , ( * core . Glusterfs Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Glusterfs Persistent Volume Source_To_core_Glusterfs Persistent Volume Source ( a . ( * v1 . Glusterfs Persistent Volume Source ) , b . ( * core . Glusterfs Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Glusterfs Persistent Volume Source ) ( nil ) , ( * v1 . Glusterfs Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Glusterfs Persistent Volume Source_To_v1_Glusterfs Persistent Volume Source ( a . ( * core . Glusterfs Persistent Volume Source ) , b . ( * v1 . Glusterfs Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Glusterfs Volume Source ) ( nil ) , ( * core . Glusterfs Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Glusterfs Volume Source_To_core_Glusterfs Volume Source ( a . ( * v1 . Glusterfs Volume Source ) , b . ( * core . Glusterfs Volume if err := s . Add Generated Conversion Func ( ( * core . Glusterfs Volume Source ) ( nil ) , ( * v1 . Glusterfs Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Glusterfs Volume Source_To_v1_Glusterfs Volume Source ( a . ( * core . Glusterfs Volume Source ) , b . ( * v1 . Glusterfs Volume if err := s . Add Generated Conversion Func ( ( * v1 . HTTP Get Action ) ( nil ) , ( * core . HTTP Get Action ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_HTTP Get Action_To_core_HTTP Get Action ( a . ( * v1 . HTTP Get Action ) , b . ( * core . HTTP Get if err := s . Add Generated Conversion Func ( ( * core . HTTP Get Action ) ( nil ) , ( * v1 . HTTP Get Action ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_HTTP Get Action_To_v1_HTTP Get Action ( a . ( * core . HTTP Get Action ) , b . ( * v1 . HTTP Get if err := s . Add Generated Conversion Func ( ( * v1 . HTTP Header ) ( nil ) , ( * core . HTTP Header ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_HTTP Header_To_core_HTTP Header ( a . ( * v1 . HTTP Header ) , b . ( * core . HTTP if err := s . Add Generated Conversion Func ( ( * core . HTTP Header ) ( nil ) , ( * v1 . HTTP Header ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_HTTP Header_To_v1_HTTP Header ( a . ( * core . HTTP Header ) , b . ( * v1 . HTTP if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Host Alias ) ( nil ) , ( * core . Host Alias ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Host Alias_To_core_Host Alias ( a . ( * v1 . Host Alias ) , b . ( * core . Host if err := s . Add Generated Conversion Func ( ( * core . Host Alias ) ( nil ) , ( * v1 . Host Alias ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Host Alias_To_v1_Host Alias ( a . ( * core . Host Alias ) , b . ( * v1 . Host if err := s . Add Generated Conversion Func ( ( * v1 . Host Path Volume Source ) ( nil ) , ( * core . Host Path Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Host Path Volume Source_To_core_Host Path Volume Source ( a . ( * v1 . Host Path Volume Source ) , b . ( * core . Host Path Volume if err := s . Add Generated Conversion Func ( ( * core . Host Path Volume Source ) ( nil ) , ( * v1 . Host Path Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Host Path Volume Source_To_v1_Host Path Volume Source ( a . ( * core . Host Path Volume Source ) , b . ( * v1 . Host Path Volume if err := s . Add Generated Conversion Func ( ( * v1 . ISCSI Persistent Volume Source ) ( nil ) , ( * core . ISCSI Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_ISCSI Persistent Volume Source_To_core_ISCSI Persistent Volume Source ( a . ( * v1 . ISCSI Persistent Volume Source ) , b . ( * core . ISCSI Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . ISCSI Persistent Volume Source ) ( nil ) , ( * v1 . ISCSI Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_ISCSI Persistent Volume Source_To_v1_ISCSI Persistent Volume Source ( a . ( * core . ISCSI Persistent Volume Source ) , b . ( * v1 . ISCSI Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . ISCSI Volume Source ) ( nil ) , ( * core . ISCSI Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_ISCSI Volume Source_To_core_ISCSI Volume Source ( a . ( * v1 . ISCSI Volume Source ) , b . ( * core . ISCSI Volume if err := s . Add Generated Conversion Func ( ( * core . ISCSI Volume Source ) ( nil ) , ( * v1 . ISCSI Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_ISCSI Volume Source_To_v1_ISCSI Volume Source ( a . ( * core . ISCSI Volume Source ) , b . ( * v1 . ISCSI Volume if err := s . Add Generated Conversion Func ( ( * v1 . Key To Path ) ( nil ) , ( * core . Key To Path ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Key To Path_To_core_Key To Path ( a . ( * v1 . Key To Path ) , b . ( * core . Key To if err := s . Add Generated Conversion Func ( ( * core . Key To Path ) ( nil ) , ( * v1 . Key To Path ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Key To Path_To_v1_Key To Path ( a . ( * core . Key To Path ) , b . ( * v1 . Key To if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Limit Range ) ( nil ) , ( * core . Limit Range ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Limit Range_To_core_Limit Range ( a . ( * v1 . Limit Range ) , b . ( * core . Limit if err := s . Add Generated Conversion Func ( ( * core . Limit Range ) ( nil ) , ( * v1 . Limit Range ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Limit Range_To_v1_Limit Range ( a . ( * core . Limit Range ) , b . ( * v1 . Limit if err := s . Add Generated Conversion Func ( ( * v1 . Limit Range Item ) ( nil ) , ( * core . Limit Range Item ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Limit Range Item_To_core_Limit Range Item ( a . ( * v1 . Limit Range Item ) , b . ( * core . Limit Range if err := s . Add Generated Conversion Func ( ( * core . Limit Range Item ) ( nil ) , ( * v1 . Limit Range Item ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Limit Range Item_To_v1_Limit Range Item ( a . ( * core . Limit Range Item ) , b . ( * v1 . Limit Range if err := s . Add Generated Conversion Func ( ( * v1 . Limit Range List ) ( nil ) , ( * core . Limit Range List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Limit Range List_To_core_Limit Range List ( a . ( * v1 . Limit Range List ) , b . ( * core . Limit Range if err := s . Add Generated Conversion Func ( ( * core . Limit Range List ) ( nil ) , ( * v1 . Limit Range List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Limit Range List_To_v1_Limit Range List ( a . ( * core . Limit Range List ) , b . ( * v1 . Limit Range if err := s . Add Generated Conversion Func ( ( * v1 . Limit Range Spec ) ( nil ) , ( * core . Limit Range Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Limit Range Spec_To_core_Limit Range Spec ( a . ( * v1 . Limit Range Spec ) , b . ( * core . Limit Range if err := s . Add Generated Conversion Func ( ( * core . Limit Range Spec ) ( nil ) , ( * v1 . Limit Range Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Limit Range Spec_To_v1_Limit Range Spec ( a . ( * core . Limit Range Spec ) , b . ( * v1 . Limit Range if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Load Balancer Ingress ) ( nil ) , ( * core . Load Balancer Ingress ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Load Balancer Ingress_To_core_Load Balancer Ingress ( a . ( * v1 . Load Balancer Ingress ) , b . ( * core . Load Balancer if err := s . Add Generated Conversion Func ( ( * core . Load Balancer Ingress ) ( nil ) , ( * v1 . Load Balancer Ingress ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Load Balancer Ingress_To_v1_Load Balancer Ingress ( a . ( * core . Load Balancer Ingress ) , b . ( * v1 . Load Balancer if err := s . Add Generated Conversion Func ( ( * v1 . Load Balancer Status ) ( nil ) , ( * core . Load Balancer Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Load Balancer Status_To_core_Load Balancer Status ( a . ( * v1 . Load Balancer Status ) , b . ( * core . Load Balancer if err := s . Add Generated Conversion Func ( ( * core . Load Balancer Status ) ( nil ) , ( * v1 . Load Balancer Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Load Balancer Status_To_v1_Load Balancer Status ( a . ( * core . Load Balancer Status ) , b . ( * v1 . Load Balancer if err := s . Add Generated Conversion Func ( ( * v1 . Local Object Reference ) ( nil ) , ( * core . Local Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Local Object Reference_To_core_Local Object Reference ( a . ( * v1 . Local Object Reference ) , b . ( * core . Local Object if err := s . Add Generated Conversion Func ( ( * core . Local Object Reference ) ( nil ) , ( * v1 . Local Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Local Object Reference_To_v1_Local Object Reference ( a . ( * core . Local Object Reference ) , b . ( * v1 . Local Object if err := s . Add Generated Conversion Func ( ( * v1 . Local Volume Source ) ( nil ) , ( * core . Local Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Local Volume Source_To_core_Local Volume Source ( a . ( * v1 . Local Volume Source ) , b . ( * core . Local Volume if err := s . Add Generated Conversion Func ( ( * core . Local Volume Source ) ( nil ) , ( * v1 . Local Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Local Volume Source_To_v1_Local Volume Source ( a . ( * core . Local Volume Source ) , b . ( * v1 . Local Volume if err := s . Add Generated Conversion Func ( ( * v1 . NFS Volume Source ) ( nil ) , ( * core . NFS Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_NFS Volume Source_To_core_NFS Volume Source ( a . ( * v1 . NFS Volume Source ) , b . ( * core . NFS Volume if err := s . Add Generated Conversion Func ( ( * core . NFS Volume Source ) ( nil ) , ( * v1 . NFS Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_NFS Volume Source_To_v1_NFS Volume Source ( a . ( * core . NFS Volume Source ) , b . ( * v1 . NFS Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Namespace List ) ( nil ) , ( * core . Namespace List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Namespace List_To_core_Namespace List ( a . ( * v1 . Namespace List ) , b . ( * core . Namespace if err := s . Add Generated Conversion Func ( ( * core . Namespace List ) ( nil ) , ( * v1 . Namespace List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Namespace List_To_v1_Namespace List ( a . ( * core . Namespace List ) , b . ( * v1 . Namespace if err := s . Add Generated Conversion Func ( ( * v1 . Namespace Spec ) ( nil ) , ( * core . Namespace Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Namespace Spec_To_core_Namespace Spec ( a . ( * v1 . Namespace Spec ) , b . ( * core . Namespace if err := s . Add Generated Conversion Func ( ( * core . Namespace Spec ) ( nil ) , ( * v1 . Namespace Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Namespace Spec_To_v1_Namespace Spec ( a . ( * core . Namespace Spec ) , b . ( * v1 . Namespace if err := s . Add Generated Conversion Func ( ( * v1 . Namespace Status ) ( nil ) , ( * core . Namespace Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Namespace Status_To_core_Namespace Status ( a . ( * v1 . Namespace Status ) , b . ( * core . Namespace if err := s . Add Generated Conversion Func ( ( * core . Namespace Status ) ( nil ) , ( * v1 . Namespace Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Namespace Status_To_v1_Namespace Status ( a . ( * core . Namespace Status ) , b . ( * v1 . Namespace if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Node Address ) ( nil ) , ( * core . Node Address ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Address_To_core_Node Address ( a . ( * v1 . Node Address ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Address ) ( nil ) , ( * v1 . Node Address ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Address_To_v1_Node Address ( a . ( * core . Node Address ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Affinity ) ( nil ) , ( * core . Node Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Affinity_To_core_Node Affinity ( a . ( * v1 . Node Affinity ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Affinity ) ( nil ) , ( * v1 . Node Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Affinity_To_v1_Node Affinity ( a . ( * core . Node Affinity ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Condition ) ( nil ) , ( * core . Node Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Condition_To_core_Node Condition ( a . ( * v1 . Node Condition ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Condition ) ( nil ) , ( * v1 . Node Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Condition_To_v1_Node Condition ( a . ( * core . Node Condition ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Config Source ) ( nil ) , ( * core . Node Config Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Config Source_To_core_Node Config Source ( a . ( * v1 . Node Config Source ) , b . ( * core . Node Config if err := s . Add Generated Conversion Func ( ( * core . Node Config Source ) ( nil ) , ( * v1 . Node Config Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Config Source_To_v1_Node Config Source ( a . ( * core . Node Config Source ) , b . ( * v1 . Node Config if err := s . Add Generated Conversion Func ( ( * v1 . Node Config Status ) ( nil ) , ( * core . Node Config Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Config Status_To_core_Node Config Status ( a . ( * v1 . Node Config Status ) , b . ( * core . Node Config if err := s . Add Generated Conversion Func ( ( * core . Node Config Status ) ( nil ) , ( * v1 . Node Config Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Config Status_To_v1_Node Config Status ( a . ( * core . Node Config Status ) , b . ( * v1 . Node Config if err := s . Add Generated Conversion Func ( ( * v1 . Node Daemon Endpoints ) ( nil ) , ( * core . Node Daemon Endpoints ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Daemon Endpoints_To_core_Node Daemon Endpoints ( a . ( * v1 . Node Daemon Endpoints ) , b . ( * core . Node Daemon if err := s . Add Generated Conversion Func ( ( * core . Node Daemon Endpoints ) ( nil ) , ( * v1 . Node Daemon Endpoints ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Daemon Endpoints_To_v1_Node Daemon Endpoints ( a . ( * core . Node Daemon Endpoints ) , b . ( * v1 . Node Daemon if err := s . Add Generated Conversion Func ( ( * v1 . Node List ) ( nil ) , ( * core . Node List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node List_To_core_Node List ( a . ( * v1 . Node List ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node List ) ( nil ) , ( * v1 . Node List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node List_To_v1_Node List ( a . ( * core . Node List ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Proxy Options ) ( nil ) , ( * core . Node Proxy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Proxy Options_To_core_Node Proxy Options ( a . ( * v1 . Node Proxy Options ) , b . ( * core . Node Proxy if err := s . Add Generated Conversion Func ( ( * core . Node Proxy Options ) ( nil ) , ( * v1 . Node Proxy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Proxy Options_To_v1_Node Proxy Options ( a . ( * core . Node Proxy Options ) , b . ( * v1 . Node Proxy if err := s . Add Generated Conversion Func ( ( * v1 . Node Resources ) ( nil ) , ( * core . Node Resources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Resources_To_core_Node Resources ( a . ( * v1 . Node Resources ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Resources ) ( nil ) , ( * v1 . Node Resources ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Resources_To_v1_Node Resources ( a . ( * core . Node Resources ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Selector ) ( nil ) , ( * core . Node Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Selector_To_core_Node Selector ( a . ( * v1 . Node Selector ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Selector ) ( nil ) , ( * v1 . Node Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Selector_To_v1_Node Selector ( a . ( * core . Node Selector ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Selector Requirement ) ( nil ) , ( * core . Node Selector Requirement ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Selector Requirement_To_core_Node Selector Requirement ( a . ( * v1 . Node Selector Requirement ) , b . ( * core . Node Selector if err := s . Add Generated Conversion Func ( ( * core . Node Selector Requirement ) ( nil ) , ( * v1 . Node Selector Requirement ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Selector Requirement_To_v1_Node Selector Requirement ( a . ( * core . Node Selector Requirement ) , b . ( * v1 . Node Selector if err := s . Add Generated Conversion Func ( ( * v1 . Node Selector Term ) ( nil ) , ( * core . Node Selector Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Selector Term_To_core_Node Selector Term ( a . ( * v1 . Node Selector Term ) , b . ( * core . Node Selector if err := s . Add Generated Conversion Func ( ( * core . Node Selector Term ) ( nil ) , ( * v1 . Node Selector Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Selector Term_To_v1_Node Selector Term ( a . ( * core . Node Selector Term ) , b . ( * v1 . Node Selector if err := s . Add Generated Conversion Func ( ( * v1 . Node Spec ) ( nil ) , ( * core . Node Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Spec_To_core_Node Spec ( a . ( * v1 . Node Spec ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Spec ) ( nil ) , ( * v1 . Node Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Spec_To_v1_Node Spec ( a . ( * core . Node Spec ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node Status ) ( nil ) , ( * core . Node Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node Status_To_core_Node Status ( a . ( * v1 . Node Status ) , b . ( * core . Node if err := s . Add Generated Conversion Func ( ( * core . Node Status ) ( nil ) , ( * v1 . Node Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node Status_To_v1_Node Status ( a . ( * core . Node Status ) , b . ( * v1 . Node if err := s . Add Generated Conversion Func ( ( * v1 . Node System Info ) ( nil ) , ( * core . Node System Info ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Node System Info_To_core_Node System Info ( a . ( * v1 . Node System Info ) , b . ( * core . Node System if err := s . Add Generated Conversion Func ( ( * core . Node System Info ) ( nil ) , ( * v1 . Node System Info ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Node System Info_To_v1_Node System Info ( a . ( * core . Node System Info ) , b . ( * v1 . Node System if err := s . Add Generated Conversion Func ( ( * v1 . Object Field Selector ) ( nil ) , ( * core . Object Field Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Field Selector_To_core_Object Field Selector ( a . ( * v1 . Object Field Selector ) , b . ( * core . Object Field if err := s . Add Generated Conversion Func ( ( * core . Object Field Selector ) ( nil ) , ( * v1 . Object Field Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Object Field Selector_To_v1_Object Field Selector ( a . ( * core . Object Field Selector ) , b . ( * v1 . Object Field if err := s . Add Generated Conversion Func ( ( * v1 . Object Reference ) ( nil ) , ( * core . Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Reference_To_core_Object Reference ( a . ( * v1 . Object Reference ) , b . ( * core . Object if err := s . Add Generated Conversion Func ( ( * core . Object Reference ) ( nil ) , ( * v1 . Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Object Reference_To_v1_Object Reference ( a . ( * core . Object Reference ) , b . ( * v1 . Object if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume ) ( nil ) , ( * core . Persistent Volume ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume_To_core_Persistent Volume ( a . ( * v1 . Persistent Volume ) , b . ( * core . Persistent if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume ) ( nil ) , ( * v1 . Persistent Volume ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume_To_v1_Persistent Volume ( a . ( * core . Persistent Volume ) , b . ( * v1 . Persistent if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Claim ) ( nil ) , ( * core . Persistent Volume Claim ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Claim_To_core_Persistent Volume Claim ( a . ( * v1 . Persistent Volume Claim ) , b . ( * core . Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Claim ) ( nil ) , ( * v1 . Persistent Volume Claim ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Claim_To_v1_Persistent Volume Claim ( a . ( * core . Persistent Volume Claim ) , b . ( * v1 . Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Claim Condition ) ( nil ) , ( * core . Persistent Volume Claim Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Claim Condition_To_core_Persistent Volume Claim Condition ( a . ( * v1 . Persistent Volume Claim Condition ) , b . ( * core . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Claim Condition ) ( nil ) , ( * v1 . Persistent Volume Claim Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Claim Condition_To_v1_Persistent Volume Claim Condition ( a . ( * core . Persistent Volume Claim Condition ) , b . ( * v1 . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Claim List ) ( nil ) , ( * core . Persistent Volume Claim List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Claim List_To_core_Persistent Volume Claim List ( a . ( * v1 . Persistent Volume Claim List ) , b . ( * core . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Claim List ) ( nil ) , ( * v1 . Persistent Volume Claim List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Claim List_To_v1_Persistent Volume Claim List ( a . ( * core . Persistent Volume Claim List ) , b . ( * v1 . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Claim Spec ) ( nil ) , ( * core . Persistent Volume Claim Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Claim Spec_To_core_Persistent Volume Claim Spec ( a . ( * v1 . Persistent Volume Claim Spec ) , b . ( * core . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Claim Spec ) ( nil ) , ( * v1 . Persistent Volume Claim Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Claim Spec_To_v1_Persistent Volume Claim Spec ( a . ( * core . Persistent Volume Claim Spec ) , b . ( * v1 . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Claim Status ) ( nil ) , ( * core . Persistent Volume Claim Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Claim Status_To_core_Persistent Volume Claim Status ( a . ( * v1 . Persistent Volume Claim Status ) , b . ( * core . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Claim Status ) ( nil ) , ( * v1 . Persistent Volume Claim Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Claim Status_To_v1_Persistent Volume Claim Status ( a . ( * core . Persistent Volume Claim Status ) , b . ( * v1 . Persistent Volume Claim if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Claim Volume Source ) ( nil ) , ( * core . Persistent Volume Claim Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Claim Volume Source_To_core_Persistent Volume Claim Volume Source ( a . ( * v1 . Persistent Volume Claim Volume Source ) , b . ( * core . Persistent Volume Claim Volume if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Claim Volume Source ) ( nil ) , ( * v1 . Persistent Volume Claim Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Claim Volume Source_To_v1_Persistent Volume Claim Volume Source ( a . ( * core . Persistent Volume Claim Volume Source ) , b . ( * v1 . Persistent Volume Claim Volume if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume List ) ( nil ) , ( * core . Persistent Volume List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume List_To_core_Persistent Volume List ( a . ( * v1 . Persistent Volume List ) , b . ( * core . Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume List ) ( nil ) , ( * v1 . Persistent Volume List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume List_To_v1_Persistent Volume List ( a . ( * core . Persistent Volume List ) , b . ( * v1 . Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Source ) ( nil ) , ( * core . Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Source_To_core_Persistent Volume Source ( a . ( * v1 . Persistent Volume Source ) , b . ( * core . Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Source ) ( nil ) , ( * v1 . Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Source_To_v1_Persistent Volume Source ( a . ( * core . Persistent Volume Source ) , b . ( * v1 . Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Spec ) ( nil ) , ( * core . Persistent Volume Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Spec_To_core_Persistent Volume Spec ( a . ( * v1 . Persistent Volume Spec ) , b . ( * core . Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Spec ) ( nil ) , ( * v1 . Persistent Volume Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Spec_To_v1_Persistent Volume Spec ( a . ( * core . Persistent Volume Spec ) , b . ( * v1 . Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Persistent Volume Status ) ( nil ) , ( * core . Persistent Volume Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Persistent Volume Status_To_core_Persistent Volume Status ( a . ( * v1 . Persistent Volume Status ) , b . ( * core . Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Persistent Volume Status ) ( nil ) , ( * v1 . Persistent Volume Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Persistent Volume Status_To_v1_Persistent Volume Status ( a . ( * core . Persistent Volume Status ) , b . ( * v1 . Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Photon Persistent Disk Volume Source ) ( nil ) , ( * core . Photon Persistent Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Photon Persistent Disk Volume Source_To_core_Photon Persistent Disk Volume Source ( a . ( * v1 . Photon Persistent Disk Volume Source ) , b . ( * core . Photon Persistent Disk Volume if err := s . Add Generated Conversion Func ( ( * core . Photon Persistent Disk Volume Source ) ( nil ) , ( * v1 . Photon Persistent Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Photon Persistent Disk Volume Source_To_v1_Photon Persistent Disk Volume Source ( a . ( * core . Photon Persistent Disk Volume Source ) , b . ( * v1 . Photon Persistent Disk Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Pod Affinity ) ( nil ) , ( * core . Pod Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Affinity_To_core_Pod Affinity ( a . ( * v1 . Pod Affinity ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod Affinity ) ( nil ) , ( * v1 . Pod Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Affinity_To_v1_Pod Affinity ( a . ( * core . Pod Affinity ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod Affinity Term ) ( nil ) , ( * core . Pod Affinity Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Affinity Term_To_core_Pod Affinity Term ( a . ( * v1 . Pod Affinity Term ) , b . ( * core . Pod Affinity if err := s . Add Generated Conversion Func ( ( * core . Pod Affinity Term ) ( nil ) , ( * v1 . Pod Affinity Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Affinity Term_To_v1_Pod Affinity Term ( a . ( * core . Pod Affinity Term ) , b . ( * v1 . Pod Affinity if err := s . Add Generated Conversion Func ( ( * v1 . Pod Anti Affinity ) ( nil ) , ( * core . Pod Anti Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Anti Affinity_To_core_Pod Anti Affinity ( a . ( * v1 . Pod Anti Affinity ) , b . ( * core . Pod Anti if err := s . Add Generated Conversion Func ( ( * core . Pod Anti Affinity ) ( nil ) , ( * v1 . Pod Anti Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Anti Affinity_To_v1_Pod Anti Affinity ( a . ( * core . Pod Anti Affinity ) , b . ( * v1 . Pod Anti if err := s . Add Generated Conversion Func ( ( * v1 . Pod Attach Options ) ( nil ) , ( * core . Pod Attach Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Attach Options_To_core_Pod Attach Options ( a . ( * v1 . Pod Attach Options ) , b . ( * core . Pod Attach if err := s . Add Generated Conversion Func ( ( * core . Pod Attach Options ) ( nil ) , ( * v1 . Pod Attach Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Attach Options_To_v1_Pod Attach Options ( a . ( * core . Pod Attach Options ) , b . ( * v1 . Pod Attach if err := s . Add Generated Conversion Func ( ( * v1 . Pod Condition ) ( nil ) , ( * core . Pod Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Condition_To_core_Pod Condition ( a . ( * v1 . Pod Condition ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod Condition ) ( nil ) , ( * v1 . Pod Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Condition_To_v1_Pod Condition ( a . ( * core . Pod Condition ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod DNS Config ) ( nil ) , ( * core . Pod DNS Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod DNS Config_To_core_Pod DNS Config ( a . ( * v1 . Pod DNS Config ) , b . ( * core . Pod DNS if err := s . Add Generated Conversion Func ( ( * core . Pod DNS Config ) ( nil ) , ( * v1 . Pod DNS Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod DNS Config_To_v1_Pod DNS Config ( a . ( * core . Pod DNS Config ) , b . ( * v1 . Pod DNS if err := s . Add Generated Conversion Func ( ( * v1 . Pod DNS Config Option ) ( nil ) , ( * core . Pod DNS Config Option ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod DNS Config Option_To_core_Pod DNS Config Option ( a . ( * v1 . Pod DNS Config Option ) , b . ( * core . Pod DNS Config if err := s . Add Generated Conversion Func ( ( * core . Pod DNS Config Option ) ( nil ) , ( * v1 . Pod DNS Config Option ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod DNS Config Option_To_v1_Pod DNS Config Option ( a . ( * core . Pod DNS Config Option ) , b . ( * v1 . Pod DNS Config if err := s . Add Generated Conversion Func ( ( * v1 . Pod Exec Options ) ( nil ) , ( * core . Pod Exec Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Exec Options_To_core_Pod Exec Options ( a . ( * v1 . Pod Exec Options ) , b . ( * core . Pod Exec if err := s . Add Generated Conversion Func ( ( * core . Pod Exec Options ) ( nil ) , ( * v1 . Pod Exec Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Exec Options_To_v1_Pod Exec Options ( a . ( * core . Pod Exec Options ) , b . ( * v1 . Pod Exec if err := s . Add Generated Conversion Func ( ( * v1 . Pod List ) ( nil ) , ( * core . Pod List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod List_To_core_Pod List ( a . ( * v1 . Pod List ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod List ) ( nil ) , ( * v1 . Pod List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod List_To_v1_Pod List ( a . ( * core . Pod List ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod Log Options ) ( nil ) , ( * core . Pod Log Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Log Options_To_core_Pod Log Options ( a . ( * v1 . Pod Log Options ) , b . ( * core . Pod Log if err := s . Add Generated Conversion Func ( ( * core . Pod Log Options ) ( nil ) , ( * v1 . Pod Log Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Log Options_To_v1_Pod Log Options ( a . ( * core . Pod Log Options ) , b . ( * v1 . Pod Log if err := s . Add Generated Conversion Func ( ( * v1 . Pod Port Forward Options ) ( nil ) , ( * core . Pod Port Forward Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Port Forward Options_To_core_Pod Port Forward Options ( a . ( * v1 . Pod Port Forward Options ) , b . ( * core . Pod Port Forward if err := s . Add Generated Conversion Func ( ( * core . Pod Port Forward Options ) ( nil ) , ( * v1 . Pod Port Forward Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Port Forward Options_To_v1_Pod Port Forward Options ( a . ( * core . Pod Port Forward Options ) , b . ( * v1 . Pod Port Forward if err := s . Add Generated Conversion Func ( ( * v1 . Pod Proxy Options ) ( nil ) , ( * core . Pod Proxy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Proxy Options_To_core_Pod Proxy Options ( a . ( * v1 . Pod Proxy Options ) , b . ( * core . Pod Proxy if err := s . Add Generated Conversion Func ( ( * core . Pod Proxy Options ) ( nil ) , ( * v1 . Pod Proxy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Proxy Options_To_v1_Pod Proxy Options ( a . ( * core . Pod Proxy Options ) , b . ( * v1 . Pod Proxy if err := s . Add Generated Conversion Func ( ( * v1 . Pod Readiness Gate ) ( nil ) , ( * core . Pod Readiness Gate ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Readiness Gate_To_core_Pod Readiness Gate ( a . ( * v1 . Pod Readiness Gate ) , b . ( * core . Pod Readiness if err := s . Add Generated Conversion Func ( ( * core . Pod Readiness Gate ) ( nil ) , ( * v1 . Pod Readiness Gate ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Readiness Gate_To_v1_Pod Readiness Gate ( a . ( * core . Pod Readiness Gate ) , b . ( * v1 . Pod Readiness if err := s . Add Generated Conversion Func ( ( * v1 . Pod Security Context ) ( nil ) , ( * core . Pod Security Context ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Security Context_To_core_Pod Security Context ( a . ( * v1 . Pod Security Context ) , b . ( * core . Pod Security if err := s . Add Generated Conversion Func ( ( * core . Pod Security Context ) ( nil ) , ( * v1 . Pod Security Context ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Security Context_To_v1_Pod Security Context ( a . ( * core . Pod Security Context ) , b . ( * v1 . Pod Security if err := s . Add Generated Conversion Func ( ( * v1 . Pod Signature ) ( nil ) , ( * core . Pod Signature ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Signature_To_core_Pod Signature ( a . ( * v1 . Pod Signature ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod Signature ) ( nil ) , ( * v1 . Pod Signature ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Signature_To_v1_Pod Signature ( a . ( * core . Pod Signature ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod Spec ) ( nil ) , ( * core . Pod Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Spec_To_core_Pod Spec ( a . ( * v1 . Pod Spec ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod Spec ) ( nil ) , ( * v1 . Pod Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Spec_To_v1_Pod Spec ( a . ( * core . Pod Spec ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod Status ) ( nil ) , ( * core . Pod Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Status_To_core_Pod Status ( a . ( * v1 . Pod Status ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod Status ) ( nil ) , ( * v1 . Pod Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Status_To_v1_Pod Status ( a . ( * core . Pod Status ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod Status Result ) ( nil ) , ( * core . Pod Status Result ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Status Result_To_core_Pod Status Result ( a . ( * v1 . Pod Status Result ) , b . ( * core . Pod Status if err := s . Add Generated Conversion Func ( ( * core . Pod Status Result ) ( nil ) , ( * v1 . Pod Status Result ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Status Result_To_v1_Pod Status Result ( a . ( * core . Pod Status Result ) , b . ( * v1 . Pod Status if err := s . Add Generated Conversion Func ( ( * v1 . Pod Template ) ( nil ) , ( * core . Pod Template ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Template_To_core_Pod Template ( a . ( * v1 . Pod Template ) , b . ( * core . Pod if err := s . Add Generated Conversion Func ( ( * core . Pod Template ) ( nil ) , ( * v1 . Pod Template ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Template_To_v1_Pod Template ( a . ( * core . Pod Template ) , b . ( * v1 . Pod if err := s . Add Generated Conversion Func ( ( * v1 . Pod Template List ) ( nil ) , ( * core . Pod Template List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Template List_To_core_Pod Template List ( a . ( * v1 . Pod Template List ) , b . ( * core . Pod Template if err := s . Add Generated Conversion Func ( ( * core . Pod Template List ) ( nil ) , ( * v1 . Pod Template List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Template List_To_v1_Pod Template List ( a . ( * core . Pod Template List ) , b . ( * v1 . Pod Template if err := s . Add Generated Conversion Func ( ( * v1 . Pod Template Spec ) ( nil ) , ( * core . Pod Template Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Template Spec_To_core_Pod Template Spec ( a . ( * v1 . Pod Template Spec ) , b . ( * core . Pod Template if err := s . Add Generated Conversion Func ( ( * core . Pod Template Spec ) ( nil ) , ( * v1 . Pod Template Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Template Spec_To_v1_Pod Template Spec ( a . ( * core . Pod Template Spec ) , b . ( * v1 . Pod Template if err := s . Add Generated Conversion Func ( ( * v1 . Portworx Volume Source ) ( nil ) , ( * core . Portworx Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Portworx Volume Source_To_core_Portworx Volume Source ( a . ( * v1 . Portworx Volume Source ) , b . ( * core . Portworx Volume if err := s . Add Generated Conversion Func ( ( * core . Portworx Volume Source ) ( nil ) , ( * v1 . Portworx Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Portworx Volume Source_To_v1_Portworx Volume Source ( a . ( * core . Portworx Volume Source ) , b . ( * v1 . Portworx Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Prefer Avoid Pods Entry ) ( nil ) , ( * core . Prefer Avoid Pods Entry ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Prefer Avoid Pods Entry_To_core_Prefer Avoid Pods Entry ( a . ( * v1 . Prefer Avoid Pods Entry ) , b . ( * core . Prefer Avoid Pods if err := s . Add Generated Conversion Func ( ( * core . Prefer Avoid Pods Entry ) ( nil ) , ( * v1 . Prefer Avoid Pods Entry ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Prefer Avoid Pods Entry_To_v1_Prefer Avoid Pods Entry ( a . ( * core . Prefer Avoid Pods Entry ) , b . ( * v1 . Prefer Avoid Pods if err := s . Add Generated Conversion Func ( ( * v1 . Preferred Scheduling Term ) ( nil ) , ( * core . Preferred Scheduling Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Preferred Scheduling Term_To_core_Preferred Scheduling Term ( a . ( * v1 . Preferred Scheduling Term ) , b . ( * core . Preferred Scheduling if err := s . Add Generated Conversion Func ( ( * core . Preferred Scheduling Term ) ( nil ) , ( * v1 . Preferred Scheduling Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Preferred Scheduling Term_To_v1_Preferred Scheduling Term ( a . ( * core . Preferred Scheduling Term ) , b . ( * v1 . Preferred Scheduling if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Projected Volume Source ) ( nil ) , ( * core . Projected Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Projected Volume Source_To_core_Projected Volume Source ( a . ( * v1 . Projected Volume Source ) , b . ( * core . Projected Volume if err := s . Add Generated Conversion Func ( ( * core . Projected Volume Source ) ( nil ) , ( * v1 . Projected Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Projected Volume Source_To_v1_Projected Volume Source ( a . ( * core . Projected Volume Source ) , b . ( * v1 . Projected Volume if err := s . Add Generated Conversion Func ( ( * v1 . Quobyte Volume Source ) ( nil ) , ( * core . Quobyte Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Quobyte Volume Source_To_core_Quobyte Volume Source ( a . ( * v1 . Quobyte Volume Source ) , b . ( * core . Quobyte Volume if err := s . Add Generated Conversion Func ( ( * core . Quobyte Volume Source ) ( nil ) , ( * v1 . Quobyte Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Quobyte Volume Source_To_v1_Quobyte Volume Source ( a . ( * core . Quobyte Volume Source ) , b . ( * v1 . Quobyte Volume if err := s . Add Generated Conversion Func ( ( * v1 . RBD Persistent Volume Source ) ( nil ) , ( * core . RBD Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_RBD Persistent Volume Source_To_core_RBD Persistent Volume Source ( a . ( * v1 . RBD Persistent Volume Source ) , b . ( * core . RBD Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . RBD Persistent Volume Source ) ( nil ) , ( * v1 . RBD Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_RBD Persistent Volume Source_To_v1_RBD Persistent Volume Source ( a . ( * core . RBD Persistent Volume Source ) , b . ( * v1 . RBD Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . RBD Volume Source ) ( nil ) , ( * core . RBD Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_RBD Volume Source_To_core_RBD Volume Source ( a . ( * v1 . RBD Volume Source ) , b . ( * core . RBD Volume if err := s . Add Generated Conversion Func ( ( * core . RBD Volume Source ) ( nil ) , ( * v1 . RBD Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_RBD Volume Source_To_v1_RBD Volume Source ( a . ( * core . RBD Volume Source ) , b . ( * v1 . RBD Volume if err := s . Add Generated Conversion Func ( ( * v1 . Range Allocation ) ( nil ) , ( * core . Range Allocation ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Range Allocation_To_core_Range Allocation ( a . ( * v1 . Range Allocation ) , b . ( * core . Range if err := s . Add Generated Conversion Func ( ( * core . Range Allocation ) ( nil ) , ( * v1 . Range Allocation ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Range Allocation_To_v1_Range Allocation ( a . ( * core . Range Allocation ) , b . ( * v1 . Range if err := s . Add Generated Conversion Func ( ( * v1 . Replication Controller ) ( nil ) , ( * core . Replication Controller ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller_To_core_Replication Controller ( a . ( * v1 . Replication Controller ) , b . ( * core . Replication if err := s . Add Generated Conversion Func ( ( * core . Replication Controller ) ( nil ) , ( * v1 . Replication Controller ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Replication Controller_To_v1_Replication Controller ( a . ( * core . Replication Controller ) , b . ( * v1 . Replication if err := s . Add Generated Conversion Func ( ( * v1 . Replication Controller Condition ) ( nil ) , ( * core . Replication Controller Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller Condition_To_core_Replication Controller Condition ( a . ( * v1 . Replication Controller Condition ) , b . ( * core . Replication Controller if err := s . Add Generated Conversion Func ( ( * core . Replication Controller Condition ) ( nil ) , ( * v1 . Replication Controller Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Replication Controller Condition_To_v1_Replication Controller Condition ( a . ( * core . Replication Controller Condition ) , b . ( * v1 . Replication Controller if err := s . Add Generated Conversion Func ( ( * v1 . Replication Controller List ) ( nil ) , ( * core . Replication Controller List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller List_To_core_Replication Controller List ( a . ( * v1 . Replication Controller List ) , b . ( * core . Replication Controller if err := s . Add Generated Conversion Func ( ( * core . Replication Controller List ) ( nil ) , ( * v1 . Replication Controller List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Replication Controller List_To_v1_Replication Controller List ( a . ( * core . Replication Controller List ) , b . ( * v1 . Replication Controller if err := s . Add Generated Conversion Func ( ( * v1 . Replication Controller Spec ) ( nil ) , ( * core . Replication Controller Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller Spec_To_core_Replication Controller Spec ( a . ( * v1 . Replication Controller Spec ) , b . ( * core . Replication Controller if err := s . Add Generated Conversion Func ( ( * core . Replication Controller Spec ) ( nil ) , ( * v1 . Replication Controller Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Replication Controller Spec_To_v1_Replication Controller Spec ( a . ( * core . Replication Controller Spec ) , b . ( * v1 . Replication Controller if err := s . Add Generated Conversion Func ( ( * v1 . Replication Controller Status ) ( nil ) , ( * core . Replication Controller Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller Status_To_core_Replication Controller Status ( a . ( * v1 . Replication Controller Status ) , b . ( * core . Replication Controller if err := s . Add Generated Conversion Func ( ( * core . Replication Controller Status ) ( nil ) , ( * v1 . Replication Controller Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Replication Controller Status_To_v1_Replication Controller Status ( a . ( * core . Replication Controller Status ) , b . ( * v1 . Replication Controller if err := s . Add Generated Conversion Func ( ( * v1 . Resource Field Selector ) ( nil ) , ( * core . Resource Field Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Field Selector_To_core_Resource Field Selector ( a . ( * v1 . Resource Field Selector ) , b . ( * core . Resource Field if err := s . Add Generated Conversion Func ( ( * core . Resource Field Selector ) ( nil ) , ( * v1 . Resource Field Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Resource Field Selector_To_v1_Resource Field Selector ( a . ( * core . Resource Field Selector ) , b . ( * v1 . Resource Field if err := s . Add Generated Conversion Func ( ( * v1 . Resource Quota ) ( nil ) , ( * core . Resource Quota ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Quota_To_core_Resource Quota ( a . ( * v1 . Resource Quota ) , b . ( * core . Resource if err := s . Add Generated Conversion Func ( ( * core . Resource Quota ) ( nil ) , ( * v1 . Resource Quota ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Resource Quota_To_v1_Resource Quota ( a . ( * core . Resource Quota ) , b . ( * v1 . Resource if err := s . Add Generated Conversion Func ( ( * v1 . Resource Quota List ) ( nil ) , ( * core . Resource Quota List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Quota List_To_core_Resource Quota List ( a . ( * v1 . Resource Quota List ) , b . ( * core . Resource Quota if err := s . Add Generated Conversion Func ( ( * core . Resource Quota List ) ( nil ) , ( * v1 . Resource Quota List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Resource Quota List_To_v1_Resource Quota List ( a . ( * core . Resource Quota List ) , b . ( * v1 . Resource Quota if err := s . Add Generated Conversion Func ( ( * v1 . Resource Quota Spec ) ( nil ) , ( * core . Resource Quota Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Quota Spec_To_core_Resource Quota Spec ( a . ( * v1 . Resource Quota Spec ) , b . ( * core . Resource Quota if err := s . Add Generated Conversion Func ( ( * core . Resource Quota Spec ) ( nil ) , ( * v1 . Resource Quota Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Resource Quota Spec_To_v1_Resource Quota Spec ( a . ( * core . Resource Quota Spec ) , b . ( * v1 . Resource Quota if err := s . Add Generated Conversion Func ( ( * v1 . Resource Quota Status ) ( nil ) , ( * core . Resource Quota Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Quota Status_To_core_Resource Quota Status ( a . ( * v1 . Resource Quota Status ) , b . ( * core . Resource Quota if err := s . Add Generated Conversion Func ( ( * core . Resource Quota Status ) ( nil ) , ( * v1 . Resource Quota Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Resource Quota Status_To_v1_Resource Quota Status ( a . ( * core . Resource Quota Status ) , b . ( * v1 . Resource Quota if err := s . Add Generated Conversion Func ( ( * v1 . Resource Requirements ) ( nil ) , ( * core . Resource Requirements ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Requirements_To_core_Resource Requirements ( a . ( * v1 . Resource Requirements ) , b . ( * core . Resource if err := s . Add Generated Conversion Func ( ( * core . Resource Requirements ) ( nil ) , ( * v1 . Resource Requirements ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Resource Requirements_To_v1_Resource Requirements ( a . ( * core . Resource Requirements ) , b . ( * v1 . Resource if err := s . Add Generated Conversion Func ( ( * v1 . SE Linux Options ) ( nil ) , ( * core . SE Linux Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_SE Linux Options_To_core_SE Linux Options ( a . ( * v1 . SE Linux Options ) , b . ( * core . SE Linux if err := s . Add Generated Conversion Func ( ( * core . SE Linux Options ) ( nil ) , ( * v1 . SE Linux Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_SE Linux Options_To_v1_SE Linux Options ( a . ( * core . SE Linux Options ) , b . ( * v1 . SE Linux if err := s . Add Generated Conversion Func ( ( * v1 . Scale IO Persistent Volume Source ) ( nil ) , ( * core . Scale IO Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Scale IO Persistent Volume Source_To_core_Scale IO Persistent Volume Source ( a . ( * v1 . Scale IO Persistent Volume Source ) , b . ( * core . Scale IO Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Scale IO Persistent Volume Source ) ( nil ) , ( * v1 . Scale IO Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Scale IO Persistent Volume Source_To_v1_Scale IO Persistent Volume Source ( a . ( * core . Scale IO Persistent Volume Source ) , b . ( * v1 . Scale IO Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Scale IO Volume Source ) ( nil ) , ( * core . Scale IO Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Scale IO Volume Source_To_core_Scale IO Volume Source ( a . ( * v1 . Scale IO Volume Source ) , b . ( * core . Scale IO Volume if err := s . Add Generated Conversion Func ( ( * core . Scale IO Volume Source ) ( nil ) , ( * v1 . Scale IO Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Scale IO Volume Source_To_v1_Scale IO Volume Source ( a . ( * core . Scale IO Volume Source ) , b . ( * v1 . Scale IO Volume if err := s . Add Generated Conversion Func ( ( * v1 . Scope Selector ) ( nil ) , ( * core . Scope Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Scope Selector_To_core_Scope Selector ( a . ( * v1 . Scope Selector ) , b . ( * core . Scope if err := s . Add Generated Conversion Func ( ( * core . Scope Selector ) ( nil ) , ( * v1 . Scope Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Scope Selector_To_v1_Scope Selector ( a . ( * core . Scope Selector ) , b . ( * v1 . Scope if err := s . Add Generated Conversion Func ( ( * v1 . Scoped Resource Selector Requirement ) ( nil ) , ( * core . Scoped Resource Selector Requirement ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Scoped Resource Selector Requirement_To_core_Scoped Resource Selector Requirement ( a . ( * v1 . Scoped Resource Selector Requirement ) , b . ( * core . Scoped Resource Selector if err := s . Add Generated Conversion Func ( ( * core . Scoped Resource Selector Requirement ) ( nil ) , ( * v1 . Scoped Resource Selector Requirement ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Scoped Resource Selector Requirement_To_v1_Scoped Resource Selector Requirement ( a . ( * core . Scoped Resource Selector Requirement ) , b . ( * v1 . Scoped Resource Selector if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Secret Env Source ) ( nil ) , ( * core . Secret Env Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secret Env Source_To_core_Secret Env Source ( a . ( * v1 . Secret Env Source ) , b . ( * core . Secret Env if err := s . Add Generated Conversion Func ( ( * core . Secret Env Source ) ( nil ) , ( * v1 . Secret Env Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Secret Env Source_To_v1_Secret Env Source ( a . ( * core . Secret Env Source ) , b . ( * v1 . Secret Env if err := s . Add Generated Conversion Func ( ( * v1 . Secret Key Selector ) ( nil ) , ( * core . Secret Key Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secret Key Selector_To_core_Secret Key Selector ( a . ( * v1 . Secret Key Selector ) , b . ( * core . Secret Key if err := s . Add Generated Conversion Func ( ( * core . Secret Key Selector ) ( nil ) , ( * v1 . Secret Key Selector ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Secret Key Selector_To_v1_Secret Key Selector ( a . ( * core . Secret Key Selector ) , b . ( * v1 . Secret Key if err := s . Add Generated Conversion Func ( ( * v1 . Secret List ) ( nil ) , ( * core . Secret List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secret List_To_core_Secret List ( a . ( * v1 . Secret List ) , b . ( * core . Secret if err := s . Add Generated Conversion Func ( ( * core . Secret List ) ( nil ) , ( * v1 . Secret List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Secret List_To_v1_Secret List ( a . ( * core . Secret List ) , b . ( * v1 . Secret if err := s . Add Generated Conversion Func ( ( * v1 . Secret Projection ) ( nil ) , ( * core . Secret Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secret Projection_To_core_Secret Projection ( a . ( * v1 . Secret Projection ) , b . ( * core . Secret if err := s . Add Generated Conversion Func ( ( * core . Secret Projection ) ( nil ) , ( * v1 . Secret Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Secret Projection_To_v1_Secret Projection ( a . ( * core . Secret Projection ) , b . ( * v1 . Secret if err := s . Add Generated Conversion Func ( ( * v1 . Secret Reference ) ( nil ) , ( * core . Secret Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secret Reference_To_core_Secret Reference ( a . ( * v1 . Secret Reference ) , b . ( * core . Secret if err := s . Add Generated Conversion Func ( ( * core . Secret Reference ) ( nil ) , ( * v1 . Secret Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Secret Reference_To_v1_Secret Reference ( a . ( * core . Secret Reference ) , b . ( * v1 . Secret if err := s . Add Generated Conversion Func ( ( * v1 . Secret Volume Source ) ( nil ) , ( * core . Secret Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Secret Volume Source_To_core_Secret Volume Source ( a . ( * v1 . Secret Volume Source ) , b . ( * core . Secret Volume if err := s . Add Generated Conversion Func ( ( * core . Secret Volume Source ) ( nil ) , ( * v1 . Secret Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Secret Volume Source_To_v1_Secret Volume Source ( a . ( * core . Secret Volume Source ) , b . ( * v1 . Secret Volume if err := s . Add Generated Conversion Func ( ( * v1 . Security Context ) ( nil ) , ( * core . Security Context ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Security Context_To_core_Security Context ( a . ( * v1 . Security Context ) , b . ( * core . Security if err := s . Add Generated Conversion Func ( ( * core . Security Context ) ( nil ) , ( * v1 . Security Context ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Security Context_To_v1_Security Context ( a . ( * core . Security Context ) , b . ( * v1 . Security if err := s . Add Generated Conversion Func ( ( * v1 . Serialized Reference ) ( nil ) , ( * core . Serialized Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Serialized Reference_To_core_Serialized Reference ( a . ( * v1 . Serialized Reference ) , b . ( * core . Serialized if err := s . Add Generated Conversion Func ( ( * core . Serialized Reference ) ( nil ) , ( * v1 . Serialized Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Serialized Reference_To_v1_Serialized Reference ( a . ( * core . Serialized Reference ) , b . ( * v1 . Serialized if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Service Account ) ( nil ) , ( * core . Service Account ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Account_To_core_Service Account ( a . ( * v1 . Service Account ) , b . ( * core . Service if err := s . Add Generated Conversion Func ( ( * core . Service Account ) ( nil ) , ( * v1 . Service Account ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Account_To_v1_Service Account ( a . ( * core . Service Account ) , b . ( * v1 . Service if err := s . Add Generated Conversion Func ( ( * v1 . Service Account List ) ( nil ) , ( * core . Service Account List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Account List_To_core_Service Account List ( a . ( * v1 . Service Account List ) , b . ( * core . Service Account if err := s . Add Generated Conversion Func ( ( * core . Service Account List ) ( nil ) , ( * v1 . Service Account List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Account List_To_v1_Service Account List ( a . ( * core . Service Account List ) , b . ( * v1 . Service Account if err := s . Add Generated Conversion Func ( ( * v1 . Service Account Token Projection ) ( nil ) , ( * core . Service Account Token Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Account Token Projection_To_core_Service Account Token Projection ( a . ( * v1 . Service Account Token Projection ) , b . ( * core . Service Account Token if err := s . Add Generated Conversion Func ( ( * core . Service Account Token Projection ) ( nil ) , ( * v1 . Service Account Token Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Account Token Projection_To_v1_Service Account Token Projection ( a . ( * core . Service Account Token Projection ) , b . ( * v1 . Service Account Token if err := s . Add Generated Conversion Func ( ( * v1 . Service List ) ( nil ) , ( * core . Service List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service List_To_core_Service List ( a . ( * v1 . Service List ) , b . ( * core . Service if err := s . Add Generated Conversion Func ( ( * core . Service List ) ( nil ) , ( * v1 . Service List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service List_To_v1_Service List ( a . ( * core . Service List ) , b . ( * v1 . Service if err := s . Add Generated Conversion Func ( ( * v1 . Service Port ) ( nil ) , ( * core . Service Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Port_To_core_Service Port ( a . ( * v1 . Service Port ) , b . ( * core . Service if err := s . Add Generated Conversion Func ( ( * core . Service Port ) ( nil ) , ( * v1 . Service Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Port_To_v1_Service Port ( a . ( * core . Service Port ) , b . ( * v1 . Service if err := s . Add Generated Conversion Func ( ( * v1 . Service Proxy Options ) ( nil ) , ( * core . Service Proxy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Proxy Options_To_core_Service Proxy Options ( a . ( * v1 . Service Proxy Options ) , b . ( * core . Service Proxy if err := s . Add Generated Conversion Func ( ( * core . Service Proxy Options ) ( nil ) , ( * v1 . Service Proxy Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Proxy Options_To_v1_Service Proxy Options ( a . ( * core . Service Proxy Options ) , b . ( * v1 . Service Proxy if err := s . Add Generated Conversion Func ( ( * v1 . Service Spec ) ( nil ) , ( * core . Service Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Spec_To_core_Service Spec ( a . ( * v1 . Service Spec ) , b . ( * core . Service if err := s . Add Generated Conversion Func ( ( * core . Service Spec ) ( nil ) , ( * v1 . Service Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Spec_To_v1_Service Spec ( a . ( * core . Service Spec ) , b . ( * v1 . Service if err := s . Add Generated Conversion Func ( ( * v1 . Service Status ) ( nil ) , ( * core . Service Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Status_To_core_Service Status ( a . ( * v1 . Service Status ) , b . ( * core . Service if err := s . Add Generated Conversion Func ( ( * core . Service Status ) ( nil ) , ( * v1 . Service Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Service Status_To_v1_Service Status ( a . ( * core . Service Status ) , b . ( * v1 . Service if err := s . Add Generated Conversion Func ( ( * v1 . Session Affinity Config ) ( nil ) , ( * core . Session Affinity Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Session Affinity Config_To_core_Session Affinity Config ( a . ( * v1 . Session Affinity Config ) , b . ( * core . Session Affinity if err := s . Add Generated Conversion Func ( ( * core . Session Affinity Config ) ( nil ) , ( * v1 . Session Affinity Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Session Affinity Config_To_v1_Session Affinity Config ( a . ( * core . Session Affinity Config ) , b . ( * v1 . Session Affinity if err := s . Add Generated Conversion Func ( ( * v1 . Storage OS Persistent Volume Source ) ( nil ) , ( * core . Storage OS Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Storage OS Persistent Volume Source_To_core_Storage OS Persistent Volume Source ( a . ( * v1 . Storage OS Persistent Volume Source ) , b . ( * core . Storage OS Persistent Volume if err := s . Add Generated Conversion Func ( ( * core . Storage OS Persistent Volume Source ) ( nil ) , ( * v1 . Storage OS Persistent Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Storage OS Persistent Volume Source_To_v1_Storage OS Persistent Volume Source ( a . ( * core . Storage OS Persistent Volume Source ) , b . ( * v1 . Storage OS Persistent Volume if err := s . Add Generated Conversion Func ( ( * v1 . Storage OS Volume Source ) ( nil ) , ( * core . Storage OS Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Storage OS Volume Source_To_core_Storage OS Volume Source ( a . ( * v1 . Storage OS Volume Source ) , b . ( * core . Storage OS Volume if err := s . Add Generated Conversion Func ( ( * core . Storage OS Volume Source ) ( nil ) , ( * v1 . Storage OS Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Storage OS Volume Source_To_v1_Storage OS Volume Source ( a . ( * core . Storage OS Volume Source ) , b . ( * v1 . Storage OS Volume if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . TCP Socket Action ) ( nil ) , ( * core . TCP Socket Action ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_TCP Socket Action_To_core_TCP Socket Action ( a . ( * v1 . TCP Socket Action ) , b . ( * core . TCP Socket if err := s . Add Generated Conversion Func ( ( * core . TCP Socket Action ) ( nil ) , ( * v1 . TCP Socket Action ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_TCP Socket Action_To_v1_TCP Socket Action ( a . ( * core . TCP Socket Action ) , b . ( * v1 . TCP Socket if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Topology Selector Label Requirement ) ( nil ) , ( * core . Topology Selector Label Requirement ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Topology Selector Label Requirement_To_core_Topology Selector Label Requirement ( a . ( * v1 . Topology Selector Label Requirement ) , b . ( * core . Topology Selector Label if err := s . Add Generated Conversion Func ( ( * core . Topology Selector Label Requirement ) ( nil ) , ( * v1 . Topology Selector Label Requirement ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Topology Selector Label Requirement_To_v1_Topology Selector Label Requirement ( a . ( * core . Topology Selector Label Requirement ) , b . ( * v1 . Topology Selector Label if err := s . Add Generated Conversion Func ( ( * v1 . Topology Selector Term ) ( nil ) , ( * core . Topology Selector Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Topology Selector Term_To_core_Topology Selector Term ( a . ( * v1 . Topology Selector Term ) , b . ( * core . Topology Selector if err := s . Add Generated Conversion Func ( ( * core . Topology Selector Term ) ( nil ) , ( * v1 . Topology Selector Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Topology Selector Term_To_v1_Topology Selector Term ( a . ( * core . Topology Selector Term ) , b . ( * v1 . Topology Selector if err := s . Add Generated Conversion Func ( ( * v1 . Typed Local Object Reference ) ( nil ) , ( * core . Typed Local Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Typed Local Object Reference_To_core_Typed Local Object Reference ( a . ( * v1 . Typed Local Object Reference ) , b . ( * core . Typed Local Object if err := s . Add Generated Conversion Func ( ( * core . Typed Local Object Reference ) ( nil ) , ( * v1 . Typed Local Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Typed Local Object Reference_To_v1_Typed Local Object Reference ( a . ( * core . Typed Local Object Reference ) , b . ( * v1 . Typed Local Object if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Volume Device ) ( nil ) , ( * core . Volume Device ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Device_To_core_Volume Device ( a . ( * v1 . Volume Device ) , b . ( * core . Volume if err := s . Add Generated Conversion Func ( ( * core . Volume Device ) ( nil ) , ( * v1 . Volume Device ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Volume Device_To_v1_Volume Device ( a . ( * core . Volume Device ) , b . ( * v1 . Volume if err := s . Add Generated Conversion Func ( ( * v1 . Volume Mount ) ( nil ) , ( * core . Volume Mount ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Mount_To_core_Volume Mount ( a . ( * v1 . Volume Mount ) , b . ( * core . Volume if err := s . Add Generated Conversion Func ( ( * core . Volume Mount ) ( nil ) , ( * v1 . Volume Mount ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Volume Mount_To_v1_Volume Mount ( a . ( * core . Volume Mount ) , b . ( * v1 . Volume if err := s . Add Generated Conversion Func ( ( * v1 . Volume Node Affinity ) ( nil ) , ( * core . Volume Node Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Node Affinity_To_core_Volume Node Affinity ( a . ( * v1 . Volume Node Affinity ) , b . ( * core . Volume Node if err := s . Add Generated Conversion Func ( ( * core . Volume Node Affinity ) ( nil ) , ( * v1 . Volume Node Affinity ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Volume Node Affinity_To_v1_Volume Node Affinity ( a . ( * core . Volume Node Affinity ) , b . ( * v1 . Volume Node if err := s . Add Generated Conversion Func ( ( * v1 . Volume Projection ) ( nil ) , ( * core . Volume Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Projection_To_core_Volume Projection ( a . ( * v1 . Volume Projection ) , b . ( * core . Volume if err := s . Add Generated Conversion Func ( ( * core . Volume Projection ) ( nil ) , ( * v1 . Volume Projection ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Volume Projection_To_v1_Volume Projection ( a . ( * core . Volume Projection ) , b . ( * v1 . Volume if err := s . Add Generated Conversion Func ( ( * v1 . Volume Source ) ( nil ) , ( * core . Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Source_To_core_Volume Source ( a . ( * v1 . Volume Source ) , b . ( * core . Volume if err := s . Add Generated Conversion Func ( ( * core . Volume Source ) ( nil ) , ( * v1 . Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Volume Source_To_v1_Volume Source ( a . ( * core . Volume Source ) , b . ( * v1 . Volume if err := s . Add Generated Conversion Func ( ( * v1 . Vsphere Virtual Disk Volume Source ) ( nil ) , ( * core . Vsphere Virtual Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Vsphere Virtual Disk Volume Source_To_core_Vsphere Virtual Disk Volume Source ( a . ( * v1 . Vsphere Virtual Disk Volume Source ) , b . ( * core . Vsphere Virtual Disk Volume if err := s . Add Generated Conversion Func ( ( * core . Vsphere Virtual Disk Volume Source ) ( nil ) , ( * v1 . Vsphere Virtual Disk Volume Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Vsphere Virtual Disk Volume Source_To_v1_Vsphere Virtual Disk Volume Source ( a . ( * core . Vsphere Virtual Disk Volume Source ) , b . ( * v1 . Vsphere Virtual Disk Volume if err := s . Add Generated Conversion Func ( ( * v1 . Weighted Pod Affinity Term ) ( nil ) , ( * core . Weighted Pod Affinity Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Weighted Pod Affinity Term_To_core_Weighted Pod Affinity Term ( a . ( * v1 . Weighted Pod Affinity Term ) , b . ( * core . Weighted Pod Affinity if err := s . Add Generated Conversion Func ( ( * core . Weighted Pod Affinity Term ) ( nil ) , ( * v1 . Weighted Pod Affinity Term ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Weighted Pod Affinity Term_To_v1_Weighted Pod Affinity Term ( a . ( * core . Weighted Pod Affinity Term ) , b . ( * v1 . Weighted Pod Affinity if err := s . Add Generated Conversion Func ( ( * v1 . Windows Security Context Options ) ( nil ) , ( * core . Windows Security Context Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Windows Security Context Options_To_core_Windows Security Context Options ( a . ( * v1 . Windows Security Context Options ) , b . ( * core . Windows Security Context if err := s . Add Generated Conversion Func ( ( * core . Windows Security Context Options ) ( nil ) , ( * v1 . Windows Security Context Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Windows Security Context Options_To_v1_Windows Security Context Options ( a . ( * core . Windows Security Context Options ) , b . ( * v1 . Windows Security Context if err := s . Add Conversion Func ( ( * apps . Replica Set Spec ) ( nil ) , ( * v1 . Replication Controller Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set Spec_To_v1_Replication Controller Spec ( a . ( * apps . Replica Set Spec ) , b . ( * v1 . Replication Controller if err := s . Add Conversion Func ( ( * apps . Replica Set Status ) ( nil ) , ( * v1 . Replication Controller Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set Status_To_v1_Replication Controller Status ( a . ( * apps . Replica Set Status ) , b . ( * v1 . Replication Controller if err := s . Add Conversion Func ( ( * apps . Replica Set ) ( nil ) , ( * v1 . Replication Controller ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set_To_v1_Replication Controller ( a . ( * apps . Replica Set ) , b . ( * v1 . Replication if err := s . Add Conversion Func ( ( * core . Pod Spec ) ( nil ) , ( * v1 . Pod Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Spec_To_v1_Pod Spec ( a . ( * core . Pod Spec ) , b . ( * v1 . Pod if err := s . Add Conversion Func ( ( * core . Pod Template Spec ) ( nil ) , ( * v1 . Pod Template Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Pod Template Spec_To_v1_Pod Template Spec ( a . ( * core . Pod Template Spec ) , b . ( * v1 . Pod Template if err := s . Add Conversion if err := s . Add Conversion Func ( ( * core . Replication Controller Spec ) ( nil ) , ( * v1 . Replication Controller Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Replication Controller Spec_To_v1_Replication Controller Spec ( a . ( * core . Replication Controller Spec ) , b . ( * v1 . Replication Controller if err := s . Add Conversion Func ( ( * v1 . Pod Spec ) ( nil ) , ( * core . Pod Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Spec_To_core_Pod Spec ( a . ( * v1 . Pod Spec ) , b . ( * core . Pod if err := s . Add Conversion Func ( ( * v1 . Pod Template Spec ) ( nil ) , ( * core . Pod Template Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pod Template Spec_To_core_Pod Template Spec ( a . ( * v1 . Pod Template Spec ) , b . ( * core . Pod Template if err := s . Add Conversion if err := s . Add Conversion Func ( ( * v1 . Replication Controller Spec ) ( nil ) , ( * apps . Replica Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller Spec_To_apps_Replica Set Spec ( a . ( * v1 . Replication Controller Spec ) , b . ( * apps . Replica Set if err := s . Add Conversion Func ( ( * v1 . Replication Controller Spec ) ( nil ) , ( * core . Replication Controller Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller Spec_To_core_Replication Controller Spec ( a . ( * v1 . Replication Controller Spec ) , b . ( * core . Replication Controller if err := s . Add Conversion Func ( ( * v1 . Replication Controller Status ) ( nil ) , ( * apps . Replica Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller Status_To_apps_Replica Set Status ( a . ( * v1 . Replication Controller Status ) , b . ( * apps . Replica Set if err := s . Add Conversion Func ( ( * v1 . Replication Controller ) ( nil ) , ( * apps . Replica Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replication Controller_To_apps_Replica Set ( a . ( * v1 . Replication Controller ) , b . ( * apps . Replica if err := s . Add Conversion Func ( ( * v1 . Resource List ) ( nil ) , ( * core . Resource List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource List_To_core_Resource List ( a . ( * v1 . Resource List ) , b . ( * core . Resource if err := s . Add Conversion } 
func Convert_v1_AWS Elastic Block Store Volume Source_To_core_AWS Elastic Block Store Volume Source ( in * v1 . AWS Elastic Block Store Volume Source , out * core . AWS Elastic Block Store Volume Source , s conversion . Scope ) error { return auto Convert_v1_AWS Elastic Block Store Volume Source_To_core_AWS Elastic Block Store Volume } 
func Convert_core_AWS Elastic Block Store Volume Source_To_v1_AWS Elastic Block Store Volume Source ( in * core . AWS Elastic Block Store Volume Source , out * v1 . AWS Elastic Block Store Volume Source , s conversion . Scope ) error { return auto Convert_core_AWS Elastic Block Store Volume Source_To_v1_AWS Elastic Block Store Volume } 
func Convert_v1_Affinity_To_core_Affinity ( in * v1 . Affinity , out * core . Affinity , s conversion . Scope ) error { return auto } 
func Convert_core_Affinity_To_v1_Affinity ( in * core . Affinity , out * v1 . Affinity , s conversion . Scope ) error { return auto } 
func Convert_v1_Attached Volume_To_core_Attached Volume ( in * v1 . Attached Volume , out * core . Attached Volume , s conversion . Scope ) error { return auto Convert_v1_Attached Volume_To_core_Attached } 
func Convert_core_Attached Volume_To_v1_Attached Volume ( in * core . Attached Volume , out * v1 . Attached Volume , s conversion . Scope ) error { return auto Convert_core_Attached Volume_To_v1_Attached } 
func Convert_v1_Avoid Pods_To_core_Avoid Pods ( in * v1 . Avoid Pods , out * core . Avoid Pods , s conversion . Scope ) error { return auto Convert_v1_Avoid Pods_To_core_Avoid } 
func Convert_core_Avoid Pods_To_v1_Avoid Pods ( in * core . Avoid Pods , out * v1 . Avoid Pods , s conversion . Scope ) error { return auto Convert_core_Avoid Pods_To_v1_Avoid } 
func Convert_v1_Azure Disk Volume Source_To_core_Azure Disk Volume Source ( in * v1 . Azure Disk Volume Source , out * core . Azure Disk Volume Source , s conversion . Scope ) error { return auto Convert_v1_Azure Disk Volume Source_To_core_Azure Disk Volume } 
func Convert_core_Azure Disk Volume Source_To_v1_Azure Disk Volume Source ( in * core . Azure Disk Volume Source , out * v1 . Azure Disk Volume Source , s conversion . Scope ) error { return auto Convert_core_Azure Disk Volume Source_To_v1_Azure Disk Volume } 
func Convert_v1_Azure File Persistent Volume Source_To_core_Azure File Persistent Volume Source ( in * v1 . Azure File Persistent Volume Source , out * core . Azure File Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Azure File Persistent Volume Source_To_core_Azure File Persistent Volume } 
func Convert_core_Azure File Persistent Volume Source_To_v1_Azure File Persistent Volume Source ( in * core . Azure File Persistent Volume Source , out * v1 . Azure File Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Azure File Persistent Volume Source_To_v1_Azure File Persistent Volume } 
func Convert_v1_Azure File Volume Source_To_core_Azure File Volume Source ( in * v1 . Azure File Volume Source , out * core . Azure File Volume Source , s conversion . Scope ) error { return auto Convert_v1_Azure File Volume Source_To_core_Azure File Volume } 
func Convert_core_Azure File Volume Source_To_v1_Azure File Volume Source ( in * core . Azure File Volume Source , out * v1 . Azure File Volume Source , s conversion . Scope ) error { return auto Convert_core_Azure File Volume Source_To_v1_Azure File Volume } 
func Convert_v1_Binding_To_core_Binding ( in * v1 . Binding , out * core . Binding , s conversion . Scope ) error { return auto } 
func Convert_core_Binding_To_v1_Binding ( in * core . Binding , out * v1 . Binding , s conversion . Scope ) error { return auto } 
func Convert_v1_CSI Persistent Volume Source_To_core_CSI Persistent Volume Source ( in * v1 . CSI Persistent Volume Source , out * core . CSI Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_CSI Persistent Volume Source_To_core_CSI Persistent Volume } 
func Convert_core_CSI Persistent Volume Source_To_v1_CSI Persistent Volume Source ( in * core . CSI Persistent Volume Source , out * v1 . CSI Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_CSI Persistent Volume Source_To_v1_CSI Persistent Volume } 
func Convert_v1_CSI Volume Source_To_core_CSI Volume Source ( in * v1 . CSI Volume Source , out * core . CSI Volume Source , s conversion . Scope ) error { return auto Convert_v1_CSI Volume Source_To_core_CSI Volume } 
func Convert_core_CSI Volume Source_To_v1_CSI Volume Source ( in * core . CSI Volume Source , out * v1 . CSI Volume Source , s conversion . Scope ) error { return auto Convert_core_CSI Volume Source_To_v1_CSI Volume } 
func Convert_v1_Capabilities_To_core_Capabilities ( in * v1 . Capabilities , out * core . Capabilities , s conversion . Scope ) error { return auto } 
func Convert_core_Capabilities_To_v1_Capabilities ( in * core . Capabilities , out * v1 . Capabilities , s conversion . Scope ) error { return auto } 
func Convert_v1_Ceph FS Persistent Volume Source_To_core_Ceph FS Persistent Volume Source ( in * v1 . Ceph FS Persistent Volume Source , out * core . Ceph FS Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Ceph FS Persistent Volume Source_To_core_Ceph FS Persistent Volume } 
func Convert_core_Ceph FS Persistent Volume Source_To_v1_Ceph FS Persistent Volume Source ( in * core . Ceph FS Persistent Volume Source , out * v1 . Ceph FS Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Ceph FS Persistent Volume Source_To_v1_Ceph FS Persistent Volume } 
func Convert_v1_Ceph FS Volume Source_To_core_Ceph FS Volume Source ( in * v1 . Ceph FS Volume Source , out * core . Ceph FS Volume Source , s conversion . Scope ) error { return auto Convert_v1_Ceph FS Volume Source_To_core_Ceph FS Volume } 
func Convert_core_Ceph FS Volume Source_To_v1_Ceph FS Volume Source ( in * core . Ceph FS Volume Source , out * v1 . Ceph FS Volume Source , s conversion . Scope ) error { return auto Convert_core_Ceph FS Volume Source_To_v1_Ceph FS Volume } 
func Convert_v1_Cinder Persistent Volume Source_To_core_Cinder Persistent Volume Source ( in * v1 . Cinder Persistent Volume Source , out * core . Cinder Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Cinder Persistent Volume Source_To_core_Cinder Persistent Volume } 
func Convert_core_Cinder Persistent Volume Source_To_v1_Cinder Persistent Volume Source ( in * core . Cinder Persistent Volume Source , out * v1 . Cinder Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Cinder Persistent Volume Source_To_v1_Cinder Persistent Volume } 
func Convert_v1_Cinder Volume Source_To_core_Cinder Volume Source ( in * v1 . Cinder Volume Source , out * core . Cinder Volume Source , s conversion . Scope ) error { return auto Convert_v1_Cinder Volume Source_To_core_Cinder Volume } 
func Convert_core_Cinder Volume Source_To_v1_Cinder Volume Source ( in * core . Cinder Volume Source , out * v1 . Cinder Volume Source , s conversion . Scope ) error { return auto Convert_core_Cinder Volume Source_To_v1_Cinder Volume } 
func Convert_v1_Client IP Config_To_core_Client IP Config ( in * v1 . Client IP Config , out * core . Client IP Config , s conversion . Scope ) error { return auto Convert_v1_Client IP Config_To_core_Client IP } 
func Convert_core_Client IP Config_To_v1_Client IP Config ( in * core . Client IP Config , out * v1 . Client IP Config , s conversion . Scope ) error { return auto Convert_core_Client IP Config_To_v1_Client IP } 
func Convert_v1_Component Condition_To_core_Component Condition ( in * v1 . Component Condition , out * core . Component Condition , s conversion . Scope ) error { return auto Convert_v1_Component Condition_To_core_Component } 
func Convert_core_Component Condition_To_v1_Component Condition ( in * core . Component Condition , out * v1 . Component Condition , s conversion . Scope ) error { return auto Convert_core_Component Condition_To_v1_Component } 
func Convert_v1_Component Status_To_core_Component Status ( in * v1 . Component Status , out * core . Component Status , s conversion . Scope ) error { return auto Convert_v1_Component Status_To_core_Component } 
func Convert_core_Component Status_To_v1_Component Status ( in * core . Component Status , out * v1 . Component Status , s conversion . Scope ) error { return auto Convert_core_Component Status_To_v1_Component } 
func Convert_v1_Component Status List_To_core_Component Status List ( in * v1 . Component Status List , out * core . Component Status List , s conversion . Scope ) error { return auto Convert_v1_Component Status List_To_core_Component Status } 
func Convert_core_Component Status List_To_v1_Component Status List ( in * core . Component Status List , out * v1 . Component Status List , s conversion . Scope ) error { return auto Convert_core_Component Status List_To_v1_Component Status } 
func Convert_v1_Config Map_To_core_Config Map ( in * v1 . Config Map , out * core . Config Map , s conversion . Scope ) error { return auto Convert_v1_Config Map_To_core_Config } 
func Convert_core_Config Map_To_v1_Config Map ( in * core . Config Map , out * v1 . Config Map , s conversion . Scope ) error { return auto Convert_core_Config Map_To_v1_Config } 
func Convert_v1_Config Map Env Source_To_core_Config Map Env Source ( in * v1 . Config Map Env Source , out * core . Config Map Env Source , s conversion . Scope ) error { return auto Convert_v1_Config Map Env Source_To_core_Config Map Env } 
func Convert_core_Config Map Env Source_To_v1_Config Map Env Source ( in * core . Config Map Env Source , out * v1 . Config Map Env Source , s conversion . Scope ) error { return auto Convert_core_Config Map Env Source_To_v1_Config Map Env } 
func Convert_v1_Config Map Key Selector_To_core_Config Map Key Selector ( in * v1 . Config Map Key Selector , out * core . Config Map Key Selector , s conversion . Scope ) error { return auto Convert_v1_Config Map Key Selector_To_core_Config Map Key } 
func Convert_core_Config Map Key Selector_To_v1_Config Map Key Selector ( in * core . Config Map Key Selector , out * v1 . Config Map Key Selector , s conversion . Scope ) error { return auto Convert_core_Config Map Key Selector_To_v1_Config Map Key } 
func Convert_v1_Config Map List_To_core_Config Map List ( in * v1 . Config Map List , out * core . Config Map List , s conversion . Scope ) error { return auto Convert_v1_Config Map List_To_core_Config Map } 
func Convert_core_Config Map List_To_v1_Config Map List ( in * core . Config Map List , out * v1 . Config Map List , s conversion . Scope ) error { return auto Convert_core_Config Map List_To_v1_Config Map } 
func Convert_v1_Config Map Node Config Source_To_core_Config Map Node Config Source ( in * v1 . Config Map Node Config Source , out * core . Config Map Node Config Source , s conversion . Scope ) error { return auto Convert_v1_Config Map Node Config Source_To_core_Config Map Node Config } 
func Convert_core_Config Map Node Config Source_To_v1_Config Map Node Config Source ( in * core . Config Map Node Config Source , out * v1 . Config Map Node Config Source , s conversion . Scope ) error { return auto Convert_core_Config Map Node Config Source_To_v1_Config Map Node Config } 
func Convert_v1_Config Map Projection_To_core_Config Map Projection ( in * v1 . Config Map Projection , out * core . Config Map Projection , s conversion . Scope ) error { return auto Convert_v1_Config Map Projection_To_core_Config Map } 
func Convert_core_Config Map Projection_To_v1_Config Map Projection ( in * core . Config Map Projection , out * v1 . Config Map Projection , s conversion . Scope ) error { return auto Convert_core_Config Map Projection_To_v1_Config Map } 
func Convert_v1_Config Map Volume Source_To_core_Config Map Volume Source ( in * v1 . Config Map Volume Source , out * core . Config Map Volume Source , s conversion . Scope ) error { return auto Convert_v1_Config Map Volume Source_To_core_Config Map Volume } 
func Convert_core_Config Map Volume Source_To_v1_Config Map Volume Source ( in * core . Config Map Volume Source , out * v1 . Config Map Volume Source , s conversion . Scope ) error { return auto Convert_core_Config Map Volume Source_To_v1_Config Map Volume } 
func Convert_v1_Container_To_core_Container ( in * v1 . Container , out * core . Container , s conversion . Scope ) error { return auto } 
func Convert_core_Container_To_v1_Container ( in * core . Container , out * v1 . Container , s conversion . Scope ) error { return auto } 
func Convert_v1_Container Image_To_core_Container Image ( in * v1 . Container Image , out * core . Container Image , s conversion . Scope ) error { return auto Convert_v1_Container Image_To_core_Container } 
func Convert_core_Container Image_To_v1_Container Image ( in * core . Container Image , out * v1 . Container Image , s conversion . Scope ) error { return auto Convert_core_Container Image_To_v1_Container } 
func Convert_v1_Container Port_To_core_Container Port ( in * v1 . Container Port , out * core . Container Port , s conversion . Scope ) error { return auto Convert_v1_Container Port_To_core_Container } 
func Convert_core_Container Port_To_v1_Container Port ( in * core . Container Port , out * v1 . Container Port , s conversion . Scope ) error { return auto Convert_core_Container Port_To_v1_Container } 
func Convert_v1_Container State_To_core_Container State ( in * v1 . Container State , out * core . Container State , s conversion . Scope ) error { return auto Convert_v1_Container State_To_core_Container } 
func Convert_core_Container State_To_v1_Container State ( in * core . Container State , out * v1 . Container State , s conversion . Scope ) error { return auto Convert_core_Container State_To_v1_Container } 
func Convert_v1_Container State Running_To_core_Container State Running ( in * v1 . Container State Running , out * core . Container State Running , s conversion . Scope ) error { return auto Convert_v1_Container State Running_To_core_Container State } 
func Convert_core_Container State Running_To_v1_Container State Running ( in * core . Container State Running , out * v1 . Container State Running , s conversion . Scope ) error { return auto Convert_core_Container State Running_To_v1_Container State } 
func Convert_v1_Container State Terminated_To_core_Container State Terminated ( in * v1 . Container State Terminated , out * core . Container State Terminated , s conversion . Scope ) error { return auto Convert_v1_Container State Terminated_To_core_Container State } 
func Convert_core_Container State Terminated_To_v1_Container State Terminated ( in * core . Container State Terminated , out * v1 . Container State Terminated , s conversion . Scope ) error { return auto Convert_core_Container State Terminated_To_v1_Container State } 
func Convert_v1_Container State Waiting_To_core_Container State Waiting ( in * v1 . Container State Waiting , out * core . Container State Waiting , s conversion . Scope ) error { return auto Convert_v1_Container State Waiting_To_core_Container State } 
func Convert_core_Container State Waiting_To_v1_Container State Waiting ( in * core . Container State Waiting , out * v1 . Container State Waiting , s conversion . Scope ) error { return auto Convert_core_Container State Waiting_To_v1_Container State } 
func Convert_v1_Container Status_To_core_Container Status ( in * v1 . Container Status , out * core . Container Status , s conversion . Scope ) error { return auto Convert_v1_Container Status_To_core_Container } 
func Convert_core_Container Status_To_v1_Container Status ( in * core . Container Status , out * v1 . Container Status , s conversion . Scope ) error { return auto Convert_core_Container Status_To_v1_Container } 
func Convert_v1_Daemon Endpoint_To_core_Daemon Endpoint ( in * v1 . Daemon Endpoint , out * core . Daemon Endpoint , s conversion . Scope ) error { return auto Convert_v1_Daemon Endpoint_To_core_Daemon } 
func Convert_core_Daemon Endpoint_To_v1_Daemon Endpoint ( in * core . Daemon Endpoint , out * v1 . Daemon Endpoint , s conversion . Scope ) error { return auto Convert_core_Daemon Endpoint_To_v1_Daemon } 
func Convert_v1_Downward API Projection_To_core_Downward API Projection ( in * v1 . Downward API Projection , out * core . Downward API Projection , s conversion . Scope ) error { return auto Convert_v1_Downward API Projection_To_core_Downward API } 
func Convert_core_Downward API Projection_To_v1_Downward API Projection ( in * core . Downward API Projection , out * v1 . Downward API Projection , s conversion . Scope ) error { return auto Convert_core_Downward API Projection_To_v1_Downward API } 
func Convert_v1_Downward API Volume File_To_core_Downward API Volume File ( in * v1 . Downward API Volume File , out * core . Downward API Volume File , s conversion . Scope ) error { return auto Convert_v1_Downward API Volume File_To_core_Downward API Volume } 
func Convert_core_Downward API Volume File_To_v1_Downward API Volume File ( in * core . Downward API Volume File , out * v1 . Downward API Volume File , s conversion . Scope ) error { return auto Convert_core_Downward API Volume File_To_v1_Downward API Volume } 
func Convert_v1_Downward API Volume Source_To_core_Downward API Volume Source ( in * v1 . Downward API Volume Source , out * core . Downward API Volume Source , s conversion . Scope ) error { return auto Convert_v1_Downward API Volume Source_To_core_Downward API Volume } 
func Convert_core_Downward API Volume Source_To_v1_Downward API Volume Source ( in * core . Downward API Volume Source , out * v1 . Downward API Volume Source , s conversion . Scope ) error { return auto Convert_core_Downward API Volume Source_To_v1_Downward API Volume } 
func Convert_v1_Empty Dir Volume Source_To_core_Empty Dir Volume Source ( in * v1 . Empty Dir Volume Source , out * core . Empty Dir Volume Source , s conversion . Scope ) error { return auto Convert_v1_Empty Dir Volume Source_To_core_Empty Dir Volume } 
func Convert_core_Empty Dir Volume Source_To_v1_Empty Dir Volume Source ( in * core . Empty Dir Volume Source , out * v1 . Empty Dir Volume Source , s conversion . Scope ) error { return auto Convert_core_Empty Dir Volume Source_To_v1_Empty Dir Volume } 
func Convert_v1_Endpoint Address_To_core_Endpoint Address ( in * v1 . Endpoint Address , out * core . Endpoint Address , s conversion . Scope ) error { return auto Convert_v1_Endpoint Address_To_core_Endpoint } 
func Convert_core_Endpoint Address_To_v1_Endpoint Address ( in * core . Endpoint Address , out * v1 . Endpoint Address , s conversion . Scope ) error { return auto Convert_core_Endpoint Address_To_v1_Endpoint } 
func Convert_v1_Endpoint Port_To_core_Endpoint Port ( in * v1 . Endpoint Port , out * core . Endpoint Port , s conversion . Scope ) error { return auto Convert_v1_Endpoint Port_To_core_Endpoint } 
func Convert_core_Endpoint Port_To_v1_Endpoint Port ( in * core . Endpoint Port , out * v1 . Endpoint Port , s conversion . Scope ) error { return auto Convert_core_Endpoint Port_To_v1_Endpoint } 
func Convert_v1_Endpoint Subset_To_core_Endpoint Subset ( in * v1 . Endpoint Subset , out * core . Endpoint Subset , s conversion . Scope ) error { return auto Convert_v1_Endpoint Subset_To_core_Endpoint } 
func Convert_core_Endpoint Subset_To_v1_Endpoint Subset ( in * core . Endpoint Subset , out * v1 . Endpoint Subset , s conversion . Scope ) error { return auto Convert_core_Endpoint Subset_To_v1_Endpoint } 
func Convert_v1_Endpoints_To_core_Endpoints ( in * v1 . Endpoints , out * core . Endpoints , s conversion . Scope ) error { return auto } 
func Convert_core_Endpoints_To_v1_Endpoints ( in * core . Endpoints , out * v1 . Endpoints , s conversion . Scope ) error { return auto } 
func Convert_v1_Endpoints List_To_core_Endpoints List ( in * v1 . Endpoints List , out * core . Endpoints List , s conversion . Scope ) error { return auto Convert_v1_Endpoints List_To_core_Endpoints } 
func Convert_core_Endpoints List_To_v1_Endpoints List ( in * core . Endpoints List , out * v1 . Endpoints List , s conversion . Scope ) error { return auto Convert_core_Endpoints List_To_v1_Endpoints } 
func Convert_v1_Env From Source_To_core_Env From Source ( in * v1 . Env From Source , out * core . Env From Source , s conversion . Scope ) error { return auto Convert_v1_Env From Source_To_core_Env From } 
func Convert_core_Env From Source_To_v1_Env From Source ( in * core . Env From Source , out * v1 . Env From Source , s conversion . Scope ) error { return auto Convert_core_Env From Source_To_v1_Env From } 
func Convert_v1_Env Var_To_core_Env Var ( in * v1 . Env Var , out * core . Env Var , s conversion . Scope ) error { return auto Convert_v1_Env Var_To_core_Env } 
func Convert_core_Env Var_To_v1_Env Var ( in * core . Env Var , out * v1 . Env Var , s conversion . Scope ) error { return auto Convert_core_Env Var_To_v1_Env } 
func Convert_v1_Env Var Source_To_core_Env Var Source ( in * v1 . Env Var Source , out * core . Env Var Source , s conversion . Scope ) error { return auto Convert_v1_Env Var Source_To_core_Env Var } 
func Convert_core_Env Var Source_To_v1_Env Var Source ( in * core . Env Var Source , out * v1 . Env Var Source , s conversion . Scope ) error { return auto Convert_core_Env Var Source_To_v1_Env Var } 
func Convert_v1_Event_To_core_Event ( in * v1 . Event , out * core . Event , s conversion . Scope ) error { return auto } 
func Convert_core_Event_To_v1_Event ( in * core . Event , out * v1 . Event , s conversion . Scope ) error { return auto } 
func Convert_v1_Event List_To_core_Event List ( in * v1 . Event List , out * core . Event List , s conversion . Scope ) error { return auto Convert_v1_Event List_To_core_Event } 
func Convert_core_Event List_To_v1_Event List ( in * core . Event List , out * v1 . Event List , s conversion . Scope ) error { return auto Convert_core_Event List_To_v1_Event } 
func Convert_v1_Event Series_To_core_Event Series ( in * v1 . Event Series , out * core . Event Series , s conversion . Scope ) error { return auto Convert_v1_Event Series_To_core_Event } 
func Convert_core_Event Series_To_v1_Event Series ( in * core . Event Series , out * v1 . Event Series , s conversion . Scope ) error { return auto Convert_core_Event Series_To_v1_Event } 
func Convert_v1_Event Source_To_core_Event Source ( in * v1 . Event Source , out * core . Event Source , s conversion . Scope ) error { return auto Convert_v1_Event Source_To_core_Event } 
func Convert_core_Event Source_To_v1_Event Source ( in * core . Event Source , out * v1 . Event Source , s conversion . Scope ) error { return auto Convert_core_Event Source_To_v1_Event } 
func Convert_v1_Exec Action_To_core_Exec Action ( in * v1 . Exec Action , out * core . Exec Action , s conversion . Scope ) error { return auto Convert_v1_Exec Action_To_core_Exec } 
func Convert_core_Exec Action_To_v1_Exec Action ( in * core . Exec Action , out * v1 . Exec Action , s conversion . Scope ) error { return auto Convert_core_Exec Action_To_v1_Exec } 
func Convert_v1_FC Volume Source_To_core_FC Volume Source ( in * v1 . FC Volume Source , out * core . FC Volume Source , s conversion . Scope ) error { return auto Convert_v1_FC Volume Source_To_core_FC Volume } 
func Convert_core_FC Volume Source_To_v1_FC Volume Source ( in * core . FC Volume Source , out * v1 . FC Volume Source , s conversion . Scope ) error { return auto Convert_core_FC Volume Source_To_v1_FC Volume } 
func Convert_v1_Flex Persistent Volume Source_To_core_Flex Persistent Volume Source ( in * v1 . Flex Persistent Volume Source , out * core . Flex Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Flex Persistent Volume Source_To_core_Flex Persistent Volume } 
func Convert_core_Flex Persistent Volume Source_To_v1_Flex Persistent Volume Source ( in * core . Flex Persistent Volume Source , out * v1 . Flex Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Flex Persistent Volume Source_To_v1_Flex Persistent Volume } 
func Convert_v1_Flex Volume Source_To_core_Flex Volume Source ( in * v1 . Flex Volume Source , out * core . Flex Volume Source , s conversion . Scope ) error { return auto Convert_v1_Flex Volume Source_To_core_Flex Volume } 
func Convert_core_Flex Volume Source_To_v1_Flex Volume Source ( in * core . Flex Volume Source , out * v1 . Flex Volume Source , s conversion . Scope ) error { return auto Convert_core_Flex Volume Source_To_v1_Flex Volume } 
func Convert_v1_Flocker Volume Source_To_core_Flocker Volume Source ( in * v1 . Flocker Volume Source , out * core . Flocker Volume Source , s conversion . Scope ) error { return auto Convert_v1_Flocker Volume Source_To_core_Flocker Volume } 
func Convert_core_Flocker Volume Source_To_v1_Flocker Volume Source ( in * core . Flocker Volume Source , out * v1 . Flocker Volume Source , s conversion . Scope ) error { return auto Convert_core_Flocker Volume Source_To_v1_Flocker Volume } 
func Convert_v1_GCE Persistent Disk Volume Source_To_core_GCE Persistent Disk Volume Source ( in * v1 . GCE Persistent Disk Volume Source , out * core . GCE Persistent Disk Volume Source , s conversion . Scope ) error { return auto Convert_v1_GCE Persistent Disk Volume Source_To_core_GCE Persistent Disk Volume } 
func Convert_core_GCE Persistent Disk Volume Source_To_v1_GCE Persistent Disk Volume Source ( in * core . GCE Persistent Disk Volume Source , out * v1 . GCE Persistent Disk Volume Source , s conversion . Scope ) error { return auto Convert_core_GCE Persistent Disk Volume Source_To_v1_GCE Persistent Disk Volume } 
func Convert_v1_Git Repo Volume Source_To_core_Git Repo Volume Source ( in * v1 . Git Repo Volume Source , out * core . Git Repo Volume Source , s conversion . Scope ) error { return auto Convert_v1_Git Repo Volume Source_To_core_Git Repo Volume } 
func Convert_core_Git Repo Volume Source_To_v1_Git Repo Volume Source ( in * core . Git Repo Volume Source , out * v1 . Git Repo Volume Source , s conversion . Scope ) error { return auto Convert_core_Git Repo Volume Source_To_v1_Git Repo Volume } 
func Convert_v1_Glusterfs Persistent Volume Source_To_core_Glusterfs Persistent Volume Source ( in * v1 . Glusterfs Persistent Volume Source , out * core . Glusterfs Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Glusterfs Persistent Volume Source_To_core_Glusterfs Persistent Volume } 
func Convert_core_Glusterfs Persistent Volume Source_To_v1_Glusterfs Persistent Volume Source ( in * core . Glusterfs Persistent Volume Source , out * v1 . Glusterfs Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Glusterfs Persistent Volume Source_To_v1_Glusterfs Persistent Volume } 
func Convert_v1_Glusterfs Volume Source_To_core_Glusterfs Volume Source ( in * v1 . Glusterfs Volume Source , out * core . Glusterfs Volume Source , s conversion . Scope ) error { return auto Convert_v1_Glusterfs Volume Source_To_core_Glusterfs Volume } 
func Convert_core_Glusterfs Volume Source_To_v1_Glusterfs Volume Source ( in * core . Glusterfs Volume Source , out * v1 . Glusterfs Volume Source , s conversion . Scope ) error { return auto Convert_core_Glusterfs Volume Source_To_v1_Glusterfs Volume } 
func Convert_v1_HTTP Get Action_To_core_HTTP Get Action ( in * v1 . HTTP Get Action , out * core . HTTP Get Action , s conversion . Scope ) error { return auto Convert_v1_HTTP Get Action_To_core_HTTP Get } 
func Convert_core_HTTP Get Action_To_v1_HTTP Get Action ( in * core . HTTP Get Action , out * v1 . HTTP Get Action , s conversion . Scope ) error { return auto Convert_core_HTTP Get Action_To_v1_HTTP Get } 
func Convert_v1_HTTP Header_To_core_HTTP Header ( in * v1 . HTTP Header , out * core . HTTP Header , s conversion . Scope ) error { return auto Convert_v1_HTTP Header_To_core_HTTP } 
func Convert_core_HTTP Header_To_v1_HTTP Header ( in * core . HTTP Header , out * v1 . HTTP Header , s conversion . Scope ) error { return auto Convert_core_HTTP Header_To_v1_HTTP } 
func Convert_v1_Handler_To_core_Handler ( in * v1 . Handler , out * core . Handler , s conversion . Scope ) error { return auto } 
func Convert_core_Handler_To_v1_Handler ( in * core . Handler , out * v1 . Handler , s conversion . Scope ) error { return auto } 
func Convert_v1_Host Alias_To_core_Host Alias ( in * v1 . Host Alias , out * core . Host Alias , s conversion . Scope ) error { return auto Convert_v1_Host Alias_To_core_Host } 
func Convert_core_Host Alias_To_v1_Host Alias ( in * core . Host Alias , out * v1 . Host Alias , s conversion . Scope ) error { return auto Convert_core_Host Alias_To_v1_Host } 
func Convert_v1_Host Path Volume Source_To_core_Host Path Volume Source ( in * v1 . Host Path Volume Source , out * core . Host Path Volume Source , s conversion . Scope ) error { return auto Convert_v1_Host Path Volume Source_To_core_Host Path Volume } 
func Convert_core_Host Path Volume Source_To_v1_Host Path Volume Source ( in * core . Host Path Volume Source , out * v1 . Host Path Volume Source , s conversion . Scope ) error { return auto Convert_core_Host Path Volume Source_To_v1_Host Path Volume } 
func Convert_v1_ISCSI Persistent Volume Source_To_core_ISCSI Persistent Volume Source ( in * v1 . ISCSI Persistent Volume Source , out * core . ISCSI Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_ISCSI Persistent Volume Source_To_core_ISCSI Persistent Volume } 
func Convert_core_ISCSI Persistent Volume Source_To_v1_ISCSI Persistent Volume Source ( in * core . ISCSI Persistent Volume Source , out * v1 . ISCSI Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_ISCSI Persistent Volume Source_To_v1_ISCSI Persistent Volume } 
func Convert_v1_ISCSI Volume Source_To_core_ISCSI Volume Source ( in * v1 . ISCSI Volume Source , out * core . ISCSI Volume Source , s conversion . Scope ) error { return auto Convert_v1_ISCSI Volume Source_To_core_ISCSI Volume } 
func Convert_core_ISCSI Volume Source_To_v1_ISCSI Volume Source ( in * core . ISCSI Volume Source , out * v1 . ISCSI Volume Source , s conversion . Scope ) error { return auto Convert_core_ISCSI Volume Source_To_v1_ISCSI Volume } 
func Convert_v1_Key To Path_To_core_Key To Path ( in * v1 . Key To Path , out * core . Key To Path , s conversion . Scope ) error { return auto Convert_v1_Key To Path_To_core_Key To } 
func Convert_core_Key To Path_To_v1_Key To Path ( in * core . Key To Path , out * v1 . Key To Path , s conversion . Scope ) error { return auto Convert_core_Key To Path_To_v1_Key To } 
func Convert_v1_Lifecycle_To_core_Lifecycle ( in * v1 . Lifecycle , out * core . Lifecycle , s conversion . Scope ) error { return auto } 
func Convert_core_Lifecycle_To_v1_Lifecycle ( in * core . Lifecycle , out * v1 . Lifecycle , s conversion . Scope ) error { return auto } 
func Convert_v1_Limit Range_To_core_Limit Range ( in * v1 . Limit Range , out * core . Limit Range , s conversion . Scope ) error { return auto Convert_v1_Limit Range_To_core_Limit } 
func Convert_core_Limit Range_To_v1_Limit Range ( in * core . Limit Range , out * v1 . Limit Range , s conversion . Scope ) error { return auto Convert_core_Limit Range_To_v1_Limit } 
func Convert_v1_Limit Range Item_To_core_Limit Range Item ( in * v1 . Limit Range Item , out * core . Limit Range Item , s conversion . Scope ) error { return auto Convert_v1_Limit Range Item_To_core_Limit Range } 
func Convert_core_Limit Range Item_To_v1_Limit Range Item ( in * core . Limit Range Item , out * v1 . Limit Range Item , s conversion . Scope ) error { return auto Convert_core_Limit Range Item_To_v1_Limit Range } 
func Convert_v1_Limit Range List_To_core_Limit Range List ( in * v1 . Limit Range List , out * core . Limit Range List , s conversion . Scope ) error { return auto Convert_v1_Limit Range List_To_core_Limit Range } 
func Convert_core_Limit Range List_To_v1_Limit Range List ( in * core . Limit Range List , out * v1 . Limit Range List , s conversion . Scope ) error { return auto Convert_core_Limit Range List_To_v1_Limit Range } 
func Convert_v1_Limit Range Spec_To_core_Limit Range Spec ( in * v1 . Limit Range Spec , out * core . Limit Range Spec , s conversion . Scope ) error { return auto Convert_v1_Limit Range Spec_To_core_Limit Range } 
func Convert_core_Limit Range Spec_To_v1_Limit Range Spec ( in * core . Limit Range Spec , out * v1 . Limit Range Spec , s conversion . Scope ) error { return auto Convert_core_Limit Range Spec_To_v1_Limit Range } 
func Convert_v1_List_To_core_List ( in * v1 . List , out * core . List , s conversion . Scope ) error { return auto } 
func Convert_core_List_To_v1_List ( in * core . List , out * v1 . List , s conversion . Scope ) error { return auto } 
func Convert_v1_Load Balancer Ingress_To_core_Load Balancer Ingress ( in * v1 . Load Balancer Ingress , out * core . Load Balancer Ingress , s conversion . Scope ) error { return auto Convert_v1_Load Balancer Ingress_To_core_Load Balancer } 
func Convert_core_Load Balancer Ingress_To_v1_Load Balancer Ingress ( in * core . Load Balancer Ingress , out * v1 . Load Balancer Ingress , s conversion . Scope ) error { return auto Convert_core_Load Balancer Ingress_To_v1_Load Balancer } 
func Convert_v1_Load Balancer Status_To_core_Load Balancer Status ( in * v1 . Load Balancer Status , out * core . Load Balancer Status , s conversion . Scope ) error { return auto Convert_v1_Load Balancer Status_To_core_Load Balancer } 
func Convert_core_Load Balancer Status_To_v1_Load Balancer Status ( in * core . Load Balancer Status , out * v1 . Load Balancer Status , s conversion . Scope ) error { return auto Convert_core_Load Balancer Status_To_v1_Load Balancer } 
func Convert_v1_Local Object Reference_To_core_Local Object Reference ( in * v1 . Local Object Reference , out * core . Local Object Reference , s conversion . Scope ) error { return auto Convert_v1_Local Object Reference_To_core_Local Object } 
func Convert_core_Local Object Reference_To_v1_Local Object Reference ( in * core . Local Object Reference , out * v1 . Local Object Reference , s conversion . Scope ) error { return auto Convert_core_Local Object Reference_To_v1_Local Object } 
func Convert_v1_Local Volume Source_To_core_Local Volume Source ( in * v1 . Local Volume Source , out * core . Local Volume Source , s conversion . Scope ) error { return auto Convert_v1_Local Volume Source_To_core_Local Volume } 
func Convert_core_Local Volume Source_To_v1_Local Volume Source ( in * core . Local Volume Source , out * v1 . Local Volume Source , s conversion . Scope ) error { return auto Convert_core_Local Volume Source_To_v1_Local Volume } 
func Convert_v1_NFS Volume Source_To_core_NFS Volume Source ( in * v1 . NFS Volume Source , out * core . NFS Volume Source , s conversion . Scope ) error { return auto Convert_v1_NFS Volume Source_To_core_NFS Volume } 
func Convert_core_NFS Volume Source_To_v1_NFS Volume Source ( in * core . NFS Volume Source , out * v1 . NFS Volume Source , s conversion . Scope ) error { return auto Convert_core_NFS Volume Source_To_v1_NFS Volume } 
func Convert_v1_Namespace_To_core_Namespace ( in * v1 . Namespace , out * core . Namespace , s conversion . Scope ) error { return auto } 
func Convert_core_Namespace_To_v1_Namespace ( in * core . Namespace , out * v1 . Namespace , s conversion . Scope ) error { return auto } 
func Convert_v1_Namespace List_To_core_Namespace List ( in * v1 . Namespace List , out * core . Namespace List , s conversion . Scope ) error { return auto Convert_v1_Namespace List_To_core_Namespace } 
func Convert_core_Namespace List_To_v1_Namespace List ( in * core . Namespace List , out * v1 . Namespace List , s conversion . Scope ) error { return auto Convert_core_Namespace List_To_v1_Namespace } 
func Convert_v1_Namespace Spec_To_core_Namespace Spec ( in * v1 . Namespace Spec , out * core . Namespace Spec , s conversion . Scope ) error { return auto Convert_v1_Namespace Spec_To_core_Namespace } 
func Convert_core_Namespace Spec_To_v1_Namespace Spec ( in * core . Namespace Spec , out * v1 . Namespace Spec , s conversion . Scope ) error { return auto Convert_core_Namespace Spec_To_v1_Namespace } 
func Convert_v1_Namespace Status_To_core_Namespace Status ( in * v1 . Namespace Status , out * core . Namespace Status , s conversion . Scope ) error { return auto Convert_v1_Namespace Status_To_core_Namespace } 
func Convert_core_Namespace Status_To_v1_Namespace Status ( in * core . Namespace Status , out * v1 . Namespace Status , s conversion . Scope ) error { return auto Convert_core_Namespace Status_To_v1_Namespace } 
func Convert_v1_Node_To_core_Node ( in * v1 . Node , out * core . Node , s conversion . Scope ) error { return auto } 
func Convert_core_Node_To_v1_Node ( in * core . Node , out * v1 . Node , s conversion . Scope ) error { return auto } 
func Convert_v1_Node Address_To_core_Node Address ( in * v1 . Node Address , out * core . Node Address , s conversion . Scope ) error { return auto Convert_v1_Node Address_To_core_Node } 
func Convert_core_Node Address_To_v1_Node Address ( in * core . Node Address , out * v1 . Node Address , s conversion . Scope ) error { return auto Convert_core_Node Address_To_v1_Node } 
func Convert_v1_Node Affinity_To_core_Node Affinity ( in * v1 . Node Affinity , out * core . Node Affinity , s conversion . Scope ) error { return auto Convert_v1_Node Affinity_To_core_Node } 
func Convert_core_Node Affinity_To_v1_Node Affinity ( in * core . Node Affinity , out * v1 . Node Affinity , s conversion . Scope ) error { return auto Convert_core_Node Affinity_To_v1_Node } 
func Convert_v1_Node Condition_To_core_Node Condition ( in * v1 . Node Condition , out * core . Node Condition , s conversion . Scope ) error { return auto Convert_v1_Node Condition_To_core_Node } 
func Convert_core_Node Condition_To_v1_Node Condition ( in * core . Node Condition , out * v1 . Node Condition , s conversion . Scope ) error { return auto Convert_core_Node Condition_To_v1_Node } 
func Convert_v1_Node Config Source_To_core_Node Config Source ( in * v1 . Node Config Source , out * core . Node Config Source , s conversion . Scope ) error { return auto Convert_v1_Node Config Source_To_core_Node Config } 
func Convert_core_Node Config Source_To_v1_Node Config Source ( in * core . Node Config Source , out * v1 . Node Config Source , s conversion . Scope ) error { return auto Convert_core_Node Config Source_To_v1_Node Config } 
func Convert_v1_Node Config Status_To_core_Node Config Status ( in * v1 . Node Config Status , out * core . Node Config Status , s conversion . Scope ) error { return auto Convert_v1_Node Config Status_To_core_Node Config } 
func Convert_core_Node Config Status_To_v1_Node Config Status ( in * core . Node Config Status , out * v1 . Node Config Status , s conversion . Scope ) error { return auto Convert_core_Node Config Status_To_v1_Node Config } 
func Convert_v1_Node Daemon Endpoints_To_core_Node Daemon Endpoints ( in * v1 . Node Daemon Endpoints , out * core . Node Daemon Endpoints , s conversion . Scope ) error { return auto Convert_v1_Node Daemon Endpoints_To_core_Node Daemon } 
func Convert_core_Node Daemon Endpoints_To_v1_Node Daemon Endpoints ( in * core . Node Daemon Endpoints , out * v1 . Node Daemon Endpoints , s conversion . Scope ) error { return auto Convert_core_Node Daemon Endpoints_To_v1_Node Daemon } 
func Convert_v1_Node List_To_core_Node List ( in * v1 . Node List , out * core . Node List , s conversion . Scope ) error { return auto Convert_v1_Node List_To_core_Node } 
func Convert_core_Node List_To_v1_Node List ( in * core . Node List , out * v1 . Node List , s conversion . Scope ) error { return auto Convert_core_Node List_To_v1_Node } 
func Convert_v1_Node Proxy Options_To_core_Node Proxy Options ( in * v1 . Node Proxy Options , out * core . Node Proxy Options , s conversion . Scope ) error { return auto Convert_v1_Node Proxy Options_To_core_Node Proxy } 
func Convert_core_Node Proxy Options_To_v1_Node Proxy Options ( in * core . Node Proxy Options , out * v1 . Node Proxy Options , s conversion . Scope ) error { return auto Convert_core_Node Proxy Options_To_v1_Node Proxy } 
func Convert_v1_Node Resources_To_core_Node Resources ( in * v1 . Node Resources , out * core . Node Resources , s conversion . Scope ) error { return auto Convert_v1_Node Resources_To_core_Node } 
func Convert_core_Node Resources_To_v1_Node Resources ( in * core . Node Resources , out * v1 . Node Resources , s conversion . Scope ) error { return auto Convert_core_Node Resources_To_v1_Node } 
func Convert_v1_Node Selector_To_core_Node Selector ( in * v1 . Node Selector , out * core . Node Selector , s conversion . Scope ) error { return auto Convert_v1_Node Selector_To_core_Node } 
func Convert_core_Node Selector_To_v1_Node Selector ( in * core . Node Selector , out * v1 . Node Selector , s conversion . Scope ) error { return auto Convert_core_Node Selector_To_v1_Node } 
func Convert_v1_Node Selector Requirement_To_core_Node Selector Requirement ( in * v1 . Node Selector Requirement , out * core . Node Selector Requirement , s conversion . Scope ) error { return auto Convert_v1_Node Selector Requirement_To_core_Node Selector } 
func Convert_core_Node Selector Requirement_To_v1_Node Selector Requirement ( in * core . Node Selector Requirement , out * v1 . Node Selector Requirement , s conversion . Scope ) error { return auto Convert_core_Node Selector Requirement_To_v1_Node Selector } 
func Convert_v1_Node Selector Term_To_core_Node Selector Term ( in * v1 . Node Selector Term , out * core . Node Selector Term , s conversion . Scope ) error { return auto Convert_v1_Node Selector Term_To_core_Node Selector } 
func Convert_core_Node Selector Term_To_v1_Node Selector Term ( in * core . Node Selector Term , out * v1 . Node Selector Term , s conversion . Scope ) error { return auto Convert_core_Node Selector Term_To_v1_Node Selector } 
func Convert_v1_Node Spec_To_core_Node Spec ( in * v1 . Node Spec , out * core . Node Spec , s conversion . Scope ) error { return auto Convert_v1_Node Spec_To_core_Node } 
func Convert_core_Node Spec_To_v1_Node Spec ( in * core . Node Spec , out * v1 . Node Spec , s conversion . Scope ) error { return auto Convert_core_Node Spec_To_v1_Node } 
func Convert_v1_Node Status_To_core_Node Status ( in * v1 . Node Status , out * core . Node Status , s conversion . Scope ) error { return auto Convert_v1_Node Status_To_core_Node } 
func Convert_core_Node Status_To_v1_Node Status ( in * core . Node Status , out * v1 . Node Status , s conversion . Scope ) error { return auto Convert_core_Node Status_To_v1_Node } 
func Convert_v1_Node System Info_To_core_Node System Info ( in * v1 . Node System Info , out * core . Node System Info , s conversion . Scope ) error { return auto Convert_v1_Node System Info_To_core_Node System } 
func Convert_core_Node System Info_To_v1_Node System Info ( in * core . Node System Info , out * v1 . Node System Info , s conversion . Scope ) error { return auto Convert_core_Node System Info_To_v1_Node System } 
func Convert_v1_Object Field Selector_To_core_Object Field Selector ( in * v1 . Object Field Selector , out * core . Object Field Selector , s conversion . Scope ) error { return auto Convert_v1_Object Field Selector_To_core_Object Field } 
func Convert_core_Object Field Selector_To_v1_Object Field Selector ( in * core . Object Field Selector , out * v1 . Object Field Selector , s conversion . Scope ) error { return auto Convert_core_Object Field Selector_To_v1_Object Field } 
func Convert_v1_Object Reference_To_core_Object Reference ( in * v1 . Object Reference , out * core . Object Reference , s conversion . Scope ) error { return auto Convert_v1_Object Reference_To_core_Object } 
func Convert_core_Object Reference_To_v1_Object Reference ( in * core . Object Reference , out * v1 . Object Reference , s conversion . Scope ) error { return auto Convert_core_Object Reference_To_v1_Object } 
func Convert_v1_Persistent Volume_To_core_Persistent Volume ( in * v1 . Persistent Volume , out * core . Persistent Volume , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume_To_core_Persistent } 
func Convert_core_Persistent Volume_To_v1_Persistent Volume ( in * core . Persistent Volume , out * v1 . Persistent Volume , s conversion . Scope ) error { return auto Convert_core_Persistent Volume_To_v1_Persistent } 
func Convert_v1_Persistent Volume Claim_To_core_Persistent Volume Claim ( in * v1 . Persistent Volume Claim , out * core . Persistent Volume Claim , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Claim_To_core_Persistent Volume } 
func Convert_core_Persistent Volume Claim_To_v1_Persistent Volume Claim ( in * core . Persistent Volume Claim , out * v1 . Persistent Volume Claim , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Claim_To_v1_Persistent Volume } 
func Convert_v1_Persistent Volume Claim Condition_To_core_Persistent Volume Claim Condition ( in * v1 . Persistent Volume Claim Condition , out * core . Persistent Volume Claim Condition , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Claim Condition_To_core_Persistent Volume Claim } 
func Convert_core_Persistent Volume Claim Condition_To_v1_Persistent Volume Claim Condition ( in * core . Persistent Volume Claim Condition , out * v1 . Persistent Volume Claim Condition , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Claim Condition_To_v1_Persistent Volume Claim } 
func Convert_v1_Persistent Volume Claim List_To_core_Persistent Volume Claim List ( in * v1 . Persistent Volume Claim List , out * core . Persistent Volume Claim List , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Claim List_To_core_Persistent Volume Claim } 
func Convert_core_Persistent Volume Claim List_To_v1_Persistent Volume Claim List ( in * core . Persistent Volume Claim List , out * v1 . Persistent Volume Claim List , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Claim List_To_v1_Persistent Volume Claim } 
func Convert_v1_Persistent Volume Claim Spec_To_core_Persistent Volume Claim Spec ( in * v1 . Persistent Volume Claim Spec , out * core . Persistent Volume Claim Spec , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Claim Spec_To_core_Persistent Volume Claim } 
func Convert_core_Persistent Volume Claim Spec_To_v1_Persistent Volume Claim Spec ( in * core . Persistent Volume Claim Spec , out * v1 . Persistent Volume Claim Spec , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Claim Spec_To_v1_Persistent Volume Claim } 
func Convert_v1_Persistent Volume Claim Status_To_core_Persistent Volume Claim Status ( in * v1 . Persistent Volume Claim Status , out * core . Persistent Volume Claim Status , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Claim Status_To_core_Persistent Volume Claim } 
func Convert_core_Persistent Volume Claim Status_To_v1_Persistent Volume Claim Status ( in * core . Persistent Volume Claim Status , out * v1 . Persistent Volume Claim Status , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Claim Status_To_v1_Persistent Volume Claim } 
func Convert_v1_Persistent Volume Claim Volume Source_To_core_Persistent Volume Claim Volume Source ( in * v1 . Persistent Volume Claim Volume Source , out * core . Persistent Volume Claim Volume Source , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Claim Volume Source_To_core_Persistent Volume Claim Volume } 
func Convert_core_Persistent Volume Claim Volume Source_To_v1_Persistent Volume Claim Volume Source ( in * core . Persistent Volume Claim Volume Source , out * v1 . Persistent Volume Claim Volume Source , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Claim Volume Source_To_v1_Persistent Volume Claim Volume } 
func Convert_v1_Persistent Volume List_To_core_Persistent Volume List ( in * v1 . Persistent Volume List , out * core . Persistent Volume List , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume List_To_core_Persistent Volume } 
func Convert_core_Persistent Volume List_To_v1_Persistent Volume List ( in * core . Persistent Volume List , out * v1 . Persistent Volume List , s conversion . Scope ) error { return auto Convert_core_Persistent Volume List_To_v1_Persistent Volume } 
func Convert_v1_Persistent Volume Source_To_core_Persistent Volume Source ( in * v1 . Persistent Volume Source , out * core . Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Source_To_core_Persistent Volume } 
func Convert_core_Persistent Volume Source_To_v1_Persistent Volume Source ( in * core . Persistent Volume Source , out * v1 . Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Source_To_v1_Persistent Volume } 
func Convert_v1_Persistent Volume Spec_To_core_Persistent Volume Spec ( in * v1 . Persistent Volume Spec , out * core . Persistent Volume Spec , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Spec_To_core_Persistent Volume } 
func Convert_core_Persistent Volume Spec_To_v1_Persistent Volume Spec ( in * core . Persistent Volume Spec , out * v1 . Persistent Volume Spec , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Spec_To_v1_Persistent Volume } 
func Convert_v1_Persistent Volume Status_To_core_Persistent Volume Status ( in * v1 . Persistent Volume Status , out * core . Persistent Volume Status , s conversion . Scope ) error { return auto Convert_v1_Persistent Volume Status_To_core_Persistent Volume } 
func Convert_core_Persistent Volume Status_To_v1_Persistent Volume Status ( in * core . Persistent Volume Status , out * v1 . Persistent Volume Status , s conversion . Scope ) error { return auto Convert_core_Persistent Volume Status_To_v1_Persistent Volume } 
func Convert_v1_Photon Persistent Disk Volume Source_To_core_Photon Persistent Disk Volume Source ( in * v1 . Photon Persistent Disk Volume Source , out * core . Photon Persistent Disk Volume Source , s conversion . Scope ) error { return auto Convert_v1_Photon Persistent Disk Volume Source_To_core_Photon Persistent Disk Volume } 
func Convert_core_Photon Persistent Disk Volume Source_To_v1_Photon Persistent Disk Volume Source ( in * core . Photon Persistent Disk Volume Source , out * v1 . Photon Persistent Disk Volume Source , s conversion . Scope ) error { return auto Convert_core_Photon Persistent Disk Volume Source_To_v1_Photon Persistent Disk Volume } 
func Convert_v1_Pod Affinity_To_core_Pod Affinity ( in * v1 . Pod Affinity , out * core . Pod Affinity , s conversion . Scope ) error { return auto Convert_v1_Pod Affinity_To_core_Pod } 
func Convert_core_Pod Affinity_To_v1_Pod Affinity ( in * core . Pod Affinity , out * v1 . Pod Affinity , s conversion . Scope ) error { return auto Convert_core_Pod Affinity_To_v1_Pod } 
func Convert_v1_Pod Affinity Term_To_core_Pod Affinity Term ( in * v1 . Pod Affinity Term , out * core . Pod Affinity Term , s conversion . Scope ) error { return auto Convert_v1_Pod Affinity Term_To_core_Pod Affinity } 
func Convert_core_Pod Affinity Term_To_v1_Pod Affinity Term ( in * core . Pod Affinity Term , out * v1 . Pod Affinity Term , s conversion . Scope ) error { return auto Convert_core_Pod Affinity Term_To_v1_Pod Affinity } 
func Convert_v1_Pod Anti Affinity_To_core_Pod Anti Affinity ( in * v1 . Pod Anti Affinity , out * core . Pod Anti Affinity , s conversion . Scope ) error { return auto Convert_v1_Pod Anti Affinity_To_core_Pod Anti } 
func Convert_core_Pod Anti Affinity_To_v1_Pod Anti Affinity ( in * core . Pod Anti Affinity , out * v1 . Pod Anti Affinity , s conversion . Scope ) error { return auto Convert_core_Pod Anti Affinity_To_v1_Pod Anti } 
func Convert_v1_Pod Attach Options_To_core_Pod Attach Options ( in * v1 . Pod Attach Options , out * core . Pod Attach Options , s conversion . Scope ) error { return auto Convert_v1_Pod Attach Options_To_core_Pod Attach } 
func Convert_core_Pod Attach Options_To_v1_Pod Attach Options ( in * core . Pod Attach Options , out * v1 . Pod Attach Options , s conversion . Scope ) error { return auto Convert_core_Pod Attach Options_To_v1_Pod Attach } 
func Convert_v1_Pod Condition_To_core_Pod Condition ( in * v1 . Pod Condition , out * core . Pod Condition , s conversion . Scope ) error { return auto Convert_v1_Pod Condition_To_core_Pod } 
func Convert_core_Pod Condition_To_v1_Pod Condition ( in * core . Pod Condition , out * v1 . Pod Condition , s conversion . Scope ) error { return auto Convert_core_Pod Condition_To_v1_Pod } 
func Convert_v1_Pod DNS Config_To_core_Pod DNS Config ( in * v1 . Pod DNS Config , out * core . Pod DNS Config , s conversion . Scope ) error { return auto Convert_v1_Pod DNS Config_To_core_Pod DNS } 
func Convert_core_Pod DNS Config_To_v1_Pod DNS Config ( in * core . Pod DNS Config , out * v1 . Pod DNS Config , s conversion . Scope ) error { return auto Convert_core_Pod DNS Config_To_v1_Pod DNS } 
func Convert_v1_Pod DNS Config Option_To_core_Pod DNS Config Option ( in * v1 . Pod DNS Config Option , out * core . Pod DNS Config Option , s conversion . Scope ) error { return auto Convert_v1_Pod DNS Config Option_To_core_Pod DNS Config } 
func Convert_core_Pod DNS Config Option_To_v1_Pod DNS Config Option ( in * core . Pod DNS Config Option , out * v1 . Pod DNS Config Option , s conversion . Scope ) error { return auto Convert_core_Pod DNS Config Option_To_v1_Pod DNS Config } 
func Convert_v1_Pod Exec Options_To_core_Pod Exec Options ( in * v1 . Pod Exec Options , out * core . Pod Exec Options , s conversion . Scope ) error { return auto Convert_v1_Pod Exec Options_To_core_Pod Exec } 
func Convert_core_Pod Exec Options_To_v1_Pod Exec Options ( in * core . Pod Exec Options , out * v1 . Pod Exec Options , s conversion . Scope ) error { return auto Convert_core_Pod Exec Options_To_v1_Pod Exec } 
func Convert_v1_Pod List_To_core_Pod List ( in * v1 . Pod List , out * core . Pod List , s conversion . Scope ) error { return auto Convert_v1_Pod List_To_core_Pod } 
func Convert_core_Pod List_To_v1_Pod List ( in * core . Pod List , out * v1 . Pod List , s conversion . Scope ) error { return auto Convert_core_Pod List_To_v1_Pod } 
func Convert_v1_Pod Log Options_To_core_Pod Log Options ( in * v1 . Pod Log Options , out * core . Pod Log Options , s conversion . Scope ) error { return auto Convert_v1_Pod Log Options_To_core_Pod Log } 
func Convert_core_Pod Log Options_To_v1_Pod Log Options ( in * core . Pod Log Options , out * v1 . Pod Log Options , s conversion . Scope ) error { return auto Convert_core_Pod Log Options_To_v1_Pod Log } 
func Convert_v1_Pod Port Forward Options_To_core_Pod Port Forward Options ( in * v1 . Pod Port Forward Options , out * core . Pod Port Forward Options , s conversion . Scope ) error { return auto Convert_v1_Pod Port Forward Options_To_core_Pod Port Forward } 
func Convert_core_Pod Port Forward Options_To_v1_Pod Port Forward Options ( in * core . Pod Port Forward Options , out * v1 . Pod Port Forward Options , s conversion . Scope ) error { return auto Convert_core_Pod Port Forward Options_To_v1_Pod Port Forward } 
func Convert_v1_Pod Proxy Options_To_core_Pod Proxy Options ( in * v1 . Pod Proxy Options , out * core . Pod Proxy Options , s conversion . Scope ) error { return auto Convert_v1_Pod Proxy Options_To_core_Pod Proxy } 
func Convert_core_Pod Proxy Options_To_v1_Pod Proxy Options ( in * core . Pod Proxy Options , out * v1 . Pod Proxy Options , s conversion . Scope ) error { return auto Convert_core_Pod Proxy Options_To_v1_Pod Proxy } 
func Convert_v1_Pod Readiness Gate_To_core_Pod Readiness Gate ( in * v1 . Pod Readiness Gate , out * core . Pod Readiness Gate , s conversion . Scope ) error { return auto Convert_v1_Pod Readiness Gate_To_core_Pod Readiness } 
func Convert_core_Pod Readiness Gate_To_v1_Pod Readiness Gate ( in * core . Pod Readiness Gate , out * v1 . Pod Readiness Gate , s conversion . Scope ) error { return auto Convert_core_Pod Readiness Gate_To_v1_Pod Readiness } 
func Convert_v1_Pod Security Context_To_core_Pod Security Context ( in * v1 . Pod Security Context , out * core . Pod Security Context , s conversion . Scope ) error { return auto Convert_v1_Pod Security Context_To_core_Pod Security } 
func Convert_core_Pod Security Context_To_v1_Pod Security Context ( in * core . Pod Security Context , out * v1 . Pod Security Context , s conversion . Scope ) error { return auto Convert_core_Pod Security Context_To_v1_Pod Security } 
func Convert_v1_Pod Signature_To_core_Pod Signature ( in * v1 . Pod Signature , out * core . Pod Signature , s conversion . Scope ) error { return auto Convert_v1_Pod Signature_To_core_Pod } 
func Convert_core_Pod Signature_To_v1_Pod Signature ( in * core . Pod Signature , out * v1 . Pod Signature , s conversion . Scope ) error { return auto Convert_core_Pod Signature_To_v1_Pod } 
func Convert_v1_Pod Status_To_core_Pod Status ( in * v1 . Pod Status , out * core . Pod Status , s conversion . Scope ) error { return auto Convert_v1_Pod Status_To_core_Pod } 
func Convert_core_Pod Status_To_v1_Pod Status ( in * core . Pod Status , out * v1 . Pod Status , s conversion . Scope ) error { return auto Convert_core_Pod Status_To_v1_Pod } 
func Convert_v1_Pod Status Result_To_core_Pod Status Result ( in * v1 . Pod Status Result , out * core . Pod Status Result , s conversion . Scope ) error { return auto Convert_v1_Pod Status Result_To_core_Pod Status } 
func Convert_core_Pod Status Result_To_v1_Pod Status Result ( in * core . Pod Status Result , out * v1 . Pod Status Result , s conversion . Scope ) error { return auto Convert_core_Pod Status Result_To_v1_Pod Status } 
func Convert_v1_Pod Template_To_core_Pod Template ( in * v1 . Pod Template , out * core . Pod Template , s conversion . Scope ) error { return auto Convert_v1_Pod Template_To_core_Pod } 
func Convert_core_Pod Template_To_v1_Pod Template ( in * core . Pod Template , out * v1 . Pod Template , s conversion . Scope ) error { return auto Convert_core_Pod Template_To_v1_Pod } 
func Convert_v1_Pod Template List_To_core_Pod Template List ( in * v1 . Pod Template List , out * core . Pod Template List , s conversion . Scope ) error { return auto Convert_v1_Pod Template List_To_core_Pod Template } 
func Convert_core_Pod Template List_To_v1_Pod Template List ( in * core . Pod Template List , out * v1 . Pod Template List , s conversion . Scope ) error { return auto Convert_core_Pod Template List_To_v1_Pod Template } 
func Convert_v1_Portworx Volume Source_To_core_Portworx Volume Source ( in * v1 . Portworx Volume Source , out * core . Portworx Volume Source , s conversion . Scope ) error { return auto Convert_v1_Portworx Volume Source_To_core_Portworx Volume } 
func Convert_core_Portworx Volume Source_To_v1_Portworx Volume Source ( in * core . Portworx Volume Source , out * v1 . Portworx Volume Source , s conversion . Scope ) error { return auto Convert_core_Portworx Volume Source_To_v1_Portworx Volume } 
func Convert_v1_Preconditions_To_core_Preconditions ( in * v1 . Preconditions , out * core . Preconditions , s conversion . Scope ) error { return auto } 
func Convert_core_Preconditions_To_v1_Preconditions ( in * core . Preconditions , out * v1 . Preconditions , s conversion . Scope ) error { return auto } 
func Convert_v1_Prefer Avoid Pods Entry_To_core_Prefer Avoid Pods Entry ( in * v1 . Prefer Avoid Pods Entry , out * core . Prefer Avoid Pods Entry , s conversion . Scope ) error { return auto Convert_v1_Prefer Avoid Pods Entry_To_core_Prefer Avoid Pods } 
func Convert_core_Prefer Avoid Pods Entry_To_v1_Prefer Avoid Pods Entry ( in * core . Prefer Avoid Pods Entry , out * v1 . Prefer Avoid Pods Entry , s conversion . Scope ) error { return auto Convert_core_Prefer Avoid Pods Entry_To_v1_Prefer Avoid Pods } 
func Convert_v1_Preferred Scheduling Term_To_core_Preferred Scheduling Term ( in * v1 . Preferred Scheduling Term , out * core . Preferred Scheduling Term , s conversion . Scope ) error { return auto Convert_v1_Preferred Scheduling Term_To_core_Preferred Scheduling } 
func Convert_core_Preferred Scheduling Term_To_v1_Preferred Scheduling Term ( in * core . Preferred Scheduling Term , out * v1 . Preferred Scheduling Term , s conversion . Scope ) error { return auto Convert_core_Preferred Scheduling Term_To_v1_Preferred Scheduling } 
func Convert_v1_Probe_To_core_Probe ( in * v1 . Probe , out * core . Probe , s conversion . Scope ) error { return auto } 
func Convert_core_Probe_To_v1_Probe ( in * core . Probe , out * v1 . Probe , s conversion . Scope ) error { return auto } 
func Convert_v1_Projected Volume Source_To_core_Projected Volume Source ( in * v1 . Projected Volume Source , out * core . Projected Volume Source , s conversion . Scope ) error { return auto Convert_v1_Projected Volume Source_To_core_Projected Volume } 
func Convert_core_Projected Volume Source_To_v1_Projected Volume Source ( in * core . Projected Volume Source , out * v1 . Projected Volume Source , s conversion . Scope ) error { return auto Convert_core_Projected Volume Source_To_v1_Projected Volume } 
func Convert_v1_Quobyte Volume Source_To_core_Quobyte Volume Source ( in * v1 . Quobyte Volume Source , out * core . Quobyte Volume Source , s conversion . Scope ) error { return auto Convert_v1_Quobyte Volume Source_To_core_Quobyte Volume } 
func Convert_core_Quobyte Volume Source_To_v1_Quobyte Volume Source ( in * core . Quobyte Volume Source , out * v1 . Quobyte Volume Source , s conversion . Scope ) error { return auto Convert_core_Quobyte Volume Source_To_v1_Quobyte Volume } 
func Convert_v1_RBD Persistent Volume Source_To_core_RBD Persistent Volume Source ( in * v1 . RBD Persistent Volume Source , out * core . RBD Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_RBD Persistent Volume Source_To_core_RBD Persistent Volume } 
func Convert_core_RBD Persistent Volume Source_To_v1_RBD Persistent Volume Source ( in * core . RBD Persistent Volume Source , out * v1 . RBD Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_RBD Persistent Volume Source_To_v1_RBD Persistent Volume } 
func Convert_v1_RBD Volume Source_To_core_RBD Volume Source ( in * v1 . RBD Volume Source , out * core . RBD Volume Source , s conversion . Scope ) error { return auto Convert_v1_RBD Volume Source_To_core_RBD Volume } 
func Convert_core_RBD Volume Source_To_v1_RBD Volume Source ( in * core . RBD Volume Source , out * v1 . RBD Volume Source , s conversion . Scope ) error { return auto Convert_core_RBD Volume Source_To_v1_RBD Volume } 
func Convert_v1_Range Allocation_To_core_Range Allocation ( in * v1 . Range Allocation , out * core . Range Allocation , s conversion . Scope ) error { return auto Convert_v1_Range Allocation_To_core_Range } 
func Convert_core_Range Allocation_To_v1_Range Allocation ( in * core . Range Allocation , out * v1 . Range Allocation , s conversion . Scope ) error { return auto Convert_core_Range Allocation_To_v1_Range } 
func Convert_v1_Replication Controller_To_core_Replication Controller ( in * v1 . Replication Controller , out * core . Replication Controller , s conversion . Scope ) error { return auto Convert_v1_Replication Controller_To_core_Replication } 
func Convert_core_Replication Controller_To_v1_Replication Controller ( in * core . Replication Controller , out * v1 . Replication Controller , s conversion . Scope ) error { return auto Convert_core_Replication Controller_To_v1_Replication } 
func Convert_v1_Replication Controller Condition_To_core_Replication Controller Condition ( in * v1 . Replication Controller Condition , out * core . Replication Controller Condition , s conversion . Scope ) error { return auto Convert_v1_Replication Controller Condition_To_core_Replication Controller } 
func Convert_core_Replication Controller Condition_To_v1_Replication Controller Condition ( in * core . Replication Controller Condition , out * v1 . Replication Controller Condition , s conversion . Scope ) error { return auto Convert_core_Replication Controller Condition_To_v1_Replication Controller } 
func Convert_v1_Replication Controller List_To_core_Replication Controller List ( in * v1 . Replication Controller List , out * core . Replication Controller List , s conversion . Scope ) error { return auto Convert_v1_Replication Controller List_To_core_Replication Controller } 
func Convert_core_Replication Controller List_To_v1_Replication Controller List ( in * core . Replication Controller List , out * v1 . Replication Controller List , s conversion . Scope ) error { return auto Convert_core_Replication Controller List_To_v1_Replication Controller } 
func Convert_v1_Replication Controller Status_To_core_Replication Controller Status ( in * v1 . Replication Controller Status , out * core . Replication Controller Status , s conversion . Scope ) error { return auto Convert_v1_Replication Controller Status_To_core_Replication Controller } 
func Convert_core_Replication Controller Status_To_v1_Replication Controller Status ( in * core . Replication Controller Status , out * v1 . Replication Controller Status , s conversion . Scope ) error { return auto Convert_core_Replication Controller Status_To_v1_Replication Controller } 
func Convert_v1_Resource Field Selector_To_core_Resource Field Selector ( in * v1 . Resource Field Selector , out * core . Resource Field Selector , s conversion . Scope ) error { return auto Convert_v1_Resource Field Selector_To_core_Resource Field } 
func Convert_core_Resource Field Selector_To_v1_Resource Field Selector ( in * core . Resource Field Selector , out * v1 . Resource Field Selector , s conversion . Scope ) error { return auto Convert_core_Resource Field Selector_To_v1_Resource Field } 
func Convert_v1_Resource Quota_To_core_Resource Quota ( in * v1 . Resource Quota , out * core . Resource Quota , s conversion . Scope ) error { return auto Convert_v1_Resource Quota_To_core_Resource } 
func Convert_core_Resource Quota_To_v1_Resource Quota ( in * core . Resource Quota , out * v1 . Resource Quota , s conversion . Scope ) error { return auto Convert_core_Resource Quota_To_v1_Resource } 
func Convert_v1_Resource Quota List_To_core_Resource Quota List ( in * v1 . Resource Quota List , out * core . Resource Quota List , s conversion . Scope ) error { return auto Convert_v1_Resource Quota List_To_core_Resource Quota } 
func Convert_core_Resource Quota List_To_v1_Resource Quota List ( in * core . Resource Quota List , out * v1 . Resource Quota List , s conversion . Scope ) error { return auto Convert_core_Resource Quota List_To_v1_Resource Quota } 
func Convert_v1_Resource Quota Spec_To_core_Resource Quota Spec ( in * v1 . Resource Quota Spec , out * core . Resource Quota Spec , s conversion . Scope ) error { return auto Convert_v1_Resource Quota Spec_To_core_Resource Quota } 
func Convert_core_Resource Quota Spec_To_v1_Resource Quota Spec ( in * core . Resource Quota Spec , out * v1 . Resource Quota Spec , s conversion . Scope ) error { return auto Convert_core_Resource Quota Spec_To_v1_Resource Quota } 
func Convert_v1_Resource Quota Status_To_core_Resource Quota Status ( in * v1 . Resource Quota Status , out * core . Resource Quota Status , s conversion . Scope ) error { return auto Convert_v1_Resource Quota Status_To_core_Resource Quota } 
func Convert_core_Resource Quota Status_To_v1_Resource Quota Status ( in * core . Resource Quota Status , out * v1 . Resource Quota Status , s conversion . Scope ) error { return auto Convert_core_Resource Quota Status_To_v1_Resource Quota } 
func Convert_v1_Resource Requirements_To_core_Resource Requirements ( in * v1 . Resource Requirements , out * core . Resource Requirements , s conversion . Scope ) error { return auto Convert_v1_Resource Requirements_To_core_Resource } 
func Convert_core_Resource Requirements_To_v1_Resource Requirements ( in * core . Resource Requirements , out * v1 . Resource Requirements , s conversion . Scope ) error { return auto Convert_core_Resource Requirements_To_v1_Resource } 
func Convert_v1_SE Linux Options_To_core_SE Linux Options ( in * v1 . SE Linux Options , out * core . SE Linux Options , s conversion . Scope ) error { return auto Convert_v1_SE Linux Options_To_core_SE Linux } 
func Convert_core_SE Linux Options_To_v1_SE Linux Options ( in * core . SE Linux Options , out * v1 . SE Linux Options , s conversion . Scope ) error { return auto Convert_core_SE Linux Options_To_v1_SE Linux } 
func Convert_v1_Scale IO Persistent Volume Source_To_core_Scale IO Persistent Volume Source ( in * v1 . Scale IO Persistent Volume Source , out * core . Scale IO Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Scale IO Persistent Volume Source_To_core_Scale IO Persistent Volume } 
func Convert_core_Scale IO Persistent Volume Source_To_v1_Scale IO Persistent Volume Source ( in * core . Scale IO Persistent Volume Source , out * v1 . Scale IO Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Scale IO Persistent Volume Source_To_v1_Scale IO Persistent Volume } 
func Convert_v1_Scale IO Volume Source_To_core_Scale IO Volume Source ( in * v1 . Scale IO Volume Source , out * core . Scale IO Volume Source , s conversion . Scope ) error { return auto Convert_v1_Scale IO Volume Source_To_core_Scale IO Volume } 
func Convert_core_Scale IO Volume Source_To_v1_Scale IO Volume Source ( in * core . Scale IO Volume Source , out * v1 . Scale IO Volume Source , s conversion . Scope ) error { return auto Convert_core_Scale IO Volume Source_To_v1_Scale IO Volume } 
func Convert_v1_Scope Selector_To_core_Scope Selector ( in * v1 . Scope Selector , out * core . Scope Selector , s conversion . Scope ) error { return auto Convert_v1_Scope Selector_To_core_Scope } 
func Convert_core_Scope Selector_To_v1_Scope Selector ( in * core . Scope Selector , out * v1 . Scope Selector , s conversion . Scope ) error { return auto Convert_core_Scope Selector_To_v1_Scope } 
func Convert_v1_Scoped Resource Selector Requirement_To_core_Scoped Resource Selector Requirement ( in * v1 . Scoped Resource Selector Requirement , out * core . Scoped Resource Selector Requirement , s conversion . Scope ) error { return auto Convert_v1_Scoped Resource Selector Requirement_To_core_Scoped Resource Selector } 
func Convert_core_Scoped Resource Selector Requirement_To_v1_Scoped Resource Selector Requirement ( in * core . Scoped Resource Selector Requirement , out * v1 . Scoped Resource Selector Requirement , s conversion . Scope ) error { return auto Convert_core_Scoped Resource Selector Requirement_To_v1_Scoped Resource Selector } 
func Convert_core_Secret_To_v1_Secret ( in * core . Secret , out * v1 . Secret , s conversion . Scope ) error { return auto } 
func Convert_v1_Secret Env Source_To_core_Secret Env Source ( in * v1 . Secret Env Source , out * core . Secret Env Source , s conversion . Scope ) error { return auto Convert_v1_Secret Env Source_To_core_Secret Env } 
func Convert_core_Secret Env Source_To_v1_Secret Env Source ( in * core . Secret Env Source , out * v1 . Secret Env Source , s conversion . Scope ) error { return auto Convert_core_Secret Env Source_To_v1_Secret Env } 
func Convert_v1_Secret Key Selector_To_core_Secret Key Selector ( in * v1 . Secret Key Selector , out * core . Secret Key Selector , s conversion . Scope ) error { return auto Convert_v1_Secret Key Selector_To_core_Secret Key } 
func Convert_core_Secret Key Selector_To_v1_Secret Key Selector ( in * core . Secret Key Selector , out * v1 . Secret Key Selector , s conversion . Scope ) error { return auto Convert_core_Secret Key Selector_To_v1_Secret Key } 
func Convert_v1_Secret List_To_core_Secret List ( in * v1 . Secret List , out * core . Secret List , s conversion . Scope ) error { return auto Convert_v1_Secret List_To_core_Secret } 
func Convert_core_Secret List_To_v1_Secret List ( in * core . Secret List , out * v1 . Secret List , s conversion . Scope ) error { return auto Convert_core_Secret List_To_v1_Secret } 
func Convert_v1_Secret Projection_To_core_Secret Projection ( in * v1 . Secret Projection , out * core . Secret Projection , s conversion . Scope ) error { return auto Convert_v1_Secret Projection_To_core_Secret } 
func Convert_core_Secret Projection_To_v1_Secret Projection ( in * core . Secret Projection , out * v1 . Secret Projection , s conversion . Scope ) error { return auto Convert_core_Secret Projection_To_v1_Secret } 
func Convert_v1_Secret Reference_To_core_Secret Reference ( in * v1 . Secret Reference , out * core . Secret Reference , s conversion . Scope ) error { return auto Convert_v1_Secret Reference_To_core_Secret } 
func Convert_core_Secret Reference_To_v1_Secret Reference ( in * core . Secret Reference , out * v1 . Secret Reference , s conversion . Scope ) error { return auto Convert_core_Secret Reference_To_v1_Secret } 
func Convert_v1_Secret Volume Source_To_core_Secret Volume Source ( in * v1 . Secret Volume Source , out * core . Secret Volume Source , s conversion . Scope ) error { return auto Convert_v1_Secret Volume Source_To_core_Secret Volume } 
func Convert_core_Secret Volume Source_To_v1_Secret Volume Source ( in * core . Secret Volume Source , out * v1 . Secret Volume Source , s conversion . Scope ) error { return auto Convert_core_Secret Volume Source_To_v1_Secret Volume } 
func Convert_v1_Security Context_To_core_Security Context ( in * v1 . Security Context , out * core . Security Context , s conversion . Scope ) error { return auto Convert_v1_Security Context_To_core_Security } 
func Convert_core_Security Context_To_v1_Security Context ( in * core . Security Context , out * v1 . Security Context , s conversion . Scope ) error { return auto Convert_core_Security Context_To_v1_Security } 
func Convert_v1_Serialized Reference_To_core_Serialized Reference ( in * v1 . Serialized Reference , out * core . Serialized Reference , s conversion . Scope ) error { return auto Convert_v1_Serialized Reference_To_core_Serialized } 
func Convert_core_Serialized Reference_To_v1_Serialized Reference ( in * core . Serialized Reference , out * v1 . Serialized Reference , s conversion . Scope ) error { return auto Convert_core_Serialized Reference_To_v1_Serialized } 
func Convert_v1_Service_To_core_Service ( in * v1 . Service , out * core . Service , s conversion . Scope ) error { return auto } 
func Convert_core_Service_To_v1_Service ( in * core . Service , out * v1 . Service , s conversion . Scope ) error { return auto } 
func Convert_v1_Service Account_To_core_Service Account ( in * v1 . Service Account , out * core . Service Account , s conversion . Scope ) error { return auto Convert_v1_Service Account_To_core_Service } 
func Convert_core_Service Account_To_v1_Service Account ( in * core . Service Account , out * v1 . Service Account , s conversion . Scope ) error { return auto Convert_core_Service Account_To_v1_Service } 
func Convert_v1_Service Account List_To_core_Service Account List ( in * v1 . Service Account List , out * core . Service Account List , s conversion . Scope ) error { return auto Convert_v1_Service Account List_To_core_Service Account } 
func Convert_core_Service Account List_To_v1_Service Account List ( in * core . Service Account List , out * v1 . Service Account List , s conversion . Scope ) error { return auto Convert_core_Service Account List_To_v1_Service Account } 
func Convert_v1_Service Account Token Projection_To_core_Service Account Token Projection ( in * v1 . Service Account Token Projection , out * core . Service Account Token Projection , s conversion . Scope ) error { return auto Convert_v1_Service Account Token Projection_To_core_Service Account Token } 
func Convert_core_Service Account Token Projection_To_v1_Service Account Token Projection ( in * core . Service Account Token Projection , out * v1 . Service Account Token Projection , s conversion . Scope ) error { return auto Convert_core_Service Account Token Projection_To_v1_Service Account Token } 
func Convert_v1_Service List_To_core_Service List ( in * v1 . Service List , out * core . Service List , s conversion . Scope ) error { return auto Convert_v1_Service List_To_core_Service } 
func Convert_core_Service List_To_v1_Service List ( in * core . Service List , out * v1 . Service List , s conversion . Scope ) error { return auto Convert_core_Service List_To_v1_Service } 
func Convert_v1_Service Port_To_core_Service Port ( in * v1 . Service Port , out * core . Service Port , s conversion . Scope ) error { return auto Convert_v1_Service Port_To_core_Service } 
func Convert_core_Service Port_To_v1_Service Port ( in * core . Service Port , out * v1 . Service Port , s conversion . Scope ) error { return auto Convert_core_Service Port_To_v1_Service } 
func Convert_v1_Service Proxy Options_To_core_Service Proxy Options ( in * v1 . Service Proxy Options , out * core . Service Proxy Options , s conversion . Scope ) error { return auto Convert_v1_Service Proxy Options_To_core_Service Proxy } 
func Convert_core_Service Proxy Options_To_v1_Service Proxy Options ( in * core . Service Proxy Options , out * v1 . Service Proxy Options , s conversion . Scope ) error { return auto Convert_core_Service Proxy Options_To_v1_Service Proxy } 
func Convert_v1_Service Spec_To_core_Service Spec ( in * v1 . Service Spec , out * core . Service Spec , s conversion . Scope ) error { return auto Convert_v1_Service Spec_To_core_Service } 
func Convert_core_Service Spec_To_v1_Service Spec ( in * core . Service Spec , out * v1 . Service Spec , s conversion . Scope ) error { return auto Convert_core_Service Spec_To_v1_Service } 
func Convert_v1_Service Status_To_core_Service Status ( in * v1 . Service Status , out * core . Service Status , s conversion . Scope ) error { return auto Convert_v1_Service Status_To_core_Service } 
func Convert_core_Service Status_To_v1_Service Status ( in * core . Service Status , out * v1 . Service Status , s conversion . Scope ) error { return auto Convert_core_Service Status_To_v1_Service } 
func Convert_v1_Session Affinity Config_To_core_Session Affinity Config ( in * v1 . Session Affinity Config , out * core . Session Affinity Config , s conversion . Scope ) error { return auto Convert_v1_Session Affinity Config_To_core_Session Affinity } 
func Convert_core_Session Affinity Config_To_v1_Session Affinity Config ( in * core . Session Affinity Config , out * v1 . Session Affinity Config , s conversion . Scope ) error { return auto Convert_core_Session Affinity Config_To_v1_Session Affinity } 
func Convert_v1_Storage OS Persistent Volume Source_To_core_Storage OS Persistent Volume Source ( in * v1 . Storage OS Persistent Volume Source , out * core . Storage OS Persistent Volume Source , s conversion . Scope ) error { return auto Convert_v1_Storage OS Persistent Volume Source_To_core_Storage OS Persistent Volume } 
func Convert_core_Storage OS Persistent Volume Source_To_v1_Storage OS Persistent Volume Source ( in * core . Storage OS Persistent Volume Source , out * v1 . Storage OS Persistent Volume Source , s conversion . Scope ) error { return auto Convert_core_Storage OS Persistent Volume Source_To_v1_Storage OS Persistent Volume } 
func Convert_v1_Storage OS Volume Source_To_core_Storage OS Volume Source ( in * v1 . Storage OS Volume Source , out * core . Storage OS Volume Source , s conversion . Scope ) error { return auto Convert_v1_Storage OS Volume Source_To_core_Storage OS Volume } 
func Convert_core_Storage OS Volume Source_To_v1_Storage OS Volume Source ( in * core . Storage OS Volume Source , out * v1 . Storage OS Volume Source , s conversion . Scope ) error { return auto Convert_core_Storage OS Volume Source_To_v1_Storage OS Volume } 
func Convert_v1_Sysctl_To_core_Sysctl ( in * v1 . Sysctl , out * core . Sysctl , s conversion . Scope ) error { return auto } 
func Convert_core_Sysctl_To_v1_Sysctl ( in * core . Sysctl , out * v1 . Sysctl , s conversion . Scope ) error { return auto } 
func Convert_v1_TCP Socket Action_To_core_TCP Socket Action ( in * v1 . TCP Socket Action , out * core . TCP Socket Action , s conversion . Scope ) error { return auto Convert_v1_TCP Socket Action_To_core_TCP Socket } 
func Convert_core_TCP Socket Action_To_v1_TCP Socket Action ( in * core . TCP Socket Action , out * v1 . TCP Socket Action , s conversion . Scope ) error { return auto Convert_core_TCP Socket Action_To_v1_TCP Socket } 
func Convert_v1_Taint_To_core_Taint ( in * v1 . Taint , out * core . Taint , s conversion . Scope ) error { return auto } 
func Convert_core_Taint_To_v1_Taint ( in * core . Taint , out * v1 . Taint , s conversion . Scope ) error { return auto } 
func Convert_v1_Toleration_To_core_Toleration ( in * v1 . Toleration , out * core . Toleration , s conversion . Scope ) error { return auto } 
func Convert_core_Toleration_To_v1_Toleration ( in * core . Toleration , out * v1 . Toleration , s conversion . Scope ) error { return auto } 
func Convert_v1_Topology Selector Label Requirement_To_core_Topology Selector Label Requirement ( in * v1 . Topology Selector Label Requirement , out * core . Topology Selector Label Requirement , s conversion . Scope ) error { return auto Convert_v1_Topology Selector Label Requirement_To_core_Topology Selector Label } 
func Convert_core_Topology Selector Label Requirement_To_v1_Topology Selector Label Requirement ( in * core . Topology Selector Label Requirement , out * v1 . Topology Selector Label Requirement , s conversion . Scope ) error { return auto Convert_core_Topology Selector Label Requirement_To_v1_Topology Selector Label } 
func Convert_v1_Topology Selector Term_To_core_Topology Selector Term ( in * v1 . Topology Selector Term , out * core . Topology Selector Term , s conversion . Scope ) error { return auto Convert_v1_Topology Selector Term_To_core_Topology Selector } 
func Convert_core_Topology Selector Term_To_v1_Topology Selector Term ( in * core . Topology Selector Term , out * v1 . Topology Selector Term , s conversion . Scope ) error { return auto Convert_core_Topology Selector Term_To_v1_Topology Selector } 
func Convert_v1_Typed Local Object Reference_To_core_Typed Local Object Reference ( in * v1 . Typed Local Object Reference , out * core . Typed Local Object Reference , s conversion . Scope ) error { return auto Convert_v1_Typed Local Object Reference_To_core_Typed Local Object } 
func Convert_core_Typed Local Object Reference_To_v1_Typed Local Object Reference ( in * core . Typed Local Object Reference , out * v1 . Typed Local Object Reference , s conversion . Scope ) error { return auto Convert_core_Typed Local Object Reference_To_v1_Typed Local Object } 
func Convert_v1_Volume_To_core_Volume ( in * v1 . Volume , out * core . Volume , s conversion . Scope ) error { return auto } 
func Convert_core_Volume_To_v1_Volume ( in * core . Volume , out * v1 . Volume , s conversion . Scope ) error { return auto } 
func Convert_v1_Volume Device_To_core_Volume Device ( in * v1 . Volume Device , out * core . Volume Device , s conversion . Scope ) error { return auto Convert_v1_Volume Device_To_core_Volume } 
func Convert_core_Volume Device_To_v1_Volume Device ( in * core . Volume Device , out * v1 . Volume Device , s conversion . Scope ) error { return auto Convert_core_Volume Device_To_v1_Volume } 
func Convert_v1_Volume Mount_To_core_Volume Mount ( in * v1 . Volume Mount , out * core . Volume Mount , s conversion . Scope ) error { return auto Convert_v1_Volume Mount_To_core_Volume } 
func Convert_core_Volume Mount_To_v1_Volume Mount ( in * core . Volume Mount , out * v1 . Volume Mount , s conversion . Scope ) error { return auto Convert_core_Volume Mount_To_v1_Volume } 
func Convert_v1_Volume Node Affinity_To_core_Volume Node Affinity ( in * v1 . Volume Node Affinity , out * core . Volume Node Affinity , s conversion . Scope ) error { return auto Convert_v1_Volume Node Affinity_To_core_Volume Node } 
func Convert_core_Volume Node Affinity_To_v1_Volume Node Affinity ( in * core . Volume Node Affinity , out * v1 . Volume Node Affinity , s conversion . Scope ) error { return auto Convert_core_Volume Node Affinity_To_v1_Volume Node } 
func Convert_v1_Volume Projection_To_core_Volume Projection ( in * v1 . Volume Projection , out * core . Volume Projection , s conversion . Scope ) error { return auto Convert_v1_Volume Projection_To_core_Volume } 
func Convert_core_Volume Projection_To_v1_Volume Projection ( in * core . Volume Projection , out * v1 . Volume Projection , s conversion . Scope ) error { return auto Convert_core_Volume Projection_To_v1_Volume } 
func Convert_v1_Volume Source_To_core_Volume Source ( in * v1 . Volume Source , out * core . Volume Source , s conversion . Scope ) error { return auto Convert_v1_Volume Source_To_core_Volume } 
func Convert_core_Volume Source_To_v1_Volume Source ( in * core . Volume Source , out * v1 . Volume Source , s conversion . Scope ) error { return auto Convert_core_Volume Source_To_v1_Volume } 
func Convert_v1_Vsphere Virtual Disk Volume Source_To_core_Vsphere Virtual Disk Volume Source ( in * v1 . Vsphere Virtual Disk Volume Source , out * core . Vsphere Virtual Disk Volume Source , s conversion . Scope ) error { return auto Convert_v1_Vsphere Virtual Disk Volume Source_To_core_Vsphere Virtual Disk Volume } 
func Convert_core_Vsphere Virtual Disk Volume Source_To_v1_Vsphere Virtual Disk Volume Source ( in * core . Vsphere Virtual Disk Volume Source , out * v1 . Vsphere Virtual Disk Volume Source , s conversion . Scope ) error { return auto Convert_core_Vsphere Virtual Disk Volume Source_To_v1_Vsphere Virtual Disk Volume } 
func Convert_v1_Weighted Pod Affinity Term_To_core_Weighted Pod Affinity Term ( in * v1 . Weighted Pod Affinity Term , out * core . Weighted Pod Affinity Term , s conversion . Scope ) error { return auto Convert_v1_Weighted Pod Affinity Term_To_core_Weighted Pod Affinity } 
func Convert_core_Weighted Pod Affinity Term_To_v1_Weighted Pod Affinity Term ( in * core . Weighted Pod Affinity Term , out * v1 . Weighted Pod Affinity Term , s conversion . Scope ) error { return auto Convert_core_Weighted Pod Affinity Term_To_v1_Weighted Pod Affinity } 
func Convert_v1_Windows Security Context Options_To_core_Windows Security Context Options ( in * v1 . Windows Security Context Options , out * core . Windows Security Context Options , s conversion . Scope ) error { return auto Convert_v1_Windows Security Context Options_To_core_Windows Security Context } 
func Convert_core_Windows Security Context Options_To_v1_Windows Security Context Options ( in * core . Windows Security Context Options , out * v1 . Windows Security Context Options , s conversion . Scope ) error { return auto Convert_core_Windows Security Context Options_To_v1_Windows Security Context } 
func ( c * Admissionregistration V1beta1Client ) REST return c . rest } 
func Drop Disabled Fields ( pv Spec * api . Persistent Volume Spec , old PV Spec * api . Persistent Volume Spec ) { if ! utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) && ! volume Mode In Use ( old PV Spec ) { pv Spec . Volume } 
func ( in * Metric List Options ) Deep Copy Into ( out * Metric List out . Type Meta = in . Type } 
func ( in * Metric List Options ) Deep Copy ( ) * Metric List out := new ( Metric List in . Deep Copy } 
func ( in * Metric List Options ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Metric Value ) Deep Copy Into ( out * Metric out . Type Meta = in . Type out . Described Object = in . Described in . Timestamp . Deep Copy if in . Window Seconds != nil { in , out := & in . Window Seconds , & out . Window out . Value = in . Value . Deep * out = new ( v1 . Label ( * in ) . Deep Copy } 
func ( in * Metric Value ) Deep Copy ( ) * Metric out := new ( Metric in . Deep Copy } 
func ( in * Metric Value ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Metric Value List ) Deep Copy Into ( out * Metric Value out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Metric for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Metric Value List ) Deep Copy ( ) * Metric Value out := new ( Metric Value in . Deep Copy } 
func ( in * Metric Value List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func New Create Job Options ( io Streams genericclioptions . IO Streams ) * Create Job Options { return & Create Job Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , IO Streams : io } 
func New Cmd Create Job ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Create Job Options ( io cmd := & cobra . Command { Use : " " , Short : job Long , Long : job Long , Example : job Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check o . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Dry Run cmd . Flags ( ) . String cmd . Flags ( ) . String } 
func ( o * Create Job Options ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command client Config , err := f . To REST o . Client , err = batchv1client . New For Config ( client o . Namespace , _ , err = f . To Raw Kube Config o . Builder = f . New o . Dry Run = cmdutil . Get Dry Run if o . Dry Run { o . Print printer , err := o . Print Flags . To o . Print Obj = func ( obj runtime . Object ) error { return printer . Print } 
func ( o * Create Job } 
func ( o * Create Job if len ( o . Image ) > 0 { job = o . create } else { infos , err := o . Builder . Unstructured ( ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . Resource Type Or Name uncast Versioned Obj , err := scheme . Scheme . Convert To Version ( infos [ 0 ] . Object , batchv1beta1 . Scheme Group cron Job , ok := uncast Versioned Obj . ( * batchv1beta1 . Cron job = o . create Job From Cron Job ( cron if ! o . Dry return o . Print } 
func New ( indexer cache . Indexer , gvr schema . Group Version Resource ) Lister { return & dynamic } 
func ( l * dynamic Lister ) List ( selector labels . Selector ) ( ret [ ] * unstructured . Unstructured , err error ) { err = cache . List } 
func ( l * dynamic Lister ) Namespace ( namespace string ) Namespace Lister { return & dynamic Namespace } 
func ( l * dynamic Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * unstructured . Unstructured , err error ) { err = cache . List All By } 
func ( l * dynamic Namespace Lister ) Get ( name string ) ( * unstructured . Unstructured , error ) { obj , exists , err := l . indexer . Get By if ! exists { return nil , errors . New Not Found ( l . gvr . Group } 
func ( c * Clientset ) Apiregistration ( ) apiregistrationinternalversion . Apiregistration Interface { return & fakeapiregistrationinternalversion . Fake } 
func ( c * deployments ) Patch ( name string , pt types . Patch err = c . client . Patch ( pt ) . Namespace ( c . ns ) . Resource ( " " ) . Sub } 
func New Server ( hostname string , recorder record . Event Recorder , listener Listener , http Server Factory HTTP Server Factory ) Server { if listener == nil { listener = std Net if http Server Factory == nil { http Server Factory = std HTTP Server return & server { hostname : hostname , recorder : recorder , listener : listener , http Factory : http Server Factory , services : map [ types . Namespaced Name ] * hc } 
func New Default Healthz Server ( addr string , health Timeout time . Duration , recorder record . Event Recorder , node Ref * v1 . Object Reference ) * Healthz Server { return new Healthz Server ( nil , nil , nil , addr , health Timeout , recorder , node } 
func ( hs * Healthz Server ) Run ( ) { serve Mux := http . New Serve serve Mux . Handle ( " " , healthz server := hs . http Factory . New ( hs . addr , serve if hs . recorder != nil { hs . recorder . Eventf ( hs . node Ref , api . Event Type } , node Healthz Retry Interval , wait . Never } 
func Make Payload ( mappings [ ] v1 . Key To Path , secret * v1 . Secret , default Mode * int32 , optional bool ) ( map [ string ] volumeutil . File Projection , error ) { if default payload := make ( map [ string ] volumeutil . File var file Projection volumeutil . File if len ( mappings ) == 0 { for name , data := range secret . Data { file file Projection . Mode = * default payload [ name ] = file err klog . Errorf ( err return nil , fmt . Errorf ( err file if ktp . Mode != nil { file } else { file Projection . Mode = * default payload [ ktp . Path ] = file } 
func Rollback V3To V2 ( migrate Datadir string , ttl time . Duration ) error { dbpath := path . Join ( migrate // etcd3 store backend. We will use it to parse v3 data files and extract information. be := backend . New Default tx := be . Batch // etcd2 store backend. We will use v3 data to update this and then save snapshot to disk. st := store . New ( etcdserver . Store Cluster Prefix , etcdserver . Store Keys expire err := tx . Unsafe For Each ( [ ] byte ( " " ) , func ( k , v [ ] byte ) error { kv := & mvccpb . Key // This is compact key. if ! strings . Has ttl Opt := store . TTL Option if kv . Lease != 0 { ttl Opt = store . TTL Option Set { Expire Time : expire if ! is Tombstone ( k ) { sk := path . Join ( strings . Trim ( etcdserver . Store Keys _ , err := st . Set ( sk , false , string ( kv . Value ) , ttl if err := traverse And Delete Empty // rebuild cluster state. metadata , hardstate , old St , err := rebuild ( migrate // In the following, it's low level logic that saves metadata and data into v2 snapshot. backup Path := migrate if err := os . Rename ( migrate Datadir , backup if err := os . Mkdir All ( path . Join ( migrate wal Dir := path . Join ( migrate w , err := oldwal . Create ( wal err = w . Save event , err := old St . Get ( etcdserver . Store Cluster // nodes (members info) for Conf traverse Metadata ( event . Node , func ( n * store . Node Extern ) { if n . Key != etcdserver . Store Cluster if n . Key == path . Join ( etcdserver . Store Cluster Prefix , " " ) { v = rollback if _ , err := st . Set ( n . Key , n . Dir , v , store . TTL Option if len ( fields ) == 4 && fields [ 2 ] == " " { node ID , err := strconv . Parse nodes = append ( nodes , node raft Snap := raftpb . Snapshot { Data : data , Metadata : raftpb . Snapshot Metadata { Index : hardstate . Commit , Term : hardstate . Term , Conf State : raftpb . Conf snapshotter := snap . New ( path . Join ( migrate if err := snapshotter . Save Snap ( raft } 
func New Heuristic Watch Cache Sizes ( expected RAM Capacity MB int ) map [ schema . Group Resource ] int { // From our documentation, we officially recommend 120GB machines for // 2000 nodes, and we scale from that point. Thus we assume ~60MB of // capacity per node. // TODO: Revisit this heuristics cluster Size := expected RAM Capacity // We should specify cache size for a given resource only if it // is supposed to have non-default value. // // TODO: Figure out which resource we should have non-default value. watch Cache Sizes := make ( map [ schema . Group watch Cache Sizes [ schema . Group Resource { Resource : " " } ] = max Int ( 5 * cluster watch Cache Sizes [ schema . Group Resource { Resource : " " } ] = max Int ( 10 * cluster watch Cache Sizes [ schema . Group Resource { Resource : " " } ] = max Int ( 5 * cluster watch Cache Sizes [ schema . Group Resource { Resource : " " } ] = max Int ( 50 * cluster watch Cache Sizes [ schema . Group Resource { Resource : " " } ] = max Int ( 5 * cluster watch Cache Sizes [ schema . Group watch Cache Sizes [ schema . Group Resource { Resource : " " , Group : " " } ] = max Int ( 5 * cluster return watch Cache } 
func ( s * service Lister ) Get Pod Services ( pod * v1 . Pod ) ( [ ] * v1 . Service , error ) { all for i := range all Services { service := all selector := labels . Set ( service . Spec . Selector ) . As Selector Pre } 
func ( h * Manager Stub ) Start ( active Pods Active Pods Func , sources Ready config . Sources } 
func ( h * Manager Stub ) Allocate ( node * schedulernodeinfo . Node Info , attrs * lifecycle . Pod Admit } 
func ( h * Manager Stub ) Get Device Run Container Options ( pod * v1 . Pod , container * v1 . Container ) ( * Device Run Container } 
func ( h * Manager Stub ) Get Capacity ( ) ( v1 . Resource List , v1 . Resource } 
func new Endpoint Impl ( socket Path , resource Name string , callback monitor Callback ) ( * endpoint Impl , error ) { client , c , err := dial ( socket if err != nil { klog . Errorf ( " " , socket return & endpoint Impl { client : client , client Conn : c , socket Path : socket Path , resource Name : resource } 
func new Stopped Endpoint Impl ( resource Name string ) * endpoint Impl { return & endpoint Impl { resource Name : resource Name , stop } 
func ( e * endpoint Impl ) run ( ) { stream , err := e . client . List And if err != nil { klog . Errorf ( err List And Watch , e . resource if err != nil { klog . Errorf ( err List And Watch , e . resource klog . V ( 2 ) . Infof ( " " , e . resource var new for _ , d := range devs { new Devs = append ( new e . callback ( e . resource Name , new } 
func ( e * endpoint Impl ) set Stop e . stop } 
func ( e * endpoint Impl ) allocate ( devs [ ] string ) ( * pluginapi . Allocate Response , error ) { if e . is Stopped ( ) { return nil , fmt . Errorf ( err Endpoint return e . client . Allocate ( context . Background ( ) , & pluginapi . Allocate Request { Container Requests : [ ] * pluginapi . Container Allocate Request { { Devices I } 
func ( e * endpoint Impl ) pre Start Container ( devs [ ] string ) ( * pluginapi . Pre Start Container Response , error ) { if e . is Stopped ( ) { return nil , fmt . Errorf ( err Endpoint ctx , cancel := context . With Timeout ( context . Background ( ) , pluginapi . Kubelet Pre Start Container RPC Timeout In return e . client . Pre Start Container ( ctx , & pluginapi . Pre Start Container Request { Devices I } 
func dial ( unix Socket Path string ) ( pluginapi . Device Plugin Client , * grpc . Client Conn , error ) { ctx , cancel := context . With c , err := grpc . Dial Context ( ctx , unix Socket Path , grpc . With Insecure ( ) , grpc . With Block ( ) , grpc . With Dialer ( func ( addr string , timeout time . Duration ) ( net . Conn , error ) { return net . Dial if err != nil { return nil , nil , fmt . Errorf ( err Failed To Dial Device return pluginapi . New Device Plugin } 
func Probe Volume Plugins ( ) [ ] volume . Volume Plugin { p := & csi Plugin { host : nil , block Enabled : utilfeature . Default Feature Gate . Enabled ( features . CSI Block return [ ] volume . Volume } 
func ( h * Registration Handler ) Validate Plugin ( plugin Name string , endpoint string , versions [ ] string , found In Deprecated Dir bool ) error { klog . Infof ( log ( " " , plugin Name , endpoint , strings . Join ( versions , " " ) , found In Deprecated if found In Deprecated Dir { // CSI 0.x drivers used /var/lib/kubelet/plugins as the socket dir. // This was deprecated as the socket dir for kubelet drivers, in lieu of a dedicated dir /var/lib/kubelet/plugins_registry // The deprecated dir will only be allowed for a whitelisted set of old versions. // CSI 1.x drivers should use the /var/lib/kubelet/plugins_registry if ! is Deprecated Socket Dir Allowed ( versions ) { err := fmt . Errorf ( " " , plugin _ , err := h . validate Versions ( " " , plugin } 
func ( h * Registration Handler ) Register Plugin ( plugin Name string , endpoint string , versions [ ] string ) error { klog . Infof ( log ( " " , plugin highest Supported Version , err := h . validate Versions ( " " , plugin // Storing endpoint of newly registered CSI driver into the map, where CSI driver name will be the key // all other CSI components will be able to get the actual socket of CSI drivers by its name. csi Drivers . Set ( plugin Name , Driver { endpoint : endpoint , highest Supported Version : highest Supported // Get node info from the driver. csi , err := new Csi Driver Client ( csi Driver Name ( plugin ctx , cancel := context . With Timeout ( context . Background ( ) , csi driver Node ID , max Volume Per Node , accessible Topology , err := csi . Node Get if err != nil { if unreg Err := unregister Driver ( plugin Name ) ; unreg Err != nil { klog . Error ( log ( " " , unreg err = nim . Install CSI Driver ( plugin Name , driver Node ID , max Volume Per Node , accessible if err != nil { if unreg Err := unregister Driver ( plugin Name ) ; unreg Err != nil { klog . Error ( log ( " " , unreg } 
func ( h * Registration Handler ) De Register Plugin ( plugin Name string ) { klog . V ( 4 ) . Info ( log ( " " , plugin if err := unregister Driver ( plugin } 
func ( p * csi Plugin ) Get Volume Name ( spec * volume . Spec ) ( string , error ) { csi , err := get PV Source From // return driver Name<separator>volume Handle return fmt . Sprintf ( " " , csi . Driver , vol Name Sep , csi . Volume } 
func ( p * csi Plugin ) construct Vol Source Spec ( vol Spec Name , driver Name string ) * volume . Spec { vol := & api . Volume { Name : vol Spec Name , Volume Source : api . Volume Source { CSI : & api . CSI Volume Source { Driver : driver return volume . New Spec From } 
func ( p * csi Plugin ) construct PV Source Spec ( vol Spec Name , driver Name , volume Handle string ) * volume . Spec { fs Mode := api . Persistent Volume pv := & api . Persistent Volume { Object Meta : meta . Object Meta { Name : vol Spec Name , } , Spec : api . Persistent Volume Spec { Persistent Volume Source : api . Persistent Volume Source { CSI : & api . CSI Persistent Volume Source { Driver : driver Name , Volume Handle : volume Handle , } , } , Volume Mode : & fs return volume . New Spec From Persistent } 
func ( p * csi Plugin ) Can Device Mount ( spec * volume . Spec ) ( bool , error ) { driver Mode , err := p . get Driver if driver Mode == ephemeral Driver } 
func ( p * csi Plugin ) skip Attach ( driver string ) ( bool , error ) { if ! utilfeature . Default Feature Gate . Enabled ( features . CSI Driver klet Host , ok := p . host . ( volume . Kubelet Volume if ok { klet Host . Wait For Cache if p . csi Driver csi Driver , err := p . csi Driver if err != nil { if apierrs . Is Not Found ( err ) { // Don't skip attach if CSI if csi Driver . Spec . Attach Required != nil && * csi Driver . Spec . Attach } 
func ( p * csi Plugin ) get Driver Mode ( spec * volume . Spec ) ( driver Mode , error ) { // TODO (vladimirvivien) ultimately, mode will be retrieved from CSI Driver.Spec.Mode. // However, in alpha version, mode is determined by the volume source: // 1) if volume.Spec.Volume.CSI != nil -> mode is ephemeral // 2) if volume.Spec.Persistent Volume.Spec.CSI != nil -> persistent vol Src , _ , err := get Source From if vol Src != nil && utilfeature . Default Feature Gate . Enabled ( features . CSI Inline Volume ) { return ephemeral Driver return persistent Driver } 
func highest Supported // Sort by lowest to highest version sort . Slice ( versions , func ( i , j int ) bool { parsed Version I , err := utilversion . Parse parsed Version J , err := utilversion . Parse return parsed Version I . Less Than ( parsed Version for i := len ( versions ) - 1 ; i >= 0 ; i -- { highest Supported Version , err := utilversion . Parse if highest Supported Version . Major ( ) <= 1 { return highest Supported } 
func is Deprecated Socket Dir Allowed ( versions [ ] string ) bool { for _ , version := range versions { if is } 
func Parse RFC3339 ( s string , now Fn func ( ) metav1 . Time ) ( metav1 . Time , error ) { if t , time Err := time . Parse ( time . RFC3339Nano , s ) ; time } 
func Parse Literal Source ( source string ) ( key // split after the first equal (so values can have the = character) items := strings . Split } 
func Label Zones To Set ( label Zones Value string ) ( sets . String , error ) { return string To Set ( label Zones Value , cloudvolume . Label Multi Zone } 
func Zones Set To Label Value ( str Set sets . String ) string { return strings . Join ( str Set . Unsorted List ( ) , cloudvolume . Label Multi Zone } 
func Zones To Set ( zones String string ) ( sets . String , error ) { zones , err := string To Set ( zones if err != nil { return nil , fmt . Errorf ( " " , zones } 
func string To Set ( str , delimiter string ) ( sets . String , error ) { zones zones for _ , zone := range zones Slice { trimmed Zone := strings . Trim if trimmed zones Set . Insert ( trimmed return zones } 
func string To List ( str , delimiter string ) ( [ ] string , error ) { zones for _ , zone := range strings . Split ( str , delimiter ) { trimmed Zone := strings . Trim if trimmed zones Slice = append ( zones Slice , trimmed return zones } 
func Select Zone For Volume ( zone Parameter Present , zones Parameter Present bool , zone Parameter string , zones Parameter , zones With Nodes sets . String , node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term , pvc Name string ) ( string , error ) { zones , err := Select Zones For Volume ( zone Parameter Present , zones Parameter Present , zone Parameter , zones Parameter , zones With Nodes , node , allowed Topologies , pvc zone , ok := zones . Pop } 
func Select Zones For Volume ( zone Parameter Present , zones Parameter Present bool , zone Parameter string , zones Parameter , zones With Nodes sets . String , node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term , pvc Name string , num Replicas uint32 ) ( sets . String , error ) { if zone Parameter Present && zones Parameter var zone From // pick one zone from node if present if node != nil { // Volume Scheduling implicit since node is not nil if zone Parameter Present || zones Parameter zone From Node , ok = node . Object Meta . Labels [ v1 . Label Zone Failure if ! ok { return nil , fmt . Errorf ( " " , v1 . Label Zone Failure // if single replica volume and node with zone found, return immediately if num Replicas == 1 { return sets . New String ( zone From // pick zone from allowed Zones if specified allowed Zones , err := Zones From Allowed Topologies ( allowed if ( len ( allowed Topologies ) > 0 ) && ( allowed Zones . Len ( ) == 0 ) { return nil , fmt . Errorf ( " " , v1 . Label Zone Failure Domain , v1 . Label Zone Failure if allowed Zones . Len ( ) > 0 { // Volume Scheduling implicit since allowed Zones present if zone Parameter Present || zones Parameter // scheduler will guarantee if node != null above, zone From Node is member of allowed Zones. // so if zone From Node != "", we can safely assume it is part of allowed Zones. zones , err := choose Zones For Volume Including Zone ( allowed Zones , pvc Name , zone From Node , num // pick zone from parameters if present if zone Parameter Present { if num return sets . New String ( zone if zones Parameter Present { if uint32 ( zones Parameter . Len ( ) ) < num Replicas { return nil , fmt . Errorf ( " " , num Replicas , zones Parameter . Len ( ) , num // directly choose from zones parameter; no zone from node need to be considered return Choose Zones For Volume ( zones Parameter , pvc Name , num // pick zone from zones with nodes if zones With Nodes . Len ( ) > 0 { // If node != null (and thus zone From Node != ""), zone From Node will be member of zones With Nodes zones , err := choose Zones For Volume Including Zone ( zones With Nodes , pvc Name , zone From Node , num } 
func Zones From Allowed Topologies ( allowed Topologies [ ] v1 . Topology Selector for _ , term := range allowed Topologies { for _ , exp := range term . Match Label Expressions { if exp . Key == v1 . Label Zone Failure } 
func choose Zones For Volume Including Zone ( zones sets . String , pvc Name , zone To Include string , num Replicas uint32 ) ( sets . String , error ) { if num if uint32 ( zones . Len ( ) ) < num Replicas { return nil , fmt . Errorf ( " " , num Replicas , num Replicas , num if zone To Include != " " && ! zones . Has ( zone To Include ) { return nil , fmt . Errorf ( " " , zone To if uint32 ( zones . Len ( ) ) == num if zone To Include != " " { zones . Delete ( zone To num Replicas = num zones Chosen := Choose Zones For Volume ( zones , pvc Name , num if zone To Include != " " { zones Chosen . Insert ( zone To return zones } 
func Choose Zones For Volume ( zones sets . String , pvc Name string , num Zones uint32 ) sets . String { // No zones available, return empty set. replica Zones := sets . New if zones . Len ( ) == 0 { return replica // We create the volume in a zone determined by the name // Eventually the scheduler will coordinate placement into an available zone hash , index := get PVC Name Hash And Index Offset ( pvc // Zones.List returns zones in a consistent order (sorted) // We do have a potential failure case where volumes will not be properly spread, // if the set of zones changes during Stateful Set volume creation. However, this is // probably relatively unlikely because we expect the set of zones to be essentially // static for clusters. // Hopefully we can address this problem if/when we do full scheduler integration of // PVC placement (which could also e.g. avoid putting volumes in overloaded or // unhealthy zones) zone starting Index := index * num for index = starting Index ; index < starting Index + num Zones ; index ++ { zone := zone Slice [ ( hash + index ) % uint32 ( len ( zone replica klog . V ( 2 ) . Infof ( " " , pvc Name , replica Zones . Unsorted List ( ) , zone return replica } 
func Recommended Default Namespace Controller Configuration ( obj * kubectrlmgrconfigv1alpha1 . Namespace Controller if obj . Concurrent Namespace Syncs == 0 { obj . Concurrent Namespace if obj . Namespace Sync Period == zero { obj . Namespace Sync } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Token Review ) ( nil ) , ( * authentication . Token Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Token Review_To_authentication_Token Review ( a . ( * v1beta1 . Token Review ) , b . ( * authentication . Token if err := s . Add Generated Conversion Func ( ( * authentication . Token Review ) ( nil ) , ( * v1beta1 . Token Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Review_To_v1beta1_Token Review ( a . ( * authentication . Token Review ) , b . ( * v1beta1 . Token if err := s . Add Generated Conversion Func ( ( * v1beta1 . Token Review Spec ) ( nil ) , ( * authentication . Token Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Token Review Spec_To_authentication_Token Review Spec ( a . ( * v1beta1 . Token Review Spec ) , b . ( * authentication . Token Review if err := s . Add Generated Conversion Func ( ( * authentication . Token Review Spec ) ( nil ) , ( * v1beta1 . Token Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Review Spec_To_v1beta1_Token Review Spec ( a . ( * authentication . Token Review Spec ) , b . ( * v1beta1 . Token Review if err := s . Add Generated Conversion Func ( ( * v1beta1 . Token Review Status ) ( nil ) , ( * authentication . Token Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Token Review Status_To_authentication_Token Review Status ( a . ( * v1beta1 . Token Review Status ) , b . ( * authentication . Token Review if err := s . Add Generated Conversion Func ( ( * authentication . Token Review Status ) ( nil ) , ( * v1beta1 . Token Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Review Status_To_v1beta1_Token Review Status ( a . ( * authentication . Token Review Status ) , b . ( * v1beta1 . Token Review if err := s . Add Generated Conversion Func ( ( * v1beta1 . User Info ) ( nil ) , ( * authentication . User Info ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_User Info_To_authentication_User Info ( a . ( * v1beta1 . User Info ) , b . ( * authentication . User if err := s . Add Generated Conversion Func ( ( * authentication . User Info ) ( nil ) , ( * v1beta1 . User Info ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_User Info_To_v1beta1_User Info ( a . ( * authentication . User Info ) , b . ( * v1beta1 . User } 
func Convert_v1beta1_Token Review_To_authentication_Token Review ( in * v1beta1 . Token Review , out * authentication . Token Review , s conversion . Scope ) error { return auto Convert_v1beta1_Token Review_To_authentication_Token } 
func Convert_authentication_Token Review_To_v1beta1_Token Review ( in * authentication . Token Review , out * v1beta1 . Token Review , s conversion . Scope ) error { return auto Convert_authentication_Token Review_To_v1beta1_Token } 
func Convert_v1beta1_Token Review Spec_To_authentication_Token Review Spec ( in * v1beta1 . Token Review Spec , out * authentication . Token Review Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Token Review Spec_To_authentication_Token Review } 
func Convert_authentication_Token Review Spec_To_v1beta1_Token Review Spec ( in * authentication . Token Review Spec , out * v1beta1 . Token Review Spec , s conversion . Scope ) error { return auto Convert_authentication_Token Review Spec_To_v1beta1_Token Review } 
func Convert_v1beta1_Token Review Status_To_authentication_Token Review Status ( in * v1beta1 . Token Review Status , out * authentication . Token Review Status , s conversion . Scope ) error { return auto Convert_v1beta1_Token Review Status_To_authentication_Token Review } 
func Convert_authentication_Token Review Status_To_v1beta1_Token Review Status ( in * authentication . Token Review Status , out * v1beta1 . Token Review Status , s conversion . Scope ) error { return auto Convert_authentication_Token Review Status_To_v1beta1_Token Review } 
func Convert_v1beta1_User Info_To_authentication_User Info ( in * v1beta1 . User Info , out * authentication . User Info , s conversion . Scope ) error { return auto Convert_v1beta1_User Info_To_authentication_User } 
func Convert_authentication_User Info_To_v1beta1_User Info ( in * authentication . User Info , out * v1beta1 . User Info , s conversion . Scope ) error { return auto Convert_authentication_User Info_To_v1beta1_User } 
func ( v * version ) Priority Classes ( ) Priority Class Informer { return & priority Class Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( c * Fake Nodes ) Get ( name string , options v1 . Get Options ) ( result * corev1 . Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( nodes } 
func ( c * Fake Nodes ) List ( opts v1 . List Options ) ( result * corev1 . Node List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( nodes Resource , nodes Kind , opts ) , & corev1 . Node label , _ , _ := testing . Extract From List list := & corev1 . Node List { List Meta : obj . ( * corev1 . Node List ) . List for _ , item := range obj . ( * corev1 . Node } 
func ( c * Fake Nodes ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( nodes } 
func ( c * Fake Nodes ) Create ( node * corev1 . Node ) ( result * corev1 . Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( nodes } 
func ( c * Fake Nodes ) Update ( node * corev1 . Node ) ( result * corev1 . Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( nodes } 
func ( c * Fake Nodes ) Update Status ( node * corev1 . Node ) ( * corev1 . Node , error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Subresource Action ( nodes } 
func ( c * Fake Nodes ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( nodes } 
func ( c * Fake Nodes ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( nodes Resource , list _ , err := c . Fake . Invokes ( action , & corev1 . Node } 
func ( c * Fake Nodes ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * corev1 . Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( nodes } 
func New Config Factory ( args * Config Factory Args ) Configurator { stop Everything := args . Stop if stop Everything == nil { stop Everything = wait . Never scheduler Cache := internalcache . New ( 30 * time . Second , stop // TODO(bsalamat): config files should be passed to the framework. framework , err := framework . New // storage Class Informer is only enabled through Volume Scheduling feature gate var storage Class Lister storagelisters . Storage Class if args . Storage Class Informer != nil { storage Class Lister = args . Storage Class c := & config Factory { client : args . Client , pod Lister : scheduler Cache , pod Queue : internalqueue . New Scheduling Queue ( stop Everything ) , node Lister : args . Node Informer . Lister ( ) , p V Lister : args . Pv Informer . Lister ( ) , p VC Lister : args . Pvc Informer . Lister ( ) , service Lister : args . Service Informer . Lister ( ) , controller Lister : args . Replication Controller Informer . Lister ( ) , replica Set Lister : args . Replica Set Informer . Lister ( ) , stateful Set Lister : args . Stateful Set Informer . Lister ( ) , pdb Lister : args . Pdb Informer . Lister ( ) , storage Class Lister : storage Class Lister , framework : framework , scheduler Cache : scheduler Cache , Stop Everything : stop Everything , scheduler Name : args . Scheduler Name , hard Pod Affinity Symmetric Weight : args . Hard Pod Affinity Symmetric Weight , disable Preemption : args . Disable Preemption , percentage Of Nodes To Score : args . Percentage Of Nodes To Score , bind Timeout Seconds : args . Bind Timeout // Setup volume binder c . volume Binder = volumebinder . New Volume Binder ( args . Client , args . Node Informer , args . Pvc Informer , args . Pv Informer , args . Storage Class Informer , time . Duration ( args . Bind Timeout c . scheduled Pods Has Synced = args . Pod Informer . Informer ( ) . Has // Scheduled Pod Lister is something we provide to plug-in functions that // they may need to call. c . scheduled Pod Lister = assigned Pod Lister { args . Pod // Setup cache debugger debugger := cachedebugger . New ( args . Node Informer . Lister ( ) , args . Pod Informer . Lister ( ) , c . scheduler Cache , c . pod debugger . Listen For Signal ( c . Stop go func ( ) { <- c . Stop c . pod } 
func ( c * config Factory ) Create From Provider ( provider Name string ) ( * Config , error ) { klog . V ( 2 ) . Infof ( " " , provider provider , err := Get Algorithm Provider ( provider return c . Create From Keys ( provider . Fit Predicate Keys , provider . Priority Function Keys , [ ] algorithm . Scheduler } 
func ( c * config Factory ) Create From // validate the policy configuration if err := validation . Validate predicate Keys := sets . New if policy . Predicates == nil { klog . V ( 2 ) . Infof ( " " , Default provider , err := Get Algorithm Provider ( Default predicate Keys = provider . Fit Predicate predicate Keys . Insert ( Register Custom Fit priority Keys := sets . New if policy . Priorities == nil { klog . V ( 2 ) . Infof ( " " , Default provider , err := Get Algorithm Provider ( Default priority Keys = provider . Priority Function priority Keys . Insert ( Register Custom Priority var extenders [ ] algorithm . Scheduler if len ( policy . Extender Configs ) != 0 { ignored Extended Resources := sets . New for ii := range policy . Extender Configs { klog . V ( 2 ) . Infof ( " " , policy . Extender extender , err := core . New HTTP Extender ( & policy . Extender for _ , r := range policy . Extender Configs [ ii ] . Managed Resources { if r . Ignored By Scheduler { ignored Extended predicates . Register Predicate Metadata Producer With Extended Resource Options ( ignored Extended // Providing Hard Pod Affinity Symmetric Weight in the policy config is the new and preferred way of providing the value. // Give it higher precedence than scheduler CLI configuration when it is provided. if policy . Hard Pod Affinity Symmetric Weight != 0 { c . hard Pod Affinity Symmetric Weight = policy . Hard Pod Affinity Symmetric // When Always Check All Predicates is set to true, scheduler checks all the configured // predicates even after one or more of them fails. if policy . Always Check All Predicates { c . always Check All Predicates = policy . Always Check All return c . Create From Keys ( predicate Keys , priority } 
func ( c * config Factory ) Create From Keys ( predicate Keys , priority Keys sets . String , extenders [ ] algorithm . Scheduler Extender ) ( * Config , error ) { klog . V ( 2 ) . Infof ( " " , predicate Keys , priority if c . Get Hard Pod Affinity Symmetric Weight ( ) < 1 || c . Get Hard Pod Affinity Symmetric Weight ( ) > 100 { return nil , fmt . Errorf ( " " , c . Get Hard Pod Affinity Symmetric predicate Funcs , err := c . Get Predicates ( predicate priority Configs , err := c . Get Priority Function Configs ( priority priority Meta Producer , err := c . Get Priority Metadata predicate Meta Producer , err := c . Get Predicate Metadata algo := core . New Generic Scheduler ( c . scheduler Cache , c . pod Queue , predicate Funcs , predicate Meta Producer , priority Configs , priority Meta Producer , c . framework , extenders , c . volume Binder , c . p VC Lister , c . pdb Lister , c . always Check All Predicates , c . disable Preemption , c . percentage Of Nodes To return & Config { Scheduler Cache : c . scheduler Cache , // The scheduler only needs to consider schedulable nodes. Node Lister : & node Lister { c . node Lister } , Algorithm : algo , Get Binder : get Binder Func ( c . client , extenders ) , Pod Condition Updater : & pod Condition Updater { c . client } , Pod Preemptor : & pod Preemptor { c . client } , Framework : c . framework , Wait For Cache Sync : func ( ) bool { return cache . Wait For Cache Sync ( c . Stop Everything , c . scheduled Pods Has } , Next Pod : internalqueue . Make Next Pod Func ( c . pod Queue ) , Error : Make Default Error Func ( c . client , c . pod Queue , c . scheduler Cache , c . Stop Everything ) , Stop Everything : c . Stop Everything , Volume Binder : c . volume Binder , Scheduling Queue : c . pod } 
func get Binder Func ( client clientset . Interface , extenders [ ] algorithm . Scheduler Extender ) func ( pod * v1 . Pod ) Binder { var extender Binder algorithm . Scheduler for i := range extenders { if extenders [ i ] . Is Binder ( ) { extender default return func ( pod * v1 . Pod ) Binder { if extender Binder != nil && extender Binder . Is Interested ( pod ) { return extender return default } 
func ( l assigned Pod Lister ) List ( selector labels . Selector ) ( [ ] * v1 . Pod , error ) { list , err := l . Pod for _ , pod := range list { if len ( pod . Spec . Node } 
func ( l assigned Pod Lister ) Pods ( namespace string ) corelisters . Pod Namespace Lister { return assigned Pod Namespace Lister { l . Pod } 
func ( l assigned Pod Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Pod , err error ) { list , err := l . Pod Namespace for _ , pod := range list { if len ( pod . Spec . Node } 
func ( l assigned Pod Namespace Lister ) Get ( name string ) ( * v1 . Pod , error ) { pod , err := l . Pod Namespace if len ( pod . Spec . Node return nil , errors . New Not Found ( schema . Group Resource { Resource : string ( v1 . Resource } 
func New Pod Informer ( client clientset . Interface , resync Period time . Duration ) coreinformers . Pod Informer { selector := fields . Parse Selector Or Die ( " " + string ( v1 . Pod Succeeded ) + " " + string ( v1 . Pod lw := cache . New List Watch From Client ( client . Core V1 ( ) . REST Client ( ) , string ( v1 . Resource Pods ) , metav1 . Namespace return & pod Informer { informer : cache . New Shared Index Informer ( lw , & v1 . Pod { } , resync Period , cache . Indexers { cache . Namespace Index : cache . Meta Namespace Index } 
func Make Default Error Func ( client clientset . Interface , pod Queue internalqueue . Scheduling Queue , scheduler Cache internalcache . Cache , stop Everything <- chan struct { } ) func ( pod * v1 . Pod , err error ) { return func ( pod * v1 . Pod , err error ) { if err == core . Err No Nodes } else { if _ , ok := err . ( * core . Fit } else if errors . Is Not Found ( err ) { if err Status , ok := err . ( errors . API Status ) ; ok && err Status . Status ( ) . Details . Kind == " " { node Name := err // when node is not found, We do not remove the node right away. Trying again to get // the node and if the node is still not found, then remove it from the scheduler cache. _ , err := client . Core V1 ( ) . Nodes ( ) . Get ( node Name , metav1 . Get if err != nil && errors . Is Not Found ( err ) { node := v1 . Node { Object Meta : metav1 . Object Meta { Name : node scheduler Cache . Remove pod Scheduling Cycle := pod Queue . Scheduling // Retry asynchronously. // Note that this is extremely rudimentary and we need a more real error handling path. go func ( ) { defer runtime . Handle pod ID := types . Namespaced // An unschedulable pod will be placed in the unschedulable queue. // This ensures that if the pod is nominated to run on a node, // scheduler takes the pod into account when running predicates for the node. // Get the pod again; it may have changed/been scheduled already. get Backoff := initial Get for { pod , err := client . Core V1 ( ) . Pods ( pod ID . Namespace ) . Get ( pod ID . Name , metav1 . Get if err == nil { if len ( pod . Spec . Node Name ) == 0 { pod Queue . Add Unschedulable If Not Present ( pod , pod Scheduling if errors . Is Not Found ( err ) { klog . Warningf ( " " , pod klog . Errorf ( " " , pod if get Backoff = get Backoff * 2 ; get Backoff > maximal Get Backoff { get Backoff = maximal Get time . Sleep ( get } 
return b . Client . Core } 
func New Create Options ( io Streams genericclioptions . IO Streams ) * Create Options { return & Create Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , Record Flags : genericclioptions . New Record Flags ( ) , Recorder : genericclioptions . Noop Recorder { } , IO Streams : io } 
func New Cmd Create ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Create Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : create Long , Example : create Example , Run : func ( cmd * cobra . Command , args [ ] string ) { if cmdutil . Is Filename Slice Empty ( o . Filename Options . Filenames , o . Filename Options . Kustomize ) { io Streams . Err default Run Func := cmdutil . Default Sub Command Run ( io Streams . Err default Run cmdutil . Check cmdutil . Check Err ( o . Validate cmdutil . Check Err ( o . Run // bind flag structs o . Record Flags . Add cmdutil . Add Filename Option Flags ( cmd , & o . Filename cmdutil . Add Validate cmd . Flags ( ) . Bool Var ( & o . Edit Before Create , " " , o . Edit Before cmdutil . Add Apply Annotation cmdutil . Add Dry Run cmd . Flags ( ) . String Var cmd . Flags ( ) . String o . Print Flags . Add // create subcommands cmd . Add Command ( New Cmd Create Namespace ( f , io cmd . Add Command ( New Cmd Create Quota ( f , io cmd . Add Command ( New Cmd Create Secret ( f , io cmd . Add Command ( New Cmd Create Config Map ( f , io cmd . Add Command ( New Cmd Create Service Account ( f , io cmd . Add Command ( New Cmd Create Service ( f , io cmd . Add Command ( New Cmd Create Deployment ( f , io cmd . Add Command ( New Cmd Create Cluster Role ( f , io cmd . Add Command ( New Cmd Create Cluster Role Binding ( f , io cmd . Add Command ( New Cmd Create Role ( f , io cmd . Add Command ( New Cmd Create Role Binding ( f , io cmd . Add Command ( New Cmd Create Pod Disruption Budget ( f , io cmd . Add Command ( New Cmd Create Priority Class ( f , io cmd . Add Command ( New Cmd Create Job ( f , io cmd . Add Command ( New Cmd Create Cron Job ( f , io } 
func ( o * Create Options ) Validate Args ( cmd * cobra . Command , args [ ] string ) error { if len ( args ) != 0 { return cmdutil . Usage if len ( o . Raw ) > 0 { if o . Edit Before Create { return cmdutil . Usage if len ( o . Filename Options . Filenames ) != 1 { return cmdutil . Usage if strings . Index ( o . Filename Options . Filenames [ 0 ] , " " ) == 0 || strings . Index ( o . Filename Options . Filenames [ 0 ] , " " ) == 0 { return cmdutil . Usage if o . Filename Options . Recursive { return cmdutil . Usage if len ( o . Selector ) > 0 { return cmdutil . Usage if len ( cmdutil . Get Flag String ( cmd , " " ) ) > 0 { return cmdutil . Usage if _ , err := url . Parse Request URI ( o . Raw ) ; err != nil { return cmdutil . Usage } 
func ( o * Create o . Record o . Recorder , err = o . Record Flags . To o . Dry Run = cmdutil . Get Dry Run if o . Dry Run { o . Print printer , err := o . Print Flags . To o . Print Obj = func ( obj kruntime . Object ) error { return printer . Print } 
func ( o * Create Options ) Run if o . Edit Before Create { return Run Edit On Create ( f , o . Print Flags , o . Record Flags , o . IO Streams , cmd , & o . Filename schema , err := f . Validator ( cmdutil . Get Flag cmd Namespace , enforce Namespace , err := f . To Raw Kube Config r := f . New Builder ( ) . Unstructured ( ) . Schema ( schema ) . Continue On Error ( ) . Namespace Param ( cmd Namespace ) . Default Namespace ( ) . Filename Param ( enforce Namespace , & o . Filename Options ) . Label Selector if err := kubectl . Create Or Update Annotation ( cmdutil . Get Flag Bool ( cmd , cmdutil . Apply Annotations Flag ) , info . Object , scheme . Default JSON Encoder ( ) ) ; err != nil { return cmdutil . Add Source To if ! o . Dry Run { if err := create And Refresh ( info ) ; err != nil { return cmdutil . Add Source To return o . Print } 
func ( o * Create Options ) raw ( f cmdutil . Factory ) error { rest Client , err := f . REST var data io . Read if o . Filename } else { data , err = os . Open ( o . Filename // TODO post content with stream. Right now it ignores body content result := rest Client . Post ( ) . Request } 
func Run Edit On Create ( f cmdutil . Factory , print Flags * genericclioptions . Print Flags , record Flags * genericclioptions . Record Flags , io Streams genericclioptions . IO Streams , cmd * cobra . Command , options * resource . Filename Options ) error { edit Options := editor . New Edit Options ( editor . Edit Before Create Mode , io edit Options . Filename edit Options . Validate Options = cmdutil . Validate Options { Enable Validation : cmdutil . Get Flag edit Options . Print Flags = print edit Options . Apply Annotation = cmdutil . Get Flag Bool ( cmd , cmdutil . Apply Annotations edit Options . Record Flags = record err := edit return edit } 
func Name From Command Args ( cmd * cobra . Command , args [ ] string ) ( string , error ) { args Len := cmd . Args Len At // Args Len At Dash returns -1 when -- was not specified if args Len == - 1 { args if args Len != 1 { return " " , cmdutil . Usage Errorf ( cmd , " " , args } 
func New Create Subcommand Options ( io Streams genericclioptions . IO Streams ) * Create Subcommand Options { return & Create Subcommand Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , IO Streams : io } 
func ( o * Create Subcommand Options ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string , generator generate . Structured Generator ) error { name , err := Name From Command o . Structured o . Dry Run = cmdutil . Get Dry Run o . Create Annotation = cmdutil . Get Flag Bool ( cmd , cmdutil . Apply Annotations if o . Dry Run { o . Print printer , err := o . Print Flags . To o . Print Obj = func ( obj kruntime . Object , out io . Writer ) error { return printer . Print o . Namespace , o . Enforce Namespace , err = f . To Raw Kube Config o . Dynamic Client , err = f . Dynamic o . Mapper , err = f . To REST } 
func ( o * Create Subcommand Options ) Run ( ) error { obj , err := o . Structured Generator . Structured if ! o . Dry Run { // create subcommands have compiled knowledge of things they create, so type them directly gvks , _ , err := scheme . Scheme . Object mapping , err := o . Mapper . REST Mapping ( schema . Group if err := kubectl . Create Or Update Annotation ( o . Create Annotation , obj , scheme . Default JSON as if err := scheme . Scheme . Convert ( obj , as if mapping . Scope . Name ( ) == meta . REST Scope Name actual Object , err := o . Dynamic Client . Resource ( mapping . Resource ) . Namespace ( o . Namespace ) . Create ( as Unstructured , metav1 . Create // ensure we pass a versioned object to the printer obj = actual } else { if meta , err := meta . Accessor ( obj ) ; err == nil && o . Enforce Namespace { meta . Set return o . Print } 
func Translate In Tree Storage Class Parameters To CSI ( in Tree Plugin Name string , sc Parameters map [ string ] string ) ( map [ string ] string , error ) { for _ , cur Plugin := range in Tree Plugins { if in Tree Plugin Name == cur Plugin . Get In Tree Plugin Name ( ) { return cur Plugin . Translate In Tree Storage Class Parameters To CSI ( sc return nil , fmt . Errorf ( " " , in Tree Plugin } 
func Translate In Tree PV To CSI ( pv * v1 . Persistent Volume ) ( * v1 . Persistent copied PV := pv . Deep for _ , cur Plugin := range in Tree Plugins { if cur Plugin . Can Support ( copied PV ) { return cur Plugin . Translate In Tree PV To CSI ( copied return nil , fmt . Errorf ( " " , copied } 
func Translate CSIPV To In Tree ( pv * v1 . Persistent Volume ) ( * v1 . Persistent copied PV := pv . Deep for driver Name , cur Plugin := range in Tree Plugins { if copied PV . Spec . CSI . Driver == driver Name { return cur Plugin . Translate CSIPV To In Tree ( copied return nil , fmt . Errorf ( " " , copied } 
func Is Migratable Intree Plugin By Name ( in Tree Plugin Name string ) bool { for _ , cur Plugin := range in Tree Plugins { if cur Plugin . Get In Tree Plugin Name ( ) == in Tree Plugin } 
func Is Migrated CSI Driver By Name ( csi Plugin Name string ) bool { if _ , ok := in Tree Plugins [ csi Plugin } 
func Get In Tree Plugin Name From Spec ( pv * v1 . Persistent Volume , vol * v1 . Volume ) ( string , error ) { if pv != nil { for _ , cur Plugin := range in Tree Plugins { if cur Plugin . Can Support ( pv ) { return cur Plugin . Get In Tree Plugin } 
func Get CSI Name From In Tree Name ( plugin Name string ) ( string , error ) { for csi Driver Name , cur Plugin := range in Tree Plugins { if cur Plugin . Get In Tree Plugin Name ( ) == plugin Name { return csi Driver return " " , fmt . Errorf ( " " , plugin } 
func Get In Tree Name From CSI Name ( plugin Name string ) ( string , error ) { if plugin , ok := in Tree Plugins [ plugin Name ] ; ok { return plugin . Get In Tree Plugin return " " , fmt . Errorf ( " " , plugin } 
func Is PV Migratable ( pv * v1 . Persistent Volume ) bool { for _ , cur Plugin := range in Tree Plugins { if cur Plugin . Can } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1 . Storage Class { } , func ( obj interface { } ) { Set Object Defaults_Storage Class ( obj . ( * v1 . Storage scheme . Add Type Defaulting Func ( & v1 . Storage Class List { } , func ( obj interface { } ) { Set Object Defaults_Storage Class List ( obj . ( * v1 . Storage Class } 
func ( m * Base Controller Ref Manager ) Claim Object ( obj metav1 . Object , match func ( metav1 . Object ) bool , adopt , release func ( metav1 . Object ) error ) ( bool , error ) { controller Ref := metav1 . Get Controller if controller Ref != nil { if controller Ref . UID != m . Controller . Get // Owned by us but selector doesn't match. // Try to release, unless we're being deleted. if m . Controller . Get Deletion if err := release ( obj ) ; err != nil { // If the pod no longer exists, ignore the error. if errors . Is Not // It's an orphan. if m . Controller . Get Deletion if obj . Get Deletion // Selector matches. Try to adopt. if err := adopt ( obj ) ; err != nil { // If the pod no longer exists, ignore the error. if errors . Is Not } 
func New Pod Controller Ref Manager ( pod Control Pod Control Interface , controller metav1 . Object , selector labels . Selector , controller Kind schema . Group Version Kind , can Adopt func ( ) error , ) * Pod Controller Ref Manager { return & Pod Controller Ref Manager { Base Controller Ref Manager : Base Controller Ref Manager { Controller : controller , Selector : selector , Can Adopt Func : can Adopt , } , controller Kind : controller Kind , pod Control : pod } 
func ( m * Pod Controller Ref Manager ) Claim adopt := func ( obj metav1 . Object ) error { return m . Adopt release := func ( obj metav1 . Object ) error { return m . Release for _ , pod := range pods { ok , err := m . Claim return claimed , utilerrors . New } 
func ( m * Pod Controller Ref Manager ) Adopt Pod ( pod * v1 . Pod ) error { if err := m . Can // Note that Validate Owner References() will reject this patch if another // Owner Reference exists with controller=true. add Controller Patch := fmt . Sprintf ( `{"metadata":{"owner References":[{"api Version":"%s","kind":"%s","name":"%s","uid":"%s","controller":true,"block Owner Deletion":true}],"uid":"%s"}}` , m . controller Kind . Group Version ( ) , m . controller Kind . Kind , m . Controller . Get Name ( ) , m . Controller . Get return m . pod Control . Patch Pod ( pod . Namespace , pod . Name , [ ] byte ( add Controller } 
func ( m * Pod Controller Ref Manager ) Release Pod ( pod * v1 . Pod ) error { klog . V ( 2 ) . Infof ( " " , pod . Namespace , pod . Name , m . controller Kind . Group Version ( ) , m . controller Kind . Kind , m . Controller . Get delete Owner Ref Patch := fmt . Sprintf ( `{"metadata":{"owner References":[{"$patch":"delete","uid":"%s"}],"uid":"%s"}}` , m . Controller . Get err := m . pod Control . Patch Pod ( pod . Namespace , pod . Name , [ ] byte ( delete Owner Ref if err != nil { if errors . Is Not if errors . Is } 
func New Replica Set Controller Ref Manager ( rs Control RS Control Interface , controller metav1 . Object , selector labels . Selector , controller Kind schema . Group Version Kind , can Adopt func ( ) error , ) * Replica Set Controller Ref Manager { return & Replica Set Controller Ref Manager { Base Controller Ref Manager : Base Controller Ref Manager { Controller : controller , Selector : selector , Can Adopt Func : can Adopt , } , controller Kind : controller Kind , rs Control : rs } 
func ( m * Replica Set Controller Ref Manager ) Claim Replica Sets ( sets [ ] * apps . Replica Set ) ( [ ] * apps . Replica Set , error ) { var claimed [ ] * apps . Replica match := func ( obj metav1 . Object ) bool { return m . Selector . Matches ( labels . Set ( obj . Get adopt := func ( obj metav1 . Object ) error { return m . Adopt Replica Set ( obj . ( * apps . Replica release := func ( obj metav1 . Object ) error { return m . Release Replica Set ( obj . ( * apps . Replica for _ , rs := range sets { ok , err := m . Claim return claimed , utilerrors . New } 
func ( m * Replica Set Controller Ref Manager ) Adopt Replica Set ( rs * apps . Replica Set ) error { if err := m . Can // Note that Validate Owner References() will reject this patch if another // Owner Reference exists with controller=true. add Controller Patch := fmt . Sprintf ( `{"metadata":{"owner References":[{"api Version":"%s","kind":"%s","name":"%s","uid":"%s","controller":true,"block Owner Deletion":true}],"uid":"%s"}}` , m . controller Kind . Group Version ( ) , m . controller Kind . Kind , m . Controller . Get Name ( ) , m . Controller . Get return m . rs Control . Patch Replica Set ( rs . Namespace , rs . Name , [ ] byte ( add Controller } 
func ( m * Replica Set Controller Ref Manager ) Release Replica Set ( replica Set * apps . Replica Set ) error { klog . V ( 2 ) . Infof ( " " , replica Set . Namespace , replica Set . Name , m . controller Kind . Group Version ( ) , m . controller Kind . Kind , m . Controller . Get delete Owner Ref Patch := fmt . Sprintf ( `{"metadata":{"owner References":[{"$patch":"delete","uid":"%s"}],"uid":"%s"}}` , m . Controller . Get UID ( ) , replica err := m . rs Control . Patch Replica Set ( replica Set . Namespace , replica Set . Name , [ ] byte ( delete Owner Ref if err != nil { if errors . Is Not Found ( err ) { // If the Replica if errors . Is Invalid ( err ) { // Invalid error will be returned in two cases: 1. the Replica Set // has no owner reference, 2. the uid of the Replica Set doesn't // match, which means the Replica } 
func Recheck Deletion Timestamp ( get Object func ( ) ( metav1 . Object , error ) ) func ( ) error { return func ( ) error { obj , err := get if obj . Get Deletion Timestamp ( ) != nil { return fmt . Errorf ( " " , obj . Get Namespace ( ) , obj . Get Name ( ) , obj . Get Deletion } 
func New Controller Revision Controller Ref Manager ( cr Control Controller Revision Control Interface , controller metav1 . Object , selector labels . Selector , controller Kind schema . Group Version Kind , can Adopt func ( ) error , ) * Controller Revision Controller Ref Manager { return & Controller Revision Controller Ref Manager { Base Controller Ref Manager : Base Controller Ref Manager { Controller : controller , Selector : selector , Can Adopt Func : can Adopt , } , controller Kind : controller Kind , cr Control : cr } 
func ( m * Controller Revision Controller Ref Manager ) Claim Controller Revisions ( histories [ ] * apps . Controller Revision ) ( [ ] * apps . Controller Revision , error ) { var claimed [ ] * apps . Controller match := func ( obj metav1 . Object ) bool { return m . Selector . Matches ( labels . Set ( obj . Get adopt := func ( obj metav1 . Object ) error { return m . Adopt Controller Revision ( obj . ( * apps . Controller release := func ( obj metav1 . Object ) error { return m . Release Controller Revision ( obj . ( * apps . Controller for _ , h := range histories { ok , err := m . Claim return claimed , utilerrors . New } 
func ( m * Controller Revision Controller Ref Manager ) Adopt Controller Revision ( history * apps . Controller Revision ) error { if err := m . Can // Note that Validate Owner References() will reject this patch if another // Owner Reference exists with controller=true. add Controller Patch := fmt . Sprintf ( `{"metadata":{"owner References":[{"api Version":"%s","kind":"%s","name":"%s","uid":"%s","controller":true,"block Owner Deletion":true}],"uid":"%s"}}` , m . controller Kind . Group Version ( ) , m . controller Kind . Kind , m . Controller . Get Name ( ) , m . Controller . Get return m . cr Control . Patch Controller Revision ( history . Namespace , history . Name , [ ] byte ( add Controller } 
func ( m * Controller Revision Controller Ref Manager ) Release Controller Revision ( history * apps . Controller Revision ) error { klog . V ( 2 ) . Infof ( " " , history . Namespace , history . Name , m . controller Kind . Group Version ( ) , m . controller Kind . Kind , m . Controller . Get delete Owner Ref Patch := fmt . Sprintf ( `{"metadata":{"owner References":[{"$patch":"delete","uid":"%s"}],"uid":"%s"}}` , m . Controller . Get err := m . cr Control . Patch Controller Revision ( history . Namespace , history . Name , [ ] byte ( delete Owner Ref if err != nil { if errors . Is Not Found ( err ) { // If the Controller if errors . Is Invalid ( err ) { // Invalid error will be returned in two cases: 1. the Controller Revision // has no owner reference, 2. the uid of the Controller Revision doesn't // match, which means the Controller } 
func ( f * shared Informer Factory ) For Resource ( resource schema . Group Version Resource ) ( Generic Informer , error ) { switch resource { // Group=apiregistration.k8s.io, Version=internal Version case apiregistration . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group Resource ( ) , informer : f . Apiregistration ( ) . Internal Version ( ) . API } 
func New Inter Pod Anti Affinity ( ) * Plugin { return & Plugin { Handler : admission . New } 
func ( p * Plugin ) Validate ( attributes admission . Attributes , o admission . Object Interfaces ) ( err error ) { // Ignore all calls to subresources or resources other than pods. if len ( attributes . Get Subresource ( ) ) != 0 || attributes . Get Resource ( ) . Group pod , ok := attributes . Get if ! ok { return apierrors . New Bad if affinity != nil && affinity . Pod Anti Affinity != nil { var pod Anti Affinity Terms [ ] api . Pod Affinity if len ( affinity . Pod Anti Affinity . Required During Scheduling Ignored During Execution ) != 0 { pod Anti Affinity Terms = affinity . Pod Anti Affinity . Required During Scheduling Ignored During // TODO: Uncomment this block when implement Required During Scheduling Required During Execution. //if len(affinity.Pod Anti Affinity.Required During Scheduling Required During Execution) != 0 { // pod Anti Affinity Terms = append(pod Anti Affinity Terms, affinity.Pod Anti Affinity.Required During Scheduling Required During Execution...) //} for _ , v := range pod Anti Affinity Terms { if v . Topology Key != v1 . Label Hostname { return apierrors . New Forbidden ( attributes . Get Resource ( ) . Group Resource ( ) , pod . Name , fmt . Errorf ( " " , v . Topology Key , v1 . Label } 
func ( c * controller Revisions ) Create ( controller Revision * v1beta2 . Controller Revision ) ( result * v1beta2 . Controller Revision , err error ) { result = & v1beta2 . Controller err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( controller } 
func New Checker ( policy * audit . Policy ) Checker { for i , rule := range policy . Rules { policy . Rules [ i ] . Omit Stages = union Stages ( policy . Omit Stages , rule . Omit return & policy } 
func Fake Checker ( level audit . Level , stage [ ] audit . Stage ) Checker { return & fake } 
func rule Matches ( r * audit . Policy Rule , attrs authorizer . Attributes ) bool { user := attrs . Get if len ( r . Users ) > 0 { if user == nil || ! has String ( r . Users , user . Get if len ( r . User for _ , group := range user . Get Groups ( ) { if has String ( r . User if len ( r . Verbs ) > 0 { if ! has String ( r . Verbs , attrs . Get if len ( r . Namespaces ) > 0 || len ( r . Resources ) > 0 { return rule Matches if len ( r . Non Resource UR Ls ) > 0 { return rule Matches Non } 
func rule Matches Non Resource ( r * audit . Policy Rule , attrs authorizer . Attributes ) bool { if attrs . Is Resource path := attrs . Get for _ , spec := range r . Non Resource UR Ls { if path } 
func path // Allow a trailing * subpath match if strings . Has Suffix ( spec , " " ) && strings . Has Prefix ( path , strings . Trim } 
func rule Matches Resource ( r * audit . Policy Rule , attrs authorizer . Attributes ) bool { if ! attrs . Is Resource if len ( r . Namespaces ) > 0 { if ! has String ( r . Namespaces , attrs . Get api Group := attrs . Get API resource := attrs . Get subresource := attrs . Get combined // If subresource, the resource in the policy must match "(resource)/(subresource)" if subresource != " " { combined name := attrs . Get for _ , gr := range r . Resources { if gr . Group == api for _ , res := range gr . Resources { if len ( gr . Resource Names ) == 0 || has String ( gr . Resource Names , name ) { // match "*" if res == combined // match "*/subresource" if len ( subresource ) > 0 && strings . Has Prefix ( res , " " ) && subresource == strings . Trim // match "resource/*" if strings . Has Suffix ( res , " " ) && resource == strings . Trim } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Persistent Volume Binder Controller Configuration ) ( nil ) , ( * config . Persistent Volume Binder Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Persistent Volume Binder Controller Configuration_To_config_Persistent Volume Binder Controller Configuration ( a . ( * v1alpha1 . Persistent Volume Binder Controller Configuration ) , b . ( * config . Persistent Volume Binder Controller if err := s . Add Generated Conversion Func ( ( * config . Persistent Volume Binder Controller Configuration ) ( nil ) , ( * v1alpha1 . Persistent Volume Binder Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Persistent Volume Binder Controller Configuration_To_v1alpha1_Persistent Volume Binder Controller Configuration ( a . ( * config . Persistent Volume Binder Controller Configuration ) , b . ( * v1alpha1 . Persistent Volume Binder Controller if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Persistent Volume Recycler Configuration ) ( nil ) , ( * config . Persistent Volume Recycler Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Persistent Volume Recycler Configuration_To_config_Persistent Volume Recycler Configuration ( a . ( * v1alpha1 . Persistent Volume Recycler Configuration ) , b . ( * config . Persistent Volume Recycler if err := s . Add Generated Conversion Func ( ( * config . Persistent Volume Recycler Configuration ) ( nil ) , ( * v1alpha1 . Persistent Volume Recycler Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Persistent Volume Recycler Configuration_To_v1alpha1_Persistent Volume Recycler Configuration ( a . ( * config . Persistent Volume Recycler Configuration ) , b . ( * v1alpha1 . Persistent Volume Recycler if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Configuration ) ( nil ) , ( * config . Volume Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Configuration_To_config_Volume Configuration ( a . ( * v1alpha1 . Volume Configuration ) , b . ( * config . Volume if err := s . Add Generated Conversion Func ( ( * config . Volume Configuration ) ( nil ) , ( * v1alpha1 . Volume Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Volume Configuration_To_v1alpha1_Volume Configuration ( a . ( * config . Volume Configuration ) , b . ( * v1alpha1 . Volume if err := s . Add Conversion Func ( ( * config . Persistent Volume Binder Controller Configuration ) ( nil ) , ( * v1alpha1 . Persistent Volume Binder Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Persistent Volume Binder Controller Configuration_To_v1alpha1_Persistent Volume Binder Controller Configuration ( a . ( * config . Persistent Volume Binder Controller Configuration ) , b . ( * v1alpha1 . Persistent Volume Binder Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Persistent Volume Binder Controller Configuration ) ( nil ) , ( * config . Persistent Volume Binder Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Persistent Volume Binder Controller Configuration_To_config_Persistent Volume Binder Controller Configuration ( a . ( * v1alpha1 . Persistent Volume Binder Controller Configuration ) , b . ( * config . Persistent Volume Binder Controller } 
func Convert_v1alpha1_Persistent Volume Recycler Configuration_To_config_Persistent Volume Recycler Configuration ( in * v1alpha1 . Persistent Volume Recycler Configuration , out * config . Persistent Volume Recycler Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Persistent Volume Recycler Configuration_To_config_Persistent Volume Recycler } 
func Convert_config_Persistent Volume Recycler Configuration_To_v1alpha1_Persistent Volume Recycler Configuration ( in * config . Persistent Volume Recycler Configuration , out * v1alpha1 . Persistent Volume Recycler Configuration , s conversion . Scope ) error { return auto Convert_config_Persistent Volume Recycler Configuration_To_v1alpha1_Persistent Volume Recycler } 
func Convert_v1alpha1_Volume Configuration_To_config_Volume Configuration ( in * v1alpha1 . Volume Configuration , out * config . Volume Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Configuration_To_config_Volume } 
func Convert_config_Volume Configuration_To_v1alpha1_Volume Configuration ( in * config . Volume Configuration , out * v1alpha1 . Volume Configuration , s conversion . Scope ) error { return auto Convert_config_Volume Configuration_To_v1alpha1_Volume } 
func ( s Secret For Docker Registry Generator V1 ) Generate ( generic Params map [ string ] interface { } ) ( runtime . Object , error ) { err := generate . Validate Params ( s . Param Names ( ) , generic delegate := & Secret For Docker Registry Generator hash Param , found := generic if found { hash Bool , is Bool := hash if ! is Bool { return nil , fmt . Errorf ( " " , hash delegate . Append Hash = hash delete ( generic for key , value := range generic Params { str Val , is if ! is params [ key ] = str return delegate . Structured } 
func ( s Secret For Docker Registry Generator V1 ) Structured secret . Type = v1 . Secret Type Docker Config if len ( s . File Sources ) > 0 { if err := handle From File Sources ( secret , s . File if len ( s . File Sources ) == 0 { dockercfg JSON Content , err := handle Docker Cfg JSON secret . Data [ v1 . Docker Config Json Key ] = dockercfg JSON if s . Append Hash { h , err := hash . Secret } 
func ( s Secret For Docker Registry Generator V1 ) Param Names ( ) [ ] generate . Generator Param { return [ ] generate . Generator } 
func ( s Secret For Docker Registry Generator if len ( s . File } 
func handle Docker Cfg JSON Content ( username , password , email , server string ) ( [ ] byte , error ) { dockercfg Auth := Docker Config Entry { Username : username , Password : password , Email : email , Auth : encode Docker Config Field docker Cfg JSON := Docker Config JSON { Auths : map [ string ] Docker Config Entry { server : dockercfg return json . Marshal ( docker Cfg } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . Kubelet Configuration { } , func ( obj interface { } ) { Set Object Defaults_Kubelet Configuration ( obj . ( * v1beta1 . Kubelet } 
func New Cmd Apply ( base Name string , f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Apply Options ( io // Store base Name for use in printing warnings / messages involving the base command name. // This is useful for downstream command that wrap this one. o . cmd Base Name = base cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : apply Long , Example : apply Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check Err ( validate cmdutil . Check Err ( validate Prune cmdutil . Check // bind flag structs o . Delete Flags . Add o . Record Flags . Add o . Print Flags . Add cmd . Flags ( ) . Bool cmd . Flags ( ) . Bool cmdutil . Add Validate cmd . Flags ( ) . String Var cmd . Flags ( ) . Bool cmd . Flags ( ) . String Array Var ( & o . Prune Whitelist , " " , o . Prune cmd . Flags ( ) . Bool Var ( & o . Open API Patch , " " , o . Open API cmd . Flags ( ) . Bool Var ( & o . Server Dry Run , " " , o . Server Dry cmdutil . Add Include Uninitialized cmdutil . Add Server Side Apply // apply subcommands cmd . Add Command ( New Cmd Apply View Last Applied ( f , io cmd . Add Command ( New Cmd Apply Set Last Applied ( f , io cmd . Add Command ( New Cmd Apply Edit Last Applied ( f , io } 
func ( v * Dry Run Verifier ) Has Support ( gvk schema . Group Version Kind ) error { oapi , err := v . Open API Getter . Open API supports , err := openapi . Supports Dry if err != nil { // We assume that we couldn't find the type, then check for namespace: supports , _ = openapi . Supports Dry Run ( oapi , schema . Group Version // If namespace supports dry Run, then we will support dry Run for CR Ds only. if supports { supports , err = v . Finder . Has CRD ( gvk . Group } 
func access Modes Index Func ( obj interface { } ) ( [ ] string , error ) { if pv , ok := obj . ( * v1 . Persistent Volume ) ; ok { modes := v1helper . Get Access Modes As String ( pv . Spec . Access } 
func ( pv Index * persistent Volume Ordered Index ) list By Access Modes ( modes [ ] v1 . Persistent Volume Access Mode ) ( [ ] * v1 . Persistent Volume , error ) { pv := & v1 . Persistent Volume { Spec : v1 . Persistent Volume Spec { Access objs , err := pv volumes := make ( [ ] * v1 . Persistent for i , obj := range objs { volumes [ i ] = obj . ( * v1 . Persistent } 
func ( pv Index * persistent Volume Ordered Index ) find By Claim ( claim * v1 . Persistent Volume Claim , delay Binding bool ) ( * v1 . Persistent Volume , error ) { // P Vs are indexed by their access modes to allow easier searching. Each // index is the string representation of a set of access modes. There is a // finite number of possible sets and P Vs will only be indexed in one of // them (whichever index matches the PV's modes). // // A request for resources will always specify its desired access modes. // Any matching PV must have at least that number of access modes, but it // can have more. For example, a user asks for Read Write Once but a GCEPD // is available, which is Read Write Once+Read Only Many. // // Searches are performed against a set of access modes, so we can attempt // not only the exact matching modes but also potential matches (the GCEPD // example above). all Possible Modes := pv Index . all Possible Matching Access Modes ( claim . Spec . Access for _ , modes := range all Possible Modes { volumes , err := pv Index . list By Access best Vol , err := find Matching Volume ( claim , volumes , nil /* node for topology binding*/ , nil /* exclusion map */ , delay if best Vol != nil { return best } 
func find Matching Volume ( claim * v1 . Persistent Volume Claim , volumes [ ] * v1 . Persistent Volume , node * v1 . Node , excluded Volumes map [ string ] * v1 . Persistent Volume , delay Binding bool ) ( * v1 . Persistent Volume , error ) { var smallest Volume * v1 . Persistent var smallest Volume requested Qty := claim . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource requested Class := v1helper . Get Persistent Volume Claim if claim . Spec . Selector != nil { internal Selector , err := metav1 . Label Selector As if err != nil { // should be unreachable code due to validation return nil , fmt . Errorf ( " " , claim To Claim selector = internal // Go through all available volumes with two goals: // - find a volume that is either pre-bound by user or dynamically // provisioned for this claim. Because of this we need to loop through // all volumes. // - find the smallest matching one if there is no volume pre-bound to // the claim. for _ , volume := range volumes { if _ , ok := excluded volume Qty := volume . Spec . Capacity [ v1 . Resource // check if volume Modes do not match (feature gate protected) is Mismatch , err := check Volume Mode // filter out mismatching volume Modes if is // check if PV's Deletion Time Stamp is set, if so, skip this volume. if utilfeature . Default Feature Gate . Enabled ( features . Storage Object In Use Protection ) { if volume . Object Meta . Deletion node Affinity if node != nil { // Scheduler path, check that the PV Node Affinity // is satisfied by the node err := volumeutil . Check Node if err != nil { node Affinity if Is Volume Bound To Claim ( volume , claim ) { // this claim and volume are pre-bound; return // the volume if the size request is satisfied, // otherwise continue searching for a match if volume Qty . Cmp ( requested // If PV node affinity is invalid, return no match. // This means the prebound PV (and therefore PVC) // is not suitable for this node. if ! node Affinity if node == nil && delay // filter out: // - volumes in non-available phase // - volumes bound to another claim // - volumes whose labels don't match the claim's selector, if specified // - volumes in Class that is not requested // - volumes whose Node Affinity does not match the node if volume . Status . Phase != v1 . Volume } else if volume . Spec . Claim if v1helper . Get Persistent Volume Class ( volume ) != requested if ! node Affinity if node != nil { // Scheduler path // Check that the access modes match if ! check Access if volume Qty . Cmp ( requested Qty ) >= 0 { if smallest Volume == nil || smallest Volume Qty . Cmp ( volume Qty ) > 0 { smallest smallest Volume Qty = volume if smallest Volume != nil { // Found a matching volume return smallest } 
func check Volume Mode Mismatches ( pvc Spec * v1 . Persistent Volume Claim Spec , pv Spec * v1 . Persistent Volume Spec ) ( bool , error ) { if ! utilfeature . Default Feature Gate . Enabled ( features . Block // In HA upgrades, we cannot guarantee that the apiserver is on a version >= controller-manager. // So we default a nil volume Mode to filesystem requested Volume Mode := v1 . Persistent Volume if pvc Spec . Volume Mode != nil { requested Volume Mode = * pvc Spec . Volume pv Volume Mode := v1 . Persistent Volume if pv Spec . Volume Mode != nil { pv Volume Mode = * pv Spec . Volume return requested Volume Mode != pv Volume } 
func ( pv Index * persistent Volume Ordered Index ) find Best Match For Claim ( claim * v1 . Persistent Volume Claim , delay Binding bool ) ( * v1 . Persistent Volume , error ) { return pv Index . find By Claim ( claim , delay } 
func ( pv Index * persistent Volume Ordered Index ) all Possible Matching Access Modes ( requested Modes [ ] v1 . Persistent Volume Access Mode ) [ ] [ ] v1 . Persistent Volume Access Mode { matched Modes := [ ] [ ] v1 . Persistent Volume Access keys := pv Index . store . List Index Func for _ , key := range keys { indexed Modes := v1helper . Get Access Modes From if volumeutil . Access Modes Contained In All ( indexed Modes , requested Modes ) { matched Modes = append ( matched Modes , indexed // sort by the number of modes in each array with the fewest number of // modes coming first. this allows searching for volumes by the minimum // number of modes required of the possible matches. sort . Sort ( by Access Modes { matched return matched } 
func check Access Modes ( claim * v1 . Persistent Volume Claim , volume * v1 . Persistent Volume ) bool { pv Modes Map := map [ v1 . Persistent Volume Access for _ , mode := range volume . Spec . Access Modes { pv Modes for _ , mode := range claim . Spec . Access Modes { _ , ok := pv Modes } 
} 
} 
func ( p * Factory Impl ) Make Patch Transformer ( slice [ ] * resource . Resource , rf * resource . Factory ) ( transformers . Transformer , error ) { return patch . New Patch } 
func ( c * Fake Deployments ) List ( opts v1 . List Options ) ( result * v1beta1 . Deployment List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( deployments Resource , deployments Kind , c . ns , opts ) , & v1beta1 . Deployment label , _ , _ := testing . Extract From List list := & v1beta1 . Deployment List { List Meta : obj . ( * v1beta1 . Deployment List ) . List for _ , item := range obj . ( * v1beta1 . Deployment } 
func ( util * Disk Util ) Attach Disk ( b * cinder Volume Mounter , global PD if b . read cloud , err := b . plugin . get Cloud instanceid , err := cloud . Instance diskid , err := cloud . Attach Disk ( instanceid , b . pd var device num for { device Path = cloud . Get Device probe Attached _ , err := os . Stat ( device if err != nil && ! os . Is Not num if num notmnt , err := b . mounter . Is Likely Not Mount Point ( global PD if err != nil { if os . Is Not Exist ( err ) { if err := os . Mkdir All ( global PD if notmnt { err = b . block Device Mounter . Format And Mount ( device Path , global PD Path , b . fs if err != nil { os . Remove ( global PD klog . V ( 2 ) . Infof ( " \n " , device } 
func ( util * Disk Util ) Detach Disk ( cd * cinder Volume Unmounter ) error { global PD Path := make Global PD Name ( cd . plugin . host , cd . pd if err := cd . mounter . Unmount ( global PD if err := os . Remove ( global PD klog . V ( 2 ) . Infof ( " \n " , global PD cloud , err := cd . plugin . get Cloud instanceid , err := cloud . Instance if err = cloud . Detach Disk ( instanceid , cd . pd klog . V ( 2 ) . Infof ( " " , cd . pd } 
func ( util * Disk Util ) Delete Volume ( cd * cinder Volume Deleter ) error { cloud , err := cd . plugin . get Cloud if err = cloud . Delete Volume ( cd . pd Name ) ; err != nil { // Open Stack cloud provider returns volume.try Again Error when necessary, // no handling needed here. klog . V ( 2 ) . Infof ( " " , cd . pd klog . V ( 2 ) . Infof ( " " , cd . pd } 
func ( util * Disk Util ) Create Volume ( c * cinder Volume Provisioner , node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term ) ( volume ID string , volume Size GB int , volume Labels map [ string ] string , fstype string , err error ) { cloud , err := c . plugin . get Cloud capacity := c . options . PVC . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource // Cinder works with gigabytes, convert to Gi B with rounding up vol Size Gi B , err := volumehelpers . Round Up To Gi B name := volutil . Generate Volume Name ( c . options . Cluster Name , c . options . PV // Apply Provisioner Parameters (case-insensitive). We leave validation of // the values to the cloud provider. for k , v := range c . options . Parameters { switch strings . To case volume . Volume Parameter FS default : return " " , 0 , nil , " " , fmt . Errorf ( " " , k , c . plugin . Get Plugin if availability == " " { // No zone specified, choose one randomly in the same region zones , err := get Zones From Nodes ( c . plugin . host . Get Kube // if we did not get any zones, lets leave it blank and gophercloud will // use zone "nova" as default if len ( zones ) > 0 { availability , err = volumehelpers . Select Zone For Volume ( false , false , " " , nil , zones , node , allowed volume ID , volume AZ , volume Region , Ignore Volume AZ , err := cloud . Create Volume ( name , vol Size Gi B , vtype , availability , c . options . Cloud klog . V ( 2 ) . Infof ( " " , volume // these are needed that pod is spawning to same AZ volume if Ignore Volume AZ == false { if volume AZ != " " { volume Labels [ v1 . Label Zone Failure Domain ] = volume if volume Region != " " { volume Labels [ v1 . Label Zone Region ] = volume return volume ID , vol Size Gi B , volume } 
func ( in * Flunder Spec ) Deep Copy Into ( out * Flunder if in . Reference Type != nil { in , out := & in . Reference Type , & out . Reference * out = new ( Reference } 
func New Work Args ( name , namespace string ) * Work Args { return & Work Args { types . Namespaced } 
func Create Worker ( args * Work Args , created At time . Time , fire At time . Time , f func ( args * Work Args ) error ) * Timed Worker { delay := fire At . Sub ( created timer := time . After return & Timed Worker { Work Item : args , Created At : created At , Fire At : fire } 
func Create Worker Queue ( f func ( args * Work Args ) error ) * Timed Worker Queue { return & Timed Worker Queue { workers : make ( map [ string ] * Timed Worker ) , work } 
func ( q * Timed Worker Queue ) Add Work ( args * Work Args , created At time . Time , fire At time . Time ) { key := args . Key From Work klog . V ( 4 ) . Infof ( " " , key , created At , fire worker := Create Worker ( args , created At , fire At , q . get Wrapped Worker } 
func ( q * Timed Worker Queue ) Cancel } 
func ( q * Timed Worker Queue ) Get Worker Unsafe ( key string ) * Timed } 
func ( s * cluster Role Binding Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Cluster Role Binding , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Cluster Role } 
func ( s * cluster Role Binding Lister ) Get ( name string ) ( * v1 . Cluster Role Binding , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1 . Cluster Role } 
func ( rs Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { rs := obj . ( * apps . Replica rs . Status = apps . Replica Set pod . Drop Disabled Template } 
func ( rs Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new RS := obj . ( * apps . Replica old RS := old . ( * apps . Replica // update is not allowed to set status new RS . Status = old pod . Drop Disabled Template Fields ( & new RS . Spec . Template , & old // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. We push // the burden of managing the status onto the clients because we can't (in general) // know here what version of spec the writer of the status has seen. It may seem like // we can at first -- since obj contains spec -- but in the future we will probably make // status its own object, and even if we don't, writes may be the result of a // read-update-write loop, so the contents of spec may not actually be the spec that // the Replica Set has *seen*. if ! apiequality . Semantic . Deep Equal ( old RS . Spec , new RS . Spec ) { new RS . Generation = old } 
func ( rs Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { rs := obj . ( * apps . Replica all Errs := validation . Validate Replica all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & rs . Spec . Template , nil , field . New return all } 
func ( rs Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new Replica Set := obj . ( * apps . Replica old Replica Set := old . ( * apps . Replica all Errs := validation . Validate Replica Set ( obj . ( * apps . Replica all Errs = append ( all Errs , validation . Validate Replica Set Update ( new Replica Set , old Replica all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & new Replica Set . Spec . Template , & old Replica Set . Spec . Template , field . New // Update is not allowed to set Spec.Selector for all groups/versions except extensions/v1beta1. // If Request Info is nil, it is better to revert to old behavior (i.e. allow update to set Spec.Selector) // to prevent unintentionally breaking users who may rely on the old behavior. // TODO(#50791): after extensions/v1beta1 is removed, move selector immutability check inside Validate Replica Set Update(). if request Info , found := genericapirequest . Request Info From ( ctx ) ; found { group Version := schema . Group Version { Group : request Info . API Group , Version : request Info . API switch group Version { case extensionsv1beta1 . Scheme Group Version : // no-op for compatibility default : // disallow mutation of selector all Errs = append ( all Errs , apivalidation . Validate Immutable Field ( new Replica Set . Spec . Selector , old Replica Set . Spec . Selector , field . New return all } 
func Replica Set To Selectable Fields ( rs * apps . Replica Set ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & rs . Object rs Specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , rs Specific Fields } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { rs , ok := obj . ( * apps . Replica return labels . Set ( rs . Object Meta . Labels ) , Replica Set To Selectable } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Pod { } , & Replica } 
func ( c * Authentication V1Client ) REST return c . rest } 
func ( autoscaler Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { new HPA := obj . ( * autoscaling . Horizontal Pod // create cannot set status new HPA . Status = autoscaling . Horizontal Pod Autoscaler } 
func ( autoscaler Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { autoscaler := obj . ( * autoscaling . Horizontal Pod return validation . Validate Horizontal Pod } 
func ( autoscaler Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new HPA := obj . ( * autoscaling . Horizontal Pod old HPA := old . ( * autoscaling . Horizontal Pod // Update is not allowed to set status new HPA . Status = old } 
func ( autoscaler Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return validation . Validate Horizontal Pod Autoscaler Update ( obj . ( * autoscaling . Horizontal Pod Autoscaler ) , old . ( * autoscaling . Horizontal Pod } 
func ( c * cluster Roles ) Create ( cluster Role * v1beta1 . Cluster Role ) ( result * v1beta1 . Cluster Role , err error ) { result = & v1beta1 . Cluster err = c . client . Post ( ) . Resource ( " " ) . Body ( cluster } 
func Add Certificate Dir Flag ( fs * pflag . Flag Set , certs Dir * string ) { fs . String Var ( certs Dir , Certificates Dir , * certs } 
func Add CSR Flag ( fs * pflag . Flag Set , csr * bool ) { fs . Bool Var ( csr , CSR } 
func Add CSR Dir Flag ( fs * pflag . Flag Set , csr Dir * string ) { fs . String Var ( csr Dir , CSR Dir , * csr } 
func ( in * Runtime Class ) Deep Copy ( ) * Runtime out := new ( Runtime in . Deep Copy } 
func ( in * Runtime Class ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Runtime Class List ) Deep Copy Into ( out * Runtime Class out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Runtime for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Runtime Class List ) Deep Copy ( ) * Runtime Class out := new ( Runtime Class in . Deep Copy } 
func ( in * Runtime Class List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func New Storage ( opts Getter generic . REST Options Getter ) * Volume Attachment Storage { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & storageapi . Volume Attachment { } } , New List Func : func ( ) runtime . Object { return & storageapi . Volume Attachment List { } } , Default Qualified Resource : storageapi . Resource ( " " ) , Create Strategy : volumeattachment . Strategy , Update Strategy : volumeattachment . Strategy , Delete Strategy : volumeattachment . Strategy , Return Deleted Object : true , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts if err := store . Complete With status status Store . Update Strategy = volumeattachment . Status return & Volume Attachment Storage { Volume Attachment : & REST { store } , Status : & Status REST { store : & status } 
func Load Client Config ( kubeconfig Path , bootstrap Path , cert Dir string ) ( cert Config , user Config * restclient . Config , err error ) { if len ( bootstrap Path ) == 0 { client Config , err := load REST Client Config ( kubeconfig return client Config , restclient . Copy Config ( client store , err := certificate . New File Store ( " " , cert Dir , cert ok , err := is Client Config Still Valid ( kubeconfig // use the current client config if ok { client Config , err := load REST Client Config ( kubeconfig return client Config , restclient . Copy Config ( client bootstrap Client Config , err := load REST Client Config ( bootstrap client Config := restclient . Anonymous Client Config ( bootstrap Client pem Path := store . Current client Config . Key File = pem client Config . Cert File = pem if err := write Kubeconfig From Bootstrapping ( client Config , kubeconfig Path , pem return bootstrap Client Config , client } 
func Load Client Cert ( kubeconfig Path , bootstrap Path , cert Dir string , node Name types . Node Name ) error { // Short-circuit if the kubeconfig file exists and is valid. ok , err := is Client Config Still Valid ( kubeconfig if ok { klog . V ( 2 ) . Infof ( " " , kubeconfig bootstrap Client Config , err := load REST Client Config ( bootstrap bootstrap Client , err := certificatesv1beta1 . New For Config ( bootstrap Client store , err := certificate . New File Store ( " " , cert Dir , cert var key if cert , err := store . Current ( ) ; err == nil { if cert . Private Key != nil { key Data , err = keyutil . Marshal Private Key To PEM ( cert . Private if err != nil { key // Cache the private key in a separate file until CSR succeeds. This has to // be a separate file because store.Current Path() points to a symlink // managed by the store. priv Key Path := filepath . Join ( cert Dir , tmp Private Key if ! verify Key Data ( key // Note: always call Load Or Generate Key File so that private key is // reused on next startup if CSR request fails. key Data , _ , err = keyutil . Load Or Generate Key File ( priv Key if err := wait For Server ( * bootstrap Client cert Data , err := request Node Certificate ( bootstrap Client . Certificate Signing Requests ( ) , key Data , node if _ , err := store . Update ( cert Data , key if err := os . Remove ( priv Key Path ) ; err != nil && ! os . Is Not Exist ( err ) { klog . V ( 2 ) . Infof ( " " , priv Key return write Kubeconfig From Bootstrapping ( bootstrap Client Config , kubeconfig Path , store . Current } 
func is Client Config Still Valid ( kubeconfig Path string ) ( bool , error ) { _ , err := os . Stat ( kubeconfig if os . Is Not if err != nil { return false , fmt . Errorf ( " " , kubeconfig bootstrap Client Config , err := load REST Client Config ( kubeconfig if err != nil { utilruntime . Handle transport Config , err := bootstrap Client Config . Transport if err != nil { utilruntime . Handle // has side effect of populating transport config data fields if _ , err := transport . TLS Config For ( transport Config ) ; err != nil { utilruntime . Handle certs , err := certutil . Parse Certs PEM ( transport Config . TLS . Cert if err != nil { utilruntime . Handle if len ( certs ) == 0 { utilruntime . Handle for _ , cert := range certs { if now . After ( cert . Not After ) { utilruntime . Handle Error ( fmt . Errorf ( " " , cert . Not } 
func request Node Certificate ( client certificatesv1beta1 . Certificate Signing Request Interface , private Key Data [ ] byte , node Name types . Node Name ) ( cert Data [ ] byte , err error ) { subject := & pkix . Name { Organization : [ ] string { " " } , Common Name : " " + string ( node private Key , err := keyutil . Parse Private Key PEM ( private Key csr Data , err := certutil . Make CSR ( private usages := [ ] certificates . Key Usage { certificates . Usage Digital Signature , certificates . Usage Key Encipherment , certificates . Usage Client name := digested Name ( private Key req , err := csr . Request Certificate ( client , csr Data , name , usages , private return csr . Wait For } 
func digested Name ( private Key Data [ ] byte , subject * pkix . Name , usages [ ] certificates . Key // Here we make sure two different inputs can't write the same stream // to the hash. This delimiter is not in the base64.URL encode := base64 . Raw URL Encoding . Encode To write ( private Key write ( [ ] byte ( subject . Common } 
func To Valid Operation ID ( s string , capitalize First capitalize := capitalize First for i , r := range s { if unicode . Is Letter ( r ) || r == '_' || ( i != 0 && unicode . Is Digit ( r ) ) { if capitalize { buffer . Write Rune ( unicode . To } else { buffer . Write } 
func Get Operation ID And prefix , exists := verbs . Get if len ( parts ) >= 2 && parts [ 0 ] == " " { trimmed := strings . Trim prefix = prefix + To Valid Operation tag := To Valid Operation if len ( parts ) > 2 { prefix = prefix + To Valid Operation tag = tag + " " + To Valid Operation } else if len ( parts ) >= 1 { tags = append ( tags , To Valid Operation return prefix + To Valid Operation } 
func New Definition Namer ( schemes ... * runtime . Scheme ) * Definition Namer { ret := & Definition Namer { type Group Version Kinds : map [ string ] group Version for _ , s := range schemes { for gvk , rtype := range s . All Known Types ( ) { new GVK := gvk for _ , existing GVK := range ret . type Group Version Kinds [ type Name ( rtype ) ] { if new GVK == existing if ! exists { ret . type Group Version Kinds [ type Name ( rtype ) ] = append ( ret . type Group Version Kinds [ type Name ( rtype ) ] , new for _ , gvk := range ret . type Group Version } 
func ( d * Definition Namer ) Get Definition Name ( name string ) ( string , spec . Extensions ) { if group Version Kinds , ok := d . type Group Version Kinds [ name ] ; ok { return friendly Name ( name ) , spec . Extensions { extension GVK : group Version return friendly } 
func New Version Converter ( t Type Converter , o runtime . Object Convertor , h schema . Group Version ) merge . Converter { return & version Converter { type Converter : t , object Convertor : o , hub Getter : func ( from schema . Group Version ) schema . Group Version { return schema . Group } 
func New CRD Version Converter ( t Type Converter , o runtime . Object Convertor , h schema . Group Version ) merge . Converter { return & version Converter { type Converter : t , object Convertor : o , hub Getter : func ( from schema . Group Version ) schema . Group } 
func ( v * version Converter ) Convert ( object typed . Typed Value , version fieldpath . API Version ) ( typed . Typed Value , error ) { // Convert the smd typed value to a kubernetes object. object To Convert , err := v . type Converter . Typed To // Parse the target group Version. group Version , err := schema . Parse Group // If attempting to convert to the same version as we already have, just return it. from Version := object To Convert . Get Object Kind ( ) . Group Version Kind ( ) . Group if from Version == group // Convert to internal internal Object , err := v . object Convertor . Convert To Version ( object To Convert , v . hub Getter ( from // Convert the object into the target version converted Object , err := v . object Convertor . Convert To Version ( internal Object , group // Convert the object back to a smd typed value and return it. return v . type Converter . Object To Typed ( converted } 
func Index Func To Key Func Adapter ( index Func Index Func ) Key Func { return func ( obj interface { } ) ( string , error ) { index Keys , err := index if len ( index Keys ) > 1 { return " " , fmt . Errorf ( " " , index if len ( index return index } 
func Meta Namespace Index return [ ] string { meta . Get } 
func Create Listener ( endpoint string ) ( net . Listener , error ) { protocol , addr , err := parse Endpoint With Fallback Protocol ( endpoint , unix if protocol != unix if err != nil && ! os . Is Not } 
func Get Address And Dialer ( endpoint string ) ( string , func ( addr string , timeout time . Duration ) ( net . Conn , error ) , error ) { protocol , addr , err := parse Endpoint With Fallback Protocol ( endpoint , unix if protocol != unix } 
func Local Endpoint ( path , file string ) string { u := url . URL { Scheme : unix } 
func ( c * Apiregistration V1Client ) REST return c . rest } 
func ( daemon Set Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { daemon Set := obj . ( * apps . Daemon daemon Set . Status = apps . Daemon Set daemon if daemon Set . Spec . Template Generation < 1 { daemon Set . Spec . Template pod . Drop Disabled Template Fields ( & daemon } 
func ( daemon Set Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Daemon Set := obj . ( * apps . Daemon old Daemon Set := old . ( * apps . Daemon pod . Drop Disabled Template Fields ( & new Daemon Set . Spec . Template , & old Daemon // update is not allowed to set status new Daemon Set . Status = old Daemon // update is not allowed to set Template Generation new Daemon Set . Spec . Template Generation = old Daemon Set . Spec . Template // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. We push // the burden of managing the status onto the clients because we can't (in general) // know here what version of spec the writer of the status has seen. It may seem like // we can at first -- since obj contains spec -- but in the future we will probably make // status its own object, and even if we don't, writes may be the result of a // read-update-write loop, so the contents of spec may not actually be the spec that // the manager has *seen*. // // TODO: Any changes to a part of the object that represents desired state (labels, // annotations etc) should also increment the generation. if ! apiequality . Semantic . Deep Equal ( old Daemon Set . Spec . Template , new Daemon Set . Spec . Template ) { new Daemon Set . Spec . Template Generation = old Daemon Set . Spec . Template new Daemon Set . Generation = old Daemon if ! apiequality . Semantic . Deep Equal ( old Daemon Set . Spec , new Daemon Set . Spec ) { new Daemon Set . Generation = old Daemon } 
func ( daemon Set Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { daemon Set := obj . ( * apps . Daemon all Errs := validation . Validate Daemon Set ( daemon all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & daemon Set . Spec . Template , nil , field . New return all } 
func ( daemon Set Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new Daemon Set := obj . ( * apps . Daemon old Daemon Set := old . ( * apps . Daemon all Errs := validation . Validate Daemon Set ( obj . ( * apps . Daemon all Errs = append ( all Errs , validation . Validate Daemon Set Update ( new Daemon Set , old Daemon all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & new Daemon Set . Spec . Template , & old Daemon Set . Spec . Template , field . New // Update is not allowed to set Spec.Selector for apps/v1 and apps/v1beta2 (allowed for extensions/v1beta1). // If Request Info is nil, it is better to revert to old behavior (i.e. allow update to set Spec.Selector) // to prevent unintentionally breaking users who may rely on the old behavior. // TODO(#50791): after extensions/v1beta1 is removed, move selector immutability check inside Validate Daemon Set Update(). if request Info , found := genericapirequest . Request Info From ( ctx ) ; found { group Version := schema . Group Version { Group : request Info . API Group , Version : request Info . API switch group Version { case extensionsv1beta1 . Scheme Group Version : // no-op for compatibility default : // disallow mutation of selector all Errs = append ( all Errs , apivalidation . Validate Immutable Field ( new Daemon Set . Spec . Selector , old Daemon Set . Spec . Selector , field . New return all } 
func Verify Against t := Convert Toleration To A w := Convert Toleration To A for k1 , v1 := range t { if v2 , ok := w [ k1 ] ; ! ok || ! Are } 
func Is Conflict ( first [ ] api . Toleration , second [ ] api . Toleration ) bool { first Map := Convert Toleration To A second Map := Convert Toleration To A for k1 , v1 := range first Map { if v2 , ok := second Map [ k1 ] ; ok && ! Are } 
func Merge Tolerations ( first [ ] api . Toleration , second [ ] api . Toleration ) [ ] api . Toleration { var merged merged Tolerations = append ( merged first Map := Convert Toleration To A second Map := Convert Toleration To A for k1 , v1 := range first Map { if _ , ok := second Map [ k1 ] ; ! ok { merged Tolerations = append ( merged return merged } 
func Convert Toleration To A } 
func Are Equal ( first , second api . Toleration ) bool { if first . Key == second . Key && first . Operator == second . Operator && first . Value == second . Value && first . Effect == second . Effect && Are Toleration Seconds Equal ( first . Toleration Seconds , second . Toleration } 
func Are Toleration Seconds } 
func ( g * Cloud ) Get Zone ( ctx context . Context ) ( cloudprovider . Zone , error ) { return cloudprovider . Zone { Failure Domain : g . local } 
func ( g * Cloud ) Get Zone By Provider ID ( ctx context . Context , provider ID string ) ( cloudprovider . Zone , error ) { _ , zone , _ , err := split Provider ID ( provider region , err := Get GCE return cloudprovider . Zone { Failure } 
func ( g * Cloud ) Get Zone By Node Name ( ctx context . Context , node Name types . Node Name ) ( cloudprovider . Zone , error ) { instance Name := map Node Name To Instance Name ( node instance , err := g . get Instance By Name ( instance region , err := Get GCE return cloudprovider . Zone { Failure } 
func ( g * Cloud ) List Zones In Region ( region string ) ( [ ] * compute . Zone , error ) { ctx , cancel := cloud . Context With Call mc := new Zones Metric list , err := g . c . Zones ( ) . List ( ctx , filter . Regexp ( " " , g . get Region } 
func New Volume Attachment Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Volume Attachment Informer ( client , resync } 
func New Filtered Volume Attachment Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Storage V1beta1 ( ) . Volume } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Storage V1beta1 ( ) . Volume } , } , & storagev1beta1 . Volume Attachment { } , resync } 
func ( in * Controller Revision ) Deep Copy Into ( out * Controller out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Data . Deep Copy } 
func ( in * Rolling Update Stateful Set Strategy ) Deep Copy Into ( out * Rolling Update Stateful Set } 
func ( in * Stateful Set Update Strategy ) Deep Copy Into ( out * Stateful Set Update if in . Rolling Update != nil { in , out := & in . Rolling Update , & out . Rolling * out = new ( Rolling Update Stateful Set ( * in ) . Deep Copy } 
func New Cmd Create Priority Class ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Priority Class Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Aliases : [ ] string { " " } , Short : i18n . T ( " " ) , Long : pc Long , Example : pc Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Priority Class V1Alpha1Generator } 
func ( o * Priority Class Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Priority Class V1Alpha1Generator Name : generator = & generateversioned . Priority Class V1Generator { Name : name , Value : cmdutil . Get Flag Int32 ( cmd , " " ) , Global Default : cmdutil . Get Flag Bool ( cmd , " " ) , Description : cmdutil . Get Flag default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Example { } , & Example metav1 . Add To Group Version ( scheme , Scheme Group } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Cross Version Object Reference ) ( nil ) , ( * autoscaling . Cross Version Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cross Version Object Reference_To_autoscaling_Cross Version Object Reference ( a . ( * v1 . Cross Version Object Reference ) , b . ( * autoscaling . Cross Version Object if err := s . Add Generated Conversion Func ( ( * autoscaling . Cross Version Object Reference ) ( nil ) , ( * v1 . Cross Version Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Cross Version Object Reference_To_v1_Cross Version Object Reference ( a . ( * autoscaling . Cross Version Object Reference ) , b . ( * v1 . Cross Version Object if err := s . Add Generated Conversion Func ( ( * v1 . External Metric Source ) ( nil ) , ( * autoscaling . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_External Metric Source_To_autoscaling_External Metric Source ( a . ( * v1 . External Metric Source ) , b . ( * autoscaling . External Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . External Metric Source ) ( nil ) , ( * v1 . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Source_To_v1_External Metric Source ( a . ( * autoscaling . External Metric Source ) , b . ( * v1 . External Metric if err := s . Add Generated Conversion Func ( ( * v1 . External Metric Status ) ( nil ) , ( * autoscaling . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_External Metric Status_To_autoscaling_External Metric Status ( a . ( * v1 . External Metric Status ) , b . ( * autoscaling . External Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . External Metric Status ) ( nil ) , ( * v1 . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Status_To_v1_External Metric Status ( a . ( * autoscaling . External Metric Status ) , b . ( * v1 . External Metric if err := s . Add Generated Conversion Func ( ( * v1 . Horizontal Pod Autoscaler ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler_To_autoscaling_Horizontal Pod Autoscaler ( a . ( * v1 . Horizontal Pod Autoscaler ) , b . ( * autoscaling . Horizontal Pod if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler_To_v1_Horizontal Pod Autoscaler ( a . ( * autoscaling . Horizontal Pod Autoscaler ) , b . ( * v1 . Horizontal Pod if err := s . Add Generated Conversion Func ( ( * v1 . Horizontal Pod Autoscaler Condition ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler Condition_To_autoscaling_Horizontal Pod Autoscaler Condition ( a . ( * v1 . Horizontal Pod Autoscaler Condition ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Condition ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Condition_To_v1_Horizontal Pod Autoscaler Condition ( a . ( * autoscaling . Horizontal Pod Autoscaler Condition ) , b . ( * v1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v1 . Horizontal Pod Autoscaler List ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler List_To_autoscaling_Horizontal Pod Autoscaler List ( a . ( * v1 . Horizontal Pod Autoscaler List ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler List ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler List_To_v1_Horizontal Pod Autoscaler List ( a . ( * autoscaling . Horizontal Pod Autoscaler List ) , b . ( * v1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v1 . Horizontal Pod Autoscaler Spec ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler Spec_To_autoscaling_Horizontal Pod Autoscaler Spec ( a . ( * v1 . Horizontal Pod Autoscaler Spec ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Spec ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Spec_To_v1_Horizontal Pod Autoscaler Spec ( a . ( * autoscaling . Horizontal Pod Autoscaler Spec ) , b . ( * v1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v1 . Horizontal Pod Autoscaler Status ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler Status_To_autoscaling_Horizontal Pod Autoscaler Status ( a . ( * v1 . Horizontal Pod Autoscaler Status ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Status ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Status_To_v1_Horizontal Pod Autoscaler Status ( a . ( * autoscaling . Horizontal Pod Autoscaler Status ) , b . ( * v1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v1 . Metric Spec ) ( nil ) , ( * autoscaling . Metric Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Metric Spec_To_autoscaling_Metric Spec ( a . ( * v1 . Metric Spec ) , b . ( * autoscaling . Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Metric Spec ) ( nil ) , ( * v1 . Metric Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Metric Spec_To_v1_Metric Spec ( a . ( * autoscaling . Metric Spec ) , b . ( * v1 . Metric if err := s . Add Generated Conversion Func ( ( * v1 . Metric Status ) ( nil ) , ( * autoscaling . Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Metric Status_To_autoscaling_Metric Status ( a . ( * v1 . Metric Status ) , b . ( * autoscaling . Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Metric Status ) ( nil ) , ( * v1 . Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Metric Status_To_v1_Metric Status ( a . ( * autoscaling . Metric Status ) , b . ( * v1 . Metric if err := s . Add Generated Conversion Func ( ( * v1 . Object Metric Source ) ( nil ) , ( * autoscaling . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Metric Source_To_autoscaling_Object Metric Source ( a . ( * v1 . Object Metric Source ) , b . ( * autoscaling . Object Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Object Metric Source ) ( nil ) , ( * v1 . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Source_To_v1_Object Metric Source ( a . ( * autoscaling . Object Metric Source ) , b . ( * v1 . Object Metric if err := s . Add Generated Conversion Func ( ( * v1 . Object Metric Status ) ( nil ) , ( * autoscaling . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Metric Status_To_autoscaling_Object Metric Status ( a . ( * v1 . Object Metric Status ) , b . ( * autoscaling . Object Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Object Metric Status ) ( nil ) , ( * v1 . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Status_To_v1_Object Metric Status ( a . ( * autoscaling . Object Metric Status ) , b . ( * v1 . Object Metric if err := s . Add Generated Conversion Func ( ( * v1 . Pods Metric Source ) ( nil ) , ( * autoscaling . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pods Metric Source_To_autoscaling_Pods Metric Source ( a . ( * v1 . Pods Metric Source ) , b . ( * autoscaling . Pods Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Pods Metric Source ) ( nil ) , ( * v1 . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Source_To_v1_Pods Metric Source ( a . ( * autoscaling . Pods Metric Source ) , b . ( * v1 . Pods Metric if err := s . Add Generated Conversion Func ( ( * v1 . Pods Metric Status ) ( nil ) , ( * autoscaling . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pods Metric Status_To_autoscaling_Pods Metric Status ( a . ( * v1 . Pods Metric Status ) , b . ( * autoscaling . Pods Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Pods Metric Status ) ( nil ) , ( * v1 . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Status_To_v1_Pods Metric Status ( a . ( * autoscaling . Pods Metric Status ) , b . ( * v1 . Pods Metric if err := s . Add Generated Conversion Func ( ( * v1 . Resource Metric Source ) ( nil ) , ( * autoscaling . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Metric Source_To_autoscaling_Resource Metric Source ( a . ( * v1 . Resource Metric Source ) , b . ( * autoscaling . Resource Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Resource Metric Source ) ( nil ) , ( * v1 . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Source_To_v1_Resource Metric Source ( a . ( * autoscaling . Resource Metric Source ) , b . ( * v1 . Resource Metric if err := s . Add Generated Conversion Func ( ( * v1 . Resource Metric Status ) ( nil ) , ( * autoscaling . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Metric Status_To_autoscaling_Resource Metric Status ( a . ( * v1 . Resource Metric Status ) , b . ( * autoscaling . Resource Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Resource Metric Status ) ( nil ) , ( * v1 . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Status_To_v1_Resource Metric Status ( a . ( * autoscaling . Resource Metric Status ) , b . ( * v1 . Resource Metric if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Scale Spec ) ( nil ) , ( * autoscaling . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Scale Spec_To_autoscaling_Scale Spec ( a . ( * v1 . Scale Spec ) , b . ( * autoscaling . Scale if err := s . Add Generated Conversion Func ( ( * autoscaling . Scale Spec ) ( nil ) , ( * v1 . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Scale Spec_To_v1_Scale Spec ( a . ( * autoscaling . Scale Spec ) , b . ( * v1 . Scale if err := s . Add Generated Conversion Func ( ( * v1 . Scale Status ) ( nil ) , ( * autoscaling . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Scale Status_To_autoscaling_Scale Status ( a . ( * v1 . Scale Status ) , b . ( * autoscaling . Scale if err := s . Add Generated Conversion Func ( ( * autoscaling . Scale Status ) ( nil ) , ( * v1 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Scale Status_To_v1_Scale Status ( a . ( * autoscaling . Scale Status ) , b . ( * v1 . Scale if err := s . Add Conversion Func ( ( * autoscaling . External Metric Source ) ( nil ) , ( * v1 . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Source_To_v1_External Metric Source ( a . ( * autoscaling . External Metric Source ) , b . ( * v1 . External Metric if err := s . Add Conversion Func ( ( * autoscaling . External Metric Status ) ( nil ) , ( * v1 . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Status_To_v1_External Metric Status ( a . ( * autoscaling . External Metric Status ) , b . ( * v1 . External Metric if err := s . Add Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Spec ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Spec_To_v1_Horizontal Pod Autoscaler Spec ( a . ( * autoscaling . Horizontal Pod Autoscaler Spec ) , b . ( * v1 . Horizontal Pod Autoscaler if err := s . Add Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Status ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Status_To_v1_Horizontal Pod Autoscaler Status ( a . ( * autoscaling . Horizontal Pod Autoscaler Status ) , b . ( * v1 . Horizontal Pod Autoscaler if err := s . Add Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler ) ( nil ) , ( * v1 . Horizontal Pod Autoscaler ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler_To_v1_Horizontal Pod Autoscaler ( a . ( * autoscaling . Horizontal Pod Autoscaler ) , b . ( * v1 . Horizontal Pod if err := s . Add Conversion Func ( ( * autoscaling . Metric Target ) ( nil ) , ( * v1 . Cross Version Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Metric Target_To_v1_Cross Version Object Reference ( a . ( * autoscaling . Metric Target ) , b . ( * v1 . Cross Version Object if err := s . Add Conversion Func ( ( * autoscaling . Object Metric Source ) ( nil ) , ( * v1 . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Source_To_v1_Object Metric Source ( a . ( * autoscaling . Object Metric Source ) , b . ( * v1 . Object Metric if err := s . Add Conversion Func ( ( * autoscaling . Object Metric Status ) ( nil ) , ( * v1 . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Status_To_v1_Object Metric Status ( a . ( * autoscaling . Object Metric Status ) , b . ( * v1 . Object Metric if err := s . Add Conversion Func ( ( * autoscaling . Pods Metric Source ) ( nil ) , ( * v1 . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Source_To_v1_Pods Metric Source ( a . ( * autoscaling . Pods Metric Source ) , b . ( * v1 . Pods Metric if err := s . Add Conversion Func ( ( * autoscaling . Pods Metric Status ) ( nil ) , ( * v1 . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Status_To_v1_Pods Metric Status ( a . ( * autoscaling . Pods Metric Status ) , b . ( * v1 . Pods Metric if err := s . Add Conversion Func ( ( * autoscaling . Resource Metric Source ) ( nil ) , ( * v1 . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Source_To_v1_Resource Metric Source ( a . ( * autoscaling . Resource Metric Source ) , b . ( * v1 . Resource Metric if err := s . Add Conversion Func ( ( * autoscaling . Resource Metric Status ) ( nil ) , ( * v1 . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Status_To_v1_Resource Metric Status ( a . ( * autoscaling . Resource Metric Status ) , b . ( * v1 . Resource Metric if err := s . Add Conversion Func ( ( * v1 . Cross Version Object Reference ) ( nil ) , ( * autoscaling . Metric Target ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Cross Version Object Reference_To_autoscaling_Metric Target ( a . ( * v1 . Cross Version Object Reference ) , b . ( * autoscaling . Metric if err := s . Add Conversion Func ( ( * v1 . External Metric Source ) ( nil ) , ( * autoscaling . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_External Metric Source_To_autoscaling_External Metric Source ( a . ( * v1 . External Metric Source ) , b . ( * autoscaling . External Metric if err := s . Add Conversion Func ( ( * v1 . External Metric Status ) ( nil ) , ( * autoscaling . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_External Metric Status_To_autoscaling_External Metric Status ( a . ( * v1 . External Metric Status ) , b . ( * autoscaling . External Metric if err := s . Add Conversion Func ( ( * v1 . Horizontal Pod Autoscaler Spec ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler Spec_To_autoscaling_Horizontal Pod Autoscaler Spec ( a . ( * v1 . Horizontal Pod Autoscaler Spec ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Conversion Func ( ( * v1 . Horizontal Pod Autoscaler Status ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler Status_To_autoscaling_Horizontal Pod Autoscaler Status ( a . ( * v1 . Horizontal Pod Autoscaler Status ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Conversion Func ( ( * v1 . Horizontal Pod Autoscaler ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Horizontal Pod Autoscaler_To_autoscaling_Horizontal Pod Autoscaler ( a . ( * v1 . Horizontal Pod Autoscaler ) , b . ( * autoscaling . Horizontal Pod if err := s . Add Conversion Func ( ( * v1 . Object Metric Source ) ( nil ) , ( * autoscaling . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Metric Source_To_autoscaling_Object Metric Source ( a . ( * v1 . Object Metric Source ) , b . ( * autoscaling . Object Metric if err := s . Add Conversion Func ( ( * v1 . Object Metric Status ) ( nil ) , ( * autoscaling . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Object Metric Status_To_autoscaling_Object Metric Status ( a . ( * v1 . Object Metric Status ) , b . ( * autoscaling . Object Metric if err := s . Add Conversion Func ( ( * v1 . Pods Metric Source ) ( nil ) , ( * autoscaling . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pods Metric Source_To_autoscaling_Pods Metric Source ( a . ( * v1 . Pods Metric Source ) , b . ( * autoscaling . Pods Metric if err := s . Add Conversion Func ( ( * v1 . Pods Metric Status ) ( nil ) , ( * autoscaling . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Pods Metric Status_To_autoscaling_Pods Metric Status ( a . ( * v1 . Pods Metric Status ) , b . ( * autoscaling . Pods Metric if err := s . Add Conversion Func ( ( * v1 . Resource Metric Source ) ( nil ) , ( * autoscaling . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Metric Source_To_autoscaling_Resource Metric Source ( a . ( * v1 . Resource Metric Source ) , b . ( * autoscaling . Resource Metric if err := s . Add Conversion Func ( ( * v1 . Resource Metric Status ) ( nil ) , ( * autoscaling . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Metric Status_To_autoscaling_Resource Metric Status ( a . ( * v1 . Resource Metric Status ) , b . ( * autoscaling . Resource Metric } 
func Convert_v1_Cross Version Object Reference_To_autoscaling_Cross Version Object Reference ( in * v1 . Cross Version Object Reference , out * autoscaling . Cross Version Object Reference , s conversion . Scope ) error { return auto Convert_v1_Cross Version Object Reference_To_autoscaling_Cross Version Object } 
func Convert_autoscaling_Cross Version Object Reference_To_v1_Cross Version Object Reference ( in * autoscaling . Cross Version Object Reference , out * v1 . Cross Version Object Reference , s conversion . Scope ) error { return auto Convert_autoscaling_Cross Version Object Reference_To_v1_Cross Version Object } 
func Convert_v1_Horizontal Pod Autoscaler Condition_To_autoscaling_Horizontal Pod Autoscaler Condition ( in * v1 . Horizontal Pod Autoscaler Condition , out * autoscaling . Horizontal Pod Autoscaler Condition , s conversion . Scope ) error { return auto Convert_v1_Horizontal Pod Autoscaler Condition_To_autoscaling_Horizontal Pod Autoscaler } 
func Convert_autoscaling_Horizontal Pod Autoscaler Condition_To_v1_Horizontal Pod Autoscaler Condition ( in * autoscaling . Horizontal Pod Autoscaler Condition , out * v1 . Horizontal Pod Autoscaler Condition , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler Condition_To_v1_Horizontal Pod Autoscaler } 
func Convert_v1_Horizontal Pod Autoscaler List_To_autoscaling_Horizontal Pod Autoscaler List ( in * v1 . Horizontal Pod Autoscaler List , out * autoscaling . Horizontal Pod Autoscaler List , s conversion . Scope ) error { return auto Convert_v1_Horizontal Pod Autoscaler List_To_autoscaling_Horizontal Pod Autoscaler } 
func Convert_autoscaling_Horizontal Pod Autoscaler List_To_v1_Horizontal Pod Autoscaler List ( in * autoscaling . Horizontal Pod Autoscaler List , out * v1 . Horizontal Pod Autoscaler List , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler List_To_v1_Horizontal Pod Autoscaler } 
func Convert_v1_Metric Spec_To_autoscaling_Metric Spec ( in * v1 . Metric Spec , out * autoscaling . Metric Spec , s conversion . Scope ) error { return auto Convert_v1_Metric Spec_To_autoscaling_Metric } 
func Convert_autoscaling_Metric Spec_To_v1_Metric Spec ( in * autoscaling . Metric Spec , out * v1 . Metric Spec , s conversion . Scope ) error { return auto Convert_autoscaling_Metric Spec_To_v1_Metric } 
func Convert_v1_Metric Status_To_autoscaling_Metric Status ( in * v1 . Metric Status , out * autoscaling . Metric Status , s conversion . Scope ) error { return auto Convert_v1_Metric Status_To_autoscaling_Metric } 
func Convert_autoscaling_Metric Status_To_v1_Metric Status ( in * autoscaling . Metric Status , out * v1 . Metric Status , s conversion . Scope ) error { return auto Convert_autoscaling_Metric Status_To_v1_Metric } 
func Convert_v1_Scale_To_autoscaling_Scale ( in * v1 . Scale , out * autoscaling . Scale , s conversion . Scope ) error { return auto } 
func Convert_autoscaling_Scale_To_v1_Scale ( in * autoscaling . Scale , out * v1 . Scale , s conversion . Scope ) error { return auto } 
func Convert_v1_Scale Spec_To_autoscaling_Scale Spec ( in * v1 . Scale Spec , out * autoscaling . Scale Spec , s conversion . Scope ) error { return auto Convert_v1_Scale Spec_To_autoscaling_Scale } 
func Convert_autoscaling_Scale Spec_To_v1_Scale Spec ( in * autoscaling . Scale Spec , out * v1 . Scale Spec , s conversion . Scope ) error { return auto Convert_autoscaling_Scale Spec_To_v1_Scale } 
func Convert_v1_Scale Status_To_autoscaling_Scale Status ( in * v1 . Scale Status , out * autoscaling . Scale Status , s conversion . Scope ) error { return auto Convert_v1_Scale Status_To_autoscaling_Scale } 
func Convert_autoscaling_Scale Status_To_v1_Scale Status ( in * autoscaling . Scale Status , out * v1 . Scale Status , s conversion . Scope ) error { return auto Convert_autoscaling_Scale Status_To_v1_Scale } 
func api Versions To API Group ( api Versions * metav1 . API Versions ) ( api Group metav1 . API Group ) { group Versions := [ ] metav1 . Group Version For for _ , version := range api Versions . Versions { group Version := metav1 . Group Version For Discovery { Group group Versions = append ( group Versions , group api Group . Versions = group // There should be only one group Version returned at /api api Group . Preferred Version = group } 
func ( d * Discovery Client ) Server Groups ( ) ( api Group List * metav1 . API Group List , err error ) { // Get the group Versions exposed at /api v := & metav1 . API err = d . rest Client . Get ( ) . Abs Path ( d . Legacy api Group := metav1 . API if err == nil && len ( v . Versions ) != 0 { api Group = api Versions To API if err != nil && ! errors . Is Not Found ( err ) && ! errors . Is // Get the group Versions exposed at /apis api Group List = & metav1 . API Group err = d . rest Client . Get ( ) . Abs Path ( " " ) . Do ( ) . Into ( api Group if err != nil && ! errors . Is Not Found ( err ) && ! errors . Is // to be compatible with a v1.0 server, if it's a 403 or 404, ignore and return whatever we got from /api if err != nil && ( errors . Is Not Found ( err ) || errors . Is Forbidden ( err ) ) { api Group List = & metav1 . API Group // prepend the group retrieved from /api to the list if not empty if len ( v . Versions ) != 0 { api Group List . Groups = append ( [ ] metav1 . API Group { api Group } , api Group return api Group } 
func ( d * Discovery Client ) Server Resources For Group Version ( group Version string ) ( resources * metav1 . API Resource if len ( group if len ( d . Legacy Prefix ) > 0 && group Version == " " { url . Path = d . Legacy Prefix + " " + group } else { url . Path = " " + group resources = & metav1 . API Resource List { Group Version : group err = d . rest Client . Get ( ) . Abs if err != nil { // ignore 403 or 404 error to be compatible with an v1.0 server. if group Version == " " && ( errors . Is Not Found ( err ) || errors . Is } 
func ( d * Discovery Client ) Server Groups And Resources ( ) ( [ ] * metav1 . API Group , [ ] * metav1 . API Resource List , error ) { return with Retries ( default Retries , func ( ) ( [ ] * metav1 . API Group , [ ] * metav1 . API Resource List , error ) { return Server Groups And } 
func ( e * Err Group Discovery } 
func Is Group Discovery Failed Error ( err error ) bool { _ , ok := err . ( * Err Group Discovery } 
func Server Resources ( d Discovery Interface ) ( [ ] * metav1 . API Resource List , error ) { _ , rs , err := Server Groups And } 
func Server Preferred Resources ( d Discovery Interface ) ( [ ] * metav1 . API Resource List , error ) { server Group List , err := d . Server group Version Resources , failed Groups := fetch Group Version Resources ( d , server Group result := [ ] * metav1 . API Resource gr Versions := map [ schema . Group Resource ] string { } // selected version of a Group gr API Resources := map [ schema . Group Resource ] * metav1 . API Resource { } // selected API Resource for a Group gv API Resource Lists := map [ schema . Group Version ] * metav1 . API Resource List { } // blueprint for a API Resource for _ , api Group := range server Group List . Groups { for _ , version := range api Group . Versions { group Version := schema . Group Version { Group : api api Resource List , ok := group Version Resources [ group // create empty list which is filled later in another loop empty API Resource List := metav1 . API Resource List { Group Version : version . Group gv API Resource Lists [ group Version ] = & empty API Resource result = append ( result , & empty API Resource for i := range api Resource List . API Resources { api Resource := & api Resource List . API if strings . Contains ( api gv := schema . Group Resource { Group : api Group . Name , Resource : api if _ , ok := gr API Resources [ gv ] ; ok && version . Version != api Group . Preferred gr gr API Resources [ gv ] = api // group selected API Resources according to Group Version into API Resource Lists for group Resource , api Resource := range gr API Resources { version := gr Versions [ group group Version := schema . Group Version { Group : group api Resource List := gv API Resource Lists [ group api Resource List . API Resources = append ( api Resource List . API Resources , * api if len ( failed return result , & Err Group Discovery Failed { Groups : failed } 
func fetch Group Version Resources ( d Discovery Interface , api Groups * metav1 . API Group List ) ( map [ schema . Group Version ] * metav1 . API Resource List , map [ schema . Group Version ] error ) { group Version Resources := make ( map [ schema . Group Version ] * metav1 . API Resource failed Groups := make ( map [ schema . Group wg := & sync . Wait result for _ , api Group := range api Groups . Groups { for _ , version := range api Group . Versions { group Version := schema . Group Version { Group : api defer utilruntime . Handle api Resource List , err := d . Server Resources For Group Version ( group // lock to record results result defer result if err != nil { // TODO: maybe restrict this to Not Found errors failed Groups [ group if api Resource List != nil { // even in case of error, some fallback might have been returned group Version Resources [ group Version ] = api Resource return group Version Resources , failed } 
func ( d * Discovery Client ) Server Preferred Resources ( ) ( [ ] * metav1 . API Resource List , error ) { _ , rs , err := with Retries ( default Retries , func ( ) ( [ ] * metav1 . API Group , [ ] * metav1 . API Resource List , error ) { rs , err := Server Preferred } 
func Server Preferred Namespaced Resources ( d Discovery Interface ) ( [ ] * metav1 . API Resource List , error ) { all , err := Server Preferred return Filtered By ( Resource Predicate Func ( func ( group Version string , r * metav1 . API } 
func ( d * Discovery Client ) Server Version ( ) ( * version . Info , error ) { body , err := d . rest Client . Get ( ) . Abs } 
func ( d * Discovery Client ) Open API Schema ( ) ( * openapi_v2 . Document , error ) { data , err := d . rest Client . Get ( ) . Abs Path ( " " ) . Set Header ( " " , mime if err != nil { if errors . Is Forbidden ( err ) || errors . Is Not Found ( err ) || errors . Is Not Acceptable ( err ) { // single endpoint not found/registered in old server, try to fetch old endpoint // TODO: remove this when kubectl/client-go don't work with 1.9 server data , err = d . rest Client . Get ( ) . Abs } 
func with Retries ( max Retries int , f func ( ) ( [ ] * metav1 . API Group , [ ] * metav1 . API Resource List , error ) ) ( [ ] * metav1 . API Group , [ ] * metav1 . API Resource List , error ) { var result [ ] * metav1 . API Resource var result Groups [ ] * metav1 . API for i := 0 ; i < max Retries ; i ++ { result if err == nil { return result if _ , ok := err . ( * Err Group Discovery return result } 
func New Discovery Client For Config ( c * restclient . Config ) ( * Discovery if err := set Discovery client , err := restclient . Unversioned REST Client return & Discovery Client { rest Client : client , Legacy } 
func ( d * Discovery Client ) REST return d . rest } 
func ( o * Pod GC Controller Options ) Add Flags ( fs * pflag . Flag fs . Int32Var ( & o . Terminated Pod GC Threshold , " " , o . Terminated Pod GC } 
func ( o * Pod GC Controller Options ) Apply To ( cfg * podgcconfig . Pod GC Controller cfg . Terminated Pod GC Threshold = o . Terminated Pod GC } 
func ( o * Pod GC Controller } 
func watch Error Stream ( error Stream io . Reader , d error Stream Decoder ) chan error { error go func ( ) { defer runtime . Handle message , err := ioutil . Read All ( error switch { case err != nil && err != io . EOF : error case len ( message ) > 0 : error default : error close ( error return error } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Aggregation Rule ) ( nil ) , ( * rbac . Aggregation Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Aggregation Rule_To_rbac_Aggregation Rule ( a . ( * v1beta1 . Aggregation Rule ) , b . ( * rbac . Aggregation if err := s . Add Generated Conversion Func ( ( * rbac . Aggregation Rule ) ( nil ) , ( * v1beta1 . Aggregation Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Aggregation Rule_To_v1beta1_Aggregation Rule ( a . ( * rbac . Aggregation Rule ) , b . ( * v1beta1 . Aggregation if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cluster Role ) ( nil ) , ( * rbac . Cluster Role ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cluster Role_To_rbac_Cluster Role ( a . ( * v1beta1 . Cluster Role ) , b . ( * rbac . Cluster if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role ) ( nil ) , ( * v1beta1 . Cluster Role ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role_To_v1beta1_Cluster Role ( a . ( * rbac . Cluster Role ) , b . ( * v1beta1 . Cluster if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cluster Role Binding ) ( nil ) , ( * rbac . Cluster Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cluster Role Binding_To_rbac_Cluster Role Binding ( a . ( * v1beta1 . Cluster Role Binding ) , b . ( * rbac . Cluster Role if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role Binding ) ( nil ) , ( * v1beta1 . Cluster Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role Binding_To_v1beta1_Cluster Role Binding ( a . ( * rbac . Cluster Role Binding ) , b . ( * v1beta1 . Cluster Role if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cluster Role Binding List ) ( nil ) , ( * rbac . Cluster Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cluster Role Binding List_To_rbac_Cluster Role Binding List ( a . ( * v1beta1 . Cluster Role Binding List ) , b . ( * rbac . Cluster Role Binding if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role Binding List ) ( nil ) , ( * v1beta1 . Cluster Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role Binding List_To_v1beta1_Cluster Role Binding List ( a . ( * rbac . Cluster Role Binding List ) , b . ( * v1beta1 . Cluster Role Binding if err := s . Add Generated Conversion Func ( ( * v1beta1 . Cluster Role List ) ( nil ) , ( * rbac . Cluster Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Cluster Role List_To_rbac_Cluster Role List ( a . ( * v1beta1 . Cluster Role List ) , b . ( * rbac . Cluster Role if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role List ) ( nil ) , ( * v1beta1 . Cluster Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role List_To_v1beta1_Cluster Role List ( a . ( * rbac . Cluster Role List ) , b . ( * v1beta1 . Cluster Role if err := s . Add Generated Conversion Func ( ( * v1beta1 . Policy Rule ) ( nil ) , ( * rbac . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Policy Rule_To_rbac_Policy Rule ( a . ( * v1beta1 . Policy Rule ) , b . ( * rbac . Policy if err := s . Add Generated Conversion Func ( ( * rbac . Policy Rule ) ( nil ) , ( * v1beta1 . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Policy Rule_To_v1beta1_Policy Rule ( a . ( * rbac . Policy Rule ) , b . ( * v1beta1 . Policy if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Role Binding ) ( nil ) , ( * rbac . Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Role Binding_To_rbac_Role Binding ( a . ( * v1beta1 . Role Binding ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role Binding ) ( nil ) , ( * v1beta1 . Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Binding_To_v1beta1_Role Binding ( a . ( * rbac . Role Binding ) , b . ( * v1beta1 . Role if err := s . Add Generated Conversion Func ( ( * v1beta1 . Role Binding List ) ( nil ) , ( * rbac . Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Role Binding List_To_rbac_Role Binding List ( a . ( * v1beta1 . Role Binding List ) , b . ( * rbac . Role Binding if err := s . Add Generated Conversion Func ( ( * rbac . Role Binding List ) ( nil ) , ( * v1beta1 . Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Binding List_To_v1beta1_Role Binding List ( a . ( * rbac . Role Binding List ) , b . ( * v1beta1 . Role Binding if err := s . Add Generated Conversion Func ( ( * v1beta1 . Role List ) ( nil ) , ( * rbac . Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Role List_To_rbac_Role List ( a . ( * v1beta1 . Role List ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role List ) ( nil ) , ( * v1beta1 . Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role List_To_v1beta1_Role List ( a . ( * rbac . Role List ) , b . ( * v1beta1 . Role if err := s . Add Generated Conversion Func ( ( * v1beta1 . Role Ref ) ( nil ) , ( * rbac . Role Ref ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Role Ref_To_rbac_Role Ref ( a . ( * v1beta1 . Role Ref ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role Ref ) ( nil ) , ( * v1beta1 . Role Ref ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Ref_To_v1beta1_Role Ref ( a . ( * rbac . Role Ref ) , b . ( * v1beta1 . Role if err := s . Add Generated Conversion if err := s . Add Generated Conversion } 
func Convert_v1beta1_Aggregation Rule_To_rbac_Aggregation Rule ( in * v1beta1 . Aggregation Rule , out * rbac . Aggregation Rule , s conversion . Scope ) error { return auto Convert_v1beta1_Aggregation Rule_To_rbac_Aggregation } 
func Convert_rbac_Aggregation Rule_To_v1beta1_Aggregation Rule ( in * rbac . Aggregation Rule , out * v1beta1 . Aggregation Rule , s conversion . Scope ) error { return auto Convert_rbac_Aggregation Rule_To_v1beta1_Aggregation } 
func Convert_v1beta1_Cluster Role_To_rbac_Cluster Role ( in * v1beta1 . Cluster Role , out * rbac . Cluster Role , s conversion . Scope ) error { return auto Convert_v1beta1_Cluster Role_To_rbac_Cluster } 
func Convert_rbac_Cluster Role_To_v1beta1_Cluster Role ( in * rbac . Cluster Role , out * v1beta1 . Cluster Role , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role_To_v1beta1_Cluster } 
func Convert_v1beta1_Cluster Role Binding_To_rbac_Cluster Role Binding ( in * v1beta1 . Cluster Role Binding , out * rbac . Cluster Role Binding , s conversion . Scope ) error { return auto Convert_v1beta1_Cluster Role Binding_To_rbac_Cluster Role } 
func Convert_rbac_Cluster Role Binding_To_v1beta1_Cluster Role Binding ( in * rbac . Cluster Role Binding , out * v1beta1 . Cluster Role Binding , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role Binding_To_v1beta1_Cluster Role } 
func Convert_v1beta1_Cluster Role Binding List_To_rbac_Cluster Role Binding List ( in * v1beta1 . Cluster Role Binding List , out * rbac . Cluster Role Binding List , s conversion . Scope ) error { return auto Convert_v1beta1_Cluster Role Binding List_To_rbac_Cluster Role Binding } 
func Convert_rbac_Cluster Role Binding List_To_v1beta1_Cluster Role Binding List ( in * rbac . Cluster Role Binding List , out * v1beta1 . Cluster Role Binding List , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role Binding List_To_v1beta1_Cluster Role Binding } 
func Convert_v1beta1_Cluster Role List_To_rbac_Cluster Role List ( in * v1beta1 . Cluster Role List , out * rbac . Cluster Role List , s conversion . Scope ) error { return auto Convert_v1beta1_Cluster Role List_To_rbac_Cluster Role } 
func Convert_rbac_Cluster Role List_To_v1beta1_Cluster Role List ( in * rbac . Cluster Role List , out * v1beta1 . Cluster Role List , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role List_To_v1beta1_Cluster Role } 
func Convert_v1beta1_Policy Rule_To_rbac_Policy Rule ( in * v1beta1 . Policy Rule , out * rbac . Policy Rule , s conversion . Scope ) error { return auto Convert_v1beta1_Policy Rule_To_rbac_Policy } 
func Convert_rbac_Policy Rule_To_v1beta1_Policy Rule ( in * rbac . Policy Rule , out * v1beta1 . Policy Rule , s conversion . Scope ) error { return auto Convert_rbac_Policy Rule_To_v1beta1_Policy } 
func Convert_v1beta1_Role_To_rbac_Role ( in * v1beta1 . Role , out * rbac . Role , s conversion . Scope ) error { return auto } 
func Convert_rbac_Role_To_v1beta1_Role ( in * rbac . Role , out * v1beta1 . Role , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Role Binding_To_rbac_Role Binding ( in * v1beta1 . Role Binding , out * rbac . Role Binding , s conversion . Scope ) error { return auto Convert_v1beta1_Role Binding_To_rbac_Role } 
func Convert_rbac_Role Binding_To_v1beta1_Role Binding ( in * rbac . Role Binding , out * v1beta1 . Role Binding , s conversion . Scope ) error { return auto Convert_rbac_Role Binding_To_v1beta1_Role } 
func Convert_v1beta1_Role Binding List_To_rbac_Role Binding List ( in * v1beta1 . Role Binding List , out * rbac . Role Binding List , s conversion . Scope ) error { return auto Convert_v1beta1_Role Binding List_To_rbac_Role Binding } 
func Convert_rbac_Role Binding List_To_v1beta1_Role Binding List ( in * rbac . Role Binding List , out * v1beta1 . Role Binding List , s conversion . Scope ) error { return auto Convert_rbac_Role Binding List_To_v1beta1_Role Binding } 
func Convert_v1beta1_Role List_To_rbac_Role List ( in * v1beta1 . Role List , out * rbac . Role List , s conversion . Scope ) error { return auto Convert_v1beta1_Role List_To_rbac_Role } 
func Convert_rbac_Role List_To_v1beta1_Role List ( in * rbac . Role List , out * v1beta1 . Role List , s conversion . Scope ) error { return auto Convert_rbac_Role List_To_v1beta1_Role } 
func Convert_v1beta1_Role Ref_To_rbac_Role Ref ( in * v1beta1 . Role Ref , out * rbac . Role Ref , s conversion . Scope ) error { return auto Convert_v1beta1_Role Ref_To_rbac_Role } 
func Convert_rbac_Role Ref_To_v1beta1_Role Ref ( in * rbac . Role Ref , out * v1beta1 . Role Ref , s conversion . Scope ) error { return auto Convert_rbac_Role Ref_To_v1beta1_Role } 
func Convert_v1beta1_Subject_To_rbac_Subject ( in * v1beta1 . Subject , out * rbac . Subject , s conversion . Scope ) error { return auto } 
func Convert_rbac_Subject_To_v1beta1_Subject ( in * rbac . Subject , out * v1beta1 . Subject , s conversion . Scope ) error { return auto } 
func ( ow * real OOM Watcher ) Start ( ref * v1 . Object Reference ) error { oom out Stream := make ( chan * oomparser . Oom go oom Log . Stream Ooms ( out go func ( ) { defer runtime . Handle for event := range out Stream { if event . Container ow . recorder . Past Eventf ( ref , metav1 . Time { Time : event . Time Of Death } , v1 . Event Type Warning , system OOM } 
func New Table Printer ( options Print Options ) * Human Readable Printer { printer := & Human Readable Printer { handler Map : make ( map [ reflect . Type ] * handler } 
func ( h * Human Readable Printer ) Print if ! found { w = Get New Tab // Case 1: Parameter "obj" is a table from server; print it. // display tables following the rules of options if table , ok := obj . ( * metav1beta1 . Table ) ; ok { // Do not print headers if this table has no column definitions, or they are the same as the last ones we printed local if len ( table . Column Definitions ) == 0 || reflect . Deep Equal ( table . Column Definitions , h . last Columns ) { local Options . No if len ( table . Column Definitions ) == 0 { // If this table has no column definitions, use the columns from the last table we printed for decoration and layout. // This is done when receiving tables in watch events to save bandwidth. local Options . No table . Column Definitions = h . last } else { // If this table has column definitions, remember them for future use. h . last Columns = table . Column if err := decorate Table ( table , local return Print Table ( table , output , local // Case 2: Parameter "obj" is not a table; search for a handler to print it. // TODO(seans3): Remove this case in 1.16, since table should be returned from server-side printing. // print with a registered handler t := reflect . Type if handler := h . handler Map [ t ] ; handler != nil { include Headers := h . last Type != t && ! h . options . No if h . last Type != nil && h . last Type != t && ! h . options . No if err := print Rows For Handler Entry ( output , handler , obj , h . options , include h . last // Case 3: Could not find print handler for "obj"; use the default print handler. // print with the default handler if set, and use the columns from the last time if h . default Handler != nil { include Headers := h . last Type != h . default Handler && ! h . options . No if h . last Type != nil && h . last Type != h . default Handler && ! h . options . No if err := print Rows For Handler Entry ( output , h . default Handler , obj , h . options , include h . last Type = h . default } 
func Print Table ( table * metav1beta1 . Table , output io . Writer , options Print Options ) error { if ! options . No for _ , column := range table . Column fmt . Fprint ( output , strings . To for i , cell := range row . Cells { if i >= len ( table . Column column := table . Column } 
func decorate Table ( table * metav1beta1 . Table , options Print Options ) error { width := len ( table . Column Definitions ) + len ( options . Column if options . With if options . Show columns := table . Column name if options . With Kind && ! options . Kind . Empty ( ) { for i := range columns { if columns [ i ] . Format == " " && columns [ i ] . Type == " " { name if width != len ( table . Column Definitions ) { columns = make ( [ ] metav1beta1 . Table Column if options . With Namespace { columns = append ( columns , metav1beta1 . Table Column columns = append ( columns , table . Column for _ , label := range format Label Headers ( options . Column Labels ) { columns = append ( columns , metav1beta1 . Table Column if options . Show Labels { columns = append ( columns , metav1beta1 . Table Column include Labels := len ( options . Column Labels ) > 0 || options . Show if include Labels || options . With Namespace || name if name Column != - 1 { row . Cells [ name Column ] = fmt . Sprintf ( " " , strings . To Lower ( options . Kind . String ( ) ) , row . Cells [ name // if we can't get an accessor, fill out the appropriate columns with empty spaces if m == nil { if options . With if options . With r [ 0 ] = m . Get if include Labels { row . Cells = append Label Cells ( row . Cells , m . Get table . Column } 
func print Rows For Handler Entry ( output io . Writer , handler * handler Entry , obj runtime . Object , options Print Options , include args := [ ] reflect . Value { reflect . Value Of ( obj ) , reflect . Value results = handler . print if ! results [ 1 ] . Is if include for _ , column := range handler . column headers = append ( headers , strings . To headers = append ( headers , format Label Headers ( options . Column // LABELS is always the last column. headers = append ( headers , format Show Labels Header ( options . Show if options . With Namespace { headers = append ( with Namespace Prefix print if results [ 1 ] . Is Nil ( ) { rows := results [ 0 ] . Interface ( ) . ( [ ] metav1beta1 . Table print } 
func print Rows ( output io . Writer , rows [ ] metav1beta1 . Table Row , options Print Options ) { for _ , row := range rows { if options . With Namespace { if obj := row . Object . Object ; obj != nil { if m , err := meta . Accessor ( obj ) ; err == nil { fmt . Fprint ( output , m . Get } else { // TODO: remove this once we drop the legacy printers if options . With Kind && ! options . Kind . Empty ( ) { fmt . Fprintf ( output , " " , strings . To has Labels := len ( options . Column if obj := row . Object . Object ; obj != nil && ( has Labels || options . Show Labels ) { if m , err := meta . Accessor ( obj ) ; err == nil { for _ , value := range label Values ( m . Get } 
func label Values ( item Labels map [ string ] string , opts Print for _ , key := range opts . Column Labels { values = append ( values , item if opts . Show Labels { values = append ( values , labels . Format Labels ( item } 
func ( ds * docker Service ) Reopen Container Log ( _ context . Context , _ * runtimeapi . Reopen Container Log Request ) ( * runtimeapi . Reopen Container Log } 
func New ( dev Entries [ ] Pod Devices Entry , devices map [ string ] [ ] string ) Device Manager Checkpoint { return & Data { Data : checkpoint Data { Pod Device Entries : dev Entries , Registered } 
func ( cp * Data ) Unmarshal } 
func ( cp * Data ) Get Data ( ) ( [ ] Pod Devices Entry , map [ string ] [ ] string ) { return cp . Data . Pod Device Entries , cp . Data . Registered } 
func ( m * sio Mgr ) get Client ( ) ( sio configs := m . config username := configs [ conf password := configs [ conf gateway := configs [ conf b , err := strconv . Parse Bool ( configs [ conf Key . ssl certs client , err := new Sio Client ( gateway , username , password , certs client . sys Name = configs [ conf client . pd Name = configs [ conf Key . protection client . sp Name = configs [ conf Key . storage client . sdc Path = configs [ conf Key . sdc Root client . provision Mode = configs [ conf Key . storage client . sdc GUID = configs [ conf Key . sdc } 
func ( m * sio Mgr ) Create Volume ( vol Name string , size GB int64 ) ( * siotypes . Volume , error ) { client , err := m . get klog . V ( 4 ) . Infof ( " " , vol vol , err := client . Create Volume ( vol Name , size if err != nil { klog . V ( 4 ) . Infof ( " " , vol klog . V ( 4 ) . Infof ( " " , vol } 
func ( m * sio Mgr ) Attach Volume ( vol Name string , multiple Mappings bool ) ( string , error ) { client , err := m . get klog . V ( 4 ) . Infoln ( log ( " " , vol klog . V ( 4 ) . Info ( log ( " " , vol vol , err := client . Find Volume ( vol if err != nil { klog . Error ( log ( " " , vol // handle vol if already attached if len ( vol . Mapped Sdc Info ) > 0 { if m . is Sdc Mapped To Vol ( iid , vol ) { klog . V ( 4 ) . Info ( log ( " " , vol // attach volume, get device Name if err := client . Attach Volume ( sio Volume ID ( vol . ID ) , multiple Mappings ) ; err != nil { klog . Error ( log ( " " , vol device , err := client . Wait For Attached klog . V ( 4 ) . Info ( log ( " " , vol } 
func ( m * sio Mgr ) Is Attached ( vol Name string ) ( bool , error ) { client , err := m . get vol , err := client . Find Volume ( vol return m . is Sdc Mapped To } 
func ( m * sio Mgr ) Detach Volume ( vol Name string ) error { client , err := m . get vol , err := client . Find Volume ( vol if ! m . is Sdc Mapped To Vol ( iid , vol ) { klog . Warning ( log ( " " , vol if err := client . Detach Volume ( sio Volume ID ( vol . ID ) ) ; err != nil { klog . Error ( log ( " " , vol klog . V ( 4 ) . Info ( log ( " " , vol } 
func ( m * sio Mgr ) Delete Volume ( vol Name string ) error { client , err := m . get vol , err := client . Find Volume ( vol if err := client . Delete Volume ( sio Volume ID ( vol . ID ) ) ; err != nil { klog . Error ( log ( " " , vol klog . V ( 4 ) . Info ( log ( " " , vol } 
func ( m * sio Mgr ) is Sdc Mapped To Vol ( sdc ID string , vol * siotypes . Volume ) bool { if len ( vol . Mapped Sdc for _ , sdc Info := range vol . Mapped Sdc Info { if sdc Info . Sdc ID == sdc } 
func New Registry ( evaluators [ ] quota . Evaluator ) quota . Registry { return & simple Registry { evaluators : evaluators By Group } 
func evaluators By Group Resource ( items [ ] quota . Evaluator ) map [ schema . Group Resource ] quota . Evaluator { result := map [ schema . Group for _ , item := range items { result [ item . Group } 
func evaluators List ( input map [ schema . Group } 
func New REST ( opts Getter generic . REST Options Getter , issuer token . Token Generator , auds authenticator . Audiences , max time . Duration , pod Storage , secret Storage * genericregistry . Store ) * REST { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Service Account { } } , New List Func : func ( ) runtime . Object { return & api . Service Account List { } } , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : serviceaccount . Strategy , Update Strategy : serviceaccount . Strategy , Delete Strategy : serviceaccount . Strategy , Return Deleted Object : true , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts if err := store . Complete With var trest * Token if issuer != nil && pod Storage != nil && secret Storage != nil { trest = & Token REST { svcaccts : store , pods : pod Storage , secrets : secret Storage , issuer : issuer , auds : auds , max Expiration } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( batch . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( v2alpha1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1 . Scheme Group Version , v1beta1 . Scheme Group Version , v2alpha1 . Scheme Group } 
func Probe Volume Plugins ( ) [ ] volume . Volume Plugin { p := & sio return [ ] volume . Volume } 
func ( p * sio Plugin ) New Unmounter ( spec Name string , pod UID types . UID ) ( volume . Unmounter , error ) { klog . V ( 4 ) . Info ( log ( " " , spec return & sio Volume { pod UID : pod UID , vol Spec Name : spec } 
func New Strategy ( typer runtime . Object Typer ) rest . REST Create Update Strategy { return api Server Strategy { typer , names . Simple Name } 
func New Status Strategy ( typer runtime . Object Typer ) rest . REST Update Strategy { return api Server Status Strategy { typer , names . Simple Name } 
func ( api Server Status Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return validation . Validate API Service Status Update ( obj . ( * apiregistration . API Service ) , old . ( * apiregistration . API } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { apiserver , ok := obj . ( * apiregistration . API return labels . Set ( apiserver . Object Meta . Labels ) , To Selectable } 
func To Selectable Fields ( obj * apiregistration . API Service ) fields . Set { return generic . Object Meta Fields Set ( & obj . Object } 
func Connect Resource ( connecter rest . Connecter , scope * Request Scope , admit admission . Interface , rest Path string , is Subresource bool ) http . Handler Func { return func ( w http . Response Writer , req * http . Request ) { if is Dry Run ( req . URL ) { scope . err ( errors . New Bad ctx = request . With ae := request . Audit Event admit = admission . With opts , subpath , subpath Key := connecter . New Connect if err := get Request Options ( req , scope , opts , subpath , subpath Key , is Subresource ) ; err != nil { err = errors . New Bad if admit != nil && admit . Handles ( admission . Connect ) { user Info , _ := request . User // TODO: remove the mutating admission here as soon as we have ported all plugin that handle CONNECT if mutating Admission , ok := admit . ( admission . Mutation Interface ) ; ok { err = mutating Admission . Admit ( admission . New Attributes Record ( opts , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Connect , false , user if validating Admission , ok := admit . ( admission . Validation Interface ) ; ok { err = validating Admission . Validate ( admission . New Attributes Record ( opts , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Connect , false , user request Info , _ := request . Request Info metrics . Record Long Running ( req , request Info , metrics . API Server handler . Serve } 
func finish Request ( timeout time . Duration , fn result err panic go func ( ) { // panics don't cross goroutine boundaries, so we have to handle ourselves defer func ( ) { panic if panic panic Reason = fmt . Sprintf ( " \n " , panic // Propagate to parent goroutine panic Ch <- panic if result , err := fn ( ) ; err != nil { err select { case result = <- ch : if status , ok := result . ( * metav1 . Status ) ; ok { if status . Status != metav1 . Status Success { return nil , errors . From case err = <- err case p := <- panic case <- time . After ( timeout ) : return nil , errors . New Timeout } 
func transform Decode Error ( typer runtime . Object Typer , base Err error , into runtime . Object , gvk * schema . Group Version Kind , body [ ] byte ) error { obj GV Ks , _ , err := typer . Object if err != nil { return errors . New Bad obj GVK := obj GV if gvk != nil && len ( gvk . Kind ) > 0 { return errors . New Bad Request ( fmt . Sprintf ( " " , gvk . Kind , gvk . Version , obj GVK . Kind , base summary := summarize return errors . New Bad Request ( fmt . Sprintf ( " " , obj GVK . Kind , base } 
func set Self Link ( obj runtime . Object , request Info * request . Request Info , namer Scope Namer ) error { // TODO: Self Link generation should return a full URL? uri , err := namer . Generate Link ( request return namer . Set Self } 
func check Name ( obj runtime . Object , name , namespace string , namer Scope Namer ) error { obj Namespace , obj Name , err := namer . Object if err != nil { return errors . New Bad if obj Name != name { return errors . New Bad Request ( fmt . Sprintf ( " " , obj if len ( namespace ) > 0 { if len ( obj Namespace ) > 0 && obj Namespace != namespace { return errors . New Bad Request ( fmt . Sprintf ( " " , obj } 
func set Object Self Link ( ctx context . Context , obj runtime . Object , req * http . Request , namer Scope Namer ) error { // We only generate list links on objects that implement List Interface - historically we duck typed this // check via reflection, but as we move away from reflection we require that you not only carry Items but // List Meta into order to be identified as a list. if ! meta . Is List Type ( obj ) { request Info , ok := request . Request Info return set Self Link ( obj , request uri , err := namer . Generate List if err := namer . Set Self request Info , ok := request . Request Info err = meta . Each List return set Self Link ( obj , request if count == 0 { if err := meta . Set } 
func ( f Token Func ) Authenticate } 
func ( f Request Func ) Authenticate } 
func ( f Password Func ) Authenticate } 
func ( storage Class Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { class := obj . ( * storage . Storage storageutil . Drop Disabled } 
func ( storage Class Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Class := obj . ( * storage . Storage old Class := old . ( * storage . Storage storageutil . Drop Disabled Fields ( old Class , new } 
func ( c * Fake Roles ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Role , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( roles } 
func ( c * Fake Roles ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( roles } 
func ( s * daemon Set Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Daemon Set , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Daemon } 
func ( s daemon Set Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Daemon Set , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Daemon } 
func ( in * TLS Client Config ) Deep Copy Into ( out * TLS Client if in . Cert Data != nil { in , out := & in . Cert Data , & out . Cert if in . Key Data != nil { in , out := & in . Key Data , & out . Key if in . CA Data != nil { in , out := & in . CA Data , & out . CA } 
func ( in * TLS Client Config ) Deep Copy ( ) * TLS Client out := new ( TLS Client in . Deep Copy } 
func New Kube Controller Manager Options ( ) ( * Kube Controller Manager Options , error ) { component Config , err := New Default Component Config ( ports . Insecure Kube Controller Manager s := Kube Controller Manager Options { Generic : cmoptions . New Generic Controller Manager Configuration Options ( & component Config . Generic ) , Kube Cloud Shared : cmoptions . New Kube Cloud Shared Options ( & component Config . Kube Cloud Shared ) , Service Controller : & cmoptions . Service Controller Options { Service Controller Configuration : & component Config . Service Controller , } , Attach Detach Controller : & Attach Detach Controller Options { & component Config . Attach Detach Controller , } , CSR Signing Controller : & CSR Signing Controller Options { & component Config . CSR Signing Controller , } , Daemon Set Controller : & Daemon Set Controller Options { & component Config . Daemon Set Controller , } , Deployment Controller : & Deployment Controller Options { & component Config . Deployment Controller , } , Deprecated Flags : & Deprecated Controller Options { & component Config . Deprecated Controller , } , Endpoint Controller : & Endpoint Controller Options { & component Config . Endpoint Controller , } , Garbage Collector Controller : & Garbage Collector Controller Options { & component Config . Garbage Collector Controller , } , HPA Controller : & HPA Controller Options { & component Config . HPA Controller , } , Job Controller : & Job Controller Options { & component Config . Job Controller , } , Namespace Controller : & Namespace Controller Options { & component Config . Namespace Controller , } , Node IPAM Controller : & Node IPAM Controller Options { & component Config . Node IPAM Controller , } , Node Lifecycle Controller : & Node Lifecycle Controller Options { & component Config . Node Lifecycle Controller , } , Persistent Volume Binder Controller : & Persistent Volume Binder Controller Options { & component Config . Persistent Volume Binder Controller , } , Pod GC Controller : & Pod GC Controller Options { & component Config . Pod GC Controller , } , Replica Set Controller : & Replica Set Controller Options { & component Config . Replica Set Controller , } , Replication Controller : & Replication Controller Options { & component Config . Replication Controller , } , Resource Quota Controller : & Resource Quota Controller Options { & component Config . Resource Quota Controller , } , SA Controller : & SA Controller Options { & component Config . SA Controller , } , TTL After Finished Controller : & TTL After Finished Controller Options { & component Config . TTL After Finished Controller , } , Secure Serving : apiserveroptions . New Secure Serving Options ( ) . With Loopback ( ) , Insecure Serving : ( & apiserveroptions . Deprecated Insecure Serving Options { Bind Address : net . Parse IP ( component Config . Generic . Address ) , Bind Port : int ( component Config . Generic . Port ) , Bind Network : " " , } ) . With Loopback ( ) , Authentication : apiserveroptions . New Delegating Authentication Options ( ) , Authorization : apiserveroptions . New Delegating Authorization s . Authentication . Remote Kube Config File s . Authorization . Remote Kube Config File s . Authorization . Always Allow // Set the Pair Name but leave certificate directory blank to generate in-memory by default s . Secure Serving . Server Cert . Cert s . Secure Serving . Server Cert . Pair s . Secure Serving . Bind Port = ports . Kube Controller Manager gc Ignored Resources := make ( [ ] garbagecollectorconfig . Group Resource , 0 , len ( garbagecollector . Default Ignored for r := range garbagecollector . Default Ignored Resources ( ) { gc Ignored Resources = append ( gc Ignored Resources , garbagecollectorconfig . Group s . Garbage Collector Controller . GC Ignored Resources = gc Ignored } 
func New Default Component Config ( insecure Port int32 ) ( kubectrlmgrconfig . Kube Controller Manager Configuration , error ) { versioned := kubectrlmgrconfigv1alpha1 . Kube Controller Manager internal := kubectrlmgrconfig . Kube Controller Manager internal . Generic . Port = insecure } 
func ( s * Kube Controller Manager Options ) Flags ( all Controllers [ ] string , disabled By Default Controllers [ ] string ) cliflag . Named Flag Sets { fss := cliflag . Named Flag s . Generic . Add Flags ( & fss , all Controllers , disabled By Default s . Kube Cloud Shared . Add Flags ( fss . Flag s . Service Controller . Add Flags ( fss . Flag s . Secure Serving . Add Flags ( fss . Flag s . Insecure Serving . Add Unqualified Flags ( fss . Flag s . Authentication . Add Flags ( fss . Flag s . Authorization . Add Flags ( fss . Flag s . Attach Detach Controller . Add Flags ( fss . Flag s . CSR Signing Controller . Add Flags ( fss . Flag s . Deployment Controller . Add Flags ( fss . Flag s . Daemon Set Controller . Add Flags ( fss . Flag s . Deprecated Flags . Add Flags ( fss . Flag s . Endpoint Controller . Add Flags ( fss . Flag s . Garbage Collector Controller . Add Flags ( fss . Flag s . HPA Controller . Add Flags ( fss . Flag s . Job Controller . Add Flags ( fss . Flag s . Namespace Controller . Add Flags ( fss . Flag s . Node IPAM Controller . Add Flags ( fss . Flag s . Node Lifecycle Controller . Add Flags ( fss . Flag s . Persistent Volume Binder Controller . Add Flags ( fss . Flag s . Pod GC Controller . Add Flags ( fss . Flag s . Replica Set Controller . Add Flags ( fss . Flag s . Replication Controller . Add Flags ( fss . Flag s . Resource Quota Controller . Add Flags ( fss . Flag s . SA Controller . Add Flags ( fss . Flag s . TTL After Finished Controller . Add Flags ( fss . Flag fs := fss . Flag fs . String fs . String utilfeature . Default Mutable Feature Gate . Add Flag ( fss . Flag } 
func ( s * Kube Controller Manager Options ) Apply To ( c * kubecontrollerconfig . Config ) error { if err := s . Generic . Apply To ( & c . Component if err := s . Kube Cloud Shared . Apply To ( & c . Component Config . Kube Cloud if err := s . Attach Detach Controller . Apply To ( & c . Component Config . Attach Detach if err := s . CSR Signing Controller . Apply To ( & c . Component Config . CSR Signing if err := s . Daemon Set Controller . Apply To ( & c . Component Config . Daemon Set if err := s . Deployment Controller . Apply To ( & c . Component Config . Deployment if err := s . Deprecated Flags . Apply To ( & c . Component Config . Deprecated if err := s . Endpoint Controller . Apply To ( & c . Component Config . Endpoint if err := s . Garbage Collector Controller . Apply To ( & c . Component Config . Garbage Collector if err := s . HPA Controller . Apply To ( & c . Component Config . HPA if err := s . Job Controller . Apply To ( & c . Component Config . Job if err := s . Namespace Controller . Apply To ( & c . Component Config . Namespace if err := s . Node IPAM Controller . Apply To ( & c . Component Config . Node IPAM if err := s . Node Lifecycle Controller . Apply To ( & c . Component Config . Node Lifecycle if err := s . Persistent Volume Binder Controller . Apply To ( & c . Component Config . Persistent Volume Binder if err := s . Pod GC Controller . Apply To ( & c . Component Config . Pod GC if err := s . Replica Set Controller . Apply To ( & c . Component Config . Replica Set if err := s . Replication Controller . Apply To ( & c . Component Config . Replication if err := s . Resource Quota Controller . Apply To ( & c . Component Config . Resource Quota if err := s . SA Controller . Apply To ( & c . Component Config . SA if err := s . Service Controller . Apply To ( & c . Component Config . Service if err := s . TTL After Finished Controller . Apply To ( & c . Component Config . TTL After Finished if err := s . Insecure Serving . Apply To ( & c . Insecure Serving , & c . Loopback Client if err := s . Secure Serving . Apply To ( & c . Secure Serving , & c . Loopback Client if s . Secure Serving . Bind Port != 0 || s . Secure Serving . Listener != nil { if err := s . Authentication . Apply To ( & c . Authentication , c . Secure if err := s . Authorization . Apply // sync back to component config // TODO: find more elegant way than syncing back the values. c . Component Config . Generic . Port = int32 ( s . Insecure Serving . Bind c . Component Config . Generic . Address = s . Insecure Serving . Bind } 
func ( s * Kube Controller Manager Options ) Validate ( all Controllers [ ] string , disabled By Default errs = append ( errs , s . Generic . Validate ( all Controllers , disabled By Default errs = append ( errs , s . Kube Cloud errs = append ( errs , s . Attach Detach errs = append ( errs , s . CSR Signing errs = append ( errs , s . Daemon Set errs = append ( errs , s . Deployment errs = append ( errs , s . Deprecated errs = append ( errs , s . Endpoint errs = append ( errs , s . Garbage Collector errs = append ( errs , s . HPA errs = append ( errs , s . Job errs = append ( errs , s . Namespace errs = append ( errs , s . Node IPAM errs = append ( errs , s . Node Lifecycle errs = append ( errs , s . Persistent Volume Binder errs = append ( errs , s . Pod GC errs = append ( errs , s . Replica Set errs = append ( errs , s . Replication errs = append ( errs , s . Resource Quota errs = append ( errs , s . SA errs = append ( errs , s . Service errs = append ( errs , s . TTL After Finished errs = append ( errs , s . Secure errs = append ( errs , s . Insecure // TODO: validate component config, master and kubeconfig return utilerrors . New } 
func ( s Kube Controller Manager Options ) Config ( all Controllers [ ] string , disabled By Default Controllers [ ] string ) ( * kubecontrollerconfig . Config , error ) { if err := s . Validate ( all Controllers , disabled By Default if err := s . Secure Serving . Maybe Default With Self Signed Certs ( " " , nil , [ ] net . IP { net . Parse kubeconfig , err := clientcmd . Build Config From kubeconfig . Content Config . Content Type = s . Generic . Client Connection . Content kubeconfig . QPS = s . Generic . Client kubeconfig . Burst = int ( s . Generic . Client client , err := clientset . New For Config ( restclient . Add User Agent ( kubeconfig , Kube Controller Manager User config . Timeout = s . Generic . Leader Election . Renew leader Election Client := clientset . New For Config Or Die ( restclient . Add User event Recorder := create Recorder ( client , Kube Controller Manager User c := & kubecontrollerconfig . Config { Client : client , Kubeconfig : kubeconfig , Event Recorder : event Recorder , Leader Election Client : leader Election if err := s . Apply } 
func do Mount ( mounter mount . Interface , device Path , device Mount Path , fs Type string , options [ ] string ) error { err := mounter . Mount ( device Path , device Mount Path , fs if err != nil { klog . Errorf ( " " , device Mount Path , device } 
func assign Go Type To Proto Package ( p * protobuf Package , t * types . Type , local , global type Name Set , optional map [ types . Name ] struct { } ) { new T , is Proto := is Fundamental Proto if is Proto { t = new if other P , ok := global [ t . Name ] ; ok { if _ , ok := local [ t . Name ] ; ! ok { p . Imports . Add Type ( & types . Type { Kind : types . Protobuf , Name : other P . Proto Type if t . Name . Package == p . Package // don't recurse into existing proto types if is Proto { p . Imports . Add for _ , m := range t . Members { if namer . Is Private Go field := & proto tag := reflect . Struct if err := protobuf Tag To Field ( tag , field , m , t , p . Proto Type Name ( ) ) ; err == nil && field . Type != nil { assign Go Type To Proto assign Go Type To Proto // TODO: should methods be walked? if t . Elem != nil { assign Go Type To Proto if t . Key != nil { assign Go Type To Proto if t . Underlying != nil { if t . Kind == types . Alias && is Optional assign Go Type To Proto } 
func is Type Applicable To Protobuf ( t * types . Type ) bool { // skip functions -- we don't care about them for protobuf if t . Kind == types . Func || ( t . Kind == types . Declaration // skip private types if namer . Is Private Go } 
func New Preflight Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Short : " " , Long : " " , Example : preflight Example , Run : run Preflight , Inherit Flags : [ ] string { options . Cfg Path , options . Ignore Preflight } 
func run Preflight ( c workflow . Run Data ) error { data , ok := c . ( Init if err := preflight . Run Init Node Checks ( utilsexec . New ( ) , data . Cfg ( ) , data . Ignore Preflight if ! data . Dry if err := preflight . Run Pull Images Check ( utilsexec . New ( ) , data . Cfg ( ) , data . Ignore Preflight } 
func New Foo Informer ( client versioned . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Foo Informer ( client , namespace , resync } 
func New Filtered Foo Informer ( client versioned . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Samplecontroller } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Samplecontroller } , } , & samplecontrollerv1alpha1 . Foo { } , resync } 
func New Prometheus Resource Metric Collector ( provider Summary Provider , config Resource Metrics Config ) prometheus . Collector { return & resource Metric Collector { provider : provider , config : config , errors : prometheus . New Gauge ( prometheus . Gauge } 
func ( rc * resource Metric for _ , metric := range rc . config . Node for _ , metric := range rc . config . Container } 
func ( rc * resource Metric summary , err := rc . provider . Get CPU And Memory for _ , metric := range rc . config . Node Metrics { if value , timestamp := metric . Value Fn ( summary . Node ) ; value != nil { ch <- prometheus . New Metric With Timestamp ( timestamp , prometheus . Must New Const Metric ( metric . desc ( ) , prometheus . Gauge for _ , pod := range summary . Pods { for _ , container := range pod . Containers { for _ , metric := range rc . config . Container Metrics { if value , timestamp := metric . Value Fn ( container ) ; value != nil { ch <- prometheus . New Metric With Timestamp ( timestamp , prometheus . Must New Const Metric ( metric . desc ( ) , prometheus . Gauge Value , * value , container . Name , pod . Pod Ref . Name , pod . Pod } 
func find Rule ( chain * fake } 
func New Service Account Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Service Account Informer ( client , namespace , resync } 
func New Cluster Role Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Cluster Role Informer ( client , resync } 
func IP Net Equal ( ipnet1 , ipnet2 * net . IP if reflect . Deep } 
func Is Connection Reset ( err error ) bool { if url Err , ok := err . ( * url . Error ) ; ok { err = url if op Err , ok := err . ( * net . Op Error ) ; ok { err = op if os Err , ok := err . ( * os . Syscall Error ) ; ok { err = os } 
func Recommended Default Attach Detach Controller Configuration ( obj * kubectrlmgrconfigv1alpha1 . Attach Detach Controller if obj . Reconciler Sync Loop Period == zero { obj . Reconciler Sync Loop } 
func ( in * Pod GC Controller Configuration ) Deep Copy ( ) * Pod GC Controller out := new ( Pod GC Controller in . Deep Copy } 
func Validate Path No Backsteps ( target Path string ) error { parts := strings . Split ( filepath . To Slash ( target } 
func Get First Pod ( client coreclient . Pods Getter , namespace string , selector string , timeout time . Duration , sort By func ( [ ] * corev1 . Pod ) sort . Interface ) ( * corev1 . Pod , int , error ) { options := metav1 . List Options { Label pod for i := range pod List . Items { pod := pod if len ( pods ) > 0 { sort . Sort ( sort return pods [ 0 ] , len ( pod // Watch until we observe a pod options . Resource Version = pod List . Resource ctx , cancel := watchtools . Context With Optional event , err := watchtools . Until Without } 
func Selectors For Object ( object runtime . Object ) ( namespace string , selector labels . Selector , err error ) { switch t := object . ( type ) { case * extensionsv1beta1 . Replica selector , err = metav1 . Label Selector As case * appsv1 . Replica selector , err = metav1 . Label Selector As case * appsv1beta2 . Replica selector , err = metav1 . Label Selector As case * corev1 . Replication selector = labels . Selector From case * appsv1 . Stateful selector , err = metav1 . Label Selector As case * appsv1beta1 . Stateful selector , err = metav1 . Label Selector As case * appsv1beta2 . Stateful selector , err = metav1 . Label Selector As case * extensionsv1beta1 . Daemon selector , err = metav1 . Label Selector As case * appsv1 . Daemon selector , err = metav1 . Label Selector As case * appsv1beta2 . Daemon selector , err = metav1 . Label Selector As selector , err = metav1 . Label Selector As selector , err = metav1 . Label Selector As selector , err = metav1 . Label Selector As selector , err = metav1 . Label Selector As selector , err = metav1 . Label Selector As selector = labels . Selector From } 
func New Endpoints Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Endpoints Informer ( client , namespace , resync } 
func new Client ( stats Node Client win Node Stats Client ) ( Client , error ) { stats Client := new ( Stats stats Client . client = stats Node err := stats Client . client . start return stats } 
func ( c * Stats Client ) Win Container Infos ( ) ( map [ string ] cadvisorapiv2 . Container Info , error ) { infos := make ( map [ string ] cadvisorapiv2 . Container root Container Info , err := c . create Root Container infos [ " " ] = * root Container } 
func ( c * Stats Client ) Get Dir Fs Info ( path string ) ( cadvisorapiv2 . Fs Info , error ) { var free Bytes Available , total Number Of Bytes , total Number Of Free ret , _ , err := syscall . Syscall6 ( proc Get Disk Free Space Ex . Addr ( ) , 4 , uintptr ( unsafe . Pointer ( syscall . String To UTF16Ptr ( path ) ) ) , uintptr ( unsafe . Pointer ( & free Bytes Available ) ) , uintptr ( unsafe . Pointer ( & total Number Of Bytes ) ) , uintptr ( unsafe . Pointer ( & total Number Of Free if ret == 0 { return cadvisorapiv2 . Fs return cadvisorapiv2 . Fs Info { Timestamp : time . Now ( ) , Capacity : uint64 ( total Number Of Bytes ) , Available : uint64 ( free Bytes Available ) , Usage : uint64 ( total Number Of Bytes - free Bytes } 
func ( tracker * Import Tracker ) Add Nullable ( ) { tracker . Add } 
func ( cfg * Config ) Complete ( ) Completed Config { c := completed Config { cfg . Generic Config . Complete ( cfg . Extra Config . Versioned Informers ) , & cfg . Extra service IP Range , api Server Service IP , err := Default Service IP Range ( c . Extra Config . Service IP if c . Extra Config . Service IP Range . IP == nil { c . Extra Config . Service IP Range = service IP if c . Extra Config . API Server Service IP == nil { c . Extra Config . API Server Service IP = api Server Service discovery Addresses := discovery . Default Addresses { Default Address : c . Generic Config . External discovery Addresses . CIDR Rules = append ( discovery Addresses . CIDR Rules , discovery . CIDR Rule { IP Range : c . Extra Config . Service IP Range , Address : net . Join Host Port ( c . Extra Config . API Server Service IP . String ( ) , strconv . Itoa ( c . Extra Config . API Server Service c . Generic Config . Discovery Addresses = discovery if c . Extra Config . Service Node Port Range . Size == 0 { // TODO: Currently no way to specify an empty range (do we need to allow this?) // We should probably allow this for clouds that don't require Node Port to do load-balancing (GCE) // but then that breaks the strict nestedness of Service Type. // Review post-v1 c . Extra Config . Service Node Port Range = kubeoptions . Default Service Node Port klog . Infof ( " " , c . Extra Config . Service Node Port if c . Extra Config . Endpoint Reconciler Config . Interval == 0 { c . Extra Config . Endpoint Reconciler Config . Interval = Default Endpoint Reconciler if c . Extra Config . Master Endpoint Reconcile TTL == 0 { c . Extra Config . Master Endpoint Reconcile TTL = Default Endpoint Reconciler if c . Extra Config . Endpoint Reconciler Config . Reconciler == nil { c . Extra Config . Endpoint Reconciler Config . Reconciler = cfg . create Endpoint return Completed } 
func ( c completed Config ) New ( delegation Target genericapiserver . Delegation Target ) ( * Master , error ) { if reflect . Deep Equal ( c . Extra Config . Kubelet Client Config , kubeletclient . Kubelet Client s , err := c . Generic Config . New ( " " , delegation if c . Extra Config . Enable Logs Support { routes . Logs { } . Install ( s . Handler . Go Restful m := & Master { Generic API // install legacy rest storage if c . Extra Config . API Resource Config Source . Version Enabled ( apiv1 . Scheme Group Version ) { legacy REST Storage Provider := corerest . Legacy REST Storage Provider { Storage Factory : c . Extra Config . Storage Factory , Proxy Transport : c . Extra Config . Proxy Transport , Kubelet Client Config : c . Extra Config . Kubelet Client Config , Event TTL : c . Extra Config . Event TTL , Service IP Range : c . Extra Config . Service IP Range , Service Node Port Range : c . Extra Config . Service Node Port Range , Loopback Client Config : c . Generic Config . Loopback Client Config , Service Account Issuer : c . Extra Config . Service Account Issuer , Service Account Max Expiration : c . Extra Config . Service Account Max Expiration , API Audiences : c . Generic Config . Authentication . API m . Install Legacy API ( & c , c . Generic Config . REST Options Getter , legacy REST Storage // The order here is preserved in discovery. // If resources with identical names exist in more than one of these groups (e.g. "deployments.apps"" and "deployments.extensions"), // the order of this list determines which group an unqualified resource name (e.g. "deployments") should prefer. // This priority order is used for local discovery, but it ends up aggregated in `k8s.io/kubernetes/cmd/kube-apiserver/app/aggregator.go // with specific priorities. // TODO: describe the priority all the way down in the REST Storage Providers and plumb it back through the various discovery // handlers that we have. rest Storage Providers := [ ] REST Storage Provider { auditregistrationrest . REST Storage Provider { } , authenticationrest . REST Storage Provider { Authenticator : c . Generic Config . Authentication . Authenticator , API Audiences : c . Generic Config . Authentication . API Audiences } , authorizationrest . REST Storage Provider { Authorizer : c . Generic Config . Authorization . Authorizer , Rule Resolver : c . Generic Config . Rule Resolver } , autoscalingrest . REST Storage Provider { } , batchrest . REST Storage Provider { } , certificatesrest . REST Storage Provider { } , coordinationrest . REST Storage Provider { } , extensionsrest . REST Storage Provider { } , networkingrest . REST Storage Provider { } , noderest . REST Storage Provider { } , policyrest . REST Storage Provider { } , rbacrest . REST Storage Provider { Authorizer : c . Generic Config . Authorization . Authorizer } , schedulingrest . REST Storage Provider { } , settingsrest . REST Storage Provider { } , storagerest . REST Storage Provider { } , // keep apps after extensions so legacy clients resolve the extensions versions of shared resource names. // See https://github.com/kubernetes/kubernetes/issues/42392 appsrest . REST Storage Provider { } , admissionregistrationrest . REST Storage Provider { } , eventsrest . REST Storage Provider { TTL : c . Extra Config . Event m . Install AP Is ( c . Extra Config . API Resource Config Source , c . Generic Config . REST Options Getter , rest Storage if c . Extra Config . Tunneler != nil { m . install Tunneler ( c . Extra Config . Tunneler , corev1client . New For Config Or Die ( c . Generic Config . Loopback Client m . Generic API Server . Add Post Start Hook Or Die ( " " , c . Extra Config . Client CA Registration Hook . Post Start } 
func ( m * Master ) Install AP Is ( api Resource Config Source serverstorage . API Resource Config Source , rest Options Getter generic . REST Options Getter , rest Storage Providers ... REST Storage Provider ) { api Groups Info := [ ] * genericapiserver . API Group for _ , rest Storage Builder := range rest Storage Providers { group Name := rest Storage Builder . Group if ! api Resource Config Source . Any Version For Group Enabled ( group Name ) { klog . V ( 1 ) . Infof ( " " , group api Group Info , enabled := rest Storage Builder . New REST Storage ( api Resource Config Source , rest Options if ! enabled { klog . Warningf ( " " , group klog . V ( 1 ) . Infof ( " " , group if post Hook Provider , ok := rest Storage Builder . ( genericapiserver . Post Start Hook Provider ) ; ok { name , hook , err := post Hook Provider . Post Start m . Generic API Server . Add Post Start Hook Or api Groups Info = append ( api Groups Info , & api Group if err := m . Generic API Server . Install API Groups ( api Groups } 
func Get return labels . Set ( secret . Labels ) , Selectable } 
func Matcher ( label labels . Selector , field fields . Selector ) pkgstorage . Selection Predicate { return pkgstorage . Selection Predicate { Label : label , Field : field , Get Attrs : Get Attrs , Index } 
func Selectable Fields ( obj * api . Secret ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & obj . Object secret Specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , secret Specific Fields } 
func Probe Volume Plugins ( ) [ ] volume . Volume Plugin { return [ ] volume . Volume Plugin { & glusterfs Plugin { host : nil , gid Table : make ( map [ string ] * Min Max } 
func ( b * glusterfs Mounter ) Can Mount ( ) error { exe := b . plugin . host . Get Exec ( b . plugin . Get Plugin switch runtime . GOOS { case " " : if _ , err := exe . Run ( " " , " " , gci Linux Gluster Mount Binary Path ) ; err != nil { return fmt . Errorf ( " " , gci Linux Gluster Mount Binary } 
func ( b * glusterfs Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func get Volume Info ( spec * volume . Spec ) ( string , bool , error ) { if spec . Volume != nil && spec . Volume . Glusterfs != nil { return spec . Volume . Glusterfs . Path , spec . Volume . Glusterfs . Read } else if spec . Persistent Volume != nil && spec . Persistent Volume . Spec . Glusterfs != nil { return spec . Persistent Volume . Spec . Glusterfs . Path , spec . Read } 
func ( plugin * glusterfs Plugin ) collect Gids ( class Name string , gid Table * Min Max Allocator ) error { kube Client := plugin . host . Get Kube if kube pv List , err := kube Client . Core V1 ( ) . Persistent Volumes ( ) . List ( metav1 . List Options { Label for _ , pv := range pv List . Items { if v1helper . Get Persistent Volume Class ( & pv ) != class pv Name := pv . Object gid Str , ok := pv . Annotations [ volutil . Volume Gid Annotation if ! ok { klog . Warningf ( " " , pv gid , err := convert Gid ( gid if err != nil { klog . Errorf ( " " , gid _ , err = gid if err == Err Conflict { klog . Warningf ( " " , gid , pv } else if err != nil { klog . Errorf ( " " , gid , pv } 
func ( plugin * glusterfs Plugin ) get Gid Table ( class Name string , min int , max int ) ( * Min Max Allocator , error ) { plugin . gid Table gid Table , ok := plugin . gid Table [ class plugin . gid Table if ok { err := gid Table . Set return gid // create a new table and fill it new Gid Table , err := New Min Max Allocator ( 0 , absolute Gid // collect gids with the full range err = plugin . collect Gids ( class Name , new Gid // and only reduce the range afterwards err = new Gid Table . Set // if in the meantime a table appeared, use it plugin . gid Table defer plugin . gid Table gid Table , ok = plugin . gid Table [ class if ok { err = gid Table . Set return gid plugin . gid Table [ class Name ] = new Gid return new Gid } 
func parse Secret ( namespace , secret Name string , kube Client clientset . Interface ) ( string , error ) { secret Map , err := volutil . Get Secret For PV ( namespace , secret Name , glusterfs Plugin Name , kube if err != nil { klog . Errorf ( " " , namespace , secret return " " , fmt . Errorf ( " " , namespace , secret if len ( secret for k , v := range secret Map { if k == secret Key } 
func parse Class Parameters ( params map [ string ] string , kube Client clientset . Interface ) ( * provisioner Config , error ) { var cfg provisioner cfg . gid Min = default Gid cfg . gid Max = default Gid cfg . custom Ep Name Prefix = dynamic Ep Svc auth parse Volume parse Volume parse Volume Name parse Thin Pool Snap //thin pool snap factor default to 1.0 cfg . thin Pool Snap for k , v := range params { switch dstrings . To case " " : cfg . user case " " : cfg . secret case " " : cfg . secret case " " : if len ( v ) != 0 { cfg . cluster case " " : auth Enabled = dstrings . To case " " : parse Gid Min , err := convert if err != nil { return nil , fmt . Errorf ( " " , k , glusterfs Plugin if parse Gid Min < absolute Gid Min { return nil , fmt . Errorf ( " " , absolute Gid if parse Gid Min > absolute Gid Max { return nil , fmt . Errorf ( " " , absolute Gid cfg . gid Min = parse Gid case " " : parse Gid Max , err := convert if err != nil { return nil , fmt . Errorf ( " " , k , glusterfs Plugin if parse Gid Max < absolute Gid Min { return nil , fmt . Errorf ( " " , absolute Gid if parse Gid Max > absolute Gid Max { return nil , fmt . Errorf ( " " , absolute Gid cfg . gid Max = parse Gid case " " : parse Volume case " " : if len ( v ) != 0 { parse Volume case " " : if len ( v ) != 0 { parse Volume Name case " " : if len ( v ) != 0 { parse Thin Pool Snap case " " : // If the string has > 'max Custom Ep Name Prefix Len' chars, the final endpoint name will // exceed the limitation of 63 chars, so fail if prefix is > 'max Custom Ep Name Prefix Len' // characters. This is only applicable for 'customepnameprefix' string and default ep name // string will always pass. if len ( v ) <= max Custom Ep Name Prefix Len { cfg . custom Ep Name } else { return nil , fmt . Errorf ( " " , max Custom Ep Name Prefix default : return nil , fmt . Errorf ( " " , k , glusterfs Plugin if len ( cfg . url ) == 0 { return nil , fmt . Errorf ( " " , glusterfs Plugin if len ( parse Volume Type ) == 0 { cfg . volume Type = gapi . Volume Durability Info { Type : gapi . Durability Replicate , Replicate : gapi . Replica Durability { Replica : replica } else { parse Volume Type Info := dstrings . Split ( parse Volume switch parse Volume Type Info [ 0 ] { case " " : if len ( parse Volume Type Info ) >= 2 { new Replica Count , err := convert Volume Param ( parse Volume Type if err != nil { return nil , fmt . Errorf ( " " , parse Volume Type cfg . volume Type = gapi . Volume Durability Info { Type : gapi . Durability Replicate , Replicate : gapi . Replica Durability { Replica : new Replica } else { cfg . volume Type = gapi . Volume Durability Info { Type : gapi . Durability Replicate , Replicate : gapi . Replica Durability { Replica : replica case " " : if len ( parse Volume Type Info ) >= 3 { new Disperse Data , err := convert Volume Param ( parse Volume Type if err != nil { return nil , fmt . Errorf ( " " , parse Volume Type new Disperse Redundancy , err := convert Volume Param ( parse Volume Type if err != nil { return nil , fmt . Errorf ( " " , parse Volume Type cfg . volume Type = gapi . Volume Durability Info { Type : gapi . Durability EC , Disperse : gapi . Disperse Durability { Data : new Disperse Data , Redundancy : new Disperse } else { return nil , fmt . Errorf ( " " , glusterfs Plugin case " " : cfg . volume Type = gapi . Volume Durability Info { Type : gapi . Durability Distribute default : return nil , fmt . Errorf ( " " , glusterfs Plugin if ! auth cfg . secret cfg . secret cfg . user cfg . secret if len ( cfg . secret Name ) != 0 || len ( cfg . secret Namespace ) != 0 { // secret Name + Namespace has precedence over user Key if len ( cfg . secret Name ) != 0 && len ( cfg . secret Namespace ) != 0 { cfg . secret Value , err = parse Secret ( cfg . secret Namespace , cfg . secret Name , kube } else { return nil , fmt . Errorf ( " " , glusterfs Plugin } else { cfg . secret Value = cfg . user if cfg . gid Min > cfg . gid Max { return nil , fmt . Errorf ( " " , glusterfs Plugin if len ( parse Volume Options ) != 0 { vol Options := dstrings . Split ( parse Volume if len ( vol Options ) == 0 { return nil , fmt . Errorf ( " " , glusterfs Plugin cfg . volume Options = vol if len ( parse Volume Name Prefix ) != 0 { if dstrings . Contains ( parse Volume Name cfg . volume Name Prefix = parse Volume Name if len ( parse Thin Pool Snap Factor ) != 0 { thin Pool Snap Factor , err := strconv . Parse Float ( parse Thin Pool Snap if err != nil { return nil , fmt . Errorf ( " " , parse Thin Pool Snap if thin Pool Snap Factor < 1.0 || thin Pool Snap Factor > 100.0 { return nil , fmt . Errorf ( " " , thin Pool Snap cfg . thin Pool Snap Factor = float32 ( thin Pool Snap } 
func get Volume ID ( pv * v1 . Persistent Volume , volume Name string ) ( string , error ) { volume // Get vol ID from pvspec if available, else fill it from volumename. if pv != nil { if pv . Annotations [ heketi Vol ID Ann ] != " " { volume ID = pv . Annotations [ heketi Vol ID } else { volume ID = dstrings . Trim Prefix ( volume Name , vol } else { return volume if volume ID == " " { return volume return volume } 
func ( c * Plugin Context ) Read ( key Context Key ) ( Context return nil , errors . New ( Not } 
func ( c * Plugin Context ) Write ( key Context Key , val Context } 
func force Multi } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Audit Sink ) ( nil ) , ( * auditregistration . Audit Sink ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Audit Sink_To_auditregistration_Audit Sink ( a . ( * v1alpha1 . Audit Sink ) , b . ( * auditregistration . Audit if err := s . Add Generated Conversion Func ( ( * auditregistration . Audit Sink ) ( nil ) , ( * v1alpha1 . Audit Sink ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_auditregistration_Audit Sink_To_v1alpha1_Audit Sink ( a . ( * auditregistration . Audit Sink ) , b . ( * v1alpha1 . Audit if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Audit Sink List ) ( nil ) , ( * auditregistration . Audit Sink List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Audit Sink List_To_auditregistration_Audit Sink List ( a . ( * v1alpha1 . Audit Sink List ) , b . ( * auditregistration . Audit Sink if err := s . Add Generated Conversion Func ( ( * auditregistration . Audit Sink List ) ( nil ) , ( * v1alpha1 . Audit Sink List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_auditregistration_Audit Sink List_To_v1alpha1_Audit Sink List ( a . ( * auditregistration . Audit Sink List ) , b . ( * v1alpha1 . Audit Sink if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Audit Sink Spec ) ( nil ) , ( * auditregistration . Audit Sink Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Audit Sink Spec_To_auditregistration_Audit Sink Spec ( a . ( * v1alpha1 . Audit Sink Spec ) , b . ( * auditregistration . Audit Sink if err := s . Add Generated Conversion Func ( ( * auditregistration . Audit Sink Spec ) ( nil ) , ( * v1alpha1 . Audit Sink Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_auditregistration_Audit Sink Spec_To_v1alpha1_Audit Sink Spec ( a . ( * auditregistration . Audit Sink Spec ) , b . ( * v1alpha1 . Audit Sink if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Service Reference ) ( nil ) , ( * auditregistration . Service Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Service Reference_To_auditregistration_Service Reference ( a . ( * v1alpha1 . Service Reference ) , b . ( * auditregistration . Service if err := s . Add Generated Conversion Func ( ( * auditregistration . Service Reference ) ( nil ) , ( * v1alpha1 . Service Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_auditregistration_Service Reference_To_v1alpha1_Service Reference ( a . ( * auditregistration . Service Reference ) , b . ( * v1alpha1 . Service if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Webhook Client Config ) ( nil ) , ( * auditregistration . Webhook Client Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Webhook Client Config_To_auditregistration_Webhook Client Config ( a . ( * v1alpha1 . Webhook Client Config ) , b . ( * auditregistration . Webhook Client if err := s . Add Generated Conversion Func ( ( * auditregistration . Webhook Client Config ) ( nil ) , ( * v1alpha1 . Webhook Client Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_auditregistration_Webhook Client Config_To_v1alpha1_Webhook Client Config ( a . ( * auditregistration . Webhook Client Config ) , b . ( * v1alpha1 . Webhook Client if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Webhook Throttle Config ) ( nil ) , ( * auditregistration . Webhook Throttle Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Webhook Throttle Config_To_auditregistration_Webhook Throttle Config ( a . ( * v1alpha1 . Webhook Throttle Config ) , b . ( * auditregistration . Webhook Throttle if err := s . Add Generated Conversion Func ( ( * auditregistration . Webhook Throttle Config ) ( nil ) , ( * v1alpha1 . Webhook Throttle Config ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_auditregistration_Webhook Throttle Config_To_v1alpha1_Webhook Throttle Config ( a . ( * auditregistration . Webhook Throttle Config ) , b . ( * v1alpha1 . Webhook Throttle } 
func Convert_v1alpha1_Audit Sink_To_auditregistration_Audit Sink ( in * v1alpha1 . Audit Sink , out * auditregistration . Audit Sink , s conversion . Scope ) error { return auto Convert_v1alpha1_Audit Sink_To_auditregistration_Audit } 
func Convert_auditregistration_Audit Sink_To_v1alpha1_Audit Sink ( in * auditregistration . Audit Sink , out * v1alpha1 . Audit Sink , s conversion . Scope ) error { return auto Convert_auditregistration_Audit Sink_To_v1alpha1_Audit } 
func Convert_v1alpha1_Audit Sink List_To_auditregistration_Audit Sink List ( in * v1alpha1 . Audit Sink List , out * auditregistration . Audit Sink List , s conversion . Scope ) error { return auto Convert_v1alpha1_Audit Sink List_To_auditregistration_Audit Sink } 
func Convert_auditregistration_Audit Sink List_To_v1alpha1_Audit Sink List ( in * auditregistration . Audit Sink List , out * v1alpha1 . Audit Sink List , s conversion . Scope ) error { return auto Convert_auditregistration_Audit Sink List_To_v1alpha1_Audit Sink } 
func Convert_v1alpha1_Audit Sink Spec_To_auditregistration_Audit Sink Spec ( in * v1alpha1 . Audit Sink Spec , out * auditregistration . Audit Sink Spec , s conversion . Scope ) error { return auto Convert_v1alpha1_Audit Sink Spec_To_auditregistration_Audit Sink } 
func Convert_auditregistration_Audit Sink Spec_To_v1alpha1_Audit Sink Spec ( in * auditregistration . Audit Sink Spec , out * v1alpha1 . Audit Sink Spec , s conversion . Scope ) error { return auto Convert_auditregistration_Audit Sink Spec_To_v1alpha1_Audit Sink } 
func Convert_v1alpha1_Policy_To_auditregistration_Policy ( in * v1alpha1 . Policy , out * auditregistration . Policy , s conversion . Scope ) error { return auto } 
func Convert_auditregistration_Policy_To_v1alpha1_Policy ( in * auditregistration . Policy , out * v1alpha1 . Policy , s conversion . Scope ) error { return auto } 
func Convert_v1alpha1_Service Reference_To_auditregistration_Service Reference ( in * v1alpha1 . Service Reference , out * auditregistration . Service Reference , s conversion . Scope ) error { return auto Convert_v1alpha1_Service Reference_To_auditregistration_Service } 
func Convert_auditregistration_Service Reference_To_v1alpha1_Service Reference ( in * auditregistration . Service Reference , out * v1alpha1 . Service Reference , s conversion . Scope ) error { return auto Convert_auditregistration_Service Reference_To_v1alpha1_Service } 
func Convert_v1alpha1_Webhook_To_auditregistration_Webhook ( in * v1alpha1 . Webhook , out * auditregistration . Webhook , s conversion . Scope ) error { return auto } 
func Convert_auditregistration_Webhook_To_v1alpha1_Webhook ( in * auditregistration . Webhook , out * v1alpha1 . Webhook , s conversion . Scope ) error { return auto } 
func Convert_v1alpha1_Webhook Client Config_To_auditregistration_Webhook Client Config ( in * v1alpha1 . Webhook Client Config , out * auditregistration . Webhook Client Config , s conversion . Scope ) error { return auto Convert_v1alpha1_Webhook Client Config_To_auditregistration_Webhook Client } 
func Convert_auditregistration_Webhook Client Config_To_v1alpha1_Webhook Client Config ( in * auditregistration . Webhook Client Config , out * v1alpha1 . Webhook Client Config , s conversion . Scope ) error { return auto Convert_auditregistration_Webhook Client Config_To_v1alpha1_Webhook Client } 
func Convert_v1alpha1_Webhook Throttle Config_To_auditregistration_Webhook Throttle Config ( in * v1alpha1 . Webhook Throttle Config , out * auditregistration . Webhook Throttle Config , s conversion . Scope ) error { return auto Convert_v1alpha1_Webhook Throttle Config_To_auditregistration_Webhook Throttle } 
func Convert_auditregistration_Webhook Throttle Config_To_v1alpha1_Webhook Throttle Config ( in * auditregistration . Webhook Throttle Config , out * v1alpha1 . Webhook Throttle Config , s conversion . Scope ) error { return auto Convert_auditregistration_Webhook Throttle Config_To_v1alpha1_Webhook Throttle } 
func Get Cgroup Driver Docker ( execer utilsexec . Interface ) ( string , error ) { driver , err := call Docker return strings . Trim } 
func ( ss * scale Set ) new Vmss Cache ( ) ( * timed Cache , error ) { getter := func ( key string ) ( interface { } , error ) { ctx , cancel := get Context With result , err := ss . Virtual Machine Scale Sets Client . Get ( ctx , ss . Resource exists , message , real Err := check Resource Exists From if real Err != nil { return nil , real return new Timedcache ( vmss Cache } 
func add Resource List ( list , new api . Resource } 
func ( s event Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Event , err error ) { err = cache . List All By } 
func ( i * image Fs Info Provider ) Image Fs Info Label ( ) ( string , error ) { switch i . runtime { case types . Docker Container Runtime : return cadvisorfs . Label Docker case types . Remote Container Runtime : // This is a temporary workaround to get stats for cri-o from cadvisor // and should be removed. // Related to https://github.com/kubernetes/kubernetes/issues/51798 if i . runtime Endpoint == Crio Socket || i . runtime Endpoint == " " + Crio Socket { return cadvisorfs . Label Crio } 
func New Image Fs Info Provider ( runtime , runtime Endpoint string ) Image Fs Info Provider { return & image Fs Info Provider { runtime : runtime , runtime Endpoint : runtime } 
func New From Interface ( token Review authenticationclient . Token Review Interface , implicit Auds authenticator . Audiences ) ( * Webhook Token Authenticator , error ) { return new With Backoff ( token Review , retry Backoff , implicit } 
func New ( kube Config File string , implicit Auds authenticator . Audiences ) ( * Webhook Token Authenticator , error ) { token Review , err := token Review Interface From Kubeconfig ( kube Config return new With Backoff ( token Review , retry Backoff , implicit } 
func new With Backoff ( token Review authenticationclient . Token Review Interface , initial Backoff time . Duration , implicit Auds authenticator . Audiences ) ( * Webhook Token Authenticator , error ) { return & Webhook Token Authenticator { token Review , initial Backoff , implicit } 
func ( w * Webhook Token Authenticator ) Authenticate Token ( ctx context . Context , token string ) ( * authenticator . Response , bool , error ) { // We take implicit audiences of the API server at Webhook Token Authenticator // construction time. The outline of how we validate audience here is: // // * if the ctx is not audience limited, don't do any audience validation. // * if ctx is audience-limited, add the audiences to the tokenreview spec // * if the tokenreview returns with audiences in the status that intersect // with the audiences in the ctx, copy into the response and return success // * if the tokenreview returns without an audience in the status, ensure // the ctx audiences intersect with the implicit audiences, and set the // intersection in the response. // * otherwise return unauthenticated. want Auds , check Auds := authenticator . Audiences r := & authentication . Token Review { Spec : authentication . Token Review Spec { Token : token , Audiences : want var ( result * authentication . Token webhook . With Exponential Backoff ( w . initial Backoff , func ( ) error { result , err = w . token if check Auds { got Auds := w . implicit if len ( result . Status . Audiences ) > 0 { got auds = want Auds . Intersect ( got return & authenticator . Response { User : & user . Default } 
func token Review Interface From Kubeconfig ( kube Config File string ) ( authenticationclient . Token Review Interface , error ) { local Scheme := runtime . New if err := scheme . Add To Scheme ( local if err := local Scheme . Set Version Priority ( group gw , err := webhook . New Generic Webhook ( local Scheme , scheme . Codecs , kube Config File , group return & token Review } 
func find Name Step ( parts [ ] string , type number Of Parts In Step := find Known Value ( parts [ 1 : ] , type // if we didn't find a known value, then the entire thing must be a name if number Of Parts In Step == 0 { number Of Parts In next Parts := parts [ 0 : number Of Parts In return strings . Join ( next } 
func get Potential Type Values ( type Value reflect . Type ) ( map [ string ] reflect . Type , error ) { if type Value . Kind ( ) == reflect . Ptr { type Value = type if type Value . Kind ( ) != reflect . Struct { return nil , fmt . Errorf ( " " , type for field Index := 0 ; field Index < type Value . Num Field ( ) ; field Index ++ { field Type := type Value . Field ( field yaml Tag := field yaml Tag Name := strings . Split ( yaml ret [ yaml Tag Name ] = field } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Job { } , & Job metav1 . Add To Group Version ( scheme , Scheme Group } 
func ( h * netlink Handle ) Ensure Address Bind ( address , dev Name string ) ( exist bool , err error ) { dev , err := h . Link By Name ( dev if err != nil { return false , fmt . Errorf ( " " , dev addr := net . Parse if err := h . Addr Add ( dev , & netlink . Addr { IP Net : netlink . New IP return false , fmt . Errorf ( " " , address , dev } 
func ( h * netlink Handle ) Unbind Address ( address , dev Name string ) error { dev , err := h . Link By Name ( dev if err != nil { return fmt . Errorf ( " " , dev addr := net . Parse if err := h . Addr Del ( dev , & netlink . Addr { IP Net : netlink . New IP Net ( addr ) } ) ; err != nil { if err != unix . ENXIO { return fmt . Errorf ( " " , address , dev } 
func ( h * netlink Handle ) Ensure Dummy Device ( dev Name string ) ( bool , error ) { _ , err := h . Link By Name ( dev dummy := & netlink . Dummy { Link Attrs : netlink . Link Attrs { Name : dev return false , h . Link } 
func ( h * netlink Handle ) Delete Dummy Device ( dev Name string ) error { link , err := h . Link By Name ( dev if err != nil { _ , ok := err . ( netlink . Link Not Found return fmt . Errorf ( " " , dev return h . Link } 
func ( h * netlink Handle ) List Bind Address ( dev Name string ) ( [ ] string , error ) { dev , err := h . Link By Name ( dev if err != nil { return nil , fmt . Errorf ( " " , dev addrs , err := h . Addr if err != nil { return nil , fmt . Errorf ( " " , dev } 
func ( h * netlink Handle ) Get Local Addresses ( dev , filter Dev string ) ( sets . String , error ) { chosen Link Index , filter Link if dev != " " { link , err := h . Link By if err != nil { return nil , fmt . Errorf ( " " , filter chosen Link } else if filter Dev != " " { link , err := h . Link By Name ( filter if err != nil { return nil , fmt . Errorf ( " " , filter filter Link route filter // find chosen device if chosen Link Index != - 1 { route Filter . Link Index = chosen Link filter routes , err := h . Route List Filtered ( netlink . FAMILY_ALL , route Filter , filter res := sets . New for _ , route := range routes { if route . Link Index == filter Link if h . is I Pv6 { if route . Dst . IP . To4 ( ) == nil && ! route . Dst . IP . Is Link Local } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1alpha1 . Audit Sink { } , func ( obj interface { } ) { Set Object Defaults_Audit Sink ( obj . ( * v1alpha1 . Audit scheme . Add Type Defaulting Func ( & v1alpha1 . Audit Sink List { } , func ( obj interface { } ) { Set Object Defaults_Audit Sink List ( obj . ( * v1alpha1 . Audit Sink } 
func ( az * Cloud ) Get Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service ) ( status * v1 . Load Balancer Status , exists bool , err error ) { _ , status , exists , err = az . get Service Load Balancer ( service , cluster if ! exists { service Name := get Service klog . V ( 5 ) . Infof ( " " , cluster Name , service } 
func ( az * Cloud ) Ensure Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node ) ( * v1 . Load Balancer Status , error ) { // When a client updates the internal load balancer annotation, // the service may be switched from an internal LB to a public one, or vise versa. // Here we'll firstly ensure service do not lie in the opposite LB. service Name := get Service klog . V ( 5 ) . Infof ( " " , service Name , cluster lb , err := az . reconcile Load Balancer ( cluster Name , service , nodes , true /* want lb Status , err := az . get Service Load Balancer var service if lb Status != nil && len ( lb Status . Ingress ) > 0 { service IP = & lb klog . V ( 2 ) . Infof ( " " , service Name , log Safe ( service if _ , err := az . reconcile Security Group ( cluster Name , service , service IP , true /* want update Service := update Service Load Balancer IP ( service , to . String ( service flipped Service := flip Service Internal Annotation ( update if _ , err := az . reconcile Load Balancer ( cluster Name , flipped Service , nil , false /* want if _ , err := az . reconcile Public IP ( cluster Name , update Service , lb , true /* want return lb } 
func ( az * Cloud ) Update Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node ) error { _ , err := az . Ensure Load Balancer ( ctx , cluster } 
func ( az * Cloud ) Ensure Load Balancer Deleted ( ctx context . Context , cluster Name string , service * v1 . Service ) error { is Internal := requires Internal Load service Name := get Service klog . V ( 5 ) . Infof ( " " , service Name , cluster ignore Errors := func ( err error ) error { if ignore Status Not Found From if ignore Status Forbidden From service IP To Cleanup , err := az . find Service IP Address ( ctx , cluster Name , service , is if ignore klog . V ( 2 ) . Infof ( " " , service Name , service IP To if _ , err := az . reconcile Security Group ( cluster Name , service , & service IP To Cleanup , false /* want Lb */ ) ; err != nil { if ignore if _ , err := az . reconcile Load Balancer ( cluster Name , service , nil , false /* want Lb */ ) ; err != nil { if ignore if _ , err := az . reconcile Public IP ( cluster Name , service , nil , false /* want Lb */ ) ; err != nil { if ignore klog . V ( 2 ) . Infof ( " " , service } 
func ( az * Cloud ) Get Load Balancer Name ( ctx context . Context , cluster Name string , service * v1 . Service ) string { // TODO: replace Default Load Balancer Name to generate more meaningful loadbalancer names. return cloudprovider . Default Load Balancer } 
func ( az * Cloud ) get Service Load Balancer ( service * v1 . Service , cluster Name string , nodes [ ] * v1 . Node , want Lb bool ) ( lb * network . Load Balancer , status * v1 . Load Balancer Status , exists bool , err error ) { is Internal := requires Internal Load var default LB * network . Load primary VM Set Name := az . vm Set . Get Primary VM Set default LB Name := az . get Azure Load Balancer Name ( cluster Name , primary VM Set Name , is existing L Bs , err := az . List // check if the service already has a load balancer if existing L Bs != nil { for i := range existing L Bs { existing LB := existing L if strings . Equal Fold ( * existing LB . Name , default LB Name ) { default LB = & existing if is Internal Load Balancer ( & existing LB ) != is status , err = az . get Service Load Balancer Status ( service , & existing return & existing has Mode , _ , _ := get Service Load Balancer if az . use Standard Load Balancer ( ) && has Mode { return nil , nil , false , fmt . Errorf ( " " , Service Annotation Load Balancer // service does not have a basic load balancer, select one. // Standard load balancer doesn't need this because all backends nodes should be added to same LB. if want Lb && ! az . use Standard Load Balancer ( ) { // select new load balancer for service selected LB , exists , err := az . select Load Balancer ( cluster Name , service , & existing L return selected // create a default LB with meta data if not present if default LB == nil { default LB = & network . Load Balancer { Name : & default LB Name , Location : & az . Location , Load Balancer Properties Format : & network . Load Balancer Properties if az . use Standard Load Balancer ( ) { default LB . Sku = & network . Load Balancer Sku { Name : network . Load Balancer Sku Name return default } 
func ( az * Cloud ) select Load Balancer ( cluster Name string , service * v1 . Service , existing L Bs * [ ] network . Load Balancer , nodes [ ] * v1 . Node ) ( selected LB * network . Load Balancer , exists Lb bool , err error ) { is Internal := requires Internal Load service Name := get Service klog . V ( 2 ) . Infof ( " " , service Name , is vm Set Names , err := az . vm Set . Get VM Set if err != nil { klog . Errorf ( " " , cluster Name , service Name , is klog . Infof ( " " , cluster Name , service Name , is Internal , * vm Set map Existing L Bs := map [ string ] network . Load for _ , lb := range * existing L Bs { map Existing L selected LB Rule Count := math . Max for _ , curr AS Name := range * vm Set Names { curr LB Name := az . get Azure Load Balancer Name ( cluster Name , curr AS Name , is lb , exists := map Existing L Bs [ curr LB if ! exists { // select this LB as this is a new LB and will have minimum rules // create tmp lb struct to hold metadata for the new load-balancer selected LB = & network . Load Balancer { Name : & curr LB Name , Location : & az . Location , Load Balancer Properties Format : & network . Load Balancer Properties return selected lb Rules := * lb . Load Balancing curr LB Rule if lb Rules != nil { curr LB Rule Count = len ( lb if curr LB Rule Count < selected LB Rule Count { selected LB Rule Count = curr LB Rule selected if selected LB == nil { err = fmt . Errorf ( " " , cluster Name , service Name , is Internal , * vm Set // validate if the selected LB has not exceeded the Maximum Load Balancer Rule Count if az . Config . Maximum Load Balancer Rule Count != 0 && selected LB Rule Count >= az . Config . Maximum Load Balancer Rule Count { err = fmt . Errorf ( " " , cluster Name , service Name , is Internal , selected LB Rule Count , * vm Set return selected LB , exists return selected LB , exists } 
func ( az * Cloud ) reconcile Load Balancer ( cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node , want Lb bool ) ( * network . Load Balancer , error ) { is Internal := requires Internal Load service Name := get Service klog . V ( 2 ) . Infof ( " " , service Name , want lb , _ , _ , err := az . get Service Load Balancer ( service , cluster Name , nodes , want if err != nil { klog . Errorf ( " " , service lb klog . V ( 2 ) . Infof ( " " , service Name , lb Name , want lb Frontend IP Config Name := az . get Frontend IP Config lb Frontend IP Config ID := az . get Frontend IP Config ID ( lb Name , lb Frontend IP Config lb Backend Pool Name := get Backend Pool Name ( cluster lb Backend Pool ID := az . get Backend Pool ID ( lb Name , lb Backend Pool lb Idle Timeout , err := get Idle if want dirty // Ensure Load Balancer's Backend Pool Configuration if want Lb { new Backend Pools := [ ] network . Backend Address if lb . Backend Address Pools != nil { new Backend Pools = * lb . Backend Address found Backend for _ , bp := range new Backend Pools { if strings . Equal Fold ( * bp . Name , lb Backend Pool Name ) { klog . V ( 10 ) . Infof ( " " , service Name , want found Backend } else { klog . V ( 10 ) . Infof ( " " , service Name , want if ! found Backend Pool { new Backend Pools = append ( new Backend Pools , network . Backend Address Pool { Name : to . String Ptr ( lb Backend Pool klog . V ( 10 ) . Infof ( " " , service Name , want dirty lb . Backend Address Pools = & new Backend // Ensure Load Balancer's Frontend IP Configurations dirty new Configs := [ ] network . Frontend IP if lb . Frontend IP Configurations != nil { new Configs = * lb . Frontend IP if ! want Lb { for i := len ( new Configs ) - 1 ; i >= 0 ; i -- { config := new if az . service Owns Frontend IP ( config , service ) { klog . V ( 2 ) . Infof ( " " , service Name , want Lb , lb Frontend IP Config new Configs = append ( new Configs [ : i ] , new dirty } else { for i := len ( new Configs ) - 1 ; i >= 0 ; i -- { config := new is Fip Changed , err := az . is Frontend IP Changed ( cluster Name , config , service , lb Frontend IP Config if is Fip Changed { klog . V ( 2 ) . Infof ( " " , service Name , want new Configs = append ( new Configs [ : i ] , new dirty found for _ , config := range new Configs { if strings . Equal Fold ( * config . Name , lb Frontend IP Config Name ) { found if ! found Config { // construct Frontend IP Configuration Properties Format var fip Configuration Properties * network . Frontend IP Configuration Properties if is Internal { subnet if subnet Name == nil { subnet Name = & az . Subnet subnet , exists Subnet , err := az . get Subnet ( az . Vnet Name , * subnet if ! exists Subnet { return nil , fmt . Errorf ( " " , service Name , lb Name , az . Vnet Name , az . Subnet config Properties := network . Frontend IP Configuration Properties load Balancer IP := service . Spec . Load Balancer if load Balancer IP != " " { config Properties . Private IP Allocation config Properties . Private IP Address = & load Balancer } else { // We'll need to call Get Load Balancer later to retrieve allocated IP. config Properties . Private IP Allocation fip Configuration Properties = & config } else { pip Name , err := az . determine Public IP Name ( cluster domain Name Label := get Public IP Domain Name pip , err := az . ensure Public IP Exists ( service , pip Name , domain Name fip Configuration Properties = & network . Frontend IP Configuration Properties Format { Public IP Address : & network . Public IP new Configs = append ( new Configs , network . Frontend IP Configuration { Name : to . String Ptr ( lb Frontend IP Config Name ) , Frontend IP Configuration Properties Format : fip Configuration klog . V ( 10 ) . Infof ( " " , service Name , want Lb , lb Frontend IP Config dirty if dirty Configs { dirty lb . Frontend IP Configurations = & new // update probes/rules expected Probes , expected Rules , err := az . reconcile Load Balancer Rule ( service , want Lb , lb Frontend IP Config ID , lb Backend Pool ID , lb Name , lb Idle // remove unwanted probes dirty var updated if lb . Probes != nil { updated for i := len ( updated Probes ) - 1 ; i >= 0 ; i -- { existing Probe := updated if az . service Owns Rule ( service , * existing Probe . Name ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing keep if find Probe ( expected Probes , existing Probe ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing keep if ! keep Probe { updated Probes = append ( updated Probes [ : i ] , updated klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing dirty // add missing, wanted probes for _ , expected Probe := range expected Probes { found if find Probe ( updated Probes , expected Probe ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * expected found if ! found Probe { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * expected updated Probes = append ( updated Probes , expected dirty if dirty Probes { dirty lb . Probes = & updated // update rules dirty var updated Rules [ ] network . Load Balancing if lb . Load Balancing Rules != nil { updated Rules = * lb . Load Balancing // update rules: remove unwanted for i := len ( updated Rules ) - 1 ; i >= 0 ; i -- { existing Rule := updated if az . service Owns Rule ( service , * existing Rule . Name ) { keep klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing if find Rule ( expected Rules , existing Rule , want Lb ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing keep if ! keep Rule { klog . V ( 2 ) . Infof ( " " , service Name , want Lb , * existing updated Rules = append ( updated Rules [ : i ] , updated dirty // update rules: add needed for _ , expected Rule := range expected Rules { found if find Rule ( updated Rules , expected Rule , want Lb ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * expected found if ! found Rule { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * expected updated Rules = append ( updated Rules , expected dirty if dirty Rules { dirty lb . Load Balancing Rules = & updated // We don't care if the LB exists or not // We only care about if there is any change in the LB, which means dirty LB // If it is not exist, and no change to that, we don't Create Or Update LB if dirty Lb { if lb . Frontend IP Configurations == nil || len ( * lb . Frontend IP Configurations ) == 0 { // When Frontend IP Configurations is empty, we need to delete the Azure load balancer resource itself, // because an Azure load balancer cannot have an empty Frontend IP Configurations collection klog . V ( 2 ) . Infof ( " " , service Name , lb // Remove backend pools from vm Sets. This is required for virtual machine scale sets before removing the LB. vm Set Name := az . map Load Balancer Name To VM Set ( lb Name , cluster klog . V ( 10 ) . Infof ( " " , lb Backend Pool ID , vm Set Name , service err := az . vm Set . Ensure Backend Pool Deleted ( service , lb Backend Pool ID , vm Set Name , lb . Backend Address if err != nil { klog . Errorf ( " " , lb Backend Pool ID , service klog . V ( 10 ) . Infof ( " " , lb Backend Pool ID , service // Remove the LB. klog . V ( 10 ) . Infof ( " " , lb err = az . Delete LB ( service , lb if err != nil { klog . V ( 2 ) . Infof ( " " , service Name , lb klog . V ( 10 ) . Infof ( " " , lb } else { klog . V ( 2 ) . Infof ( " " , service Name , lb err := az . Create Or Update if err != nil { klog . V ( 2 ) . Infof ( " " , service Name , lb if is Internal { // Refresh updated lb which will be used later in other places. new LB , exist , err := az . get Azure Load Balancer ( lb if err != nil { klog . V ( 2 ) . Infof ( " " , service Name , lb if ! exist { return nil , fmt . Errorf ( " " , lb lb = & new if want Lb && nodes != nil { // Add the machines to the backend pool if they're not already vm Set Name := az . map Load Balancer Name To VM Set ( lb Name , cluster err := az . vm Set . Ensure Hosts In Pool ( service , nodes , lb Backend Pool ID , vm Set Name , is klog . V ( 2 ) . Infof ( " " , service Name , lb } 
func ( az * Cloud ) reconcile Security Group ( cluster Name string , service * v1 . Service , lb IP * string , want Lb bool ) ( * network . Security Group , error ) { service Name := get Service klog . V ( 5 ) . Infof ( " " , service Name , cluster if ports == nil { if use Shared Security ports = [ ] v1 . Service sg , err := az . get Security destination IP if want Lb && lb if lb IP != nil { destination IP Address = * lb if destination IP Address == " " { destination IP source Ranges , err := servicehelpers . Get Load Balancer Source service Tags , err := get Service var source Address if ( source Ranges == nil || servicehelpers . Is Allow All ( source Ranges ) ) && len ( service Tags ) == 0 { if ! requires Internal Load Balancer ( service ) { source Address } else { for _ , ip := range source Ranges { source Address Prefixes = append ( source Address source Address Prefixes = append ( source Address Prefixes , service expected Security Rules := [ ] network . Security if want Lb { expected Security Rules = make ( [ ] network . Security Rule , len ( ports ) * len ( source Address for i , port := range ports { _ , security Proto , _ , err := get Protocols From Kubernetes for j := range source Address Prefixes { ix := i * len ( source Address security Rule Name := az . get Security Rule Name ( service , port , source Address expected Security Rules [ ix ] = network . Security Rule { Name : to . String Ptr ( security Rule Name ) , Security Rule Properties Format : & network . Security Rule Properties Format { Protocol : * security Proto , Source Port Range : to . String Ptr ( " " ) , Destination Port Range : to . String Ptr ( strconv . Itoa ( int ( port . Port ) ) ) , Source Address Prefix : to . String Ptr ( source Address Prefixes [ j ] ) , Destination Address Prefix : to . String Ptr ( destination IP Address ) , Access : network . Security Rule Access Allow , Direction : network . Security Rule Direction for _ , r := range expected Security Rules { klog . V ( 10 ) . Infof ( " " , service . Name , * r . Source Address Prefix , * r . Source Port Range , * r . Destination Address Prefix , * r . Destination Port // update security rules dirty var updated Rules [ ] network . Security if sg . Security Group Properties Format != nil && sg . Security Group Properties Format . Security Rules != nil { updated Rules = * sg . Security Group Properties Format . Security for _ , r := range updated Rules { klog . V ( 10 ) . Infof ( " " , service . Name , log Safe ( r . Source Address Prefix ) , log Safe ( r . Source Port Range ) , log Safe Collection ( r . Destination Address Prefix , r . Destination Address Prefixes ) , log Safe ( r . Destination Port // update security rules: remove unwanted rules that belong privately // to this service for i := len ( updated Rules ) - 1 ; i >= 0 ; i -- { existing Rule := updated if az . service Owns Rule ( service , * existing Rule . Name ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing keep if find Security Rule ( expected Security Rules , existing Rule ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing keep if ! keep Rule { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * existing updated Rules = append ( updated Rules [ : i ] , updated dirty // update security rules: if the service uses a shared rule and is being deleted, // then remove it from the shared rule if use Shared Security Rule ( service ) && ! want Lb { for _ , port := range ports { for _ , source Address Prefix := range source Address Prefixes { shared Rule Name := az . get Security Rule Name ( service , port , source Address shared Index , shared Rule , shared Rule Found := find Security Rule By Name ( updated Rules , shared Rule if ! shared Rule Found { klog . V ( 4 ) . Infof ( " " , shared Rule return nil , fmt . Errorf ( " " , shared Rule if shared Rule . Destination Address existing Prefixes := * shared Rule . Destination Address address Index , found := find Index ( existing Prefixes , destination IP if ! found { klog . V ( 4 ) . Infof ( " " , destination IP Address , shared Rule return nil , fmt . Errorf ( " " , destination IP Address , shared Rule if len ( existing Prefixes ) == 1 { updated Rules = append ( updated Rules [ : shared Index ] , updated Rules [ shared } else { new Destinations := append ( existing Prefixes [ : address Index ] , existing Prefixes [ address shared Rule . Destination Address Prefixes = & new updated Rules [ shared Index ] = shared dirty // update security rules: prepare rules for consolidation for index , rule := range updated Rules { if allows Consolidation ( rule ) { updated Rules [ index ] = make for index , rule := range expected Security Rules { if allows Consolidation ( rule ) { expected Security Rules [ index ] = make // update security rules: add needed for _ , expected Rule := range expected Security Rules { found if find Security Rule ( updated Rules , expected Rule ) { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * expected found if found Rule && allows Consolidation ( expected Rule ) { index , _ := find Consolidation Candidate ( updated Rules , expected updated Rules [ index ] = consolidate ( updated Rules [ index ] , expected dirty if ! found Rule { klog . V ( 10 ) . Infof ( " " , service Name , want Lb , * expected next Available Priority , err := get Next Available Priority ( updated expected Rule . Priority = to . Int32Ptr ( next Available updated Rules = append ( updated Rules , expected dirty for _ , r := range updated Rules { klog . V ( 10 ) . Infof ( " " , service . Name , log Safe ( r . Source Address Prefix ) , log Safe ( r . Source Port Range ) , log Safe Collection ( r . Destination Address Prefix , r . Destination Address Prefixes ) , log Safe ( r . Destination Port if dirty Sg { sg . Security Rules = & updated klog . V ( 2 ) . Infof ( " " , service err := az . Create Or Update Security if err != nil { klog . V ( 2 ) . Infof ( " " , service // TODO (Nov 2017): remove when augmented security rules are out of preview // we could try to parse the response but it's not worth it for bridging a preview error if strings . Contains ( error Description , " " ) && strings . Contains ( error Description , " " ) { shared Rule Error := fmt . Errorf ( " " , error return nil , shared Rule } 
func ( az * Cloud ) reconcile Public IP ( cluster Name string , service * v1 . Service , lb * network . Load Balancer , want Lb bool ) ( * network . Public IP Address , error ) { is Internal := requires Internal Load service Name := get Service var desired Pip if ! is Internal && want Lb { desired Pip Name , err = az . determine Public IP Name ( cluster pip Resource Group := az . get Public IP Address Resource pips , err := az . List PIP ( service , pip Resource if pip . Tags != nil && ( pip . Tags ) [ " " ] != nil && * ( pip . Tags ) [ " " ] == service Name { // We need to process for pips belong to this service pip if want Lb && ! is Internal && pip Name == desired Pip Name { // This is the only case we should preserve the // Public ip resource with match service tag } else { klog . V ( 2 ) . Infof ( " " , service Name , pip err := az . safe Delete Public IP ( service , pip Resource if err != nil { klog . Errorf ( " " , pip klog . V ( 2 ) . Infof ( " " , service Name , pip if ! is Internal && want Lb { // Confirm desired public ip resource exists var pip * network . Public IP domain Name Label := get Public IP Domain Name if pip , err = az . ensure Public IP Exists ( service , desired Pip Name , domain Name } 
func ( az * Cloud ) safe Delete Public IP ( service * v1 . Service , pip Resource Group string , pip * network . Public IP Address , lb * network . Load Balancer ) error { // Remove references if pip.IP Configuration is not nil. if pip . Public IP Address Properties Format != nil && pip . Public IP Address Properties Format . IP Configuration != nil && lb != nil && lb . Load Balancer Properties Format != nil && lb . Load Balancer Properties Format . Frontend IP Configurations != nil { referenced LB Rules := [ ] network . Sub frontend IP Config load Balancer Rule // Check whether there are still frontend IP configurations referring to it. ip Configuration ID := to . String ( pip . Public IP Address Properties Format . IP if ip Configuration ID != " " { lb Frontend IP Configs := * lb . Load Balancer Properties Format . Frontend IP for i := len ( lb Frontend IP Configs ) - 1 ; i >= 0 ; i -- { config := lb Frontend IP if strings . Equal Fold ( ip Configuration ID , to . String ( config . ID ) ) { if config . Frontend IP Configuration Properties Format != nil && config . Frontend IP Configuration Properties Format . Load Balancing Rules != nil { referenced LB Rules = * config . Frontend IP Configuration Properties Format . Load Balancing frontend IP Config lb Frontend IP Configs = append ( lb Frontend IP Configs [ : i ] , lb Frontend IP if frontend IP Config Updated { lb . Load Balancer Properties Format . Frontend IP Configurations = & lb Frontend IP // Check whether there are still load balancer rules referring to it. if len ( referenced LB Rules ) > 0 { referenced LB Rule I Ds := sets . New for _ , refer := range referenced LB Rules { referenced LB Rule I if lb . Load Balancer Properties Format . Load Balancing Rules != nil { lb Rules := * lb . Load Balancer Properties Format . Load Balancing for i := len ( lb Rules ) - 1 ; i >= 0 ; i -- { rule ID := to . String ( lb if rule ID != " " && referenced LB Rule I Ds . Has ( rule ID ) { load Balancer Rule lb Rules = append ( lb Rules [ : i ] , lb if load Balancer Rule Updated { lb . Load Balancer Properties Format . Load Balancing Rules = & lb // Update load balancer when frontend IP Config Updated or load Balancer Rule Updated. if frontend IP Config Updated || load Balancer Rule Updated { err := az . Create Or Update if err != nil { klog . Errorf ( " " , get Service pip klog . V ( 10 ) . Infof ( " " , pip Resource Group , pip err := az . Delete Public IP ( service , pip Resource Group , pip if err != nil { if err = ignore Status Not Found From klog . V ( 10 ) . Infof ( " " , pip Resource Group , pip } 
func equal Load Balancing Rule Properties Format ( s * network . Load Balancing Rule Properties Format , t * network . Load Balancing Rule Properties Format , want properties := reflect . Deep Equal ( s . Protocol , t . Protocol ) && reflect . Deep Equal ( s . Frontend IP Configuration , t . Frontend IP Configuration ) && reflect . Deep Equal ( s . Backend Address Pool , t . Backend Address Pool ) && reflect . Deep Equal ( s . Load Distribution , t . Load Distribution ) && reflect . Deep Equal ( s . Frontend Port , t . Frontend Port ) && reflect . Deep Equal ( s . Backend Port , t . Backend Port ) && reflect . Deep Equal ( s . Enable Floating IP , t . Enable Floating if want LB { return properties && reflect . Deep Equal ( s . Idle Timeout In Minutes , t . Idle Timeout In } 
func find Security Rule ( rules [ ] network . Security Rule , rule network . Security Rule ) bool { for _ , existing Rule := range rules { if ! strings . Equal Fold ( to . String ( existing if existing if ! strings . Equal Fold ( to . String ( existing Rule . Source Port Range ) , to . String ( rule . Source Port if ! strings . Equal Fold ( to . String ( existing Rule . Destination Port Range ) , to . String ( rule . Destination Port if ! strings . Equal Fold ( to . String ( existing Rule . Source Address Prefix ) , to . String ( rule . Source Address if ! allows Consolidation ( existing Rule ) && ! allows Consolidation ( rule ) { if ! strings . Equal Fold ( to . String ( existing Rule . Destination Address Prefix ) , to . String ( rule . Destination Address if existing if existing } 
func requires Internal Load Balancer ( service * v1 . Service ) bool { if l , found := service . Annotations [ Service Annotation Load Balancer } 
func get Service Load Balancer Mode ( service * v1 . Service ) ( has Mode bool , is Auto bool , vm Set Names [ ] string ) { mode , has Mode := service . Annotations [ Service Annotation Load Balancer mode = strings . Trim is Auto = strings . Equal Fold ( mode , Service Annotation Load Balancer Auto Mode if ! is Auto { // Break up list of "AS1,AS2" vm Set Parsed // Trim the VM set names and remove duplicates // e.g. {"AS1"," AS2", "AS3", "AS3"} => {"AS1", "AS2", "AS3"} vm Set Name Set := sets . New for _ , v := range vm Set Parsed List { vm Set Name Set . Insert ( strings . Trim vm Set Names = vm Set Name return has Mode , is Auto , vm Set } 
func Enforce Version Policies ( version Getter Version Getter , new K8s Version Str string , new K8s Version * version . Version , allow Experimental Upgrades , allow RC Upgrades bool ) * Version Skew Policy Errors { skew Errors := & Version Skew Policy cluster Version Str , cluster Version , err := version Getter . Cluster if err != nil { // This case can't be forced: kubeadm has to be able to lookup cluster version for upgrades to work skew Errors . Mandatory = append ( skew return skew kubeadm Version Str , kubeadm Version , err := version Getter . Kubeadm if err != nil { // This case can't be forced: kubeadm has to be able to lookup its version for upgrades to work skew Errors . Mandatory = append ( skew return skew kubelet Versions , err := version Getter . Kubelet if err != nil { // This is a non-critical error; continue although kubeadm couldn't look this up skew Errors . Skippable = append ( skew // Make sure the new version is a supported version (higher than the minimum one supported) if constants . Minimum Control Plane Version . At Least ( new K8s Version ) { // This must not happen, kubeadm always supports a minimum version; and we can't go below that skew Errors . Mandatory = append ( skew Errors . Mandatory , errors . Errorf ( " " , new K8s Version Str , cluster Version // kubeadm doesn't support upgrades between two minor versions; e.g. a v1.7 -> v1.9 upgrade is not supported right away if new K8s Version . Minor ( ) > cluster Version . Minor ( ) + Maximum Allowed Minor Version Upgrade Skew { too Large Upgrade Skew Err := errors . Errorf ( " " , new K8s Version Str , Maximum Allowed Minor Version Upgrade // If the version that we're about to upgrade to is a released version, we should fully enforce this policy // If the version is a CI/dev/experimental version, it's okay to jump two minor version steps, but then require the -f flag if len ( new K8s Version . Pre Release ( ) ) == 0 { skew Errors . Mandatory = append ( skew Errors . Mandatory , too Large Upgrade Skew } else { skew Errors . Skippable = append ( skew Errors . Skippable , too Large Upgrade Skew // kubeadm doesn't support downgrades between two minor versions; e.g. a v1.9 -> v1.7 downgrade is not supported right away if new K8s Version . Minor ( ) < cluster Version . Minor ( ) - Maximum Allowed Minor Version Downgrade Skew { too Large Downgrade Skew Err := errors . Errorf ( " " , new K8s Version Str , Maximum Allowed Minor Version Downgrade // If the version that we're about to downgrade to is a released version, we should fully enforce this policy // If the version is a CI/dev/experimental version, it's okay to jump two minor version steps, but then require the -f flag if len ( new K8s Version . Pre Release ( ) ) == 0 { skew Errors . Mandatory = append ( skew Errors . Mandatory , too Large Downgrade Skew } else { skew Errors . Skippable = append ( skew Errors . Skippable , too Large Downgrade Skew // If the kubeadm version is lower than what we want to upgrade to; error if kubeadm Version . Less Than ( new K8s Version ) { if new K8s Version . Minor ( ) > kubeadm Version . Minor ( ) { too Large Kubeadm Skew := errors . Errorf ( " " , new K8s Version Str , new K8s Version . Minor ( ) , kubeadm // This is unsupported; kubeadm has no idea how it should handle a newer minor release than itself // If the version is a CI/dev/experimental version though, lower the severity of this check, but then require the -f flag if len ( new K8s Version . Pre Release ( ) ) == 0 { skew Errors . Mandatory = append ( skew Errors . Mandatory , too Large Kubeadm } else { skew Errors . Skippable = append ( skew Errors . Skippable , too Large Kubeadm } else { // Upgrading to a higher patch version than kubeadm is ok if the user specifies --force. Not recommended, but possible. skew Errors . Skippable = append ( skew Errors . Skippable , errors . Errorf ( " " , new K8s Version Str , kubeadm Version if kubeadm Version . Major ( ) > new K8s Version . Major ( ) || kubeadm Version . Minor ( ) > new K8s Version . Minor ( ) { skew Errors . Skippable = append ( skew Errors . Skippable , errors . Errorf ( " " , kubeadm Version Str , kubeadm Version . Major ( ) , kubeadm // Detect if the version is unstable and the user didn't allow that if err = detect Unstable Version Error ( new K8s Version , new K8s Version Str , allow Experimental Upgrades , allow RC Upgrades ) ; err != nil { skew Errors . Skippable = append ( skew // Detect if there are too old kubelets in the cluster // Check for nil here since this is the only case where kubelet Versions can be nil; if Kubelet Versions() returned an error // However, it's okay to skip that check if kubelet Versions != nil { if err = detect Too Old Kubelets ( new K8s Version , kubelet Versions ) ; err != nil { skew Errors . Skippable = append ( skew // If we did not see any errors, return nil if len ( skew Errors . Skippable ) == 0 && len ( skew // Uh oh, we encountered one or more errors, return them return skew } 
func detect Unstable Version Error ( new K8s Version * version . Version , new K8s Version Str string , allow Experimental Upgrades , allow RC Upgrades bool ) error { // Short-circuit quickly if this is not an unstable version if len ( new K8s Version . Pre // If the user has specified that unstable versions are fine, then no error should be returned if allow Experimental // If this is a release candidate and we allow such ones, everything's fine if strings . Has Prefix ( new K8s Version . Pre Release ( ) , " " ) && allow RC return errors . Errorf ( " " , new K8s Version } 
func detect Too Old Kubelets ( new K8s Version * version . Version , kubelet Versions map [ string ] uint16 ) error { too Old Kubelet for version Str := range kubelet Versions { kubelet Version , err := version . Parse Semantic ( version if err != nil { return errors . Errorf ( " " , version if new K8s Version . Minor ( ) > kubelet Version . Minor ( ) + Maximum Allowed Minor Version Kubelet Skew { too Old Kubelet Versions = append ( too Old Kubelet Versions , version if len ( too Old Kubelet return errors . Errorf ( " " , too Old Kubelet } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Aggregation Rule ) ( nil ) , ( * rbac . Aggregation Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Aggregation Rule_To_rbac_Aggregation Rule ( a . ( * v1alpha1 . Aggregation Rule ) , b . ( * rbac . Aggregation if err := s . Add Generated Conversion Func ( ( * rbac . Aggregation Rule ) ( nil ) , ( * v1alpha1 . Aggregation Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Aggregation Rule_To_v1alpha1_Aggregation Rule ( a . ( * rbac . Aggregation Rule ) , b . ( * v1alpha1 . Aggregation if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Cluster Role ) ( nil ) , ( * rbac . Cluster Role ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Cluster Role_To_rbac_Cluster Role ( a . ( * v1alpha1 . Cluster Role ) , b . ( * rbac . Cluster if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role ) ( nil ) , ( * v1alpha1 . Cluster Role ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role_To_v1alpha1_Cluster Role ( a . ( * rbac . Cluster Role ) , b . ( * v1alpha1 . Cluster if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Cluster Role Binding ) ( nil ) , ( * rbac . Cluster Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Cluster Role Binding_To_rbac_Cluster Role Binding ( a . ( * v1alpha1 . Cluster Role Binding ) , b . ( * rbac . Cluster Role if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role Binding ) ( nil ) , ( * v1alpha1 . Cluster Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role Binding_To_v1alpha1_Cluster Role Binding ( a . ( * rbac . Cluster Role Binding ) , b . ( * v1alpha1 . Cluster Role if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Cluster Role Binding List ) ( nil ) , ( * rbac . Cluster Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Cluster Role Binding List_To_rbac_Cluster Role Binding List ( a . ( * v1alpha1 . Cluster Role Binding List ) , b . ( * rbac . Cluster Role Binding if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role Binding List ) ( nil ) , ( * v1alpha1 . Cluster Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role Binding List_To_v1alpha1_Cluster Role Binding List ( a . ( * rbac . Cluster Role Binding List ) , b . ( * v1alpha1 . Cluster Role Binding if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Cluster Role List ) ( nil ) , ( * rbac . Cluster Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Cluster Role List_To_rbac_Cluster Role List ( a . ( * v1alpha1 . Cluster Role List ) , b . ( * rbac . Cluster Role if err := s . Add Generated Conversion Func ( ( * rbac . Cluster Role List ) ( nil ) , ( * v1alpha1 . Cluster Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Cluster Role List_To_v1alpha1_Cluster Role List ( a . ( * rbac . Cluster Role List ) , b . ( * v1alpha1 . Cluster Role if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Policy Rule ) ( nil ) , ( * rbac . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Policy Rule_To_rbac_Policy Rule ( a . ( * v1alpha1 . Policy Rule ) , b . ( * rbac . Policy if err := s . Add Generated Conversion Func ( ( * rbac . Policy Rule ) ( nil ) , ( * v1alpha1 . Policy Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Policy Rule_To_v1alpha1_Policy Rule ( a . ( * rbac . Policy Rule ) , b . ( * v1alpha1 . Policy if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Role Binding ) ( nil ) , ( * rbac . Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Role Binding_To_rbac_Role Binding ( a . ( * v1alpha1 . Role Binding ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role Binding ) ( nil ) , ( * v1alpha1 . Role Binding ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Binding_To_v1alpha1_Role Binding ( a . ( * rbac . Role Binding ) , b . ( * v1alpha1 . Role if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Role Binding List ) ( nil ) , ( * rbac . Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Role Binding List_To_rbac_Role Binding List ( a . ( * v1alpha1 . Role Binding List ) , b . ( * rbac . Role Binding if err := s . Add Generated Conversion Func ( ( * rbac . Role Binding List ) ( nil ) , ( * v1alpha1 . Role Binding List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Binding List_To_v1alpha1_Role Binding List ( a . ( * rbac . Role Binding List ) , b . ( * v1alpha1 . Role Binding if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Role List ) ( nil ) , ( * rbac . Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Role List_To_rbac_Role List ( a . ( * v1alpha1 . Role List ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role List ) ( nil ) , ( * v1alpha1 . Role List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role List_To_v1alpha1_Role List ( a . ( * rbac . Role List ) , b . ( * v1alpha1 . Role if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Role Ref ) ( nil ) , ( * rbac . Role Ref ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Role Ref_To_rbac_Role Ref ( a . ( * v1alpha1 . Role Ref ) , b . ( * rbac . Role if err := s . Add Generated Conversion Func ( ( * rbac . Role Ref ) ( nil ) , ( * v1alpha1 . Role Ref ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_rbac_Role Ref_To_v1alpha1_Role Ref ( a . ( * rbac . Role Ref ) , b . ( * v1alpha1 . Role if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Conversion if err := s . Add Conversion } 
func Convert_v1alpha1_Aggregation Rule_To_rbac_Aggregation Rule ( in * v1alpha1 . Aggregation Rule , out * rbac . Aggregation Rule , s conversion . Scope ) error { return auto Convert_v1alpha1_Aggregation Rule_To_rbac_Aggregation } 
func Convert_rbac_Aggregation Rule_To_v1alpha1_Aggregation Rule ( in * rbac . Aggregation Rule , out * v1alpha1 . Aggregation Rule , s conversion . Scope ) error { return auto Convert_rbac_Aggregation Rule_To_v1alpha1_Aggregation } 
func Convert_v1alpha1_Cluster Role_To_rbac_Cluster Role ( in * v1alpha1 . Cluster Role , out * rbac . Cluster Role , s conversion . Scope ) error { return auto Convert_v1alpha1_Cluster Role_To_rbac_Cluster } 
func Convert_rbac_Cluster Role_To_v1alpha1_Cluster Role ( in * rbac . Cluster Role , out * v1alpha1 . Cluster Role , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role_To_v1alpha1_Cluster } 
func Convert_v1alpha1_Cluster Role Binding_To_rbac_Cluster Role Binding ( in * v1alpha1 . Cluster Role Binding , out * rbac . Cluster Role Binding , s conversion . Scope ) error { return auto Convert_v1alpha1_Cluster Role Binding_To_rbac_Cluster Role } 
func Convert_rbac_Cluster Role Binding_To_v1alpha1_Cluster Role Binding ( in * rbac . Cluster Role Binding , out * v1alpha1 . Cluster Role Binding , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role Binding_To_v1alpha1_Cluster Role } 
func Convert_v1alpha1_Cluster Role Binding List_To_rbac_Cluster Role Binding List ( in * v1alpha1 . Cluster Role Binding List , out * rbac . Cluster Role Binding List , s conversion . Scope ) error { return auto Convert_v1alpha1_Cluster Role Binding List_To_rbac_Cluster Role Binding } 
func Convert_rbac_Cluster Role Binding List_To_v1alpha1_Cluster Role Binding List ( in * rbac . Cluster Role Binding List , out * v1alpha1 . Cluster Role Binding List , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role Binding List_To_v1alpha1_Cluster Role Binding } 
func Convert_v1alpha1_Cluster Role List_To_rbac_Cluster Role List ( in * v1alpha1 . Cluster Role List , out * rbac . Cluster Role List , s conversion . Scope ) error { return auto Convert_v1alpha1_Cluster Role List_To_rbac_Cluster Role } 
func Convert_rbac_Cluster Role List_To_v1alpha1_Cluster Role List ( in * rbac . Cluster Role List , out * v1alpha1 . Cluster Role List , s conversion . Scope ) error { return auto Convert_rbac_Cluster Role List_To_v1alpha1_Cluster Role } 
func Convert_v1alpha1_Policy Rule_To_rbac_Policy Rule ( in * v1alpha1 . Policy Rule , out * rbac . Policy Rule , s conversion . Scope ) error { return auto Convert_v1alpha1_Policy Rule_To_rbac_Policy } 
func Convert_rbac_Policy Rule_To_v1alpha1_Policy Rule ( in * rbac . Policy Rule , out * v1alpha1 . Policy Rule , s conversion . Scope ) error { return auto Convert_rbac_Policy Rule_To_v1alpha1_Policy } 
func Convert_v1alpha1_Role_To_rbac_Role ( in * v1alpha1 . Role , out * rbac . Role , s conversion . Scope ) error { return auto } 
func Convert_rbac_Role_To_v1alpha1_Role ( in * rbac . Role , out * v1alpha1 . Role , s conversion . Scope ) error { return auto } 
func Convert_v1alpha1_Role Binding_To_rbac_Role Binding ( in * v1alpha1 . Role Binding , out * rbac . Role Binding , s conversion . Scope ) error { return auto Convert_v1alpha1_Role Binding_To_rbac_Role } 
func Convert_rbac_Role Binding_To_v1alpha1_Role Binding ( in * rbac . Role Binding , out * v1alpha1 . Role Binding , s conversion . Scope ) error { return auto Convert_rbac_Role Binding_To_v1alpha1_Role } 
func Convert_v1alpha1_Role Binding List_To_rbac_Role Binding List ( in * v1alpha1 . Role Binding List , out * rbac . Role Binding List , s conversion . Scope ) error { return auto Convert_v1alpha1_Role Binding List_To_rbac_Role Binding } 
func Convert_rbac_Role Binding List_To_v1alpha1_Role Binding List ( in * rbac . Role Binding List , out * v1alpha1 . Role Binding List , s conversion . Scope ) error { return auto Convert_rbac_Role Binding List_To_v1alpha1_Role Binding } 
func Convert_v1alpha1_Role List_To_rbac_Role List ( in * v1alpha1 . Role List , out * rbac . Role List , s conversion . Scope ) error { return auto Convert_v1alpha1_Role List_To_rbac_Role } 
func Convert_rbac_Role List_To_v1alpha1_Role List ( in * rbac . Role List , out * v1alpha1 . Role List , s conversion . Scope ) error { return auto Convert_rbac_Role List_To_v1alpha1_Role } 
func Convert_v1alpha1_Role Ref_To_rbac_Role Ref ( in * v1alpha1 . Role Ref , out * rbac . Role Ref , s conversion . Scope ) error { return auto Convert_v1alpha1_Role Ref_To_rbac_Role } 
func Convert_rbac_Role Ref_To_v1alpha1_Role Ref ( in * rbac . Role Ref , out * v1alpha1 . Role Ref , s conversion . Scope ) error { return auto Convert_rbac_Role Ref_To_v1alpha1_Role } 
func Validate Finalizer Name ( string Value string , fld Path * field . Path ) field . Error List { all Errs := field . Error for _ , msg := range validation . Is Qualified Name ( string Value ) { all Errs = append ( all Errs , field . Invalid ( fld Path , string return all } 
func Validate Object Meta ( obj Meta * metav1 . Object Meta , requires Namespace bool , name Fn Validate Name Func , fld Path * field . Path ) field . Error List { metadata , err := meta . Accessor ( obj if err != nil { all Errs := field . Error all Errs = append ( all Errs , field . Invalid ( fld Path , obj return all return Validate Object Meta Accessor ( metadata , requires Namespace , name Fn , fld } 
func Validate Object Meta Accessor ( meta metav1 . Object , requires Namespace bool , name Fn Validate Name Func , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( meta . Get Generate Name ( ) ) != 0 { for _ , msg := range name Fn ( meta . Get Generate Name ( ) , true ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , meta . Get Generate // If the generated name validates, but the calculated value does not, it's a problem with generation, and we // report it here. This may confuse users, but indicates a programming bug and still must be validated. // If there are multiple fields out of which one is required then add an or as a separator if len ( meta . Get Name ( ) ) == 0 { all Errs = append ( all Errs , field . Required ( fld } else { for _ , msg := range name Fn ( meta . Get Name ( ) , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , meta . Get if requires Namespace { if len ( meta . Get Namespace ( ) ) == 0 { all Errs = append ( all Errs , field . Required ( fld } else { for _ , msg := range Validate Namespace Name ( meta . Get Namespace ( ) , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , meta . Get } else { if len ( meta . Get Namespace ( ) ) != 0 { all Errs = append ( all Errs , field . Forbidden ( fld if len ( meta . Get Cluster Name ( ) ) != 0 { for _ , msg := range Validate Cluster Name ( meta . Get Cluster Name ( ) , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , meta . Get Cluster for _ , entry := range meta . Get Managed Fields ( ) { all Errs = append ( all Errs , v1validation . Validate Field Manager ( entry . Manager , fld all Errs = append ( all Errs , Validate Nonnegative Field ( meta . Get Generation ( ) , fld all Errs = append ( all Errs , v1validation . Validate Labels ( meta . Get Labels ( ) , fld all Errs = append ( all Errs , Validate Annotations ( meta . Get Annotations ( ) , fld all Errs = append ( all Errs , Validate Owner References ( meta . Get Owner References ( ) , fld all Errs = append ( all Errs , Validate Finalizers ( meta . Get Finalizers ( ) , fld return all } 
func Validate Finalizers ( finalizers [ ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error has Finalizer Orphan has Finalizer Delete for _ , finalizer := range finalizers { all Errs = append ( all Errs , Validate Finalizer Name ( finalizer , fld if finalizer == metav1 . Finalizer Orphan Dependents { has Finalizer Orphan if finalizer == metav1 . Finalizer Delete Dependents { has Finalizer Delete if has Finalizer Delete Dependents && has Finalizer Orphan Dependents { all Errs = append ( all Errs , field . Invalid ( fld Path , finalizers , fmt . Sprintf ( " " , metav1 . Finalizer Orphan Dependents , metav1 . Finalizer Delete return all } 
func Validate Object Meta Update ( new Meta , old Meta * metav1 . Object Meta , fld Path * field . Path ) field . Error List { new Metadata , err := meta . Accessor ( new if err != nil { all Errs := field . Error all Errs = append ( all Errs , field . Invalid ( fld Path , new return all old Metadata , err := meta . Accessor ( old if err != nil { all Errs := field . Error all Errs = append ( all Errs , field . Invalid ( fld Path , old return all return Validate Object Meta Accessor Update ( new Metadata , old Metadata , fld } 
func New Role Binding Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Role Binding Informer ( client , namespace , resync } 
func With Timeout For Non Long Running Requests ( handler http . Handler , long Running apirequest . Long Running Request Check , timeout time . Duration ) http . Handler { if long timeout Func := func ( req * http . Request ) ( * http . Request , <- chan time . Time , func ( ) , * apierrors . Status Error ) { // TODO unify this with apiserver.Max In Flight request Info , ok := apirequest . Request Info if ! ok { // if this happens, the handler chain isn't setup correctly because there is no request info return req , time . After ( timeout ) , func ( ) { } , apierrors . New Internal if long Running ( req , request ctx , cancel := context . With req = req . With post Timeout metrics . Record ( req , request Info , metrics . API Server Component , " " , http . Status Gateway return req , time . After ( timeout ) , post Timeout Fn , apierrors . New Timeout return With Timeout ( handler , timeout } 
func With Timeout ( h http . Handler , timeout Func timeout Func ) http . Handler { return & timeout Handler { h , timeout } 
func new Hollow Node Command ( ) * cobra . Command { s := & hollow Node cmd := & cobra . Command { Use : " " , Long : " " , Run : func ( cmd * cobra . Command , args [ ] string ) { verflag . Print And Exit If s . add } 
func current Migration Rules ( ) map [ string ] string { old Recommended Home old Recommended Windows Home File := path . Join ( os . Getenv ( " " ) , Recommended Home Dir , Recommended File migration migration Rules [ Recommended Home File ] = old Recommended Home if goruntime . GOOS == " " { migration Rules [ Recommended Home File ] = old Recommended Windows Home return migration } 
func New Default Client Config Loading Rules ( ) * Client Config Loading env Var Files := os . Getenv ( Recommended Config Path Env if len ( env Var Files ) != 0 { file List := filepath . Split List ( env Var // prevent the same path load multiple times chain = append ( chain , deduplicate ( file } else { chain = append ( chain , Recommended Home return & Client Config Loading Rules { Precedence : chain , Migration Rules : current Migration } 
func ( rules * Client Config Loading Rules ) Migrate ( ) error { if rules . Migration for destination , source := range rules . Migration } else if os . Is } else if ! os . Is Not if source Info , err := os . Stat ( source ) ; err != nil { if os . Is Not Exist ( err ) || os . Is } else if source Info . Is } 
func ( rules * Client Config Loading Rules ) Get Starting Config ( ) ( * clientcmdapi . Config , error ) { client Config := New Non Interactive Deferred Loading Client Config ( rules , & Config raw Config , err := client Config . Raw if os . Is Not Exist ( err ) { return clientcmdapi . New return & raw } 
func ( rules * Client Config Loading Rules ) Get Default Filename ( ) string { // Explicit file if we have one. if rules . Is Explicit File ( ) { return rules . Get Explicit // Otherwise, first existing file from precedence. for _ , filename := range rules . Get Loading } 
func ( rules * Client Config Loading Rules ) Is Default Config ( config * restclient . Config ) bool { if rules . Default Client default Config , err := rules . Default Client Config . Client return reflect . Deep Equal ( config , default } 
func Load ( data [ ] byte ) ( * clientcmdapi . Config , error ) { config := clientcmdapi . New // if there's no data in a file, return the default object instead of failing (Decode decoded , _ , err := clientcmdlatest . Codec . Decode ( data , & schema . Group Version } 
func Write To if _ , err := os . Stat ( dir ) ; os . Is Not Exist ( err ) { if err = os . Mkdir if err := ioutil . Write } 
} 
func Resolve Local Paths ( config * clientcmdapi . Config ) error { for _ , cluster := range config . Clusters { if len ( cluster . Location Of base , err := filepath . Abs ( filepath . Dir ( cluster . Location Of if err != nil { return fmt . Errorf ( " " , cluster . Location Of if err := Resolve Paths ( Get Cluster File for _ , auth Info := range config . Auth Infos { if len ( auth Info . Location Of base , err := filepath . Abs ( filepath . Dir ( auth Info . Location Of if err != nil { return fmt . Errorf ( " " , auth Info . Location Of if err := Resolve Paths ( Get Auth Info File References ( auth } 
func Relativize Cluster Local Paths ( cluster * clientcmdapi . Cluster ) error { if len ( cluster . Location Of base , err := filepath . Abs ( filepath . Dir ( cluster . Location Of if err != nil { return fmt . Errorf ( " " , cluster . Location Of if err := Resolve Paths ( Get Cluster File if err := Relativize Path With No Backsteps ( Get Cluster File } 
func Relativize Auth Info Local Paths ( auth Info * clientcmdapi . Auth Info ) error { if len ( auth Info . Location Of Origin ) == 0 { return fmt . Errorf ( " " , auth base , err := filepath . Abs ( filepath . Dir ( auth Info . Location Of if err != nil { return fmt . Errorf ( " " , auth Info . Location Of if err := Resolve Paths ( Get Auth Info File References ( auth if err := Relativize Path With No Backsteps ( Get Auth Info File References ( auth } 
func Relativize Path With No Backsteps ( refs [ ] * string , base string ) error { for _ , ref := range refs { // Don't relativize empty paths if len ( * ref ) > 0 { rel , err := Make // if we have a backstep, don't mess with the path if strings . Has Prefix ( rel , " " ) { if filepath . Is } 
} 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Runtime Class ) ( nil ) , ( * node . Runtime Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Runtime Class_To_node_Runtime Class ( a . ( * v1alpha1 . Runtime Class ) , b . ( * node . Runtime if err := s . Add Generated Conversion Func ( ( * node . Runtime Class ) ( nil ) , ( * v1alpha1 . Runtime Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_node_Runtime Class_To_v1alpha1_Runtime Class ( a . ( * node . Runtime Class ) , b . ( * v1alpha1 . Runtime if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Runtime Class List ) ( nil ) , ( * node . Runtime Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Runtime Class List_To_node_Runtime Class List ( a . ( * v1alpha1 . Runtime Class List ) , b . ( * node . Runtime Class if err := s . Add Generated Conversion Func ( ( * node . Runtime Class List ) ( nil ) , ( * v1alpha1 . Runtime Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_node_Runtime Class List_To_v1alpha1_Runtime Class List ( a . ( * node . Runtime Class List ) , b . ( * v1alpha1 . Runtime Class if err := s . Add Conversion Func ( ( * node . Runtime Class ) ( nil ) , ( * v1alpha1 . Runtime Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_node_Runtime Class_To_v1alpha1_Runtime Class ( a . ( * node . Runtime Class ) , b . ( * v1alpha1 . Runtime if err := s . Add Conversion Func ( ( * v1alpha1 . Runtime Class ) ( nil ) , ( * node . Runtime Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Runtime Class_To_node_Runtime Class ( a . ( * v1alpha1 . Runtime Class ) , b . ( * node . Runtime } 
func Convert_v1alpha1_Runtime Class List_To_node_Runtime Class List ( in * v1alpha1 . Runtime Class List , out * node . Runtime Class List , s conversion . Scope ) error { return auto Convert_v1alpha1_Runtime Class List_To_node_Runtime Class } 
func Convert_node_Runtime Class List_To_v1alpha1_Runtime Class List ( in * node . Runtime Class List , out * v1alpha1 . Runtime Class List , s conversion . Scope ) error { return auto Convert_node_Runtime Class List_To_v1alpha1_Runtime Class } 
func ( s * stateful Set Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Stateful Set , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Stateful } 
func ( s * stateful Set Lister ) Stateful Sets ( namespace string ) Stateful Set Namespace Lister { return stateful Set Namespace } 
func scaled Value ( unscaled * big . Int , scale , new Scale int ) int64 { dif := scale - new // Handle scale down // We have to be careful about the intermediate operations. // fast path when unscaled < max.Int64 and exp(10,dif) < max.Int64 const log10Max if unscaled . Cmp ( max Int64 ) < 0 && dif < log10Max // We should only convert back to int64 when getting the result. divisor := int exp := int result := int defer func ( ) { int int int // divisor = 10^(dif) // TODO: create loop up table if exp costs too much. divisor . Exp ( big Ten , exp . Set // result = unscaled / divisor // remainder = unscaled % divisor result . Div } 
func New REST ( opts Getter generic . REST Options Getter ) * REST { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & settingsapi . Pod Preset { } } , New List Func : func ( ) runtime . Object { return & settingsapi . Pod Preset List { } } , Default Qualified Resource : settingsapi . Resource ( " " ) , Create Strategy : podpreset . Strategy , Update Strategy : podpreset . Strategy , Delete options := & generic . Store Options { REST Options : opts if err := store . Complete With } 
func ( in * Kube Scheduler Configuration ) Deep Copy Into ( out * Kube Scheduler out . Type Meta = in . Type in . Algorithm Source . Deep Copy Into ( & out . Algorithm out . Leader Election = in . Leader out . Client Connection = in . Client out . Debugging Configuration = in . Debugging if in . Bind Timeout Seconds != nil { in , out := & in . Bind Timeout Seconds , & out . Bind Timeout } 
func ( in * Kube Scheduler Configuration ) Deep Copy ( ) * Kube Scheduler out := new ( Kube Scheduler in . Deep Copy } 
func ( in * Kube Scheduler Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Kube Scheduler Leader Election Configuration ) Deep Copy Into ( out * Kube Scheduler Leader Election out . Leader Election Configuration = in . Leader Election } 
func ( in * Kube Scheduler Leader Election Configuration ) Deep Copy ( ) * Kube Scheduler Leader Election out := new ( Kube Scheduler Leader Election in . Deep Copy } 
func ( in * Scheduler Algorithm Source ) Deep Copy Into ( out * Scheduler Algorithm * out = new ( Scheduler Policy ( * in ) . Deep Copy } 
func ( in * Scheduler Algorithm Source ) Deep Copy ( ) * Scheduler Algorithm out := new ( Scheduler Algorithm in . Deep Copy } 
func ( in * Scheduler Policy Config Map Source ) Deep Copy ( ) * Scheduler Policy Config Map out := new ( Scheduler Policy Config Map in . Deep Copy } 
func ( in * Scheduler Policy File Source ) Deep Copy ( ) * Scheduler Policy File out := new ( Scheduler Policy File in . Deep Copy } 
func ( in * Scheduler Policy Source ) Deep Copy Into ( out * Scheduler Policy * out = new ( Scheduler Policy File if in . Config Map != nil { in , out := & in . Config Map , & out . Config * out = new ( Scheduler Policy Config Map } 
func ( in * Scheduler Policy Source ) Deep Copy ( ) * Scheduler Policy out := new ( Scheduler Policy in . Deep Copy } 
func Selector From for field , value := range ls { items = append ( items , & has return and } 
func Unescape Value ( s string ) ( string , error ) { // if there's no escaping or special characters, just return to avoid allocation if ! strings . Contains v := bytes . New in for _ , c := range s { if in Slash { switch c { case '\\' , ',' , '=' : // omit the \ for recognized escape sequences v . Write default : // error on unrecognized escape sequences return " " , Invalid Escape in switch c { case '\\' : in case ',' , '=' : // unescaped , and = characters are not allowed in field selector values return " " , Unescaped default : v . Write // Ending with a single backslash is an invalid sequence if in Slash { return " " , Invalid Escape } 
func Parse Selector Or Die ( s string ) Selector { selector , err := Parse } 
func Parse Selector ( selector string ) ( Selector , error ) { return parse Selector ( selector , func ( lhs , rhs string ) ( new Lhs , new } 
func Parse And Transform Selector ( selector string , fn Transform Func ) ( Selector , error ) { return parse } 
func split Terms ( field Selector string ) [ ] string { if len ( field start in for i , c := range field Selector { switch { case in Slash : in case c == '\\' : in case c == ',' : terms = append ( terms , field Selector [ start start terms = append ( terms , field Selector [ start } 
func split for _ , op := range term Operators { if strings . Has } 
func One Term Equal Selector ( k , v string ) Selector { return & has } 
func One Term Not Equal Selector ( k , v string ) Selector { return & not Has } 
func parse Group Version Kind ( s proto . Schema ) [ ] schema . Group Version Kind { extensions := s . Get gvk List Result := [ ] schema . Group Version // Get the extensions gvk Extension , ok := extensions [ group Version Kind Extension if ! ok { return [ ] schema . Group Version // gvk extension must be a list of at least 1 element. gvk List , ok := gvk if ! ok { return [ ] schema . Group Version for _ , gvk := range gvk List { // gvk extension list must be a map with group, version, and // kind fields gvk group , ok := gvk version , ok := gvk kind , ok := gvk gvk List Result = append ( gvk List Result , schema . Group Version return gvk List } 
func Get Address And } 
func new CRI Stats Provider ( cadvisor cadvisor . Interface , resource Analyzer stats . Resource Analyzer , runtime Service internalapi . Runtime Service , image Service internalapi . Image Manager Service , log Metrics Service Log Metrics Service , os Interface kubecontainer . OS Interface , ) container Stats Provider { return & cri Stats Provider { cadvisor : cadvisor , resource Analyzer : resource Analyzer , runtime Service : runtime Service , image Service : image Service , log Metrics Service : log Metrics Service , os Interface : os Interface , cpu Usage Cache : make ( map [ string ] * cpu Usage } 
func ( p * cri Stats Provider ) List Pod CPU And Memory Stats ( ) ( [ ] statsapi . Pod Stats , error ) { containers , err := p . runtime Service . List Containers ( & runtimeapi . Container // Creates pod sandbox map. pod Sandbox Map := make ( map [ string ] * runtimeapi . Pod pod Sandboxes , err := p . runtime Service . List Pod Sandbox ( & runtimeapi . Pod Sandbox for _ , s := range pod Sandboxes { pod Sandbox // sandbox ID To Pod Stats is a temporary map from sandbox ID to its pod stats. sandbox ID To Pod Stats := make ( map [ string ] * statsapi . Pod resp , err := p . runtime Service . List Container Stats ( & runtimeapi . Container Stats containers = remove Terminated // Creates container map. container for _ , c := range containers { container all Infos , err := get Cadvisor Container ca Infos := get CRI Cadvisor Stats ( all for _ , stats := range resp { container container , found := container Map [ container pod Sandbox ID := container . Pod Sandbox pod Sandbox , found := pod Sandbox Map [ pod Sandbox // Creates the stats of the pod (if not created yet) which the // container belongs to. ps , found := sandbox ID To Pod Stats [ pod Sandbox if ! found { ps = build Pod Stats ( pod sandbox ID To Pod Stats [ pod Sandbox // Fill available CPU and memory stats for full set of required pod stats cs := p . make Container CPU And Memory p . add Pod CPU Memory Stats ( ps , types . UID ( pod Sandbox . Metadata . Uid ) , all // If cadvisor stats is available for the container, use it to populate // container stats ca Stats , ca Found := ca Infos [ container if ! ca Found { klog . V ( 4 ) . Infof ( " " , container } else { p . add Cadvisor Container Stats ( cs , & ca // cleanup outdated caches. p . cleanup Outdated result := make ( [ ] statsapi . Pod Stats , 0 , len ( sandbox ID To Pod for _ , s := range sandbox ID To Pod } 
func ( p * cri Stats Provider ) Image Fs Stats ( ) ( * statsapi . Fs Stats , error ) { resp , err := p . image Service . Image Fs // CRI may return the stats of multiple image filesystems but we only // return the first one. // // TODO(yguo0905): Support returning stats of multiple image filesystems. for _ , fs := range resp { s := & statsapi . Fs Stats { Time : metav1 . New Time ( time . Unix ( 0 , fs . Timestamp ) ) , Used Bytes : & fs . Used if fs . Inodes Used != nil { s . Inodes Used = & fs . Inodes image Fs Info := p . get Fs Info ( fs . Get Fs if image Fs Info != nil { // The image filesystem id is unknown to the local node or there's // an error on retrieving the stats. In these cases, we omit those // stats and return the best-effort partial result. See // https://github.com/kubernetes/heapster/issues/1793. s . Available Bytes = & image Fs s . Capacity Bytes = & image Fs s . Inodes Free = image Fs Info . Inodes s . Inodes = image Fs } 
func ( p * cri Stats Provider ) Image Fs Device ( ) ( string , error ) { resp , err := p . image Service . Image Fs for _ , fs := range resp { fs Info := p . get Fs Info ( fs . Get Fs if fs Info != nil { return fs } 
func ( p * cri Stats Provider ) get Fs Info ( fs ID * runtimeapi . Filesystem Identifier ) * cadvisorapiv2 . Fs Info { if fs mountpoint := fs ID . Get fs Info , err := p . cadvisor . Get Dir Fs if err == cadvisorfs . Err No Such return & fs } 
func build Pod Stats ( pod Sandbox * runtimeapi . Pod Sandbox ) * statsapi . Pod Stats { return & statsapi . Pod Stats { Pod Ref : statsapi . Pod Reference { Name : pod Sandbox . Metadata . Name , UID : pod Sandbox . Metadata . Uid , Namespace : pod Sandbox . Metadata . Namespace , } , // The Start Time in the summary API is the pod creation time. Start Time : metav1 . New Time ( time . Unix ( 0 , pod Sandbox . Created } 
func ( p * cri Stats Provider ) get Container Usage Nano Cores ( stats * runtimeapi . Container p . mutex . R defer p . mutex . R cached , ok := p . cpu Usage if ! ok || cached . usage Nano // return a copy of the usage latest Usage := * cached . usage Nano return & latest } 
func ( p * cri Stats Provider ) get And Update Container Usage Nano Cores ( stats * runtimeapi . Container Stats ) * uint64 { if stats == nil || stats . Attributes == nil || stats . Cpu == nil || stats . Cpu . Usage Core Nano cached , ok := p . cpu Usage if ! ok || cached . stats . Usage Core Nano Seconds == nil { // Cannot compute the usage now, but update the cached stats anyway p . cpu Usage Cache [ id ] = & cpu Usage Record { stats : stats . Cpu , usage Nano new cached nano Seconds := new Stats . Timestamp - cached if nano Seconds <= 0 { return nil , fmt . Errorf ( " " , new Stats . Timestamp , cached usage Nano Cores := ( new Stats . Usage Core Nano Seconds . Value - cached Stats . Usage Core Nano Seconds . Value ) * uint64 ( time . Second / time . Nanosecond ) / uint64 ( nano // Update cache with new value. usage To Update := usage Nano p . cpu Usage Cache [ id ] = & cpu Usage Record { stats : new Stats , usage Nano Cores : & usage To return & usage Nano } 
func remove Terminated Container ( containers [ ] * runtimeapi . Container ) [ ] * runtimeapi . Container { container Map := make ( map [ container // Sort order by create time sort . Slice ( containers , func ( i , j int ) bool { return containers [ i ] . Created At < containers [ j ] . Created for _ , container := range containers { ref ID := container ID { pod Ref : build Pod Ref ( container . Labels ) , container Name : kubetypes . Get Container container Map [ ref ID ] = append ( container Map [ ref for _ , refs := range container for i := 0 ; i < len ( refs ) ; i ++ { if refs [ i ] . State == runtimeapi . Container } 
func ( p * cri Stats Provider ) get Pod Log Stats ( path string , root Fs Info * cadvisorapiv2 . Fs Info ) ( * statsapi . Fs Stats , error ) { files , err := p . os Interface . Read result := & statsapi . Fs Stats { Time : metav1 . New Time ( root Fs Info . Timestamp ) , Available Bytes : & root Fs Info . Available , Capacity Bytes : & root Fs Info . Capacity , Inodes Free : root Fs Info . Inodes Free , Inodes : root Fs for _ , f := range files { if f . Is fstats , err := p . get Path Fs Stats ( fpath , root Fs result . Used Bytes = add Usage ( result . Used Bytes , fstats . Used result . Inodes Used = add Usage ( result . Inodes Used , fstats . Inodes result . Time = max Update } 
func get Min Toleration Time ( tolerations [ ] v1 . Toleration ) time . Duration { min Toleration for i := range tolerations { if tolerations [ i ] . Toleration Seconds != nil { toleration Seconds := * ( tolerations [ i ] . Toleration if toleration } else if toleration Seconds < min Toleration Time || min Toleration Time == - 1 { min Toleration Time = toleration return time . Duration ( min Toleration } 
func New No Execute Taint Manager ( c clientset . Interface , get Pod Get Pod Func , get Node Get Node Func ) * No Execute Taint Manager { event Broadcaster := record . New recorder := event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : c . Core tm := & No Execute Taint Manager { client : c , recorder : recorder , get Pod : get Pod , get Node : get Node , tainted Nodes : make ( map [ string ] [ ] v1 . Taint ) , node Update Queue : workqueue . New Named ( " " ) , pod Update Queue : workqueue . New tm . taint Eviction Queue = Create Worker Queue ( delete Pod Handler ( c , tm . emit Pod Deletion } 
func ( tc * No Execute Taint Manager ) Run ( stop for i := 0 ; i < Update Worker Size ; i ++ { tc . node Update Channels = append ( tc . node Update Channels , make ( chan node Update Item , Node Update Channel tc . pod Update Channels = append ( tc . pod Update Channels , make ( chan pod Update Item , pod Update Channel // Functions that are responsible for taking work items out of the workqueues and putting them // into channels. go func ( stop Ch <- chan struct { } ) { for { item , shutdown := tc . node Update node Update := item . ( node Update hash := hash ( node Update . node Name , Update Worker select { case <- stop Ch : tc . node Update case tc . node Update Channels [ hash ] <- node Update : // tc.node Update Queue.Done is called by the node Update } ( stop go func ( stop Ch <- chan struct { } ) { for { item , shutdown := tc . pod Update pod Update := item . ( pod Update hash := hash ( pod Update . node Name , Update Worker select { case <- stop Ch : tc . pod Update case tc . pod Update Channels [ hash ] <- pod Update : // tc.pod Update Queue.Done is called by the pod Update } ( stop wg := sync . Wait wg . Add ( Update Worker for i := 0 ; i < Update Worker Size ; i ++ { go tc . worker ( i , wg . Done , stop } 
func ( tc * No Execute Taint Manager ) Pod Updated ( old Pod * v1 . Pod , new Pod * v1 . Pod ) { pod pod node old if old Pod != nil { pod Name = old pod Namespace = old node Name = old Pod . Spec . Node old Tolerations = old new if new Pod != nil { pod Name = new pod Namespace = new node Name = new Pod . Spec . Node new Tolerations = new if old Pod != nil && new Pod != nil && helper . Semantic . Deep Equal ( old Tolerations , new Tolerations ) && old Pod . Spec . Node Name == new Pod . Spec . Node update Item := pod Update Item { pod Name : pod Name , pod Namespace : pod Namespace , node Name : node tc . pod Update Queue . Add ( update } 
func ( tc * No Execute Taint Manager ) Node Updated ( old Node * v1 . Node , new Node * v1 . Node ) { node old if old Node != nil { node Name = old old Taints = get No Execute Taints ( old new if new Node != nil { node Name = new new Taints = get No Execute Taints ( new if old Node != nil && new Node != nil && helper . Semantic . Deep Equal ( old Taints , new update Item := node Update Item { node Name : node tc . node Update Queue . Add ( update } 
func make Instance Group Name ( cluster // cluster ID might be empty for legacy clusters if cluster return fmt . Sprintf ( " " , prefix , cluster } 
func Make Health Check Firewall Name ( cluster ID , hc Name string , is Nodes Health Check bool ) string { if is Nodes Health Check { return Make Nodes Health Check Name ( cluster return " " + hc } 
func New Summary Provider ( stats Provider Provider ) Summary Provider { kubelet Creation boot Time , err := util . Get Boot if err != nil { // boot return & summary Provider Impl { kubelet Creation Time : kubelet Creation Time , system Boot Time : metav1 . New Time ( boot Time ) , provider : stats } 
func ( b * photon Persistent Disk Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func ( c * photon Persistent Disk Unmounter ) Tear Down ( ) error { err := c . Tear Down At ( c . Get remove From Scsi Subsystem ( c . vol } 
func String In } 
func ( a * Always Pull Images ) Admit ( attributes admission . Attributes , o admission . Object Interfaces ) ( err error ) { // Ignore all calls to subresources or resources other than pods. if should pod , ok := attributes . Get if ! ok { return apierrors . New Bad for i := range pod . Spec . Init Containers { pod . Spec . Init Containers [ i ] . Image Pull Policy = api . Pull for i := range pod . Spec . Containers { pod . Spec . Containers [ i ] . Image Pull Policy = api . Pull } 
func ( * Always Pull Images ) Validate ( attributes admission . Attributes , o admission . Object Interfaces ) ( err error ) { if should pod , ok := attributes . Get if ! ok { return apierrors . New Bad for i := range pod . Spec . Init Containers { if pod . Spec . Init Containers [ i ] . Image Pull Policy != api . Pull Always { return admission . New Forbidden ( attributes , field . Not Supported ( field . New Path ( " " , " " ) . Index ( i ) . Child ( " " ) , pod . Spec . Init Containers [ i ] . Image Pull Policy , [ ] string { string ( api . Pull for i := range pod . Spec . Containers { if pod . Spec . Containers [ i ] . Image Pull Policy != api . Pull Always { return admission . New Forbidden ( attributes , field . Not Supported ( field . New Path ( " " , " " ) . Index ( i ) . Child ( " " ) , pod . Spec . Containers [ i ] . Image Pull Policy , [ ] string { string ( api . Pull } 
func New Always Pull Images ( ) * Always Pull Images { return & Always Pull Images { Handler : admission . New } 
func ( v * version ) Fischers ( ) Fischer Informer { return & fischer Informer { factory : v . factory , tweak List Options : v . tweak List } 
func ( v * version ) Flunders ( ) Flunder Informer { return & flunder Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func Main Usage Template ( ) string { sections := [ ] string { " \n \n " , Section Vars , Section Aliases , Section Examples , Section Subcommands , Section Flags , Section Usage , Section Tips Help , Section Tips Global return strings . Trim Right Func ( strings . Join ( sections , " " ) , unicode . Is } 
func ( mutating Webhook Configuration Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { ic := obj . ( * admissionregistration . Mutating Webhook } 
func ( mutating Webhook Configuration Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new IC := obj . ( * admissionregistration . Mutating Webhook old IC := old . ( * admissionregistration . Mutating Webhook // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. // See metav1.Object Meta description for more information on Generation. if ! reflect . Deep Equal ( old IC . Webhooks , new IC . Webhooks ) { new IC . Generation = old } 
func ( mutating Webhook Configuration Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { ic := obj . ( * admissionregistration . Mutating Webhook return validation . Validate Mutating Webhook } 
func ( mutating Webhook Configuration Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return validation . Validate Mutating Webhook Configuration Update ( obj . ( * admissionregistration . Mutating Webhook Configuration ) , old . ( * admissionregistration . Mutating Webhook } 
func Create ( c storagebackend . Config ) ( storage . Interface , Destroy case storagebackend . Storage Type Unset , storagebackend . Storage Type ETCD3 : return new } 
func Create Health case storagebackend . Storage Type Unset , storagebackend . Storage Type ETCD3 : return new ETCD3Health } 
func ( s * replica Set Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Replica Set , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Replica } 
func Is Master Node ( node Name string ) bool { // We are trying to capture "master(-...)?$" regexp. // However, using regexp.Match String() results even in more than 35% // of all space allocations in Controller Manager spent in this function. // That's why we are trying to be a bit smarter. if strings . Has Suffix ( node if len ( node Name ) >= 10 { return strings . Has Suffix ( node Name [ : len ( node } 
func Validate Custom Resource Definition ( obj * apiextensions . Custom Resource Definition ) field . Error List { name Validation Fn := func ( name string , prefix bool ) [ ] string { ret := genericvalidation . Name Is DNS required if name != required all Errs := genericvalidation . Validate Object Meta ( & obj . Object Meta , false , name Validation Fn , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Spec ( & obj . Spec , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Status ( & obj . Status , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Stored Versions ( obj . Status . Stored Versions , obj . Spec . Versions , field . New return all } 
func Validate Custom Resource Definition Update ( obj , old Obj * apiextensions . Custom Resource Definition ) field . Error List { all Errs := genericvalidation . Validate Object Meta Update ( & obj . Object Meta , & old Obj . Object Meta , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Spec Update ( & obj . Spec , & old Obj . Spec , apiextensions . Is CRD Condition True ( old Obj , apiextensions . Established ) , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Status ( & obj . Status , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Stored Versions ( obj . Status . Stored Versions , obj . Spec . Versions , field . New return all } 
func Validate Custom Resource Definition Stored Versions ( stored Versions [ ] string , versions [ ] apiextensions . Custom Resource Definition Version , fld Path * field . Path ) field . Error List { if len ( stored Versions ) == 0 { return field . Error List { field . Invalid ( fld Path , stored all Errs := field . Error stored Versions for i , v := range stored Versions { stored Versions for _ , v := range versions { _ , ok := stored Versions if v . Storage && ! ok { all Errs = append ( all Errs , field . Invalid ( fld if ok { delete ( stored Versions for v , i := range stored Versions Map { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Update Custom Resource Definition Status ( obj , old Obj * apiextensions . Custom Resource Definition ) field . Error List { all Errs := genericvalidation . Validate Object Meta Update ( & obj . Object Meta , & old Obj . Object Meta , field . New all Errs = append ( all Errs , Validate Custom Resource Definition Status ( & obj . Status , field . New return all } 
func Validate Custom Resource Definition Version ( version * apiextensions . Custom Resource Definition Version , fld Path * field . Path , status Enabled bool ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Custom Resource Definition Validation ( version . Schema , status Enabled , fld all Errs = append ( all Errs , Validate Custom Resource Definition Subresources ( version . Subresources , fld for i := range version . Additional Printer Columns { all Errs = append ( all Errs , Validate Custom Resource Column Definition ( & version . Additional Printer Columns [ i ] , fld return all } 
func Validate Custom Resource Definition Spec ( spec * apiextensions . Custom Resource Definition Spec , fld Path * field . Path ) field . Error List { return validate Custom Resource Definition Spec ( spec , true , fld } 
func has Valid Conversion Review Version Or for _ , v := range versions { if is Accepted Conversion Review } 
func Validate Custom Resource Conversion ( conversion * apiextensions . Custom Resource Conversion , fld Path * field . Path ) field . Error List { return validate Custom Resource Conversion ( conversion , true , fld } 
func Validate Custom Resource Definition Spec Update ( spec , old Spec * apiextensions . Custom Resource Definition Spec , established bool , fld Path * field . Path ) field . Error List { require Recognized Version := old Spec . Conversion == nil || has Valid Conversion Review Version Or Empty ( old Spec . Conversion . Conversion Review all Errs := validate Custom Resource Definition Spec ( spec , require Recognized Version , fld if established { // these effect the storage and cannot be changed therefore all Errs = append ( all Errs , genericvalidation . Validate Immutable Field ( spec . Scope , old Spec . Scope , fld all Errs = append ( all Errs , genericvalidation . Validate Immutable Field ( spec . Names . Kind , old Spec . Names . Kind , fld // these affects the resource name, which is always immutable, so this can't be updated. all Errs = append ( all Errs , genericvalidation . Validate Immutable Field ( spec . Group , old Spec . Group , fld all Errs = append ( all Errs , genericvalidation . Validate Immutable Field ( spec . Names . Plural , old Spec . Names . Plural , fld return all } 
func get Subresources For Version ( crd * apiextensions . Custom Resource Definition Spec , version string ) * apiextensions . Custom Resource Subresources { if ! has Per Version } 
func has Any Status Enabled ( crd * apiextensions . Custom Resource Definition Spec ) bool { if has Status for _ , v := range crd . Versions { if has Status } 
func has Status Enabled ( subresources * apiextensions . Custom Resource } 
func has Per Version Schema ( versions [ ] apiextensions . Custom Resource Definition } 
func has Per Version Subresources ( versions [ ] apiextensions . Custom Resource Definition } 
func has Per Version Columns ( versions [ ] apiextensions . Custom Resource Definition Version ) bool { for _ , v := range versions { if len ( v . Additional Printer } 
func has Identical Per Version Schema ( versions [ ] apiextensions . Custom Resource Definition for _ , v := range versions { if v . Schema == nil || ! apiequality . Semantic . Deep } 
func has Identical Per Version Subresources ( versions [ ] apiextensions . Custom Resource Definition for _ , v := range versions { if v . Subresources == nil || ! apiequality . Semantic . Deep } 
func has Identical Per Version Columns ( versions [ ] apiextensions . Custom Resource Definition value := versions [ 0 ] . Additional Printer for _ , v := range versions { if len ( v . Additional Printer Columns ) == 0 || ! apiequality . Semantic . Deep Equal ( v . Additional Printer } 
func Validate Custom Resource Definition Status ( status * apiextensions . Custom Resource Definition Status , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Custom Resource Definition Names ( & status . Accepted Names , fld return all } 
func Validate Custom Resource Definition Names ( names * apiextensions . Custom Resource Definition Names , fld Path * field . Path ) field . Error List { all Errs := field . Error if errs := validationutil . Is DNS1035Label ( names . Plural ) ; len ( names . Plural ) > 0 && len ( errs ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld if errs := validationutil . Is DNS1035Label ( names . Singular ) ; len ( names . Singular ) > 0 && len ( errs ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld if errs := validationutil . Is DNS1035Label ( strings . To Lower ( names . Kind ) ) ; len ( names . Kind ) > 0 && len ( errs ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld if errs := validationutil . Is DNS1035Label ( strings . To Lower ( names . List Kind ) ) ; len ( names . List Kind ) > 0 && len ( errs ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , names . List for i , short Name := range names . Short Names { if errs := validationutil . Is DNS1035Label ( short Name ) ; len ( errs ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) . Index ( i ) , short // kind and list Kind may not be the same or parsing become ambiguous if len ( names . Kind ) > 0 && names . Kind == names . List Kind { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , names . List for i , category := range names . Categories { if errs := validationutil . Is DNS1035Label ( category ) ; len ( errs ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Custom Resource Column Definition ( col * apiextensions . Custom Resource Column Definition , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( col . Name ) == 0 { all Errs = append ( all Errs , field . Required ( fld if len ( col . Type ) == 0 { all Errs = append ( all Errs , field . Required ( fld Path . Child ( " " ) , fmt . Sprintf ( " " , strings . Join ( printer Column } else if ! printer Column Datatypes . Has ( col . Type ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , col . Type , fmt . Sprintf ( " " , strings . Join ( printer Column if len ( col . Format ) > 0 && ! custom Resource Column Definition Formats . Has ( col . Format ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , col . Format , fmt . Sprintf ( " " , strings . Join ( custom Resource Column Definition if len ( col . JSON Path ) == 0 { all Errs = append ( all Errs , field . Required ( fld } else if errs := validate Simple JSON Path ( col . JSON Path , fld Path . Child ( " " ) ) ; len ( errs ) > 0 { all Errs = append ( all return all } 
func Validate Custom Resource Definition Validation ( custom Resource Validation * apiextensions . Custom Resource Validation , status Subresource Enabled bool , fld Path * field . Path ) field . Error List { all Errs := field . Error if custom Resource Validation == nil { return all if schema := custom Resource Validation . Open APIV3Schema ; schema != nil { // if the status subresource is enabled, only certain fields are allowed inside the root schema. // these fields are chosen such that, if status is extracted as properties["status"], it's validation is not lost. if status Subresource Enabled { v := reflect . Value for i := 0 ; i < v . Num Field ( ) ; i ++ { // skip zero values if value := v . Field ( i ) . Interface ( ) ; reflect . Deep Equal ( value , reflect . Zero ( reflect . Type field // only "object" type is valid at root of the schema since validation schema for status is extracted as properties["status"] if field Name == " " { if schema . Type != " " { all Errs = append ( all Errs , field . Invalid ( fld if ! allowed At Root Schema ( field Name ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * schema , fmt . Sprintf ( `only %v fields are allowed at the root of the schema if the status subresource is enabled` , allowed Fields At Root if schema . Nullable { all Errs = append ( all Errs , field . Forbidden ( fld open APIV3Schema := & spec Standard Validator all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( schema , fld Path . Child ( " " ) , open // if validation passed otherwise, make sure we can actually construct a schema validator from this custom resource validation. if len ( all Errs ) == 0 { if _ , _ , err := apiservervalidation . New Schema Validator ( custom Resource Validation ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Custom Resource Definition Open API Schema ( schema * apiextensions . JSON Schema Props , fld Path * field . Path , ssv spec Standard Validator ) field . Error List { all Errs := field . Error if schema == nil { return all all Errs = append ( all Errs , ssv . validate ( schema , fld if schema . Unique Items == true { all Errs = append ( all Errs , field . Forbidden ( fld // additional Properties and properties are mutual exclusive because otherwise they // contradict Kubernetes' API convention to ignore unknown fields. // // In other words: // - properties are for structs, // - additional Properties are for map[string]interface{} // // Note: when pattern Properties is added to Open API some day, this will have to be // restricted like additional Properties. if schema . Additional Properties != nil { if len ( schema . Properties ) != 0 { if schema . Additional Properties . Allows == false || schema . Additional Properties . Schema != nil { all Errs = append ( all Errs , field . Forbidden ( fld all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( schema . Additional Properties . Schema , fld if len ( schema . Properties ) != 0 { for property , json Schema := range schema . Properties { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if len ( schema . Pattern Properties ) != 0 { for property , json Schema := range schema . Pattern Properties { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if schema . Additional Items != nil { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( schema . Additional Items . Schema , fld all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( schema . Not , fld if len ( schema . All Of ) != 0 { for i , json Schema := range schema . All Of { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if len ( schema . One Of ) != 0 { for i , json Schema := range schema . One Of { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if len ( schema . Any Of ) != 0 { for i , json Schema := range schema . Any Of { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if len ( schema . Definitions ) != 0 { for definition , json Schema := range schema . Definitions { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if schema . Items != nil { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( schema . Items . Schema , fld if len ( schema . Items . JSON Schemas ) != 0 { for i , json Schema := range schema . Items . JSON Schemas { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( & json Schema , fld if schema . Dependencies != nil { for dependency , json Schema Props Or String Array := range schema . Dependencies { all Errs = append ( all Errs , Validate Custom Resource Definition Open API Schema ( json Schema Props Or String Array . Schema , fld return all } 
func ( v * spec Standard Validator V3 ) validate ( schema * apiextensions . JSON Schema Props , fld Path * field . Path ) field . Error List { all Errs := field . Error if schema == nil { return all if schema . Default != nil { all Errs = append ( all Errs , field . Forbidden ( fld if schema . ID != " " { all Errs = append ( all Errs , field . Forbidden ( fld if schema . Additional Items != nil { all Errs = append ( all Errs , field . Forbidden ( fld if len ( schema . Pattern Properties ) != 0 { all Errs = append ( all Errs , field . Forbidden ( fld if len ( schema . Definitions ) != 0 { all Errs = append ( all Errs , field . Forbidden ( fld if schema . Dependencies != nil { all Errs = append ( all Errs , field . Forbidden ( fld if schema . Ref != nil { all Errs = append ( all Errs , field . Forbidden ( fld if schema . Type == " " { all Errs = append ( all Errs , field . Forbidden ( fld if schema . Items != nil && len ( schema . Items . JSON Schemas ) != 0 { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func Validate Custom Resource Definition Subresources ( subresources * apiextensions . Custom Resource Subresources , fld Path * field . Path ) field . Error List { all Errs := field . Error if subresources == nil { return all if subresources . Scale != nil { if len ( subresources . Scale . Spec Replicas Path ) == 0 { all Errs = append ( all Errs , field . Required ( fld } else { // should be constrained json path under .spec if errs := validate Simple JSON Path ( subresources . Scale . Spec Replicas Path , fld Path . Child ( " " ) ) ; len ( errs ) > 0 { all Errs = append ( all } else if ! strings . Has Prefix ( subresources . Scale . Spec Replicas Path , " " ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , subresources . Scale . Spec Replicas if len ( subresources . Scale . Status Replicas Path ) == 0 { all Errs = append ( all Errs , field . Required ( fld } else { // should be constrained json path under .status if errs := validate Simple JSON Path ( subresources . Scale . Status Replicas Path , fld Path . Child ( " " ) ) ; len ( errs ) > 0 { all Errs = append ( all } else if ! strings . Has Prefix ( subresources . Scale . Status Replicas Path , " " ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , subresources . Scale . Status Replicas // if label Selector Path is present, it should be a constrained json path under .status if subresources . Scale . Label Selector Path != nil && len ( * subresources . Scale . Label Selector Path ) > 0 { if errs := validate Simple JSON Path ( * subresources . Scale . Label Selector Path , fld Path . Child ( " " ) ) ; len ( errs ) > 0 { all Errs = append ( all } else if ! strings . Has Prefix ( * subresources . Scale . Label Selector Path , " " ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , subresources . Scale . Label Selector return all } 
func ( e * Status Error ) Debug Error ( ) ( string , [ ] interface { } ) { if out , err := json . Marshal Indent ( e . Err return " " , [ ] interface { } { e . Err } 
func From Object ( obj runtime . Object ) error { switch t := obj . ( type ) { case * metav1 . Status : return & Status Error { Err obj := t . Unstructured if ! reflect . Deep if err := runtime . Default Unstructured Converter . From Unstructured ( t . Unstructured if status . API Version != " " && status . API return & Status Error { Err return & Unexpected Object } 
func New Forbidden ( qualified Resource schema . Group Resource , name string , err error ) * Status if qualified } else if name == " " { message = fmt . Sprintf ( " " , qualified } else { message = fmt . Sprintf ( " " , qualified return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Forbidden , Reason : metav1 . Status Reason Forbidden , Details : & metav1 . Status Details { Group : qualified Resource . Group , Kind : qualified } 
func New Apply Conflict ( causes [ ] metav1 . Status Cause , message string ) * Status Error { return & Status Error { Err Status : metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Conflict , Reason : metav1 . Status Reason Conflict , Details : & metav1 . Status } 
func New Gone ( message string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Gone , Reason : metav1 . Status Reason } 
func New Resource Expired ( message string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Gone , Reason : metav1 . Status Reason } 
func New Bad Request ( reason string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Bad Request , Reason : metav1 . Status Reason Bad } 
func New Too Many Requests ( message string , retry After Seconds int ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Too Many Requests , Reason : metav1 . Status Reason Too Many Requests , Message : message , Details : & metav1 . Status Details { Retry After Seconds : int32 ( retry After } 
func New Service Unavailable ( reason string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Service Unavailable , Reason : metav1 . Status Reason Service } 
func New Method Not Supported ( qualified Resource schema . Group Resource , action string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Method Not Allowed , Reason : metav1 . Status Reason Method Not Allowed , Details : & metav1 . Status Details { Group : qualified Resource . Group , Kind : qualified Resource . Resource , } , Message : fmt . Sprintf ( " " , action , qualified } 
func New Server Timeout ( qualified Resource schema . Group Resource , operation string , retry After Seconds int ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Internal Server Error , Reason : metav1 . Status Reason Server Timeout , Details : & metav1 . Status Details { Group : qualified Resource . Group , Kind : qualified Resource . Resource , Name : operation , Retry After Seconds : int32 ( retry After Seconds ) , } , Message : fmt . Sprintf ( " " , operation , qualified } 
func New Server Timeout For Kind ( qualified Kind schema . Group Kind , operation string , retry After Seconds int ) * Status Error { return New Server Timeout ( schema . Group Resource { Group : qualified Kind . Group , Resource : qualified Kind . Kind } , operation , retry After } 
func New Internal Error ( err error ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Internal Server Error , Reason : metav1 . Status Reason Internal Error , Details : & metav1 . Status Details { Causes : [ ] metav1 . Status } 
func New Timeout Error ( message string , retry After Seconds int ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Gateway Timeout , Reason : metav1 . Status Reason Timeout , Message : fmt . Sprintf ( " " , message ) , Details : & metav1 . Status Details { Retry After Seconds : int32 ( retry After } 
func New Too Many Requests Error ( message string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : Status Too Many Requests , Reason : metav1 . Status Reason Too Many } 
func New Request Entity Too Large Error ( message string ) * Status Error { return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : http . Status Request Entity Too Large , Reason : metav1 . Status Reason Request Entity Too } 
func New Generic Server Response ( code int , verb string , qualified Resource schema . Group Resource , name , server Message string , retry After Seconds int , is Unexpected Response bool ) * Status Error { reason := metav1 . Status Reason switch code { case http . Status Conflict : if verb == " " { reason = metav1 . Status Reason Already } else { reason = metav1 . Status Reason case http . Status Not Found : reason = metav1 . Status Reason Not case http . Status Bad Request : reason = metav1 . Status Reason Bad case http . Status Unauthorized : reason = metav1 . Status Reason case http . Status Forbidden : reason = metav1 . Status Reason // the server message has details about who is trying to perform what action. Keep its message. message = server case http . Status Not Acceptable : reason = metav1 . Status Reason Not // the server message has details about what types are acceptable message = server case http . Status Unsupported Media Type : reason = metav1 . Status Reason Unsupported Media // the server message has details about what types are acceptable message = server case http . Status Method Not Allowed : reason = metav1 . Status Reason Method Not case http . Status Unprocessable Entity : reason = metav1 . Status Reason case http . Status Service Unavailable : reason = metav1 . Status Reason Service case http . Status Gateway Timeout : reason = metav1 . Status Reason case http . Status Too Many Requests : reason = metav1 . Status Reason Too Many default : if code >= 500 { reason = metav1 . Status Reason Internal message = fmt . Sprintf ( " " , server switch { case ! qualified Resource . Empty ( ) && len ( name ) > 0 : message = fmt . Sprintf ( " " , message , strings . To Lower ( verb ) , qualified case ! qualified Resource . Empty ( ) : message = fmt . Sprintf ( " " , message , strings . To Lower ( verb ) , qualified var causes [ ] metav1 . Status if is Unexpected Response { causes = [ ] metav1 . Status Cause { { Type : metav1 . Cause Type Unexpected Server Response , Message : server return & Status Error { metav1 . Status { Status : metav1 . Status Failure , Code : int32 ( code ) , Reason : reason , Details : & metav1 . Status Details { Group : qualified Resource . Group , Kind : qualified Resource . Resource , Name : name , Causes : causes , Retry After Seconds : int32 ( retry After } 
func Is Too Many Requests ( err error ) bool { if Reason For Error ( err ) == metav1 . Status Reason Too Many switch t := err . ( type ) { case API Status : return t . Status ( ) . Code == http . Status Too Many } 
func Is Request Entity Too Large Error ( err error ) bool { if Reason For Error ( err ) == metav1 . Status Reason Request Entity Too switch t := err . ( type ) { case API Status : return t . Status ( ) . Code == http . Status Request Entity Too } 
func Is Unexpected Server Error ( err error ) bool { switch t := err . ( type ) { case API Status : if d := t . Status ( ) . Details ; d != nil { for _ , cause := range d . Causes { if cause . Type == metav1 . Cause Type Unexpected Server } 
func Is Unexpected Object Error ( err error ) bool { _ , ok := err . ( * Unexpected Object } 
func Suggests Client Delay ( err error ) ( int , bool ) { switch t := err . ( type ) { case API Status : if t . Status ( ) . Details != nil { switch t . Status ( ) . Reason { // this Status Reason explicitly requests the caller to delay the action case metav1 . Status Reason Server Timeout : return int ( t . Status ( ) . Details . Retry After // If the client requests that we retry after a certain number of seconds if t . Status ( ) . Details . Retry After Seconds > 0 { return int ( t . Status ( ) . Details . Retry After } 
func Reason For Error ( err error ) metav1 . Status Reason { switch t := err . ( type ) { case API return metav1 . Status Reason } 
func New Client Error Reporter ( code int , verb string , reason string ) * Error Reporter { return & Error } 
func ( r * Error Reporter ) As Object ( err error ) runtime . Object { status := New Generic Server Response ( r . code , r . verb , schema . Group if status . Err Status . Details == nil { status . Err Status . Details = & metav1 . Status status . Err Status . Details . Causes = append ( status . Err Status . Details . Causes , metav1 . Status Cause { Type : metav1 . Cause return & status . Err } 
func ( a * Authenticator ) Authenticate Request ( req * http . Request ) ( * authenticator . Response , bool , error ) { username , password , found := req . Basic resp , ok , err := a . auth . Authenticate // If the password authenticator didn't error, provide a default error if ! ok && err == nil { err = err Invalid } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Runtime Class ) ( nil ) , ( * node . Runtime Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Runtime Class_To_node_Runtime Class ( a . ( * v1beta1 . Runtime Class ) , b . ( * node . Runtime if err := s . Add Generated Conversion Func ( ( * node . Runtime Class ) ( nil ) , ( * v1beta1 . Runtime Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_node_Runtime Class_To_v1beta1_Runtime Class ( a . ( * node . Runtime Class ) , b . ( * v1beta1 . Runtime if err := s . Add Generated Conversion Func ( ( * v1beta1 . Runtime Class List ) ( nil ) , ( * node . Runtime Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Runtime Class List_To_node_Runtime Class List ( a . ( * v1beta1 . Runtime Class List ) , b . ( * node . Runtime Class if err := s . Add Generated Conversion Func ( ( * node . Runtime Class List ) ( nil ) , ( * v1beta1 . Runtime Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_node_Runtime Class List_To_v1beta1_Runtime Class List ( a . ( * node . Runtime Class List ) , b . ( * v1beta1 . Runtime Class } 
func Convert_v1beta1_Runtime Class_To_node_Runtime Class ( in * v1beta1 . Runtime Class , out * node . Runtime Class , s conversion . Scope ) error { return auto Convert_v1beta1_Runtime Class_To_node_Runtime } 
func Convert_node_Runtime Class_To_v1beta1_Runtime Class ( in * node . Runtime Class , out * v1beta1 . Runtime Class , s conversion . Scope ) error { return auto Convert_node_Runtime Class_To_v1beta1_Runtime } 
func Convert_v1beta1_Runtime Class List_To_node_Runtime Class List ( in * v1beta1 . Runtime Class List , out * node . Runtime Class List , s conversion . Scope ) error { return auto Convert_v1beta1_Runtime Class List_To_node_Runtime Class } 
func Convert_node_Runtime Class List_To_v1beta1_Runtime Class List ( in * node . Runtime Class List , out * v1beta1 . Runtime Class List , s conversion . Scope ) error { return auto Convert_node_Runtime Class List_To_v1beta1_Runtime Class } 
func Normalize Reduce ( max Priority int , reverse bool ) Priority Reduce Function { return func ( _ * v1 . Pod , _ interface { } , _ map [ string ] * schedulernodeinfo . Node Info , result schedulerapi . Host Priority List ) error { var max for i := range result { if result [ i ] . Score > max Count { max if max Count == 0 { if reverse { for i := range result { result [ i ] . Score = max score = max Priority * score / max if reverse { score = max } 
func ( c * Clientset ) Wardle V1alpha1 ( ) wardlev1alpha1 . Wardle V1alpha1Interface { return & fakewardlev1alpha1 . Fake Wardle } 
func ( c * Clientset ) Wardle V1beta1 ( ) wardlev1beta1 . Wardle V1beta1Interface { return & fakewardlev1beta1 . Fake Wardle } 
if m . Namespace return utilerrors . New } 
func ( m * Matcher ) Get Namespace Labels ( attr admission . Attributes ) ( map [ string ] string , error ) { // If the request itself is creating or updating a namespace, then get the // labels from attr.Object, because namespace Lister doesn't have the latest // namespace yet. // // However, if the request is deleting a namespace, then get the label from // the namespace in the namespace Lister, because a delete request is not // going to change the object, and attr.Object will be a Delete Options // rather than a namespace object. if attr . Get Resource ( ) . Resource == " " && len ( attr . Get Subresource ( ) ) == 0 && ( attr . Get Operation ( ) == admission . Create || attr . Get Operation ( ) == admission . Update ) { accessor , err := meta . Accessor ( attr . Get return accessor . Get namespace Name := attr . Get namespace , err := m . Namespace Lister . Get ( namespace if err != nil && ! apierrors . Is Not if apierrors . Is Not Found ( err ) { // in case of latency in our caches, make a call direct to storage to verify that it truly exists or not namespace , err = m . Client . Core V1 ( ) . Namespaces ( ) . Get ( namespace Name , metav1 . Get } 
func ( m * Matcher ) Match Namespace Selector ( h * v1beta1 . Webhook , attr admission . Attributes ) ( bool , * apierrors . Status Error ) { namespace Name := attr . Get if len ( namespace Name ) == 0 && attr . Get namespace Labels , err := m . Get Namespace // this means the namespace is not found, for backwards compatibility, // return a 404 if apierrors . Is Not Found ( err ) { status , ok := err . ( apierrors . API if ! ok { return false , apierrors . New Internal return false , & apierrors . Status if err != nil { return false , apierrors . New Internal // TODO: adding an LRU cache to cache the translation selector , err := metav1 . Label Selector As Selector ( h . Namespace if err != nil { return false , apierrors . New Internal return selector . Matches ( labels . Set ( namespace } 
func New Discovery REST Mapper ( group Resources [ ] * API Group Resources ) meta . REST Mapper { union Mapper := meta . Multi REST var group // /v1 is special. It should always come first resource Priority := [ ] schema . Group Version Resource { { Group : " " , Version : " " , Resource : meta . Any kind Priority := [ ] schema . Group Version Kind { { Group : " " , Version : " " , Kind : meta . Any for _ , group := range group Resources { group Priority = append ( group // Make sure the preferred version comes first if len ( group . Group . Preferred Version . Version ) != 0 { preferred := group . Group . Preferred if _ , ok := group . Versioned Resources [ preferred ] ; ok { resource Priority = append ( resource Priority , schema . Group Version Resource { Group : group . Group . Name , Version : group . Group . Preferred Version . Version , Resource : meta . Any kind Priority = append ( kind Priority , schema . Group Version Kind { Group : group . Group . Name , Version : group . Group . Preferred Version . Version , Kind : meta . Any for _ , discovery Version := range group . Group . Versions { resources , ok := group . Versioned Resources [ discovery // Add non-preferred versions after the preferred version, in case there are resources that only exist in those versions if discovery Version . Version != group . Group . Preferred Version . Version { resource Priority = append ( resource Priority , schema . Group Version Resource { Group : group . Group . Name , Version : discovery Version . Version , Resource : meta . Any kind Priority = append ( kind Priority , schema . Group Version Kind { Group : group . Group . Name , Version : discovery Version . Version , Kind : meta . Any gv := schema . Group Version { Group : group . Group . Name , Version : discovery version Mapper := meta . New Default REST Mapper ( [ ] schema . Group for _ , resource := range resources { scope := meta . REST Scope if ! resource . Namespaced { scope = meta . REST Scope plural := gv . With singular := gv . With Resource ( resource . Singular // this is for legacy resources and servers which don't list singular forms. For those we must still guess. if len ( resource . Singular Name ) == 0 { _ , singular = meta . Unsafe Guess Kind To Resource ( gv . With version Mapper . Add Specific ( gv . With Kind ( strings . To version Mapper . Add Specific ( gv . With // TODO this is producing unsafe guesses that don't actually work, but it matches previous behavior version Mapper . Add ( gv . With // TODO why is this type not in discovery (at least for "v1") version Mapper . Add ( gv . With Kind ( " " ) , meta . REST Scope union Mapper = append ( union Mapper , version for _ , group := range group Priority { resource Priority = append ( resource Priority , schema . Group Version Resource { Group : group , Version : meta . Any Version , Resource : meta . Any kind Priority = append ( kind Priority , schema . Group Version Kind { Group : group , Version : meta . Any Version , Kind : meta . Any return meta . Priority REST Mapper { Delegate : union Mapper , Resource Priority : resource Priority , Kind Priority : kind } 
func Get API Group Resources ( cl discovery . Discovery Interface ) ( [ ] * API Group Resources , error ) { gs , rs , err := cl . Server Groups And rsm := map [ string ] * metav1 . API Resource for _ , r := range rs { rsm [ r . Group var result [ ] * API Group for _ , group := range gs { group Resources := & API Group Resources { Group : * group , Versioned Resources : make ( map [ string ] [ ] metav1 . API for _ , version := range group . Versions { resources , ok := rsm [ version . Group group Resources . Versioned Resources [ version . Version ] = resources . API result = append ( result , group } 
func ( d * Deferred Discovery REST d . init defer d . init } 
func ( d * Deferred Discovery REST Mapper ) Kind For ( resource schema . Group Version Resource ) ( gvk schema . Group Version Kind , err error ) { del , err := d . get if err != nil { return schema . Group Version gvk , err = del . Kind gvk , err = d . Kind } 
func ( d * Deferred Discovery REST Mapper ) Kinds For ( resource schema . Group Version Resource ) ( gvks [ ] schema . Group Version Kind , err error ) { del , err := d . get gvks , err = del . Kinds gvks , err = d . Kinds } 
func ( d * Deferred Discovery REST Mapper ) Resource For ( input schema . Group Version Resource ) ( gvr schema . Group Version Resource , err error ) { del , err := d . get if err != nil { return schema . Group Version gvr , err = del . Resource gvr , err = d . Resource } 
func ( d * Deferred Discovery REST Mapper ) Resources For ( input schema . Group Version Resource ) ( gvrs [ ] schema . Group Version Resource , err error ) { del , err := d . get gvrs , err = del . Resources gvrs , err = d . Resources } 
func ( d * Deferred Discovery REST Mapper ) REST Mapping ( gk schema . Group Kind , versions ... string ) ( m * meta . REST Mapping , err error ) { del , err := d . get m , err = del . REST m , err = d . REST } 
func ( d * Deferred Discovery REST Mapper ) REST Mappings ( gk schema . Group Kind , versions ... string ) ( ms [ ] * meta . REST Mapping , err error ) { del , err := d . get ms , err = del . REST ms , err = d . REST } 
func ( d * Deferred Discovery REST Mapper ) Resource Singularizer ( resource string ) ( singular string , err error ) { del , err := d . get singular , err = del . Resource singular , err = d . Resource } 
func New Desired State Of World ( volume Plugin Mgr * volume . Volume Plugin Mgr ) Desired State Of World { return & desired State Of World { nodes Managed : make ( map [ k8stypes . Node Name ] node Managed ) , volume Plugin Mgr : volume Plugin } 
func ( dsw * desired State Of World ) Get Keep Terminated Pod Volumes For Node ( node Name k8stypes . Node Name ) bool { dsw . R defer dsw . R if node if node , ok := dsw . nodes Managed [ node Name ] ; ok { return node . keep Terminated Pod } 
func get Pods From Map ( pod Map map [ types . Unique Pod Name ] pod ) [ ] * v1 . Pod { pods := make ( [ ] * v1 . Pod , 0 , len ( pod for _ , pod := range pod Map { pods = append ( pods , pod . pod } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Runtime Class { } , & Runtime Class metav1 . Add To Group Version ( scheme , Scheme Group } 
func ( o * Deprecated Options ) Add Flags ( fs * pflag . Flag Set , cfg * kubeschedulerconfig . Kube Scheduler // TODO: unify deprecation mechanism, string prefix or Mark Deprecated (the latter hides the flag. We also don't want that). fs . String Var ( & o . Algorithm Provider , " " , o . Algorithm Provider , " " + factory . List Algorithm fs . String Var ( & o . Policy Config File , " " , o . Policy Config usage := fmt . Sprintf ( " " , kubeschedulerconfig . Scheduler Policy Config Map fs . String Var ( & o . Policy Config Map Name , " " , o . Policy Config Map fs . String Var ( & o . Policy Config Map Namespace , " " , o . Policy Config Map fs . Bool Var ( & o . Use Legacy Policy Config , " " , o . Use Legacy Policy fs . Bool Var ( & cfg . Enable Profiling , " " , cfg . Enable fs . Bool Var ( & cfg . Enable Contention Profiling , " " , cfg . Enable Contention fs . String Var ( & cfg . Client Connection . Kubeconfig , " " , cfg . Client fs . String Var ( & cfg . Client Connection . Content Type , " " , cfg . Client Connection . Content fs . Float32Var ( & cfg . Client Connection . QPS , " " , cfg . Client fs . Int32Var ( & cfg . Client Connection . Burst , " " , cfg . Client fs . String Var ( & cfg . Scheduler Name , " " , cfg . Scheduler fs . String Var ( & cfg . Leader Election . Lock Object Namespace , " " , cfg . Leader Election . Lock Object fs . String Var ( & cfg . Leader Election . Lock Object Name , " " , cfg . Leader Election . Lock Object fs . Int32Var ( & cfg . Hard Pod Affinity Symmetric Weight , " " , cfg . Hard Pod Affinity Symmetric fs . Mark } 
func ( o * Deprecated if o . Use Legacy Policy Config && len ( o . Policy Config File ) == 0 { errs = append ( errs , field . Required ( field . New } 
func ( o * Deprecated Options ) Apply To ( cfg * kubeschedulerconfig . Kube Scheduler switch { case o . Use Legacy Policy Config || ( len ( o . Policy Config File ) > 0 && o . Policy Config Map Name == " " ) : cfg . Algorithm Source = kubeschedulerconfig . Scheduler Algorithm Source { Policy : & kubeschedulerconfig . Scheduler Policy Source { File : & kubeschedulerconfig . Scheduler Policy File Source { Path : o . Policy Config case len ( o . Policy Config Map Name ) > 0 : cfg . Algorithm Source = kubeschedulerconfig . Scheduler Algorithm Source { Policy : & kubeschedulerconfig . Scheduler Policy Source { Config Map : & kubeschedulerconfig . Scheduler Policy Config Map Source { Name : o . Policy Config Map Name , Namespace : o . Policy Config Map case len ( o . Algorithm Provider ) > 0 : cfg . Algorithm Source = kubeschedulerconfig . Scheduler Algorithm Source { Provider : & o . Algorithm } 
func Is Only Mutating GC Fields ( obj , old runtime . Object , equalities conversion . Equalities ) bool { if old == nil || reflect . Value Of ( old ) . Is // make a copy of the new Obj so that we can stomp for comparison copied := obj . Deep Copy copied old copied Meta . Set Owner References ( old Meta . Get Owner copied Meta . Set Finalizers ( old Meta . Get copied Meta . Set Self Link ( old Meta . Get Self return equalities . Deep } 
func New Checkpoint Manager ( checkpoint Dir string ) ( Checkpoint Manager , error ) { fstore , err := utilstore . New File Store ( checkpoint Dir , utilfs . Default return & impl { path : checkpoint } 
func ( manager * impl ) Create Checkpoint ( checkpoint blob , err := checkpoint . Marshal return manager . store . Write ( checkpoint } 
func ( manager * impl ) Get Checkpoint ( checkpoint blob , err := manager . store . Read ( checkpoint if err != nil { if err == utilstore . Err Key Not Found { return errors . Err Checkpoint Not err = checkpoint . Unmarshal if err == nil { err = checkpoint . Verify } 
func ( manager * impl ) Remove Checkpoint ( checkpoint return manager . store . Delete ( checkpoint } 
func ( manager * impl ) List } 
func search _ , cur Index , err := parse Wal if index >= cur } 
func is Valid Seq ( names [ ] string ) bool { var last for _ , name := range names { cur Seq , _ , err := parse Wal if last Seq != 0 && last Seq != cur last Seq = cur } 
func get Parent Name And sub Matches := stateful Pod Regex . Find String if len ( sub parent = sub if i , err := strconv . Parse Int ( sub } 
func get Parent Name ( pod * v1 . Pod ) string { parent , _ := get Parent Name And } 
func get Ordinal ( pod * v1 . Pod ) int { _ , ordinal := get Parent Name And } 
func get Pod Name ( set * apps . Stateful } 
func get Persistent Volume Claim Name ( set * apps . Stateful Set , claim * v1 . Persistent Volume Claim , ordinal int ) string { // NOTE: This name format is used by the heuristics for zone spreading in Choose Zone For } 
func is Member Of ( set * apps . Stateful Set , pod * v1 . Pod ) bool { return get Parent } 
func identity Matches ( set * apps . Stateful Set , pod * v1 . Pod ) bool { parent , ordinal := get Parent Name And return ordinal >= 0 && set . Name == parent && pod . Name == get Pod Name ( set , ordinal ) && pod . Namespace == set . Namespace && pod . Labels [ apps . Stateful Set Pod Name } 
func storage Matches ( set * apps . Stateful Set , pod * v1 . Pod ) bool { ordinal := get for _ , claim := range set . Spec . Volume Claim if ! found || volume . Volume Source . Persistent Volume Claim == nil || volume . Volume Source . Persistent Volume Claim . Claim Name != get Persistent Volume Claim } 
func get Persistent Volume Claims ( set * apps . Stateful Set , pod * v1 . Pod ) map [ string ] v1 . Persistent Volume Claim { ordinal := get templates := set . Spec . Volume Claim claims := make ( map [ string ] v1 . Persistent Volume claim . Name = get Persistent Volume Claim if claim . Labels != nil { for key , value := range set . Spec . Selector . Match } else { claim . Labels = set . Spec . Selector . Match } 
func update Storage ( set * apps . Stateful Set , pod * v1 . Pod ) { current claims := get Persistent Volume new for name , claim := range claims { new Volumes = append ( new Volumes , v1 . Volume { Name : name , Volume Source : v1 . Volume Source { Persistent Volume Claim : & v1 . Persistent Volume Claim Volume Source { Claim Name : claim . Name , // TODO: Use source definition to set this value when we have one. Read for i := range current Volumes { if _ , ok := claims [ current Volumes [ i ] . Name ] ; ! ok { new Volumes = append ( new Volumes , current pod . Spec . Volumes = new } 
func update Identity ( set * apps . Stateful Set , pod * v1 . Pod ) { pod . Name = get Pod Name ( set , get pod . Labels [ apps . Stateful Set Pod Name } 
func is Running And Ready ( pod * v1 . Pod ) bool { return pod . Status . Phase == v1 . Pod Running && podutil . Is Pod } 
func is Failed ( pod * v1 . Pod ) bool { return pod . Status . Phase == v1 . Pod } 
func allows Burst ( set * apps . Stateful Set ) bool { return set . Spec . Pod Management Policy == apps . Parallel Pod } 
func set Pod pod . Labels [ apps . Stateful Set Revision } 
func get Pod return pod . Labels [ apps . Stateful Set Revision } 
func new Stateful Set Pod ( set * apps . Stateful Set , ordinal int ) * v1 . Pod { pod , _ := controller . Get Pod From Template ( & set . Spec . Template , set , metav1 . New Controller Ref ( set , controller pod . Name = get Pod init update } 
func new Versioned Stateful Set Pod ( current Set , update Set * apps . Stateful Set , current Revision , update Revision string , ordinal int ) * v1 . Pod { if current Set . Spec . Update Strategy . Type == apps . Rolling Update Stateful Set Strategy Type && ( current Set . Spec . Update Strategy . Rolling Update == nil && ordinal < int ( current Set . Status . Current Replicas ) ) || ( current Set . Spec . Update Strategy . Rolling Update != nil && ordinal < int ( * current Set . Spec . Update Strategy . Rolling Update . Partition ) ) { pod := new Stateful Set Pod ( current set Pod Revision ( pod , current pod := new Stateful Set Pod ( update set Pod Revision ( pod , update } 
func Match ( ss * apps . Stateful Set , history * apps . Controller Revision ) ( bool , error ) { patch , err := get } 
func get Patch ( set * apps . Stateful Set ) ( [ ] byte , error ) { str , err := runtime . Encode ( patch obj spec spec obj Copy [ " " ] = spec patch , err := json . Marshal ( obj } 
func new Revision ( set * apps . Stateful Set , revision int64 , collision Count * int32 ) ( * apps . Controller Revision , error ) { patch , err := get cr , err := history . New Controller Revision ( set , controller Kind , set . Spec . Template . Labels , runtime . Raw Extension { Raw : patch } , revision , collision if cr . Object Meta . Annotations == nil { cr . Object for key , value := range set . Annotations { cr . Object } 
func Apply Revision ( set * apps . Stateful Set , revision * apps . Controller Revision ) ( * apps . Stateful Set , error ) { clone := set . Deep patched , err := strategicpatch . Strategic Merge Patch ( [ ] byte ( runtime . Encode Or Die ( patch } 
func next Revision ( revisions [ ] * apps . Controller } 
func inconsistent Status ( set * apps . Stateful Set , status * apps . Stateful Set Status ) bool { return status . Observed Generation > set . Status . Observed Generation || status . Replicas != set . Status . Replicas || status . Current Replicas != set . Status . Current Replicas || status . Ready Replicas != set . Status . Ready Replicas || status . Updated Replicas != set . Status . Updated Replicas || status . Current Revision != set . Status . Current Revision || status . Update Revision != set . Status . Update } 
func complete Rolling Update ( set * apps . Stateful Set , status * apps . Stateful Set Status ) { if set . Spec . Update Strategy . Type == apps . Rolling Update Stateful Set Strategy Type && status . Updated Replicas == status . Replicas && status . Ready Replicas == status . Replicas { status . Current Replicas = status . Updated status . Current Revision = status . Update } 
func Write Kubelet Dynamic Env File ( cfg * kubeadmapi . Cluster Configuration , node Reg * kubeadmapi . Node Registration Options , register Taints Using Flags bool , kubelet Dir string ) error { host Name , err := nodeutil . Get flag Opts := kubelet Flags Opts { node Reg Opts : node Reg , feature Gates : cfg . Feature Gates , pause Image : images . Get Pause Image ( cfg ) , register Taints Using Flags : register Taints Using Flags , execer : utilsexec . New ( ) , pid Of Func : procfs . Pid Of , default Hostname : host string Map := build Kubelet Arg Map ( flag arg List := kubeadmutil . Build Argument List From Map ( string Map , node Reg . Kubelet Extra env File Content := fmt . Sprintf ( " \n " , constants . Kubelet Env File Variable Name , strings . Join ( arg return write Kubelet Flag Bytes To Disk ( [ ] byte ( env File Content ) , kubelet } 
func build Kubelet Arg Map ( opts kubelet Flags Opts ) map [ string ] string { kubelet if opts . node Reg Opts . CRI Socket == constants . Default Docker CRI Socket { // These flags should only be set when running docker kubelet driver , err := kubeadmutil . Get Cgroup Driver } else { kubelet if opts . pause Image != " " { kubelet Flags [ " " ] = opts . pause } else { kubelet kubelet Flags [ " " ] = opts . node Reg Opts . CRI if opts . register Taints Using Flags && opts . node Reg Opts . Taints != nil && len ( opts . node Reg Opts . Taints ) > 0 { taint for _ , taint := range opts . node Reg Opts . Taints { taint Strs = append ( taint Strs , taint . To kubelet Flags [ " " ] = strings . Join ( taint if pids , _ := opts . pid Of Func ( " " ) ; len ( pids ) > 0 { // procfs.Pid Of only returns an error if the regex is empty or doesn't compile, so we can ignore it kubelet // Make sure the node name we're passed will work with Kubelet if opts . node Reg Opts . Name != " " && opts . node Reg Opts . Name != opts . default Hostname { klog . V ( 1 ) . Infof ( " " , opts . node Reg kubelet Flags [ " " ] = opts . node Reg // TODO: Conditionally set `--cgroup-driver` to either `systemd` or `cgroupfs` for CRI other than Docker return kubelet } 
func write Kubelet Flag Bytes To Disk ( b [ ] byte , kubelet Dir string ) error { kubelet Env File Path := filepath . Join ( kubelet Dir , constants . Kubelet Env File fmt . Printf ( " \n " , kubelet Env File // creates target folder if not already exists if err := os . Mkdir All ( kubelet Dir , 0700 ) ; err != nil { return errors . Wrapf ( err , " " , kubelet if err := ioutil . Write File ( kubelet Env File Path , b , 0644 ) ; err != nil { return errors . Wrapf ( err , " " , kubelet Env File } 
func parse var effect v1 . Taint case 2 : effect = v1 . Taint if err := validate Taint parts if len ( parts key = parts if len ( parts KV ) == 2 { value = parts if errs := validation . Is Valid Label if errs := validation . Is Qualified } 
func Reorganize Taints ( node * v1 . Node , overwrite bool , taints To Add [ ] v1 . Taint , taints To Remove [ ] v1 . Taint ) ( string , [ ] v1 . Taint , error ) { new Taints := append ( [ ] v1 . Taint { } , taints To old // add taints that already existing but not updated to new Taints added := add Taints ( old Taints , & new all Errs , deleted := delete Taints ( taints To Remove , & new if ( added && deleted ) || overwrite { return MODIFIED , new Taints , utilerrors . New Aggregate ( all } else if added { return TAINTED , new Taints , utilerrors . New Aggregate ( all return UNTAINTED , new Taints , utilerrors . New Aggregate ( all } 
func delete Taints ( taints To Remove [ ] v1 . Taint , new Taints * [ ] v1 . Taint ) ( [ ] error , bool ) { all for _ , taint To Remove := range taints To if len ( taint To Remove . Effect ) > 0 { * new Taints , removed = Delete Taint ( * new Taints , & taint To } else { * new Taints , removed = Delete Taints By Key ( * new Taints , taint To if ! removed { all Errs = append ( all Errs , fmt . Errorf ( " " , taint To Remove . To return all } 
func Check If Taints Already Exists ( old Taints [ ] v1 . Taint , taints [ ] v1 . Taint ) string { var existing Taint for _ , taint := range taints { for _ , old Taint := range old Taints { if taint . Key == old Taint . Key && taint . Effect == old Taint . Effect { existing Taint List = append ( existing Taint return strings . Join ( existing Taint } 
func Delete Taints By Key ( taints [ ] v1 . Taint , taint Key string ) ( [ ] v1 . Taint , bool ) { new for i := range taints { if taint new Taints = append ( new return new } 
func Delete Taint ( taints [ ] v1 . Taint , taint To Delete * v1 . Taint ) ( [ ] v1 . Taint , bool ) { new for i := range taints { if taint To Delete . Match new Taints = append ( new return new } 
func Remove Taint ( node * v1 . Node , taint * v1 . Taint ) ( * v1 . Node , bool , error ) { new Node := node . Deep node Taints := new if len ( node Taints ) == 0 { return new if ! Taint Exists ( node Taints , taint ) { return new new Taints , _ := Delete Taint ( node new Node . Spec . Taints = new return new } 
func Taint Exists ( taints [ ] v1 . Taint , taint To Find * v1 . Taint ) bool { for _ , taint := range taints { if taint . Match Taint ( taint To } 
func parse json := json . New if metadata . UUID == " " { return nil , Err Bad } 
func Probe Attachable Volume Plugins ( ) [ ] volume . Volume Plugin { all Plugins := [ ] volume . Volume all Plugins = append ( all Plugins , awsebs . Probe Volume all Plugins = append ( all Plugins , gcepd . Probe Volume all Plugins = append ( all Plugins , cinder . Probe Volume all Plugins = append ( all Plugins , portworx . Probe Volume all Plugins = append ( all Plugins , vsphere_volume . Probe Volume all Plugins = append ( all Plugins , azure_dd . Probe Volume all Plugins = append ( all Plugins , photon_pd . Probe Volume all Plugins = append ( all Plugins , scaleio . Probe Volume all Plugins = append ( all Plugins , storageos . Probe Volume all Plugins = append ( all Plugins , fc . Probe Volume all Plugins = append ( all Plugins , iscsi . Probe Volume all Plugins = append ( all Plugins , rbd . Probe Volume if utilfeature . Default Feature Gate . Enabled ( features . CSI Persistent Volume ) { all Plugins = append ( all Plugins , csi . Probe Volume return all } 
func Get Dynamic Plugin Prober ( config persistentvolumeconfig . Volume Configuration ) volume . Dynamic Plugin Prober { return flexvolume . Get Dynamic Plugin Prober ( config . Flex Volume Plugin } 
func Probe Expandable Volume Plugins ( config persistentvolumeconfig . Volume Configuration ) [ ] volume . Volume Plugin { all Plugins := [ ] volume . Volume all Plugins = append ( all Plugins , awsebs . Probe Volume all Plugins = append ( all Plugins , gcepd . Probe Volume all Plugins = append ( all Plugins , cinder . Probe Volume all Plugins = append ( all Plugins , portworx . Probe Volume all Plugins = append ( all Plugins , vsphere_volume . Probe Volume all Plugins = append ( all Plugins , glusterfs . Probe Volume all Plugins = append ( all Plugins , rbd . Probe Volume all Plugins = append ( all Plugins , azure_dd . Probe Volume all Plugins = append ( all Plugins , azure_file . Probe Volume all Plugins = append ( all Plugins , photon_pd . Probe Volume all Plugins = append ( all Plugins , scaleio . Probe Volume all Plugins = append ( all Plugins , storageos . Probe Volume all Plugins = append ( all Plugins , fc . Probe Volume return all } 
func Probe Controller Volume Plugins ( cloud cloudprovider . Interface , config persistentvolumeconfig . Volume Configuration ) [ ] volume . Volume Plugin { all Plugins := [ ] volume . Volume // The list of plugins to probe is decided by this binary, not // by dynamic linking or other "magic". Plugins will be analyzed and // initialized later. // Each plugin can make use of Volume Config. The single arg to this func contains *all* enumerated // options meant to configure volume plugins. From that single config, create an instance of volume.Volume Config // for a specific plugin and pass that instance to the plugin's Probe Volume Plugins(config) func. // Host Path recycling is for testing and development purposes only! host Path Config := volume . Volume Config { Recycler Minimum Timeout : int ( config . Persistent Volume Recycler Configuration . Minimum Timeout Host Path ) , Recycler Timeout Increment : int ( config . Persistent Volume Recycler Configuration . Increment Timeout Host Path ) , Recycler Pod Template : volume . New Persistent Volume Recycler Pod Template ( ) , Provisioning Enabled : config . Enable Host Path if err := Attempt To Load Recycler ( config . Persistent Volume Recycler Configuration . Pod Template File Path Host Path , & host Path Config ) ; err != nil { klog . Fatalf ( " " , config . Persistent Volume Recycler Configuration . Pod Template File Path Host all Plugins = append ( all Plugins , host_path . Probe Volume Plugins ( host Path nfs Config := volume . Volume Config { Recycler Minimum Timeout : int ( config . Persistent Volume Recycler Configuration . Minimum Timeout NFS ) , Recycler Timeout Increment : int ( config . Persistent Volume Recycler Configuration . Increment Timeout NFS ) , Recycler Pod Template : volume . New Persistent Volume Recycler Pod if err := Attempt To Load Recycler ( config . Persistent Volume Recycler Configuration . Pod Template File Path NFS , & nfs Config ) ; err != nil { klog . Fatalf ( " " , config . Persistent Volume Recycler Configuration . Pod Template File Path all Plugins = append ( all Plugins , nfs . Probe Volume Plugins ( nfs all Plugins = append ( all Plugins , glusterfs . Probe Volume // add rbd provisioner all Plugins = append ( all Plugins , rbd . Probe Volume all Plugins = append ( all Plugins , quobyte . Probe Volume all Plugins = append ( all Plugins , azure_file . Probe Volume all Plugins = append ( all Plugins , flocker . Probe Volume all Plugins = append ( all Plugins , portworx . Probe Volume all Plugins = append ( all Plugins , scaleio . Probe Volume all Plugins = append ( all Plugins , local . Probe Volume all Plugins = append ( all Plugins , storageos . Probe Volume all Plugins = append ( all Plugins , awsebs . Probe Volume all Plugins = append ( all Plugins , gcepd . Probe Volume all Plugins = append ( all Plugins , cinder . Probe Volume all Plugins = append ( all Plugins , vsphere_volume . Probe Volume all Plugins = append ( all Plugins , azure_dd . Probe Volume all Plugins = append ( all Plugins , photon_pd . Probe Volume if utilfeature . Default Feature Gate . Enabled ( features . CSI Inline Volume ) { all Plugins = append ( all Plugins , csi . Probe Volume return all } 
func Attempt To Load Recycler ( path string , config * volume . Volume Config ) error { if path != " " { recycler Pod , err := volumeutil . Load Pod From if err = volume . Validate Recycler Pod Template ( recycler config . Recycler Pod Template = recycler } 
func New Cmd Set ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : set Long , Run : cmdutil . Default Sub Command Run ( streams . Err // add subcommands cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd Service cmd . Add Command ( New Cmd } 
func add To Group Version ( scheme * runtime . Scheme , group Version schema . Group Version ) error { if err := scheme . Add Ignored Conversion Type ( & metav1 . Type Meta { } , & metav1 . Type err := scheme . Add Conversion Funcs ( metav1 . Convert_string_To_labels_Selector , metav1 . Convert_labels_Selector_To_string , metav1 . Convert_string_To_fields_Selector , metav1 . Convert_fields_Selector_To_string , metav1 . Convert_Map_string_To_string_To_v1_Label Selector , metav1 . Convert_v1_Label Selector_To_Map_string_To_string , Convert_internalversion_List Options_To_v1_List Options , Convert_v1_List Options_To_internalversion_List // List Options is the only options struct which needs conversion (it exposes labels and fields // as selectors for convenience). The other types have only a single representation today. scheme . Add Known Types ( Scheme Group Version , & List Options { } , & metav1 . Get Options { } , & metav1 . Export Options { } , & metav1 . Delete Options { } , & metav1 . Create Options { } , & metav1 . Update scheme . Add Known Types ( Scheme Group Version , & metav1beta1 . Table { } , & metav1beta1 . Table Options { } , & metav1beta1 . Partial Object Metadata { } , & metav1beta1 . Partial Object Metadata scheme . Add Known Types ( metav1beta1 . Scheme Group Version , & metav1beta1 . Table { } , & metav1beta1 . Table Options { } , & metav1beta1 . Partial Object Metadata { } , & metav1beta1 . Partial Object Metadata scheme . Add Known Types ( metav1 . Scheme Group Version , & metav1 . Table { } , & metav1 . Table Options { } , & metav1 . Partial Object Metadata { } , & metav1 . Partial Object Metadata // Allow delete options to be decoded across all version in this scheme (we may want to be more clever than this) scheme . Add Unversioned Types ( Scheme Group Version , & metav1 . Delete Options { } , & metav1 . Create Options { } , & metav1 . Update metav1 . Add To Group Version ( scheme , metav1 . Scheme Group } 
func ( s * horizontal Pod Autoscaler Lister ) List ( selector labels . Selector ) ( ret [ ] * v2beta2 . Horizontal Pod Autoscaler , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v2beta2 . Horizontal Pod } 
func ( s * horizontal Pod Autoscaler Lister ) Horizontal Pod Autoscalers ( namespace string ) Horizontal Pod Autoscaler Namespace Lister { return horizontal Pod Autoscaler Namespace } 
func ( s horizontal Pod Autoscaler Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v2beta2 . Horizontal Pod Autoscaler , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v2beta2 . Horizontal Pod } 
func New Runtime Class Informer ( client versioned . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Runtime Class Informer ( client , resync } 
func New Filtered Runtime Class Informer ( client versioned . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Node V1alpha1 ( ) . Runtime } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Node V1alpha1 ( ) . Runtime } , } , & nodev1alpha1 . Runtime Class { } , resync } 
func Register ( pv Lister PV Lister , pvc Lister PVC Lister ) { register Metrics . Do ( func ( ) { prometheus . Must Register ( new PV And PVC Count Collector ( pv Lister , pvc prometheus . Must Register ( volume Operation prometheus . Must Register ( volume Operation Errors } 
func Record Volume Operation Metric ( plugin Name , op Name string , time Taken float64 , err error ) { if plugin Name == " " { plugin if err != nil { volume Operation Errors Metric . With Label Values ( plugin Name , op volume Operation Metric . With Label Values ( plugin Name , op Name ) . Observe ( time } 
func ( n * node ) mark Being Deleted ( ) { n . being Deleted defer n . being Deleted n . being } 
func ( owner Node * node ) get Dependents ( ) [ ] * node { owner Node . dependents Lock . R defer owner Node . dependents Lock . R for dep := range owner } 
func ( n * node ) blocking Dependents ( ) [ ] * node { dependents := n . get for _ , dep := range dependents { for _ , owner := range dep . owners { if owner . UID == n . identity . UID && owner . Block Owner Deletion != nil && * owner . Block Owner } 
func ( n * node ) String ( ) string { n . dependents Lock . R defer n . dependents Lock . R } 
func ( el * Endpoints Lock ) Get ( ) ( * Leader Election Record , error ) { var record Leader Election el . e , err = el . Client . Endpoints ( el . Endpoints Meta . Namespace ) . Get ( el . Endpoints Meta . Name , metav1 . Get if record Bytes , found := el . e . Annotations [ Leader Election Record Annotation Key ] ; found { if err := json . Unmarshal ( [ ] byte ( record } 
func ( el * Endpoints Lock ) Create ( ler Leader Election Record ) error { record el . e , err = el . Client . Endpoints ( el . Endpoints Meta . Namespace ) . Create ( & v1 . Endpoints { Object Meta : metav1 . Object Meta { Name : el . Endpoints Meta . Name , Namespace : el . Endpoints Meta . Namespace , Annotations : map [ string ] string { Leader Election Record Annotation Key : string ( record } 
func ( el * Endpoints Lock ) Update ( ler Leader Election record el . e . Annotations [ Leader Election Record Annotation Key ] = string ( record el . e , err = el . Client . Endpoints ( el . Endpoints } 
func ( el * Endpoints Lock ) Record Event ( s string ) { if el . Lock Config . Event events := fmt . Sprintf ( " " , el . Lock el . Lock Config . Event Recorder . Eventf ( & v1 . Endpoints { Object Meta : el . e . Object Meta } , v1 . Event Type } 
func ( el * Endpoints Lock ) Describe ( ) string { return fmt . Sprintf ( " " , el . Endpoints Meta . Namespace , el . Endpoints } 
func New Path ( name string , more for _ , another Name := range more Names { r = & Path { name : another } 
} 
func ( p * Path ) Child ( name string , more Names ... string ) * Path { r := New Path ( name , more } 
} 
} 
// iterate, but it has to be backwards buf := bytes . New if p . parent != nil && len ( p . name ) > 0 { // This is either the root or it is a subscript. buf . Write if len ( p . name ) > 0 { buf . Write } 
func ( g * Cloud ) Create Instance Group ( ig * compute . Instance Group , zone string ) error { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric return mc . Observe ( g . c . Instance Groups ( ) . Insert ( ctx , meta . Zonal } 
func ( g * Cloud ) Delete Instance Group ( name string , zone string ) error { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric return mc . Observe ( g . c . Instance Groups ( ) . Delete ( ctx , meta . Zonal } 
func ( g * Cloud ) List Instance Groups ( zone string ) ( [ ] * compute . Instance Group , error ) { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric v , err := g . c . Instance } 
func ( g * Cloud ) List Instances In Instance Group ( name string , zone string , state string ) ( [ ] * compute . Instance With Named Ports , error ) { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric req := & compute . Instance Groups List Instances Request { Instance v , err := g . c . Instance Groups ( ) . List Instances ( ctx , meta . Zonal } 
func ( g * Cloud ) Add Instances To Instance Group ( name string , zone string , instance Refs [ ] * compute . Instance Reference ) error { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric // TODO: should cull operation above this layer. if len ( instance req := & compute . Instance Groups Add Instances Request { Instances : instance return mc . Observe ( g . c . Instance Groups ( ) . Add Instances ( ctx , meta . Zonal } 
func ( g * Cloud ) Set Named Ports Of Instance Group ( ig Name , zone string , named Ports [ ] * compute . Named Port ) error { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric req := & compute . Instance Groups Set Named Ports Request { Named Ports : named return mc . Observe ( g . c . Instance Groups ( ) . Set Named Ports ( ctx , meta . Zonal Key ( ig } 
func ( g * Cloud ) Get Instance Group ( name string , zone string ) ( * compute . Instance Group , error ) { ctx , cancel := cloud . Context With Call mc := new Instance Group Metric v , err := g . c . Instance Groups ( ) . Get ( ctx , meta . Zonal } 
func Vendorless ( p string ) string { if pos := strings . Last } 
func Lookup Container Port Number By Name ( pod v1 . Pod , name string ) ( int32 , error ) { for _ , ctr := range pod . Spec . Containers { for _ , ctrportspec := range ctr . Ports { if ctrportspec . Name == name { return ctrportspec . Container } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Webhook Admission ) ( nil ) , ( * webhookadmission . Webhook Admission ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Webhook Admission_To_webhookadmission_Webhook Admission ( a . ( * Webhook Admission ) , b . ( * webhookadmission . Webhook if err := s . Add Generated Conversion Func ( ( * webhookadmission . Webhook Admission ) ( nil ) , ( * Webhook Admission ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_webhookadmission_Webhook Admission_To_v1alpha1_Webhook Admission ( a . ( * webhookadmission . Webhook Admission ) , b . ( * Webhook } 
func Convert_v1alpha1_Webhook Admission_To_webhookadmission_Webhook Admission ( in * Webhook Admission , out * webhookadmission . Webhook Admission , s conversion . Scope ) error { return auto Convert_v1alpha1_Webhook Admission_To_webhookadmission_Webhook } 
func Convert_webhookadmission_Webhook Admission_To_v1alpha1_Webhook Admission ( in * webhookadmission . Webhook Admission , out * Webhook Admission , s conversion . Scope ) error { return auto Convert_webhookadmission_Webhook Admission_To_v1alpha1_Webhook } 
func Parse Certs PEM ( pem for len ( pem block , pem Certs = pem . Decode ( pem // Only use PEM "CERTIFICATE" blocks without extra headers if block . Type != Certificate Block cert , err := x509 . Parse } 
func Canonical if ! has Port ( addr ) { return addr + " " + port } 
func syntax Wrap ( input string ) string { return string ( operator ) + string ( reference Opener ) + input + string ( reference } 
func Mapping Func return syntax } 
for cursor := 0 ; cursor < len ( input ) ; cursor ++ { if input [ cursor ] == operator && cursor + 1 < len ( input ) { // Copy the portion of the input string since the last // checkpoint into the buffer buf . Write // Attempt to read the variable name as defined by the // syntax from the input string read , is Var , advance := try Read Variable if is Var { // We were able to read a variable name correctly; // apply the mapping to the variable name and copy the // bytes into the buffer buf . Write } else { // Not a variable name; copy the read bytes into the buffer buf . Write } 
func try Read Variable case reference Opener : // Scan to expression closer for i := 1 ; i < len ( input ) ; i ++ { if input [ i ] == reference // Incomplete reference; return it. return string ( operator ) + string ( reference } 
func ( sb * Scheme Builder ) Add To } 
func ( sb * Scheme } 
func New Scheme Builder ( funcs ... func ( * Scheme ) error ) Scheme Builder { var sb Scheme } 
func Get Supported for s := range completion } 
func New Cmd Completion ( out io . Writer , boiler Plate string ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Long : completion Long , Example : completion Example , Run : func ( cmd * cobra . Command , args [ ] string ) { err := Run Completion ( out , boiler kubeadmutil . Check } , Valid Args : Get Supported } 
func Run Completion ( out io . Writer , boiler run , found := completion if len ( boiler Plate ) == 0 { boiler Plate = default Boiler if _ , err := out . Write ( [ ] byte ( boiler } 
func Lookup Patch Metadata For Struct ( t reflect . Type , json Field string ) ( elem Type reflect . Type , patch Strategies [ ] string , patch Merge jf := [ ] byte ( json fields := cached Type if bytes . Equal ( ff . name // Do case-insensitive comparison. if f == nil && ff . equal Fold ( ff . name patch Strategy := tjf . Tag . Get ( patch Strategy Tag patch Merge Key = tjf . Tag . Get ( patch Merge Key Tag patch Strategies = strings . Split ( patch elem e = fmt . Errorf ( " " , t . Name ( ) , json } 
func dominant } 
func cached Type Fields ( t reflect . Type ) [ ] field { field Cache . R f := field field Cache . R // Compute fields without lock. // Might duplicate effort but won't hold other computations back. f = type field if field Cache . m == nil { field field field } 
func fold Func ( s [ ] byte ) func ( s , t [ ] byte ) bool { non for _ , b := range s { if b >= utf8 . Rune Self { return bytes . Equal upper := b & case if upper < 'A' || upper > 'Z' { non if special { return equal Fold if non Letter { return ascii Equal return simple Letter Equal } 
func equal Fold if tb < utf8 . Rune Self { if sb != tb { sb Upper := sb & case if 'A' <= sb Upper && sb Upper <= 'Z' { if sb Upper != tb & case // sb is ASCII and t is not. t must be either kelvin // sign or long s; sb must be s, S, k, or K. tr , size := utf8 . Decode switch sb { case 's' , 'S' : if tr != small Long } 
func ascii Equal if ( 'a' <= sb && sb <= 'z' ) || ( 'A' <= sb && sb <= 'Z' ) { if sb & case Mask != tb & case } 
func simple Letter Equal for i , b := range s { if b & case Mask != t [ i ] & case } 
func ( c * Apps V1Client ) REST return c . rest } 
func ( g * gen Fake For } 
func ( g * Cloud ) Get Beta Security Policy ( name string ) ( * computebeta . Security Policy , error ) { ctx , cancel := cloud . Context With Call mc := new Security Policy Metric Context With Version ( " " , compute Beta v , err := g . c . Beta Security Policies ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) List Beta Security Policy ( ) ( [ ] * computebeta . Security Policy , error ) { ctx , cancel := cloud . Context With Call mc := new Security Policy Metric Context With Version ( " " , compute Beta v , err := g . c . Beta Security } 
func ( g * Cloud ) Create Beta Security Policy ( sp * computebeta . Security Policy ) error { ctx , cancel := cloud . Context With Call mc := new Security Policy Metric Context With Version ( " " , compute Beta return mc . Observe ( g . c . Beta Security Policies ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) Delete Beta Security Policy ( name string ) error { ctx , cancel := cloud . Context With Call mc := new Security Policy Metric Context With Version ( " " , compute Beta return mc . Observe ( g . c . Beta Security Policies ( ) . Delete ( ctx , meta . Global } 
func ( g * Cloud ) Get Rule For Beta Security Policy ( name string ) ( * computebeta . Security Policy Rule , error ) { ctx , cancel := cloud . Context With Call mc := new Security Policy Metric Context With Version ( " " , compute Beta v , err := g . c . Beta Security Policies ( ) . Get Rule ( ctx , meta . Global } 
func ( g * Cloud ) Add Ruleto Beta Security Policy ( name string , spr * computebeta . Security Policy Rule ) error { ctx , cancel := cloud . Context With Call mc := new Security Policy Metric Context With Version ( " " , compute Beta return mc . Observe ( g . c . Beta Security Policies ( ) . Add Rule ( ctx , meta . Global } 
func ( c * Fake Runtime Classes ) List ( opts v1 . List Options ) ( result * v1beta1 . Runtime Class List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( runtimeclasses Resource , runtimeclasses Kind , opts ) , & v1beta1 . Runtime Class label , _ , _ := testing . Extract From List list := & v1beta1 . Runtime Class List { List Meta : obj . ( * v1beta1 . Runtime Class List ) . List for _ , item := range obj . ( * v1beta1 . Runtime Class } 
func ( c * Fake Runtime Classes ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( runtimeclasses } 
func ( c * Fake Runtime Classes ) Update ( runtime Class * v1beta1 . Runtime Class ) ( result * v1beta1 . Runtime Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( runtimeclasses Resource , runtime Class ) , & v1beta1 . Runtime return obj . ( * v1beta1 . Runtime } 
func ( c * Fake Runtime Classes ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( runtimeclasses Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Runtime Class } 
func ( s * deployment Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Deployment , err error ) { err = cache . List } 
func ( s * deployment Lister ) Deployments ( namespace string ) Deployment Namespace Lister { return deployment Namespace } 
func ( s deployment Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Deployment , err error ) { err = cache . List All By } 
func init Verifier ( ctx context . Context , config * oidc . Config , iss string ) ( * oidc . ID Token Verifier , error ) { provider , err := oidc . New } 
func new Async ID Token Verifier ( ctx context . Context , c * oidc . Config , iss string ) * async ID Token Verifier { t := & async ID Token // Polls indefinitely in an attempt to initialize the distributed claims // verifier, or until context canceled. init v , err := init go func ( ) { if done , _ := init Fn ( ) ; ! done { go wait . Poll Until ( time . Second * 10 , init if synchronize Token ID Verifier For } 
func ( a * async ID Token Verifier ) verifier ( ) * oidc . ID Token } 
func untrusted payload , err := base64 . Raw URL Encoding . Decode // Coalesce the legacy Google Iss with the new one. // // http://openid.net/specs/openid-connect-core-1_0.html#Google } 
func new Claim Resolver ( claim string , client * http . Client , config * oidc . Config ) * claim Resolver { return & claim Resolver { claim : claim , client : client , config : config , verifier Per Issuer : map [ string ] * async ID Token } 
func ( r * claim Resolver ) Verifier ( iss string ) ( * oidc . ID Token av := r . verifier Per if av == nil { // This lazy init should normally be very quick. // TODO: Make this context cancelable. ctx := oidc . Client av = new Async ID Token r . verifier Per } 
func ( r * claim Resolver ) expand ( c claims ) error { const ( // The claim containing a map of endpoint references per claim. // OIDC Connect Core 1.0, section 5.6.2. claim Names // The claim containing endpoint specifications. // OIDC Connect Core 1.0, section 5.6.2. claim Sources names , ok := c [ claim Names claim To if err := json . Unmarshal ( [ ] byte ( names ) , & claim To raw Sources , ok := c [ claim Sources if err := json . Unmarshal ( [ ] byte ( raw src , ok := claim To } 
func ( r * claim Resolver ) resolve ( endpoint endpoint , all Claims claims ) error { // TODO: cache resolved claims. jwt , err := get Claim JWT ( r . client , endpoint . URL , endpoint . Access untrusted Iss , err := untrusted v , err := r . Verifier ( untrusted if err != nil { return fmt . Errorf ( " " , untrusted var dist if err := t . Claims ( & dist value , ok := dist all } 
func get Claim JWT ( client * http . Client , url , access Token string ) ( string , error ) { ctx , cancel := context . With // TODO: Allow passing request body with configurable information. req , err := http . New if access Token != " " { req . Header . Set ( " " , fmt . Sprintf ( " " , access req = req . With // Report non-OK status code as an error. if response . Status Code < http . Status OK || response . Status Code > http . Status IM response Bytes , err := ioutil . Read return string ( response } 
func New Docker Client From Config ( config * Client Config ) libdocker . Interface { if config != nil { // Create docker client. client := libdocker . Connect To Docker Or Die ( config . Docker Endpoint , config . Runtime Request Timeout , config . Image Pull Progress Deadline , config . With Trace Disabled , config . Enable } 
func New Docker Service ( config * Client Config , pod Sandbox Image string , streaming Config * streaming . Config , plugin Settings * Network Plugin Settings , cgroups Name string , kube Cgroup Driver string , dockershim Root Dir string , start Local Streaming Server bool ) ( Docker Service , error ) { client := New Docker Client From c := libdocker . New Instrumented checkpoint Manager , err := checkpointmanager . New Checkpoint Manager ( filepath . Join ( dockershim Root Dir , sandbox Checkpoint ds := & docker Service { client : c , os : kubecontainer . Real OS { } , pod Sandbox Image : pod Sandbox Image , streaming Runtime : & streaming Runtime { client : client , exec Handler : & Native Exec Handler { } , } , container Manager : cm . New Container Manager ( cgroups Name , client ) , checkpoint Manager : checkpoint Manager , start Local Streaming Server : start Local Streaming Server , network Ready : make ( map [ string ] bool ) , container Cleanup Infos : make ( map [ string ] * container Cleanup // check docker version compatibility. if err = ds . check Version // create streaming server if configured. if streaming ds . streaming Server , err = streaming . New Server ( * streaming Config , ds . streaming // Determine the hairpin mode. if err := effective Hairpin Mode ( plugin klog . Infof ( " " , plugin Settings . Hairpin // dockershim currently only supports CNI plugins. plugin Settings . Plugin Bin Dirs = cni . Split Dirs ( plugin Settings . Plugin Bin Dir cni Plugins := cni . Probe Network Plugins ( plugin Settings . Plugin Conf Dir , plugin Settings . Plugin Bin cni Plugins = append ( cni Plugins , kubenet . New Plugin ( plugin Settings . Plugin Bin net Host := & docker Network Host { & namespace Getter { ds } , & port Mapping plug , err := network . Init Network Plugin ( cni Plugins , plugin Settings . Plugin Name , net Host , plugin Settings . Hairpin Mode , plugin Settings . Non Masquerade CIDR , plugin if err != nil { return nil , fmt . Errorf ( " " , plugin ds . network = network . New Plugin // NOTE: cgroup driver is only detectable in docker 1.11+ cgroup Driver := default Cgroup docker klog . Infof ( " " , docker klog . Warningf ( " " , cgroup } else if len ( docker Info . Cgroup klog . Warningf ( " " , cgroup } else { cgroup Driver = docker Info . Cgroup if len ( kube Cgroup Driver ) != 0 && kube Cgroup Driver != cgroup Driver { return nil , fmt . Errorf ( " " , kube Cgroup Driver , cgroup klog . Infof ( " " , cgroup ds . cgroup Driver = cgroup ds . version Cache = cache . New Object Cache ( func ( ) ( interface { } , error ) { return ds . get Docker } , version Cache } 
func ( ds * docker Service ) Version ( _ context . Context , r * runtimeapi . Version Request ) ( * runtimeapi . Version Response , error ) { v , err := ds . get Docker return & runtimeapi . Version Response { Version : kube API Version , Runtime Name : docker Runtime Name , Runtime Version : v . Version , Runtime Api Version : v . API } 
func ( ds * docker Service ) get Docker // Docker API version (e.g., 1.23) is not semver compatible. Add a ".0" // suffix to remedy this. v . API Version = fmt . Sprintf ( " " , v . API } 
func ( ds * docker Service ) Update Runtime Config ( _ context . Context , r * runtimeapi . Update Runtime Config Request ) ( * runtimeapi . Update Runtime Config Response , error ) { runtime Config := r . Get Runtime if runtime Config == nil { return & runtimeapi . Update Runtime Config klog . Infof ( " " , runtime if ds . network != nil && runtime Config . Network Config . Pod event [ network . NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR ] = runtime Config . Network Config . Pod return & runtimeapi . Update Runtime Config } 
func ( ds * docker Service ) Get Net NS ( pod Sandbox ID string ) ( string , error ) { r , err := ds . client . Inspect Container ( pod Sandbox return get Network } 
func ( ds * docker Service ) Get Pod Port Mappings ( pod Sandbox ID string ) ( [ ] * hostport . Port Mapping , error ) { // TODO: get portmappings from docker labels for backward compatibility checkpoint := New Pod Sandbox Checkpoint ( " " , " " , & Checkpoint err := ds . checkpoint Manager . Get Checkpoint ( pod Sandbox // Return empty port Mappings if checkpoint is not found if err != nil { if err == errors . Err Checkpoint Not err Rem := ds . checkpoint Manager . Remove Checkpoint ( pod Sandbox if err Rem != nil { klog . Errorf ( " " , pod Sandbox ID , err _ , _ , _ , checkpointed Port Mappings , _ := checkpoint . Get port Mappings := make ( [ ] * hostport . Port Mapping , 0 , len ( checkpointed Port for _ , pm := range checkpointed Port Mappings { proto := to API port Mappings = append ( port Mappings , & hostport . Port Mapping { Host Port : * pm . Host Port , Container Port : * pm . Container Port , Protocol : proto , Host IP : pm . Host return port } 
func ( ds * docker Service ) Start ( ) error { ds . init // Initialize the legacy cleanup flag. if ds . start Local Streaming Server { go func ( ) { if err := ds . streaming return ds . container } 
func ( ds * docker Service ) init Cleanup ( ) { errors := ds . platform Specific Container Init } 
func ( ds * docker Service ) Status ( _ context . Context , r * runtimeapi . Status Request ) ( * runtimeapi . Status Response , error ) { runtime Ready := & runtimeapi . Runtime Condition { Type : runtimeapi . Runtime network Ready := & runtimeapi . Runtime Condition { Type : runtimeapi . Network conditions := [ ] * runtimeapi . Runtime Condition { runtime Ready , network if _ , err := ds . client . Version ( ) ; err != nil { runtime runtime runtime if err := ds . network . Status ( ) ; err != nil { network network network status := & runtimeapi . Runtime return & runtimeapi . Status } 
func ( ds * docker Service ) Generate Expected Cgroup Parent ( cgroup Parent string ) ( string , error ) { if cgroup Parent != " " { // if docker uses the systemd cgroup driver, it expects *.slice style names for cgroup parent. // if we configured kubelet to use --cgroup-driver=cgroupfs, and docker is configured to use systemd driver // docker will fail to launch the container because the name we provide will not be a valid slice. // this is a very good thing. if ds . cgroup Driver == " " { // Pass only the last component of the cgroup path to systemd. cgroup Parent = path . Base ( cgroup klog . V ( 3 ) . Infof ( " " , cgroup return cgroup } 
func ( ds * docker Service ) check Version Compatibility ( ) error { api Version , err := ds . get Docker API min API Version , err := semver . Parse ( libdocker . Minimum Docker API // Verify the docker version. result := api Version . Compare ( min API if result < 0 { return fmt . Errorf ( " " , libdocker . Minimum Docker API } 
func ( ds * docker Service ) get Docker API if ds . version Cache != nil { dv , err = ds . get Docker Version From } else { dv , err = ds . get Docker api Version , err := semver . Parse ( dv . API return & api } 
func effective Hairpin Mode ( s * Network Plugin Settings ) error { // The hairpin mode setting doesn't matter if: // - We're not using a bridge network. This is hard to check because we might // be using a plugin. // - It's set to hairpin-veth for a container runtime that doesn't know how // to set the hairpin flag on the veth's of containers. Currently the // docker runtime is the only one that understands this. // - It's set to "none". if s . Hairpin Mode == kubeletconfig . Promiscuous Bridge || s . Hairpin Mode == kubeletconfig . Hairpin Veth { if s . Hairpin Mode == kubeletconfig . Promiscuous Bridge && s . Plugin Name != " " { // This is not a valid combination, since promiscuous-bridge only works on kubenet. Users might be using the // default values (from before the hairpin-mode flag existed) and we // should keep the old behavior. klog . Warningf ( " " , s . Hairpin Mode , kubeletconfig . Hairpin s . Hairpin Mode = kubeletconfig . Hairpin } else if s . Hairpin Mode != kubeletconfig . Hairpin None { return fmt . Errorf ( " " , s . Hairpin } 
func New Remote Image Service ( endpoint string , connection Timeout time . Duration ) ( internalapi . Image Manager addr , dailer , err := util . Get Address And ctx , cancel := context . With Timeout ( context . Background ( ) , connection conn , err := grpc . Dial Context ( ctx , addr , grpc . With Insecure ( ) , grpc . With Dialer ( dailer ) , grpc . With Default Call Options ( grpc . Max Call Recv Msg Size ( max Msg return & Remote Image Service { timeout : connection Timeout , image Client : runtimeapi . New Image Service } 
func ( r * Remote Image Service ) List Images ( filter * runtimeapi . Image Filter ) ( [ ] * runtimeapi . Image , error ) { ctx , cancel := get Context With resp , err := r . image Client . List Images ( ctx , & runtimeapi . List Images } 
func ( r * Remote Image Service ) Image Status ( image * runtimeapi . Image Spec ) ( * runtimeapi . Image , error ) { ctx , cancel := get Context With resp , err := r . image Client . Image Status ( ctx , & runtimeapi . Image Status if resp . Image != nil { if resp . Image . Id == " " || resp . Image . Size_ == 0 { error klog . Errorf ( " " , error return nil , errors . New ( error } 
func ( r * Remote Image Service ) Pull Image ( image * runtimeapi . Image Spec , auth * runtimeapi . Auth Config , pod Sandbox Config * runtimeapi . Pod Sandbox Config ) ( string , error ) { ctx , cancel := get Context With resp , err := r . image Client . Pull Image ( ctx , & runtimeapi . Pull Image Request { Image : image , Auth : auth , Sandbox Config : pod Sandbox if resp . Image Ref == " " { error klog . Errorf ( " " , error return " " , errors . New ( error return resp . Image } 
func ( r * Remote Image Service ) Remove Image ( image * runtimeapi . Image Spec ) error { ctx , cancel := get Context With _ , err := r . image Client . Remove Image ( ctx , & runtimeapi . Remove Image } 
func ( r * Remote Image Service ) Image Fs Info ( ) ( [ ] * runtimeapi . Filesystem Usage , error ) { // Do not set timeout, because `Image Fs Info` takes time. // TODO(random-liu): Should we assume runtime should cache the result, and set timeout here? ctx , cancel := get Context With resp , err := r . image Client . Image Fs Info ( ctx , & runtimeapi . Image Fs Info return resp . Get Image } 
func check Err ( err error , handle switch { case err == Err Exit : handle Err ( " " , Default Error Exit case kerrors . Is Invalid ( err ) : details := err . ( * kerrors . Status if len ( details . Causes ) > 0 { errs := status Causes To Aggr handle Err ( Multiline Error ( s + " " , errs ) , Default Error Exit } else { handle Err ( s , Default Error Exit case clientcmd . Is Configuration Invalid ( err ) : handle Err ( Multiline Error ( " " , err ) , Default Error Exit default : switch err := err . ( type ) { case * meta . No Resource Match Error : switch { case len ( err . Partial Resource . Group ) > 0 && len ( err . Partial Resource . Version ) > 0 : handle Err ( fmt . Sprintf ( " " , err . Partial Resource . Resource , err . Partial Resource . Group , err . Partial Resource . Version ) , Default Error Exit case len ( err . Partial Resource . Group ) > 0 : handle Err ( fmt . Sprintf ( " " , err . Partial Resource . Resource , err . Partial Resource . Group ) , Default Error Exit case len ( err . Partial Resource . Version ) > 0 : handle Err ( fmt . Sprintf ( " " , err . Partial Resource . Resource , err . Partial Resource . Version ) , Default Error Exit default : handle Err ( fmt . Sprintf ( " " , err . Partial Resource . Resource ) , Default Error Exit case utilerrors . Aggregate : handle Err ( Multiple Errors ( `` , err . Errors ( ) ) , Default Error Exit case utilexec . Exit Error : handle Err ( err . Error ( ) , err . Exit default : // for any other error type msg , ok := Standard Error if ! strings . Has handle Err ( msg , Default Error Exit } 
func Standard Error Message ( err error ) ( string , bool ) { if debug Err , ok := err . ( debug Error ) ; ok { klog . V ( 4 ) . Infof ( debug Err . Debug status , is Status := err . ( kerrors . API switch { case is Status : switch s := status . Status ( ) ; { case s . Reason == metav1 . Status Reason case kerrors . Is Unexpected Object } 
func Multiline case 1 : return fmt . Sprintf ( " \n " , prefix , message For for _ , err := range errs { fmt . Fprintf ( buf , " \n " , message For } 
func Print Error With Causes ( err error , err Out io . Writer ) bool { switch t := err . ( type ) { case * kerrors . Status Error : error if error Details != nil { fmt . Fprintf ( err Out , " \n \n " , error Details . Kind , error for _ , cause := range error Details . Causes { fmt . Fprintf ( err fmt . Fprintf ( err } 
func Multiple for _ , err := range errs { fmt . Fprintf ( buf , " \n " , prefix , message For } 
func message For Error ( err error ) string { msg , ok := Standard Error } 
func Get Flag String Slice ( cmd * cobra . Command , flag string ) [ ] string { s , err := cmd . Flags ( ) . Get String } 
func Get Flag String Array ( cmd * cobra . Command , flag string ) [ ] string { s , err := cmd . Flags ( ) . Get String } 
func Get Flag Int32 ( cmd * cobra . Command , flag string ) int32 { i , err := cmd . Flags ( ) . Get } 
func Get Flag Int64 ( cmd * cobra . Command , flag string ) int64 { i , err := cmd . Flags ( ) . Get } 
func Add Kustomize Flag ( flags * pflag . Flag Set , value * string ) { flags . String Var } 
func Add Generator Flags ( cmd * cobra . Command , default Generator string ) { cmd . Flags ( ) . String ( " " , default Add Dry Run } 
func Dump Reader To File ( reader io . Reader , filename string ) error { f , err := os . Open } 
func Get Resources And Pairs ( args [ ] string , pair Type string ) ( resources [ ] string , pair Args [ ] string , err error ) { found for _ , s := range args { non Resource := ( strings . Contains ( s , " " ) && s [ 0 ] != '=' ) || ( strings . Has switch { case ! found Pair && non Resource : found case found Pair && non Resource : pair Args = append ( pair case ! found Pair && ! non case found Pair && ! non Resource : err = fmt . Errorf ( " " , pair } 
func Parse Pairs ( pair Args [ ] string , pair Type string , support Remove bool ) ( new Pairs map [ string ] string , remove Pairs [ ] string , err error ) { new if support Remove { remove var invalid var invalid Buf Non for _ , pair Arg := range pair Args { if strings . Contains ( pair Arg , " " ) && pair Arg [ 0 ] != '=' { parts := strings . Split N ( pair if len ( parts ) != 2 { if invalid Buf Non Empty { invalid Buf . Write invalid Buf . Write String ( pair invalid Buf Non } else { new } else if support Remove && strings . Has Suffix ( pair Arg , " " ) && pair Arg != " " { remove Pairs = append ( remove Pairs , pair Arg [ : len ( pair } else { if invalid Buf Non Empty { invalid Buf . Write invalid Buf . Write String ( pair invalid Buf Non if invalid Buf Non Empty { err = fmt . Errorf ( " " , pair Type , invalid } 
func Is Sibling Command Exists ( cmd * cobra . Command , target Cmd Name string ) bool { for _ , c := range cmd . Parent ( ) . Commands ( ) { if c . Name ( ) == target Cmd } 
func Default Sub Command Run ( out io . Writer ) func ( c * cobra . Command , args [ ] string ) { return func ( c * cobra . Command , args [ ] string ) { c . Set Require No Check Err ( Err } 
func Require No Arguments ( c * cobra . Command , args [ ] string ) { if len ( args ) > 0 { Check Err ( Usage } 
func Strip stripped , err := yaml . To if err != nil { stripped = Manual } 
func ( c * storage Protection Plugin ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { if ! feature . Default Feature Gate . Enabled ( features . Storage Object In Use switch a . Get Resource ( ) . Group Resource ( ) { case pv Resource : return c . admit case pvc Resource : return c . admit } 
func ( f * Remote Runtime ) List Images ( ctx context . Context , req * kubeapi . List Images Request ) ( * kubeapi . List Images Response , error ) { images , err := f . Image Service . List return & kubeapi . List Images } 
func ( f * Remote Runtime ) Image Status ( ctx context . Context , req * kubeapi . Image Status Request ) ( * kubeapi . Image Status Response , error ) { status , err := f . Image Service . Image return & kubeapi . Image Status } 
func ( f * Remote Runtime ) Pull Image ( ctx context . Context , req * kubeapi . Pull Image Request ) ( * kubeapi . Pull Image Response , error ) { image , err := f . Image Service . Pull Image ( req . Image , req . Auth , req . Sandbox return & kubeapi . Pull Image Response { Image } 
func ( f * Remote Runtime ) Remove Image ( ctx context . Context , req * kubeapi . Remove Image Request ) ( * kubeapi . Remove Image Response , error ) { err := f . Image Service . Remove return & kubeapi . Remove Image } 
func ( f * Remote Runtime ) Image Fs Info ( ctx context . Context , req * kubeapi . Image Fs Info Request ) ( * kubeapi . Image Fs Info Response , error ) { fs Usage , err := f . Image Service . Image Fs return & kubeapi . Image Fs Info Response { Image Filesystems : fs } 
func ( t * os Cinder CSI Translator ) Translate In Tree Storage Class Parameters To CSI ( sc Parameters map [ string ] string ) ( map [ string ] string , error ) { return sc } 
func ( t * os Cinder CSI Translator ) Translate In Tree PV To CSI ( pv * v1 . Persistent Volume ) ( * v1 . Persistent cinder csi Source := & v1 . CSI Persistent Volume Source { Driver : Cinder Driver Name , Volume Handle : cinder Source . Volume ID , Read Only : cinder Source . Read Only , FS Type : cinder Source . FS Type , Volume pv . Spec . CSI = csi } 
func ( t * os Cinder CSI Translator ) Translate CSIPV To In Tree ( pv * v1 . Persistent Volume ) ( * v1 . Persistent csi cinder Source := & v1 . Cinder Persistent Volume Source { Volume ID : csi Source . Volume Handle , FS Type : csi Source . FS Type , Read Only : csi Source . Read pv . Spec . Cinder = cinder } 
func ( t * os Cinder CSI Translator ) Can Support ( pv * v1 . Persistent } 
func ( persistentvolume Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { pv := obj . ( * api . Persistent pv . Status = api . Persistent Volume pvutil . Drop Disabled } 
func ( persistentvolume Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Pv := obj . ( * api . Persistent old Pv := old . ( * api . Persistent new Pv . Status = old pvutil . Drop Disabled Fields ( & new Pv . Spec , & old } 
func ( persistentvolume Status Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Pv := obj . ( * api . Persistent old Pv := old . ( * api . Persistent new Pv . Spec = old } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { persistentvolume Obj , ok := obj . ( * api . Persistent return labels . Set ( persistentvolume Obj . Labels ) , Persistent Volume To Selectable Fields ( persistentvolume } 
func Persistent Volume To Selectable Fields ( persistentvolume * api . Persistent Volume ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & persistentvolume . Object specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , specific Fields } 
func Convert_v1alpha1_Generic Controller Manager Configuration_To_config_Generic Controller Manager Configuration ( in * v1alpha1 . Generic Controller Manager Configuration , out * config . Generic Controller Manager Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Generic Controller Manager Configuration_To_config_Generic Controller Manager } 
func Convert_config_Generic Controller Manager Configuration_To_v1alpha1_Generic Controller Manager Configuration ( in * config . Generic Controller Manager Configuration , out * v1alpha1 . Generic Controller Manager Configuration , s conversion . Scope ) error { return auto Convert_config_Generic Controller Manager Configuration_To_v1alpha1_Generic Controller Manager } 
func Convert_v1alpha1_Kube Cloud Shared Configuration_To_config_Kube Cloud Shared Configuration ( in * v1alpha1 . Kube Cloud Shared Configuration , out * config . Kube Cloud Shared Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Cloud Shared Configuration_To_config_Kube Cloud Shared } 
func Convert_config_Kube Cloud Shared Configuration_To_v1alpha1_Kube Cloud Shared Configuration ( in * config . Kube Cloud Shared Configuration , out * v1alpha1 . Kube Cloud Shared Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Cloud Shared Configuration_To_v1alpha1_Kube Cloud Shared } 
func Init Coverage ( name string ) { // We read the coverage destination in from the KUBE_COVERAGE_FILE env var, // or if it's empty we just use a default in /tmp coverage if coverage File == " " { coverage fmt . Println ( " " + coverage flush requested if requested Interval != " " { if duration , err := time . Parse Duration ( requested Interval ) ; err == nil { flush // Set up the unit test framework with the required arguments to activate test coverage. flag . Command Line . Parse ( [ ] string { " " , temp Coverage // Begin periodic logging go wait . Forever ( Flush Coverage , flush } 
func Flush Coverage ( ) { // We're not actually going to run any tests, but we need Go to think we did so it writes // coverage information to disk. To achieve this, we create a bunch of empty test suites and // have it "run" them. tests := [ ] testing . Internal benchmarks := [ ] testing . Internal examples := [ ] testing . Internal var deps fake Test dummy Run := testing . Main dummy // Once it writes to the temporary path, we move it to the intended path. // This gets us atomic updates from the perspective of another process trying to access // the file. if err := os . Rename ( temp Coverage Path ( ) , coverage File ) ; err != nil { klog . Errorf ( " " , coverage File , temp Coverage } 
func New Pod Security Policy Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Pod Security Policy Informer ( client , resync } 
func New Filtered Pod Security Policy Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Policy V1beta1 ( ) . Pod Security } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Policy V1beta1 ( ) . Pod Security } , } , & policyv1beta1 . Pod Security Policy { } , resync } 
func ( plugin * local Volume Plugin ) Construct Volume Spec ( volume Name , mount Path string ) ( * volume . Spec , error ) { fs := v1 . Persistent Volume local Volume := & v1 . Persistent Volume { Object Meta : metav1 . Object Meta { Name : volume Name , } , Spec : v1 . Persistent Volume Spec { Persistent Volume Source : v1 . Persistent Volume Source { Local : & v1 . Local Volume Source { Path : " " , } , } , Volume return volume . New Spec From Persistent Volume ( local } 
func ( m * local Volume Mounter ) Set Up ( fs Group * int64 ) error { return m . Set Up At ( m . Get Path ( ) , fs } 
func ( m * local Volume Mounter ) Set Up At ( dir string , fs Group * int64 ) error { m . plugin . volume Locks . Lock Key ( m . global defer m . plugin . volume Locks . Unlock Key ( m . global if m . global Path == " " { return fmt . Errorf ( " " , m . vol err := validation . Validate Path No Backsteps ( m . global if err != nil { return fmt . Errorf ( " " , m . global not Mnt , err := mount . Is Not Mount klog . V ( 4 ) . Infof ( " " , dir , m . global Path , ! not Mnt , err , m . read if err != nil && ! os . Is Not if ! not refs , err := m . mounter . Get Mount Refs ( m . global if fs Group != nil { if err != nil { klog . Errorf ( " " , m . global // Only count mounts from other pods refs = m . filter Pod if len ( refs ) > 0 { fs Group New := int64 ( * fs fs Group Old , err := m . mounter . Get FS Group ( m . global if err != nil { return fmt . Errorf ( " " , m . global if fs Group New != fs Group Old { m . plugin . recorder . Eventf ( m . pod , v1 . Event Type Warning , events . Warn Already Mounted Volume , " " , fs Group New , m . vol Name , fs Group if runtime . GOOS != " " { // skip below Mkdir All for windows since the "bind mount" logic is implemented differently in mount_wiondows.go if err := os . Mkdir if m . read mount Options := util . Join Mount Options ( options , m . mount global Path := util . Make Absolute Path ( runtime . GOOS , m . global err = m . mounter . Mount ( global Path , dir , " " , mount not Mnt , mnt Err := mount . Is Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! not Mnt { if mnt Err = m . mounter . Unmount ( dir ) ; mnt Err != nil { klog . Errorf ( " " , mnt not Mnt , mnt Err = mount . Is Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! not if ! m . read Only { // Volume owner will be written only once on the first volume mount if len ( refs ) == 0 { return volume . Set Volume Ownership ( m , fs } 
func ( m * local Volume Mounter ) filter Pod for _ , r := range refs { if strings . Has Prefix ( r , m . plugin . host . Get Pods Dir ( ) + string ( os . Path } 
func ( u * local Volume Unmounter ) Tear Down At ( dir string ) error { klog . V ( 4 ) . Infof ( " \n " , u . vol return mount . Cleanup Mount Point ( dir , u . mounter , true ) /* extensive Mount Point } 
func ( m * local Volume Mapper ) Set Up Device ( ) ( string , error ) { global Path := util . Make Absolute Path ( runtime . GOOS , m . global klog . V ( 4 ) . Infof ( " " , global return global } 
func ( u * local Volume Unmapper ) Tear Down Device ( map Path , _ string ) error { klog . V ( 4 ) . Infof ( " " , map } 
func ( l * local Volume ) Get Global Map Path ( spec * volume . Spec ) ( string , error ) { return filepath . Join ( l . plugin . host . Get Volume Device Plugin Dir ( utilstrings . Escape Qualified Name ( local Volume Plugin Name ) ) , l . vol } 
func ( l * local Volume ) Get Pod Device Map Path ( ) ( string , string ) { return l . plugin . host . Get Pod Volume Device Dir ( l . pod UID , utilstrings . Escape Qualified Name ( local Volume Plugin Name ) ) , l . vol } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . TTL After Finished Controller Configuration ) ( nil ) , ( * config . TTL After Finished Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_TTL After Finished Controller Configuration_To_config_TTL After Finished Controller Configuration ( a . ( * v1alpha1 . TTL After Finished Controller Configuration ) , b . ( * config . TTL After Finished Controller if err := s . Add Generated Conversion Func ( ( * config . TTL After Finished Controller Configuration ) ( nil ) , ( * v1alpha1 . TTL After Finished Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_TTL After Finished Controller Configuration_To_v1alpha1_TTL After Finished Controller Configuration ( a . ( * config . TTL After Finished Controller Configuration ) , b . ( * v1alpha1 . TTL After Finished Controller if err := s . Add Conversion Func ( ( * config . TTL After Finished Controller Configuration ) ( nil ) , ( * v1alpha1 . TTL After Finished Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_TTL After Finished Controller Configuration_To_v1alpha1_TTL After Finished Controller Configuration ( a . ( * config . TTL After Finished Controller Configuration ) , b . ( * v1alpha1 . TTL After Finished Controller if err := s . Add Conversion Func ( ( * v1alpha1 . TTL After Finished Controller Configuration ) ( nil ) , ( * config . TTL After Finished Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_TTL After Finished Controller Configuration_To_config_TTL After Finished Controller Configuration ( a . ( * v1alpha1 . TTL After Finished Controller Configuration ) , b . ( * config . TTL After Finished Controller } 
func New Cmd Node ( ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Run E : cmdutil . Sub Cmd Run cmd . Add Command ( New Cmd Upgrade Node cmd . Add Command ( New Cmd Upgrade Control } 
func New Cmd Upgrade Node Config ( ) * cobra . Command { flags := & node Upgrade Flags { kube Config Path : constants . Get Kubelet Kube Config Path ( ) , kubelet Version Str : " " , dry cmd := & cobra . Command { Use : " " , Short : " " , Long : upgrade Node Config Long Desc , Example : upgrade Node Config Example , Run : func ( cmd * cobra . Command , args [ ] string ) { err := Run Upgrade Node kubeadmutil . Check options . Add Kube Config Flag ( cmd . Flags ( ) , & flags . kube Config cmd . Flags ( ) . Bool Var ( & flags . dry Run , options . Dry Run , flags . dry cmd . Flags ( ) . String Var ( & flags . kubelet Version Str , " " , flags . kubelet Version } 
func New Cmd Upgrade Control Plane ( ) * cobra . Command { flags := & controlplane Upgrade Flags { kube Config Path : constants . Get Kubelet Kube Config Path ( ) , advertise Address : " " , etcd Upgrade : true , renew Certs : true , dry cmd := & cobra . Command { Use : " " , Short : " " , Long : upgrade Node Config Long Desc , Example : upgrade Node Config Example , Run : func ( cmd * cobra . Command , args [ ] string ) { if flags . node node Name , err := node . Get Hostname ( flags . node if err != nil { kubeadmutil . Check flags . node Name = node if flags . advertise Address == " " { ip , err := configutil . Choose API Server Bind if err != nil { kubeadmutil . Check flags . advertise err = Run Upgrade Control kubeadmutil . Check options . Add Kube Config Flag ( cmd . Flags ( ) , & flags . kube Config cmd . Flags ( ) . Bool Var ( & flags . dry Run , options . Dry Run , flags . dry cmd . Flags ( ) . Bool Var ( & flags . etcd Upgrade , " " , flags . etcd cmd . Flags ( ) . Bool Var ( & flags . renew Certs , " " , flags . renew } 
func Run Upgrade Node Config ( flags * node Upgrade Flags ) error { if len ( flags . kubelet Version // Set up the kubelet directory to use. If dry-running, use a fake directory kubelet Dir , err := upgrade . Get Kubelet Dir ( flags . dry client , err := get Client ( flags . kube Config Path , flags . dry if err != nil { return errors . Wrapf ( err , " " , flags . kube Config // Parse the desired kubelet version kubelet Version , err := version . Parse Semantic ( flags . kubelet Version // TODO: Checkpoint the current configuration first so that if something goes wrong it can be recovered if err := kubeletphase . Download Config ( client , kubelet Version , kubelet // If we're dry-running, print the generated manifests, otherwise do nothing if err := print Files If Dry Running ( flags . dry Run , kubelet } 
func print Files If Dry Running ( dry Run bool , kubelet Dir string ) error { if ! dry // Print the contents of the upgraded file and pretend like they were in kubeadmconstants.Kubelet Run Directory file To Print := dryrunutil . File To Print { Real Path : filepath . Join ( kubelet Dir , constants . Kubelet Configuration File Name ) , Print Path : filepath . Join ( constants . Kubelet Run Directory , constants . Kubelet Configuration File return dryrunutil . Print Dry Run Files ( [ ] dryrunutil . File To Print { file To } 
func Run Upgrade Control Plane ( flags * controlplane Upgrade Flags ) error { client , err := get Client ( flags . kube Config Path , flags . dry if err != nil { return errors . Wrapf ( err , " " , flags . kube Config waiter := apiclient . New Kube Waiter ( client , upgrade . Upgrade Manifest // Fetches the cluster configuration cfg , err := configutil . Fetch Init Configuration From // Upgrade the control plane and etcd if installed on this node fmt . Printf ( " \n " , cfg . Kubernetes if flags . dry Run { return Dry Run Static Pod if err := Perform Static Pod Upgrade ( client , waiter , cfg , flags . etcd Upgrade , flags . renew } 
func Add Kube Config Flag ( fs * pflag . Flag Set , kube Config File * string ) { fs . String Var ( kube Config File , Kubeconfig Path , * kube Config // Note that Def Value is the text shown in the terminal and not the default value assigned to the flag fs . Lookup ( Kubeconfig Path ) . Def Value = constants . Get Admin Kube Config } 
func Add Kube Config Dir Flag ( fs * pflag . Flag Set , kube Config Dir * string ) { fs . String Var ( kube Config Dir , Kubeconfig Dir , * kube Config } 
func Add Config Flag ( fs * pflag . Flag Set , cfg Path * string ) { fs . String Var ( cfg Path , Cfg Path , * cfg } 
func Add Ignore Preflight Errors Flag ( fs * pflag . Flag Set , ignore Preflight Errors * [ ] string ) { fs . String Slice Var ( ignore Preflight Errors , Ignore Preflight Errors , * ignore Preflight } 
func Add Control Plan Extra Args Flags ( fs * pflag . Flag Set , api Server Extra Args , controller Manager Extra Args , scheduler Extra Args * map [ string ] string ) { fs . Var ( cliflag . New Map String String ( api Server Extra Args ) , API Server Extra fs . Var ( cliflag . New Map String String ( controller Manager Extra Args ) , Controller Manager Extra fs . Var ( cliflag . New Map String String ( scheduler Extra Args ) , Scheduler Extra } 
func Add Image Meta Flags ( fs * pflag . Flag Set , image Repository * string ) { fs . String Var ( image Repository , Image Repository , * image } 
func Add Feature Gates String Flag ( fs * pflag . Flag Set , feature Gates String * string ) { if known Features := features . Known Features ( & features . Init Feature Gates ) ; len ( known Features ) > 0 { fs . String Var ( feature Gates String , Feature Gates String , * feature Gates String , " " + " \n " + strings . Join ( known } else { fs . String Var ( feature Gates String , Feature Gates String , * feature Gates } 
func Add Kubernetes Version Flag ( fs * pflag . Flag Set , kubernetes Version * string ) { fs . String Var ( kubernetes Version , Kubernetes Version , * kubernetes } 
func Convert_v1alpha1_Namespace Controller Configuration_To_config_Namespace Controller Configuration ( in * v1alpha1 . Namespace Controller Configuration , out * config . Namespace Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Namespace Controller Configuration_To_config_Namespace Controller } 
func Convert_config_Namespace Controller Configuration_To_v1alpha1_Namespace Controller Configuration ( in * config . Namespace Controller Configuration , out * v1alpha1 . Namespace Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Namespace Controller Configuration_To_v1alpha1_Namespace Controller } 
func ( u response Upgrader ) Upgrade Response ( w http . Response Writer , req * http . Request , new Stream Handler httpstream . New Stream Handler ) httpstream . Connection { connection Header := strings . To Lower ( req . Header . Get ( httpstream . Header upgrade Header := strings . To Lower ( req . Header . Get ( httpstream . Header if ! strings . Contains ( connection Header , strings . To Lower ( httpstream . Header Upgrade ) ) || ! strings . Contains ( upgrade Header , strings . To Lower ( Header Spdy31 ) ) { error http . Error ( w , error Msg , http . Status Bad if ! ok { error http . Error ( w , error Msg , http . Status Internal Server w . Header ( ) . Add ( httpstream . Header Connection , httpstream . Header w . Header ( ) . Add ( httpstream . Header Upgrade , Header w . Write Header ( http . Status Switching if err != nil { runtime . Handle conn With Buf := & conn Wrapper { Conn : conn , buf spdy Conn , err := New Server Connection ( conn With Buf , new Stream if err != nil { runtime . Handle return spdy } 
func Register ( ) { // Register the metrics. register Metrics . Do ( func ( ) { prometheus . Must Register ( cache Hit prometheus . Must Register ( cache Miss prometheus . Must Register ( cache Entry prometheus . Must Register ( cache Add prometheus . Must Register ( cache Get prometheus . Must Register ( etcd Request prometheus . Must Register ( object // TODO(danielqsj): Remove the following metrics, they are deprecated prometheus . Must Register ( deprecated Cache Hit prometheus . Must Register ( deprecated Cache Miss prometheus . Must Register ( deprecated Cache Entry prometheus . Must Register ( deprecated Cache Add prometheus . Must Register ( deprecated Cache Get prometheus . Must Register ( deprecated Etcd Request Latencies } 
func ( o * Deployment Controller Options ) Add Flags ( fs * pflag . Flag fs . Int32Var ( & o . Concurrent Deployment Syncs , " " , o . Concurrent Deployment fs . Duration Var ( & o . Deployment Controller Sync Period . Duration , " " , o . Deployment Controller Sync } 
func ( o * Deployment Controller Options ) Apply To ( cfg * deploymentconfig . Deployment Controller cfg . Concurrent Deployment Syncs = o . Concurrent Deployment cfg . Deployment Controller Sync Period = o . Deployment Controller Sync } 
func ( o * Deployment Controller } 
func ( m * kube Generic Runtime Manager ) apply Platform Specific Container Config ( config * runtimeapi . Container Config , container * v1 . Container , pod * v1 . Pod , uid * int64 , username string ) error { windows Config , err := m . generate Windows Container if utilfeature . Default Feature Gate . Enabled ( kubefeatures . Windows GMSA ) { determine Effective Security config . Windows = windows } 
func ( m * kube Generic Runtime Manager ) generate Windows Container Config ( container * v1 . Container , pod * v1 . Pod , uid * int64 , username string ) ( * runtimeapi . Windows Container Config , error ) { wc := & runtimeapi . Windows Container Config { Resources : & runtimeapi . Windows Container Resources { } , Security Context : & runtimeapi . Windows Container Security cpu cpu isolated By Hyperv := kubeletapis . Should Isolated By Hyper if ! cpu Limit . Is Zero ( ) { // Note that sysinfo.Num CPU() is limited to 64 CP Us on Windows due to Processor Groups, // as only 64 processors are available for execution by a given process. This causes // some oddities on systems with more than 64 processors. // Refer https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx. cpu Maximum := 10000 * cpu Limit . Milli Value ( ) / int64 ( sysinfo . Num if isolated By Hyperv { cpu Count := int64 ( cpu Limit . Milli wc . Resources . Cpu Count = cpu if cpu Count != 0 { cpu Maximum = cpu Limit . Milli Value ( ) / cpu // ensure cpu Maximum is in range [1, 10000]. if cpu Maximum < 1 { cpu } else if cpu Maximum > 10000 { cpu wc . Resources . Cpu Maximum = cpu cpu Shares := milli CPU To Shares ( cpu Limit . Milli Value ( ) , isolated By if cpu Shares == 0 { cpu Shares = milli CPU To Shares ( cpu Request . Milli Value ( ) , isolated By wc . Resources . Cpu Shares = cpu memory if memory Limit != 0 { wc . Resources . Memory Limit In Bytes = memory // setup security context effective Sc := securitycontext . Determine Effective Security // Run As User only supports int64 from Kubernetes API, but Windows containers only support username. if effective Sc . Run As User != nil { return nil , fmt . Errorf ( " " , * effective Sc . Run As if username != " " { wc . Security Context . Run As } 
func determine Effective Security Context ( config * runtimeapi . Container Config , container * v1 . Container , pod * v1 . Pod ) { var container Cred container GMSA Pod Annotation := container . Name + g MSA Container Spec Pod Annotation Key if pod . Annotations [ container GMSA Pod Annotation ] != " " { container Cred Spec = pod . Annotations [ container GMSA Pod } else if pod . Annotations [ g MSA Pod Spec Pod Annotation Key ] != " " { container Cred Spec = pod . Annotations [ g MSA Pod Spec Pod Annotation if container Cred config . Annotations [ GMSA Spec Container Annotation Key ] = container Cred } else { // the annotation shouldn't be present, but let's err on the side of caution: // it should only be set here and nowhere else delete ( config . Annotations , GMSA Spec Container Annotation } 
func ( n * network Counter ) merge Collected Data ( packets Received Per Second Data , packets Sent Per Second Data , bytes Received Per Second Data , bytes Sent Per Second Data , packets Received Discarded Data , packets Received Errors Data , packets Outbound Discarded Data , packets Outbound Errors Data map [ string ] uint64 ) { adapters := sets . New // merge the collected data and list of adapters. adapters . Insert ( n . merge Packets Received Per Second Data ( packets Received Per Second adapters . Insert ( n . merge Packets Sent Per Second Data ( packets Sent Per Second adapters . Insert ( n . merge Bytes Received Per Second Data ( bytes Received Per Second adapters . Insert ( n . merge Bytes Sent Per Second Data ( bytes Sent Per Second adapters . Insert ( n . merge Packets Received Discarded Data ( packets Received Discarded adapters . Insert ( n . merge Packets Received Errors Data ( packets Received Errors adapters . Insert ( n . merge Packets Outbound Discarded Data ( packets Outbound Discarded adapters . Insert ( n . merge Packets Outbound Errors Data ( packets Outbound Errors // delete the cache for non-existing adapters. for adapter := range n . adapter Stats { if ! adapters . Has ( adapter ) { delete ( n . adapter } 
func hard Eviction Reservation ( thresholds [ ] evictionapi . Threshold , capacity v1 . Resource List ) v1 . Resource ret := v1 . Resource for _ , threshold := range thresholds { if threshold . Operator != evictionapi . Op Less switch threshold . Signal { case evictionapi . Signal Memory Available : memory Capacity := capacity [ v1 . Resource value := evictionapi . Get Threshold Quantity ( threshold . Value , & memory ret [ v1 . Resource case evictionapi . Signal Node Fs Available : storage Capacity := capacity [ v1 . Resource Ephemeral value := evictionapi . Get Threshold Quantity ( threshold . Value , & storage ret [ v1 . Resource Ephemeral } 
func ( m * kube Generic Runtime Manager ) determine Effective Security Context ( pod * v1 . Pod , container * v1 . Container , uid * int64 , username string ) * runtimeapi . Linux Container Security Context { effective Sc := securitycontext . Determine Effective Security synthesized := convert To Runtime Security Context ( effective if synthesized == nil { synthesized = & runtimeapi . Linux Container Security Context { Masked Paths : securitycontext . Convert To Runtime Masked Paths ( effective Sc . Proc Mount ) , Readonly Paths : securitycontext . Convert To Runtime Readonly Paths ( effective Sc . Proc // set Seccomp Profile Path. synthesized . Seccomp Profile Path = m . get Seccomp Profile From // set Apparmor Profile. synthesized . Apparmor Profile = apparmor . Get Profile Name From Pod // set Run As User. if synthesized . Run As User == nil { if uid != nil { synthesized . Run As synthesized . Run As // set namespace options and supplemental groups. synthesized . Namespace Options = namespaces For pod Sc := pod . Spec . Security if pod Sc != nil { if pod Sc . FS Group != nil { synthesized . Supplemental Groups = append ( synthesized . Supplemental Groups , int64 ( * pod Sc . FS if pod Sc . Supplemental Groups != nil { for _ , sg := range pod Sc . Supplemental Groups { synthesized . Supplemental Groups = append ( synthesized . Supplemental if groups := m . runtime Helper . Get Extra Supplemental Groups For Pod ( pod ) ; len ( groups ) > 0 { synthesized . Supplemental Groups = append ( synthesized . Supplemental synthesized . No New Privs = securitycontext . Add No New Privileges ( effective synthesized . Masked Paths = securitycontext . Convert To Runtime Masked Paths ( effective Sc . Proc synthesized . Readonly Paths = securitycontext . Convert To Runtime Readonly Paths ( effective Sc . Proc } 
func verify Run As Non Root ( pod * v1 . Pod , container * v1 . Container , uid * int64 , username string ) error { effective Sc := securitycontext . Determine Effective Security // If the option is not set, or if running as root is allowed, return nil. if effective Sc == nil || effective Sc . Run As Non Root == nil || ! * effective Sc . Run As Non if effective Sc . Run As User != nil { if * effective Sc . Run As } 
func convert To Runtime Security Context ( security Context * v1 . Security Context ) * runtimeapi . Linux Container Security Context { if security sc := & runtimeapi . Linux Container Security Context { Capabilities : convert To Runtime Capabilities ( security Context . Capabilities ) , Selinux Options : convert To Runtime SE Linux Option ( security Context . SE Linux if security Context . Run As User != nil { sc . Run As User = & runtimeapi . Int64Value { Value : int64 ( * security Context . Run As if security Context . Run As Group != nil { sc . Run As Group = & runtimeapi . Int64Value { Value : int64 ( * security Context . Run As if security Context . Privileged != nil { sc . Privileged = * security if security Context . Read Only Root Filesystem != nil { sc . Readonly Rootfs = * security Context . Read Only Root } 
func convert To Runtime SE Linux Option ( opts * v1 . SE Linux Options ) * runtimeapi . SE Linux return & runtimeapi . SE Linux } 
func convert To Runtime capabilities := & runtimeapi . Capability { Add Capabilities : make ( [ ] string , len ( opts . Add ) ) , Drop for index , value := range opts . Add { capabilities . Add for index , value := range opts . Drop { capabilities . Drop } 
func New Defaults ( ) ( * args . Generator Args , * Custom Args ) { generic Args := args . Default ( ) . Without Default Flag custom Args := & Custom Args { Base Peer Dirs : Default Base Peer Dirs , Skip generic Args . Custom Args = custom generic Args . Output File Base return generic Args , custom } 
func ( ca * Custom Args ) Add Flags ( fs * pflag . Flag Set ) { pflag . Command Line . String Slice Var ( & ca . Base Peer Dirs , " " , ca . Base Peer pflag . Command Line . String Slice Var ( & ca . Extra Peer Dirs , " " , ca . Extra Peer pflag . Command Line . Bool Var ( & ca . Skip Unsafe , " " , ca . Skip } 
func ( c * Fake Custom Resource Definitions ) Get ( name string , options v1 . Get Options ) ( result * apiextensions . Custom Resource Definition , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( customresourcedefinitions Resource , name ) , & apiextensions . Custom Resource return obj . ( * apiextensions . Custom Resource } 
func ( c * Fake Custom Resource Definitions ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( customresourcedefinitions Resource , name ) , & apiextensions . Custom Resource } 
func ( c * Fake Custom Resource Definitions ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( customresourcedefinitions Resource , list _ , err := c . Fake . Invokes ( action , & apiextensions . Custom Resource Definition } 
func ( c * Fake Custom Resource Definitions ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * apiextensions . Custom Resource Definition , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( customresourcedefinitions Resource , name , pt , data , subresources ... ) , & apiextensions . Custom Resource return obj . ( * apiextensions . Custom Resource } 
func New Cmd Create Cluster Role ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { c := & Create Cluster Role Options { Create Role Options : New Create Role Options ( io Streams ) , Aggregation cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : cluster Role Long , Long : cluster Role Long , Example : cluster Role Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( c . Run Create c . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Dry Run cmd . Flags ( ) . String Slice cmd . Flags ( ) . String Slice Var ( & c . Non Resource UR Ls , " " , c . Non Resource UR cmd . Flags ( ) . String cmd . Flags ( ) . String Array Var ( & c . Resource Names , " " , c . Resource cmd . Flags ( ) . Var ( cliflag . New Map String String ( & c . Aggregation } 
func ( c * Create Cluster Role Options ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { // Remove duplicate non Resource UR Ls non Resource UR for _ , n := range c . Non Resource UR Ls { if ! array Contains ( non Resource UR Ls , n ) { non Resource UR Ls = append ( non Resource UR c . Non Resource UR Ls = non Resource UR return c . Create Role } 
func ( c * Create Cluster Role if len ( c . Aggregation Rule ) > 0 { if len ( c . Non Resource UR Ls ) > 0 || len ( c . Verbs ) > 0 || len ( c . Resources ) > 0 || len ( c . Resource if len ( c . Resources ) == 0 && len ( c . Non Resource UR // validate resources if len ( c . Resources ) > 0 { for _ , v := range c . Verbs { if ! array Contains ( valid Resource if err := c . validate //validate non-resource-url if len ( c . Non Resource UR Ls ) > 0 { for _ , v := range c . Verbs { if ! array Contains ( valid Non Resource for _ , non Resource URL := range c . Non Resource UR Ls { if non Resource if non Resource URL == " " || ! strings . Has Prefix ( non Resource if strings . Contains Rune ( non Resource URL [ : len ( non Resource } 
func ( c * Create Cluster Role Options ) Run Create Role ( ) error { cluster Role := & rbacv1 . Cluster Role { // this is ok because we know exactly how we want to be serialized Type Meta : metav1 . Type Meta { API Version : rbacv1 . Scheme Group cluster if len ( c . Aggregation Rule ) == 0 { rules , err := generate Resource Policy Rules ( c . Mapper , c . Verbs , c . Resources , c . Resource Names , c . Non Resource UR cluster } else { cluster Role . Aggregation Rule = & rbacv1 . Aggregation Rule { Cluster Role Selectors : [ ] metav1 . Label Selector { { Match Labels : c . Aggregation // Create Cluster Role. if ! c . Dry Run { cluster Role , err = c . Client . Cluster Roles ( ) . Create ( cluster return c . Print Obj ( cluster } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1 . Daemon Set { } , func ( obj interface { } ) { Set Object Defaults_Daemon Set ( obj . ( * v1 . Daemon scheme . Add Type Defaulting Func ( & v1 . Daemon Set List { } , func ( obj interface { } ) { Set Object Defaults_Daemon Set List ( obj . ( * v1 . Daemon Set scheme . Add Type Defaulting Func ( & v1 . Deployment { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Deployment List { } , func ( obj interface { } ) { Set Object Defaults_Deployment List ( obj . ( * v1 . Deployment scheme . Add Type Defaulting Func ( & v1 . Replica Set { } , func ( obj interface { } ) { Set Object Defaults_Replica Set ( obj . ( * v1 . Replica scheme . Add Type Defaulting Func ( & v1 . Replica Set List { } , func ( obj interface { } ) { Set Object Defaults_Replica Set List ( obj . ( * v1 . Replica Set scheme . Add Type Defaulting Func ( & v1 . Stateful Set { } , func ( obj interface { } ) { Set Object Defaults_Stateful Set ( obj . ( * v1 . Stateful scheme . Add Type Defaulting Func ( & v1 . Stateful Set List { } , func ( obj interface { } ) { Set Object Defaults_Stateful Set List ( obj . ( * v1 . Stateful Set } 
func New Cmd Config Set Cluster ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { options := & create Cluster Options { config Access : config cmd := & cobra . Command { Use : fmt . Sprintf ( " " , clientcmd . Flag API Server , clientcmd . Flag CA File , clientcmd . Flag Insecure ) , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : create Cluster Long , Example : create Cluster Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . insecure Skip TLS cmd . Flags ( ) . Var ( & options . server , clientcmd . Flag API Server , clientcmd . Flag API f := cmd . Flags ( ) . Var PF ( & options . insecure Skip TLS Verify , clientcmd . Flag Insecure , " " , clientcmd . Flag f . No Opt Def cmd . Flags ( ) . Var ( & options . certificate Authority , clientcmd . Flag CA File , " " + clientcmd . Flag CA cmd . Mark Flag Filename ( clientcmd . Flag CA f = cmd . Flags ( ) . Var PF ( & options . embed CA Data , clientcmd . Flag Embed Certs , " " , clientcmd . Flag Embed f . No Opt Def } 
func ( o * create Cluster Options ) modify Cluster ( existing Cluster clientcmdapi . Cluster ) clientcmdapi . Cluster { modified Cluster := existing if o . server . Provided ( ) { modified if o . insecure Skip TLS Verify . Provided ( ) { modified Cluster . Insecure Skip TLS Verify = o . insecure Skip TLS // Specifying insecure mode clears any certificate authority if modified Cluster . Insecure Skip TLS Verify { modified Cluster . Certificate modified Cluster . Certificate Authority if o . certificate Authority . Provided ( ) { ca Path := o . certificate if o . embed CA Data . Value ( ) { modified Cluster . Certificate Authority Data , _ = ioutil . Read File ( ca modified Cluster . Insecure Skip TLS modified Cluster . Certificate } else { ca Path , _ = filepath . Abs ( ca modified Cluster . Certificate Authority = ca // Specifying a certificate authority file clears certificate authority data and insecure mode if ca Path != " " { modified Cluster . Insecure Skip TLS modified Cluster . Certificate Authority return modified } 
func Convert JSON Schema Props To Open AP Iv2Schema ( in * apiextensions . JSON Schema // dirty hack to temporarily set the type at the root. See continuation at the func bottom. // TODO: remove for Kubernetes 1.15 old Root // Remove unsupported fields in Open validation . Convert JSON Schema Props With Post Process ( in , out , func ( p * spec . Schema ) error { p . One // TODO(roycaihw): preserve cases where we only have one subtree in Any Of, same for One Of p . Any // https://github.com/kubernetes/kube-openapi/pull/143/files#diff-62afddb578e9db18fb32ffb6b7802d92R104 if ! strings . Has case len ( p . Type ) > 1 : // https://github.com/kubernetes/kube-openapi/pull/143/files#diff-62afddb578e9db18fb32ffb6b7802d92R272 // We also set Properties to null to enforce parse // normalize items if p . Items != nil && len ( p . Items . Schemas ) == 1 { p . Items = & spec . Schema Or p . Additional p . Pattern if p . External Docs != nil && len ( p . External Docs . URL ) == 0 { p . External // restore root level type in input, and remove it in output if we had added it // TODO: remove with Kubernetes 1.15 in . Type = old Root if len ( old Root } 
func ( c completed Config ) New ( delegation Target genericapiserver . Delegation Target ) ( * Custom Resource Definitions , error ) { generic Server , err := c . Generic Config . New ( " " , delegation s := & Custom Resource Definitions { Generic API Server : generic api Resource Config := c . Generic Config . Merged Resource api Group Info := genericapiserver . New Default API Group Info ( apiextensions . Group Name , Scheme , metav1 . Parameter if api Resource Config . Version Enabled ( v1beta1 . Scheme Group // customresourcedefinitions custom Resource Defintion Storage := customresourcedefinition . New REST ( Scheme , c . Generic Config . REST Options storage [ " " ] = custom Resource Defintion storage [ " " ] = customresourcedefinition . New Status REST ( Scheme , custom Resource Defintion api Group Info . Versioned Resources Storage if err := s . Generic API Server . Install API Group ( & api Group crd Client , err := internalclientset . New For Config ( s . Generic API Server . Loopback Client s . Informers = internalinformers . New Shared Informer Factory ( crd delegate Handler := delegation Target . Unprotected if delegate Handler == nil { delegate Handler = http . Not Found version Discovery Handler := & version Discovery Handler { discovery : map [ schema . Group Version ] * discovery . API Version Handler { } , delegate : delegate group Discovery Handler := & group Discovery Handler { discovery : map [ string ] * discovery . API Group Handler { } , delegate : delegate establishing Controller := establish . New Establishing Controller ( s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource Definitions ( ) , crd crd Handler , err := New Custom Resource Definition Handler ( version Discovery Handler , group Discovery Handler , s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource Definitions ( ) , delegate Handler , c . Extra Config . CRDREST Options Getter , c . Generic Config . Admission Control , establishing Controller , c . Extra Config . Service Resolver , c . Extra Config . Auth Resolver Wrapper , c . Extra Config . Master Count , s . Generic API s . Generic API Server . Handler . Non Go Restful Mux . Handle ( " " , crd s . Generic API Server . Handler . Non Go Restful Mux . Handle Prefix ( " " , crd crd Controller := New Discovery Controller ( s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource Definitions ( ) , version Discovery Handler , group Discovery naming Controller := status . New Naming Condition Controller ( s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource Definitions ( ) , crd finalizing Controller := finalizer . New CRD Finalizer ( s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource Definitions ( ) , crd Client . Apiextensions ( ) , crd var openapi if utilfeature . Default Feature Gate . Enabled ( apiextensionsfeatures . Custom Resource Publish Open API ) { openapi Controller = openapicontroller . New Controller ( s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { s . Informers . Start ( context . Stop s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { if utilfeature . Default Feature Gate . Enabled ( apiextensionsfeatures . Custom Resource Publish Open API ) { go openapi Controller . Run ( s . Generic API Server . Static Open API Spec , s . Generic API Server . Open API Versioned Service , context . Stop go crd Controller . Run ( context . Stop go naming Controller . Run ( context . Stop go establishing Controller . Run ( context . Stop go finalizing Controller . Run ( 5 , context . Stop // we don't want to report healthy until we can handle all CR Ds that have already been registered. Waiting for the informer // to sync makes sure that the lister will be valid before we begin. There may still be races for CR Ds added after startup, // but we won't go healthy until we can handle the ones already present. s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { return wait . Poll Immediate Until ( 100 * time . Millisecond , func ( ) ( bool , error ) { return s . Informers . Apiextensions ( ) . Internal Version ( ) . Custom Resource Definitions ( ) . Informer ( ) . Has } , context . Stop } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Endpoint Controller Configuration ) ( nil ) , ( * config . Endpoint Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Endpoint Controller Configuration_To_config_Endpoint Controller Configuration ( a . ( * v1alpha1 . Endpoint Controller Configuration ) , b . ( * config . Endpoint Controller if err := s . Add Generated Conversion Func ( ( * config . Endpoint Controller Configuration ) ( nil ) , ( * v1alpha1 . Endpoint Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Endpoint Controller Configuration_To_v1alpha1_Endpoint Controller Configuration ( a . ( * config . Endpoint Controller Configuration ) , b . ( * v1alpha1 . Endpoint Controller if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Conversion Func ( ( * config . Endpoint Controller Configuration ) ( nil ) , ( * v1alpha1 . Endpoint Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Endpoint Controller Configuration_To_v1alpha1_Endpoint Controller Configuration ( a . ( * config . Endpoint Controller Configuration ) , b . ( * v1alpha1 . Endpoint Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Endpoint Controller Configuration ) ( nil ) , ( * config . Endpoint Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Endpoint Controller Configuration_To_config_Endpoint Controller Configuration ( a . ( * v1alpha1 . Endpoint Controller Configuration ) , b . ( * config . Endpoint Controller } 
func New Default Stateful Set Control ( pod Control Stateful Pod Control Interface , status Updater Stateful Set Status Updater Interface , controller History history . Interface , recorder record . Event Recorder ) Stateful Set Control Interface { return & default Stateful Set Control { pod Control , status Updater , controller } 
func ( ssc * default Stateful Set Control ) Update Stateful Set ( set * apps . Stateful Set , pods [ ] * v1 . Pod ) error { // list all revisions and sort them revisions , err := ssc . List history . Sort Controller // get the current, and update revisions current Revision , update Revision , collision Count , err := ssc . get Stateful Set // perform the main update function and get the status status , err := ssc . update Stateful Set ( set , current Revision , update Revision , collision // update the set's status err = ssc . update Stateful Set klog . V ( 4 ) . Infof ( " " , set . Namespace , set . Name , status . Replicas , status . Ready Replicas , status . Current Replicas , status . Updated klog . V ( 4 ) . Infof ( " " , set . Namespace , set . Name , status . Current Revision , status . Update // maintain the set's revision history limit return ssc . truncate History ( set , pods , revisions , current Revision , update } 
func ( ssc * default Stateful Set Control ) truncate History ( set * apps . Stateful Set , pods [ ] * v1 . Pod , revisions [ ] * apps . Controller Revision , current * apps . Controller Revision , update * apps . Controller Revision ) error { history := make ( [ ] * apps . Controller for i := range pods { live [ get Pod history history Limit := int ( * set . Spec . Revision History if history Len <= history // delete any non-live history to maintain the revision limit. history = history [ : ( history Len - history for i := 0 ; i < len ( history ) ; i ++ { if err := ssc . controller History . Delete Controller } 
func ( ssc * default Stateful Set Control ) get Stateful Set Revisions ( set * apps . Stateful Set , revisions [ ] * apps . Controller Revision ) ( * apps . Controller Revision , * apps . Controller Revision , int32 , error ) { var current Revision , update Revision * apps . Controller revision history . Sort Controller // Use a local copy of set.Status.Collision Count to avoid modifying set.Status directly. // This copy is returned so the value gets carried over to set.Status in update Stateful Set. var collision if set . Status . Collision Count != nil { collision Count = * set . Status . Collision // create a new revision from the current set update Revision , err := new Revision ( set , next Revision ( revisions ) , & collision if err != nil { return nil , nil , collision // find any equivalent revisions equal Revisions := history . Find Equal Revisions ( revisions , update equal Count := len ( equal if equal Count > 0 && history . Equal Revision ( revisions [ revision Count - 1 ] , equal Revisions [ equal Count - 1 ] ) { // if the equivalent revision is immediately prior the update revision has not changed update Revision = revisions [ revision } else if equal Count > 0 { // if the equivalent revision is not immediately prior we will roll back by incrementing the // Revision of the equivalent revision update Revision , err = ssc . controller History . Update Controller Revision ( equal Revisions [ equal Count - 1 ] , update if err != nil { return nil , nil , collision } else { //if there is no equivalent revision we create a new one update Revision , err = ssc . controller History . Create Controller Revision ( set , update Revision , & collision if err != nil { return nil , nil , collision // attempt to find the revision that corresponds to the current revision for i := range revisions { if revisions [ i ] . Name == set . Status . Current Revision { current // if the current revision is nil we initialize the history by setting it to the update revision if current Revision == nil { current Revision = update return current Revision , update Revision , collision } 
func ( ssc * default Stateful Set Control ) update Stateful Set ( set * apps . Stateful Set , current Revision * apps . Controller Revision , update Revision * apps . Controller Revision , collision Count int32 , pods [ ] * v1 . Pod ) ( * apps . Stateful Set Status , error ) { // get the current and update revisions of the set. current Set , err := Apply Revision ( set , current update Set , err := Apply Revision ( set , update // set the generation, and revisions in the returned status status := apps . Stateful Set status . Observed status . Current Revision = current status . Update Revision = update status . Collision * status . Collision Count = collision replica // slice that will contain all Pods such that 0 <= get Ordinal(pod) < set.Spec.Replicas replicas := make ( [ ] * v1 . Pod , replica // slice that will contain all Pods such that set.Spec.Replicas <= get first Unhealthy Ordinal := math . Max var first Unhealthy // count the number of running and ready replicas if is Running And Ready ( pods [ i ] ) { status . Ready // count the number of current and update replicas if is Created ( pods [ i ] ) && ! is Terminating ( pods [ i ] ) { if get Pod Revision ( pods [ i ] ) == current Revision . Name { status . Current if get Pod Revision ( pods [ i ] ) == update Revision . Name { status . Updated if ord := get Ordinal ( pods [ i ] ) ; 0 <= ord && ord < replica } else if ord >= replica // for any empty indices in the sequence [0,set.Spec.Replicas) create a new Pod at the correct revision for ord := 0 ; ord < replica Count ; ord ++ { if replicas [ ord ] == nil { replicas [ ord ] = new Versioned Stateful Set Pod ( current Set , update Set , current Revision . Name , update // sort the condemned Pods by their ordinals sort . Sort ( ascending // find the first unhealthy Pod for i := range replicas { if ! is if ord := get Ordinal ( replicas [ i ] ) ; ord < first Unhealthy Ordinal { first Unhealthy first Unhealthy for i := range condemned { if ! is if ord := get Ordinal ( condemned [ i ] ) ; ord < first Unhealthy Ordinal { first Unhealthy first Unhealthy if unhealthy > 0 { klog . V ( 4 ) . Infof ( " " , set . Namespace , set . Name , unhealthy , first Unhealthy // If the Stateful Set is being deleted, don't do anything other than updating // status. if set . Deletion monotonic := ! allows // Examine each replica with respect to its ordinal for i := range replicas { // delete and recreate failed pods if is Failed ( replicas [ i ] ) { ssc . recorder . Eventf ( set , v1 . Event Type if err := ssc . pod Control . Delete Stateful if get Pod Revision ( replicas [ i ] ) == current Revision . Name { status . Current if get Pod Revision ( replicas [ i ] ) == update Revision . Name { status . Updated replicas [ i ] = new Versioned Stateful Set Pod ( current Set , update Set , current Revision . Name , update // If we find a Pod that has not been created we create the Pod if ! is Created ( replicas [ i ] ) { if err := ssc . pod Control . Create Stateful if get Pod Revision ( replicas [ i ] ) == current Revision . Name { status . Current if get Pod Revision ( replicas [ i ] ) == update Revision . Name { status . Updated // If we find a Pod that is currently terminating, we must wait until graceful deletion // completes before we continue to make progress. if is // If we have a Pod that has been created but is not running and ready we can not make progress. // We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its // ordinal, are Running and Ready. if ! is Running And // Enforce the Stateful Set invariants if identity Matches ( set , replicas [ i ] ) && storage // Make a deep copy so we don't mutate the shared cache replica := replicas [ i ] . Deep if err := ssc . pod Control . Update Stateful Pod ( update // At this point, all of the current Replicas are Running and Ready, we can consider termination. // We will wait for all predecessors to be Running and Ready prior to attempting a deletion. // We will terminate Pods in a monotonically decreasing order over [len(pods),set.Spec.Replicas). // Note that we do not resurrect Pods in this interval. Also note that scaling will take precedence over // updates. for target := len ( condemned ) - 1 ; target >= 0 ; target -- { // wait for terminating pods to expire if is // if we are in monotonic mode and the condemned target is not the first unhealthy Pod block if ! is Running And Ready ( condemned [ target ] ) && monotonic && condemned [ target ] != first Unhealthy Pod { klog . V ( 4 ) . Infof ( " " , set . Namespace , set . Name , first Unhealthy if err := ssc . pod Control . Delete Stateful if get Pod Revision ( condemned [ target ] ) == current Revision . Name { status . Current if get Pod Revision ( condemned [ target ] ) == update Revision . Name { status . Updated // for the On Delete strategy we short circuit. Pods will be updated when they are manually deleted. if set . Spec . Update Strategy . Type == apps . On Delete Stateful Set Strategy // we compute the minimum ordinal of the target sequence for a destructive update based on the strategy. update if set . Spec . Update Strategy . Rolling Update != nil { update Min = int ( * set . Spec . Update Strategy . Rolling // we terminate the Pod with the largest ordinal that does not match the update revision. for target := len ( replicas ) - 1 ; target >= update Min ; target -- { // delete the Pod if it is not already terminating and does not match the update revision. if get Pod Revision ( replicas [ target ] ) != update Revision . Name && ! is err := ssc . pod Control . Delete Stateful status . Current // wait for unhealthy Pods on update if ! is } 
func ( ssc * default Stateful Set Control ) update Stateful Set Status ( set * apps . Stateful Set , status * apps . Stateful Set Status ) error { // complete any in progress rolling update if necessary complete Rolling // if the status is not inconsistent do not perform an update if ! inconsistent // copy set and update its status set = set . Deep if err := ssc . status Updater . Update Stateful Set } 
func ( e Simple Category Expander ) Expand ( category string ) ( [ ] schema . Group } 
func New Discovery Category Expander ( client discovery . Discovery Interface ) Category return discovery Category Expander { discovery } 
func ( e discovery Category Expander ) Expand ( category string ) ( [ ] schema . Group Resource , bool ) { // Get all supported resources for groups and versions from server, if no resource found, fallback anyway. api Resource Lists , _ := e . discovery Client . Server if len ( api Resource discovered Expansions := map [ string ] [ ] schema . Group for _ , api Resource List := range api Resource Lists { gv , err := schema . Parse Group Version ( api Resource List . Group // Collect Group Versions by categories for _ , api Resource := range api Resource List . API Resources { if categories := api Resource . Categories ; len ( categories ) > 0 { for _ , category := range categories { group Resource := schema . Group Resource { Group : gv . Group , Resource : api discovered Expansions [ category ] = append ( discovered Expansions [ category ] , group ret , ok := discovered } 
func ( u Union Category Expander ) Expand ( category string ) ( [ ] schema . Group Resource , bool ) { ret := [ ] schema . Group // Expand the category for each Category Expander in the list and merge/combine the results. for _ , expansion := range u { curr , curr for _ , curr for _ , existing := range ret { if existing == curr if ! found { ret = append ( ret , curr ok = ok || curr } 
func Parse Etcd Version ( s string ) ( * Etcd return & Etcd } 
func ( v * Etcd Version ) Equals ( o * Etcd } 
func ( v * Etcd Version ) Major Minor Equals ( o * Etcd } 
func Parse Etcd Storage Version ( s string ) ( Etcd Storage Version , error ) { switch s { case " " : return storage case " " : return storage default : return storage } 
func Must Parse Etcd Storage Version ( s string ) Etcd Storage Version { version , err := Parse Etcd Storage } 
func ( v Etcd Storage Version ) String ( ) string { switch v { case storage case storage } 
func Parse Etcd Version Pair ( s string ) ( * Etcd Version version , err := Parse Etcd storage Version , err := Parse Etcd Storage return & Etcd Version Pair { version , storage } 
func Must Parse Etcd Version Pair ( s string ) * Etcd Version Pair { pair , err := Parse Etcd Version } 
func ( vp * Etcd Version Pair ) String ( ) string { return fmt . Sprintf ( " " , vp . version , vp . storage } 
func ( vp * Etcd Version Pair ) Equals ( o * Etcd Version Pair ) bool { return vp . version . Equals ( o . version ) && vp . storage Version == o . storage } 
func ( sv Supported Versions ) Next Version ( current * Etcd Version ) * Etcd Version { var next Version * Etcd for i , supported Version := range sv { if current . Major Minor Equals ( supported Version ) && len ( sv ) > i + 1 { next return next } 
func ( sv Supported Versions ) Next Version Pair ( current * Etcd Version Pair ) * Etcd Version Pair { next Version := sv . Next if next storage Version := storage if next Version . Major == 2 { storage Version = storage return & Etcd Version Pair { version : next Version , storage Version : storage } 
func Parse Supported Versions ( s string ) ( Supported versions := make ( Supported for i , v := range list { versions [ i ] , err = Parse Etcd Version ( strings . Trim } 
func Must Parse Supported Versions ( s string ) Supported Versions { versions , err := Parse Supported } 
func ( in * Client Connection Configuration ) Deep Copy ( ) * Client Connection out := new ( Client Connection in . Deep Copy } 
func ( in * Debugging Configuration ) Deep Copy ( ) * Debugging out := new ( Debugging in . Deep Copy } 
func ( in * Leader Election Configuration ) Deep Copy Into ( out * Leader Election if in . Leader Elect != nil { in , out := & in . Leader Elect , & out . Leader out . Lease Duration = in . Lease out . Renew Deadline = in . Renew out . Retry Period = in . Retry } 
func ( in * Leader Election Configuration ) Deep Copy ( ) * Leader Election out := new ( Leader Election in . Deep Copy } 
func ( p * Type Setter Printer ) Print Obj ( obj runtime . Object , w io . Writer ) error { if obj == nil { return p . Delegate . Print if ! obj . Get Object Kind ( ) . Group Version Kind ( ) . Empty ( ) { return p . Delegate . Print // we were empty coming in, make sure we're empty going out. This makes the call thread-unsafe defer func ( ) { obj . Get Object Kind ( ) . Set Group Version Kind ( schema . Group Version gvks , _ , err := p . Typer . Object if len ( gvk . Version ) == 0 || gvk . Version == runtime . API Version obj . Get Object Kind ( ) . Set Group Version return p . Delegate . Print } 
func ( p * Type Setter Printer ) To Printer ( delegate Resource Printer ) Resource } 
func ( p * Type Setter Printer ) Wrap To Printer ( delegate Resource Printer , err error ) ( Resource } 
func New Backend ( c * Config ) ( audit . Backend , error ) { event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( c . Event scheme := runtime . New err := auditregv1alpha1 . Add To recorder := event Broadcaster . New Recorder ( scheme , c . Event if c . Buffered Config == nil { c . Buffered Config = New Default Webhook Batch cm , err := webhook . New Client Manager ( auditv1 . Scheme Group // TODO: need a way of injecting authentication before beta auth Info Resolver , err := webhook . New Default Authentication Info cm . Set Authentication Info Resolver ( auth Info cm . Set Service Resolver ( c . Webhook Config . Service cm . Set Authentication Info Resolver Wrapper ( c . Webhook Config . Auth Info Resolver manager := & backend { config : c , delegates : atomic . Value { } , delegate Update Mutex : sync . Mutex { } , webhook Client manager . delegates . Store ( synced c . Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { manager . add Sink ( obj . ( * auditregv1alpha1 . Audit } , Update Func : func ( old Obj , new Obj interface { } ) { manager . update Sink ( old Obj . ( * auditregv1alpha1 . Audit Sink ) , new Obj . ( * auditregv1alpha1 . Audit } , Delete Func : func ( obj interface { } ) { sink , ok := obj . ( * auditregv1alpha1 . Audit if ! ok { tombstone , ok := obj . ( cache . Deleted Final State sink , ok = tombstone . Obj . ( * auditregv1alpha1 . Audit manager . delete } 
func ( s synced } 
func ( b * backend ) Process Events ( events ... * auditinternal . Event ) bool { for _ , d := range b . Get Delegates ( ) { d . Process } 
func ( b * backend ) stop All Delegates ( ) { b . delegate Update for _ , d := range b . Get Delegates ( ) { close ( d . stop } 
func ( b * backend ) copy Delegates ( ) synced Delegates { c := make ( synced for u , s := range b . Get } 
func ( b * backend ) add Sink ( sink * auditregv1alpha1 . Audit Sink ) { b . delegate Update defer b . delegate Update delegates := b . copy d , err := b . create And Start b . recorder . Event ( sink , corev1 . Event Type b . set } 
func ( b * backend ) update Sink ( old Sink , new Sink * auditregv1alpha1 . Audit Sink ) { b . delegate Update defer b . delegate Update delegates := b . copy old Delegate , ok := delegates [ old if ! ok { klog . Errorf ( " " , old Sink . Name , old // check if spec has changed eq := reflect . Deep Equal ( old Sink . Spec , new if eq { delete ( delegates , old delegates [ new Sink . UID ] = old b . set } else { d , err := b . create And Start Delegate ( new if err != nil { msg := fmt . Sprintf ( " " , old b . recorder . Event ( new Sink , corev1 . Event Type delete ( delegates , old delegates [ new b . set // graceful shutdown in goroutine as to not block go old Delegate . graceful klog . V ( 2 ) . Infof ( " " , new } 
func ( b * backend ) delete Sink ( sink * auditregv1alpha1 . Audit Sink ) { b . delegate Update defer b . delegate Update delegates := b . copy b . set // graceful shutdown in goroutine as to not block go delegate . graceful } 
func ( b * backend ) create And Start Delegate ( sink * auditregv1alpha1 . Audit Sink ) ( * delegate , error ) { f := factory { config : b . config , webhook Client Manager : b . webhook Client delegate , err := f . Build err = delegate . Run ( delegate . stop } 
func ( b * backend ) String ( ) string { var delegate for _ , delegate := range b . Get Delegates ( ) { delegate Strings = append ( delegate return fmt . Sprintf ( " " , Plugin Name , strings . Join ( delegate } 
func New Cloud ( config Reader io . Reader ) ( cloudprovider . Interface , error ) { config , err := parse Config ( config if config . Route Table Resource Group == " " { config . Route Table Resource Group = config . Resource if config . VM Type == " " { // default to standard vm Type if not set. config . VM Type = vm Type env , err := auth . Parse Azure service Principal Token , err := auth . Get Service Principal Token ( & config . Azure Auth // operation Poll Rate Limiter.Accept() is a no-op if rate limits are configured off. operation Poll Rate Limiter := flowcontrol . New Fake Always Rate operation Poll Rate Limiter Write := flowcontrol . New Fake Always Rate // If reader is provided (and no writer) we will // use the same value for both. if config . Cloud Provider Rate Limit { // Assign rate limit defaults if no configuration was passed in if config . Cloud Provider Rate Limit QPS == 0 { config . Cloud Provider Rate Limit QPS = rate Limit QPS if config . Cloud Provider Rate Limit Bucket == 0 { config . Cloud Provider Rate Limit Bucket = rate Limit Bucket if config . Cloud Provider Rate Limit QPS Write == 0 { config . Cloud Provider Rate Limit QPS Write = rate Limit QPS if config . Cloud Provider Rate Limit Bucket Write == 0 { config . Cloud Provider Rate Limit Bucket Write = rate Limit Bucket operation Poll Rate Limiter = flowcontrol . New Token Bucket Rate Limiter ( config . Cloud Provider Rate Limit QPS , config . Cloud Provider Rate Limit operation Poll Rate Limiter Write = flowcontrol . New Token Bucket Rate Limiter ( config . Cloud Provider Rate Limit QPS Write , config . Cloud Provider Rate Limit Bucket klog . V ( 2 ) . Infof ( " " , config . Cloud Provider Rate Limit QPS , config . Cloud Provider Rate Limit klog . V ( 2 ) . Infof ( " " , config . Cloud Provider Rate Limit QPS Write , config . Cloud Provider Rate Limit Bucket // Conditionally configure resource request backoff resource Request if config . Cloud Provider Backoff { // Assign backoff defaults if no configuration was passed in if config . Cloud Provider Backoff Retries == 0 { config . Cloud Provider Backoff Retries = backoff Retries if config . Cloud Provider Backoff Duration == 0 { config . Cloud Provider Backoff Duration = backoff Duration if config . Cloud Provider Backoff Exponent == 0 { config . Cloud Provider Backoff Exponent = backoff Exponent } else if config . should Omit Cloud Provider if config . Cloud Provider Backoff Jitter == 0 { config . Cloud Provider Backoff Jitter = backoff Jitter } else if config . should Omit Cloud Provider if ! config . should Omit Cloud Provider Backoff ( ) { resource Request Backoff = wait . Backoff { Steps : config . Cloud Provider Backoff Retries , Factor : config . Cloud Provider Backoff Exponent , Duration : time . Duration ( config . Cloud Provider Backoff Duration ) * time . Second , Jitter : config . Cloud Provider Backoff klog . V ( 2 ) . Infof ( " " , config . Cloud Provider Backoff Retries , config . Cloud Provider Backoff Exponent , config . Cloud Provider Backoff Duration , config . Cloud Provider Backoff } else { // Cloud Provider Backoff Retries will be set to 1 by default as the requirements of Azure SDK. config . Cloud Provider Backoff config . Cloud Provider Backoff Duration = backoff Duration if strings . Equal Fold ( config . Load Balancer Sku , load Balancer Sku Standard ) { // Do not add master nodes to standard LB by default. if config . Exclude Master From Standard LB == nil { config . Exclude Master From Standard LB = & default Exclude Master From Standard // Enable outbound SNAT by default. if config . Disable Outbound SNAT == nil { config . Disable Outbound SNAT = & default Disable Outbound } else { if config . Disable Outbound SNAT != nil && * config . Disable Outbound az Client Config := & az Client Config { subscription ID : config . Subscription ID , resource Manager Endpoint : env . Resource Manager Endpoint , service Principal Token : service Principal Token , rate Limiter Reader : operation Poll Rate Limiter , rate Limiter Writer : operation Poll Rate Limiter Write , Cloud Provider Backoff Retries : config . Cloud Provider Backoff Retries , Cloud Provider Backoff Duration : config . Cloud Provider Backoff Duration , Should Omit Cloud Provider Backoff : config . should Omit Cloud Provider az := Cloud { Config : * config , Environment : * env , node Zones : map [ string ] sets . String { } , node Resource Groups : map [ string ] string { } , unmanaged Nodes : sets . New String ( ) , route CID Rs : map [ string ] string { } , resource Request Backoff : resource Request Backoff , Disks Client : new Az Disks Client ( az Client Config ) , Snapshots Client : new Snapshots Client ( az Client Config ) , Routes Client : new Az Routes Client ( az Client Config ) , Subnets Client : new Az Subnets Client ( az Client Config ) , Interfaces Client : new Az Interfaces Client ( az Client Config ) , Route Tables Client : new Az Route Tables Client ( az Client Config ) , Load Balancer Client : new Az Load Balancers Client ( az Client Config ) , Security Groups Client : new Az Security Groups Client ( az Client Config ) , Storage Account Client : new Az Storage Account Client ( az Client Config ) , Virtual Machines Client : new Az Virtual Machines Client ( az Client Config ) , Public IP Addresses Client : new Az Public IP Addresses Client ( az Client Config ) , Virtual Machine Sizes Client : new Az Virtual Machine Sizes Client ( az Client Config ) , Virtual Machine Scale Sets Client : new Az Virtual Machine Scale Sets Client ( az Client Config ) , Virtual Machine Scale Set V Ms Client : new Az Virtual Machine Scale Set V Ms Client ( az Client Config ) , File Client : & azure File az . metadata , err = New Instance Metadata Service ( metadata if az . Maximum Load Balancer Rule Count == 0 { az . Maximum Load Balancer Rule Count = maximum Load Balancer Rule if strings . Equal Fold ( vm Type VMSS , az . Config . VM Type ) { az . vm Set , err = new Scale } else { az . vm Set = new Availability az . vm Cache , err = az . new VM az . lb Cache , err = az . new LB az . nsg Cache , err = az . new NSG az . rt Cache , err = az . new Route Table if err := init Disk } 
func parse Config ( config if config config Contents , err := ioutil . Read All ( config err = yaml . Unmarshal ( config // The resource group name may be in different cases from different Azure AP Is, hence it is converted to lower here. // See more context at https://github.com/kubernetes/kubernetes/issues/71994. config . Resource Group = strings . To Lower ( config . Resource } 
func ( az * Cloud ) Initialize ( client Builder cloudprovider . Controller Client Builder , stop <- chan struct { } ) { az . kube Client = client Builder . Client Or az . event Broadcaster = record . New az . event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : az . kube Client . Core az . event Recorder = az . event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event } 
func configure User Agent ( client * autorest . Client ) { k8s Version := version . Get ( ) . Git client . User Agent = fmt . Sprintf ( " " , client . User Agent , k8s } 
func ( az * Cloud ) Set Informers ( informer Factory informers . Shared Informer node Informer := informer node Informer . Add Event Handler ( cache . Resource Event Handler Funcs { Add az . update Node } , Update Func : func ( prev , obj interface { } ) { prev new if new Node . Labels [ v1 . Label Zone Failure Domain ] == prev Node . Labels [ v1 . Label Zone Failure az . update Node Caches ( prev Node , new } , Delete Func : func ( obj interface { } ) { node , is // We can get Deleted Final State Unknown instead of *v1.Node here // and we need to handle that correctly. if ! is Node { deleted State , ok := obj . ( cache . Deleted Final State node , ok = deleted if ! ok { klog . Errorf ( " " , deleted az . update Node az . node Informer Synced = node Informer . Has } 
func ( az * Cloud ) update Node Caches ( prev Node , new Node * v1 . Node ) { az . node Caches defer az . node Caches if prev Node != nil { // Remove from node Zones cache. prev Zone , ok := prev Node . Object Meta . Labels [ v1 . Label Zone Failure if ok && az . is Availability Zone ( prev Zone ) { az . node Zones [ prev Zone ] . Delete ( prev Node . Object if az . node Zones [ prev Zone ] . Len ( ) == 0 { az . node Zones [ prev // Remove from node Resource Groups cache. _ , ok = prev Node . Object Meta . Labels [ external Resource Group if ok { delete ( az . node Resource Groups , prev Node . Object // Remove from unmanaged Nodes cache. managed , ok := prev Node . Object Meta . Labels [ managed By Azure if ok && managed == " " { az . unmanaged Nodes . Delete ( prev Node . Object if new Node != nil { // Add to node Zones cache. new Zone , ok := new Node . Object Meta . Labels [ v1 . Label Zone Failure if ok && az . is Availability Zone ( new Zone ) { if az . node Zones [ new Zone ] == nil { az . node Zones [ new Zone ] = sets . New az . node Zones [ new Zone ] . Insert ( new Node . Object // Add to node Resource Groups cache. new RG , ok := new Node . Object Meta . Labels [ external Resource Group if ok && len ( new RG ) > 0 { az . node Resource Groups [ new Node . Object Meta . Name ] = strings . To Lower ( new // Add to unmanaged Nodes cache. managed , ok := new Node . Object Meta . Labels [ managed By Azure if ok && managed == " " { az . unmanaged Nodes . Insert ( new Node . Object } 
func ( az * Cloud ) Get Active Zones ( ) ( sets . String , error ) { if az . node Informer az . node Caches defer az . node Caches if ! az . node Informer zones := sets . New for zone , nodes := range az . node } 
func ( az * Cloud ) Get Node Resource Group ( node Name string ) ( string , error ) { // Kubelet won't set az.node Informer Synced, always return configured resource Group. if az . node Informer Synced == nil { return az . Resource az . node Caches defer az . node Caches if ! az . node Informer // Return external resource group if it has been cached. if cached RG , ok := az . node Resource Groups [ node Name ] ; ok { return cached // Return resource group from cloud provider options. return az . Resource } 
func ( az * Cloud ) Get Resource Groups ( ) ( sets . String , error ) { // Kubelet won't set az.node Informer Synced, always return configured resource Group. if az . node Informer Synced == nil { return sets . New String ( az . Resource az . node Caches defer az . node Caches if ! az . node Informer resource Groups := sets . New String ( az . Resource for _ , rg := range az . node Resource Groups { resource return resource } 
func ( az * Cloud ) Get Unmanaged Nodes ( ) ( sets . String , error ) { // Kubelet won't set az.node Informer Synced, always return nil. if az . node Informer az . node Caches defer az . node Caches if ! az . node Informer return sets . New String ( az . unmanaged } 
func ( az * Cloud ) Should Node Excluded From Load Balancer ( node * v1 . Node ) bool { labels := node . Object if rg , ok := labels [ external Resource Group Label ] ; ok && ! strings . Equal Fold ( rg , az . Resource if managed , ok := labels [ managed By Azure } 
func ( d * Disallow Flunder ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { // we are only interested in flunders if a . Get Kind ( ) . Group if ! d . Wait For Ready ( ) { return admission . New meta Accessor , err := meta . Accessor ( a . Get flunder Name := meta Accessor . Get for _ , fischer := range fischers { for _ , disallowed Flunder := range fischer . Disallowed Flunders { if flunder Name == disallowed Flunder { return errors . New Forbidden ( a . Get Resource ( ) . Group Resource ( ) , a . Get } 
func ( d * Disallow Flunder ) Set Internal Wardle Informer Factory ( f informers . Shared Informer d . Set Ready Func ( f . Wardle ( ) . V1alpha1 ( ) . Fischers ( ) . Informer ( ) . Has } 
func ( d * Disallow Flunder ) Validate } 
func New ( ) ( * Disallow Flunder , error ) { return & Disallow Flunder { Handler : admission . New } 
func New Cmd Create Cron Job ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Create Cron Job Options ( io cmd := & cobra . Command { Use : " " , Aliases : [ ] string { " " } , Short : cronjob Long , Long : cronjob Long , Example : cronjob Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check o . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Dry Run cmd . Flags ( ) . String cmd . Flags ( ) . String cmd . Flags ( ) . String } 
func Create New Tokens ( client clientset . Interface , tokens [ ] kubeadmapi . Bootstrap Token ) error { return Update Or Create } 
func Update Or Create Tokens ( client clientset . Interface , fail If Exists bool , tokens [ ] kubeadmapi . Bootstrap Token ) error { for _ , token := range tokens { secret Name := bootstraputil . Bootstrap Token Secret secret , err := client . Core V1 ( ) . Secrets ( metav1 . Namespace System ) . Get ( secret Name , metav1 . Get if secret != nil && err == nil && fail If updated Or New Secret := token . To // Try to create or update the token with an exponential backoff err = apiclient . Try Run Command ( func ( ) error { if err := apiclient . Create Or Update Secret ( client , updated Or New Secret ) ; err != nil { return errors . Wrapf ( err , " " , secret } 
func Kubernetes Release Version ( version string ) ( string , error ) { ver := normalized Build bucket URL , version Label , err := split // revalidate, if exact build from e.g. CI bucket requested. ver = normalized Build Version ( version // kube Release Label Regex matches labels such as: latest, latest-1, latest-1.10 if kube Release Label Regex . Match String ( version Label ) { // Try to obtain a client version. // pkgversion.Get().String() should always return a correct version added by the golang // linker and the build system. The version can still be missing when doing unit tests // on individual packages. client Version , client Version Err := kubeadm // Fetch version from the internet. url := fmt . Sprintf ( " " , bucket URL , version body , err := fetch From URL ( url , get Release Version if err != nil { // If the network operaton was successful but the server did not reply with Status if client Version klog . Warningf ( " " , client return Kubernetes Release Version ( client if client Version Err != nil { if err != nil { klog . Warningf ( " " , constants . Current Kubernetes return Kubernetes Release Version ( constants . Current Kubernetes return Kubernetes Release // both the client and the remote version are obtained; validate them and pick a stable version body , err = validate Stable Version ( body , client // Re-validate received version and return. return Kubernetes Release } 
func Kubernetes Version To Image Tag ( version string ) string { allowed := regexp . Must Compile ( `[^-a-z return allowed . Replace All } 
func Kubernetes Is CI Version ( version string ) bool { subs := kube Bucket Prefixes . Find All String if len ( subs ) == 1 && len ( subs [ 0 ] ) == 4 && strings . Has } 
func normalized Build Version ( version string ) string { if kube Release Regex . Match String ( version ) { if strings . Has } 
func split Version ( version string ) ( string , string , error ) { var url subs := kube Bucket Prefixes . Find All String switch { case strings . Has Prefix ( subs [ 0 ] [ 2 ] , " " ) : // Just use whichever the user specified url default : url url := fmt . Sprintf ( " " , kube Release Bucket URL , url } 
func fetch From client := & http . Client { Timeout : timeout , Transport : netutil . Set Old Transport body , err := ioutil . Read body String := strings . Trim if resp . Status Code != http . Status return body return body } 
func kubeadm Version ( info string ) ( string , error ) { v , err := versionutil . Parse // There is no utility in versionutil to get the version without the metadata, // so this needs some manual formatting. // Discard offsets after a release label and keep the labels down to e.g. `alpha.0` instead of // including the offset e.g. `alpha.0.206`. This is done to comply with GCR image tags. pre := v . Pre v return v } 
func validate Stable Version ( remote Version , client Version string ) ( string , error ) { ver Remote , err := versionutil . Parse Generic ( remote ver Client , err := versionutil . Parse Generic ( client // If the remote Major version is bigger or if the Major versions are the same, // but the remote Minor is bigger use the client version release. This handles Major bumps too. if ver Client . Major ( ) < ver Remote . Major ( ) || ( ver Client . Major ( ) == ver Remote . Major ( ) ) && ver Client . Minor ( ) < ver Remote . Minor ( ) { estimated Release := fmt . Sprintf ( " " , ver Client . Major ( ) , ver klog . Infof ( " " , remote Version , estimated return estimated return remote } 
func Funcs ( codecs runtimeserializer . Codec Factory ) [ ] interface { } { return [ ] interface { } { fuzz Init Configuration , fuzz Cluster Configuration , fuzz Component Configs , fuzz DNS , fuzz Local Etcd , fuzz Networking , fuzz Join } 
func Convert_v1alpha1_Flunder Spec_To_wardle_Flunder Spec ( in * Flunder Spec , out * wardle . Flunder Spec , s conversion . Scope ) error { if in . Reference Type != nil { // assume that Reference Type is defaulted switch * in . Reference Type { case Flunder Reference Type : out . Reference Type = wardle . Flunder Reference out . Flunder case Fischer Reference Type : out . Reference Type = wardle . Fischer Reference out . Fischer } 
func ( c * Fake Cron Jobs ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Cron Job , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( cronjobs Resource , c . ns , name ) , & v1beta1 . Cron return obj . ( * v1beta1 . Cron } 
func ( c * Fake Cron Jobs ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( cronjobs Resource , c . ns , name ) , & v1beta1 . Cron } 
func ( c * Fake Cron Jobs ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( cronjobs Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Cron Job } 
func ( c * Fake Cron Jobs ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Cron Job , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( cronjobs Resource , c . ns , name , pt , data , subresources ... ) , & v1beta1 . Cron return obj . ( * v1beta1 . Cron } 
func Milli CPU To Quota ( milli CPU int64 , period int64 ) ( quota int64 ) { // CFS quota is measured in two values: // - cfs_period_us=100ms (the amount of time to measure usage across given by period) // - cfs_quota=20ms (the amount of cpu time allowed to be used across a period) // so in the above example, you are limited to 20% of a single CPU // for multi-cpu environments, you just scale equivalent amounts // see https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt for details if milli if ! utilfeature . Default Feature Gate . Enabled ( kubefeatures . CPUCFS Quota Period ) { period = Quota // we then convert your milli CPU to a value normalized over a period quota = ( milli CPU * period ) / Milli CPU To // quota needs to be a minimum of 1ms. if quota < Min Quota Period { quota = Min Quota } 
func Milli CPU To Shares ( milli CPU int64 ) uint64 { if milli CPU == 0 { // Docker converts zero milli CPU to unset, which maps to kernel default // for unset: 1024. Return 2 here to really match kernel default for // zero milli CPU. return Min // Conceptually (milli CPU / milli CPU To CPU) * shares Per CPU, but factored to improve rounding. shares := ( milli CPU * Shares Per CPU ) / Milli CPU To if shares < Min Shares { return Min } 
func Huge Page Limits ( resource List v1 . Resource List ) map [ int64 ] int64 { huge Page for k , v := range resource List { if v1helper . Is Huge Page Resource Name ( k ) { page Size , _ := v1helper . Huge Page Size From Resource if value , exists := huge Page Limits [ page Size . Value ( ) ] ; exists { huge Page Limits [ page } else { huge Page Limits [ page return huge Page } 
func Resource Config For Pod ( pod * v1 . Pod , enforce CPU Limits bool , cpu Period uint64 ) * Resource Config { // sum requests and limits. reqs , limits := resource . Pod Requests And cpu cpu memory if request , found := reqs [ v1 . Resource CPU ] ; found { cpu Requests = request . Milli if limit , found := limits [ v1 . Resource CPU ] ; found { cpu Limits = limit . Milli if limit , found := limits [ v1 . Resource Memory ] ; found { memory // convert to CFS values cpu Shares := Milli CPU To Shares ( cpu cpu Quota := Milli CPU To Quota ( cpu Limits , int64 ( cpu // track if limits were applied for each resource. memory Limits cpu Limits // map hugepage pagesize (bytes) to limits (bytes) huge Page for _ , container := range pod . Spec . Containers { if container . Resources . Limits . Cpu ( ) . Is Zero ( ) { cpu Limits if container . Resources . Limits . Memory ( ) . Is Zero ( ) { memory Limits container Huge Page Limits := Huge Page for k , v := range container Huge Page Limits { if value , exists := huge Page Limits [ k ] ; exists { huge Page } else { huge Page // quota is not capped when cfs quota is disabled if ! enforce CPU Limits { cpu // determine the qos class qos Class := v1qos . Get Pod // build the result result := & Resource if qos Class == v1 . Pod QOS Guaranteed { result . Cpu Shares = & cpu result . Cpu Quota = & cpu result . Cpu Period = & cpu result . Memory = & memory } else if qos Class == v1 . Pod QOS Burstable { result . Cpu Shares = & cpu if cpu Limits Declared { result . Cpu Quota = & cpu result . Cpu Period = & cpu if memory Limits Declared { result . Memory = & memory } else { shares := uint64 ( Min result . Cpu result . Huge Page Limit = huge Page } 
func Get Cgroup Subsystems ( ) ( * Cgroup Subsystems , error ) { // get all cgroup mounts. all Cgroups , err := libcontainercgroups . Get Cgroup if err != nil { return & Cgroup if len ( all Cgroups ) == 0 { return & Cgroup mount Points := make ( map [ string ] string , len ( all for _ , mount := range all Cgroups { for _ , subsystem := range mount . Subsystems { mount return & Cgroup Subsystems { Mounts : all Cgroups , Mount Points : mount } 
func get Cgroup Procs ( dir string ) ( [ ] int , error ) { procs f , err := os . Open ( procs if err != nil { if os . Is Not Exist ( err ) { // The procs s := bufio . New if err != nil { return nil , fmt . Errorf ( " " , procs } 
func New May Run As ( ranges [ ] policy . ID Range ) ( Group return & may Run } 
func ( s * may Run As ) Validate ( fld Path * field . Path , _ * api . Pod , groups [ ] int64 ) field . Error List { return Validate Groups In Ranges ( fld } 
func ( * Volume Capability ) XXX_Oneof Funcs ( ) ( func ( msg proto . Message , b * proto . Buffer ) error , func ( msg proto . Message , tag , wire int , b * proto . Buffer ) ( bool , error ) , func ( msg proto . Message ) ( n int ) , [ ] interface { } ) { return _Volume Capability_Oneof Marshaler , _Volume Capability_Oneof Unmarshaler , _Volume Capability_Oneof Sizer , [ ] interface { } { ( * Volume Capability_Block ) ( nil ) , ( * Volume } 
func ( * Controller Service Capability ) XXX_Oneof Funcs ( ) ( func ( msg proto . Message , b * proto . Buffer ) error , func ( msg proto . Message , tag , wire int , b * proto . Buffer ) ( bool , error ) , func ( msg proto . Message ) ( n int ) , [ ] interface { } ) { return _Controller Service Capability_Oneof Marshaler , _Controller Service Capability_Oneof Unmarshaler , _Controller Service Capability_Oneof Sizer , [ ] interface { } { ( * Controller Service } 
func ( * Node Service Capability ) XXX_Oneof Funcs ( ) ( func ( msg proto . Message , b * proto . Buffer ) error , func ( msg proto . Message , tag , wire int , b * proto . Buffer ) ( bool , error ) , func ( msg proto . Message ) ( n int ) , [ ] interface { } ) { return _Node Service Capability_Oneof Marshaler , _Node Service Capability_Oneof Unmarshaler , _Node Service Capability_Oneof Sizer , [ ] interface { } { ( * Node Service } 
func ( c * node Client ) Node Get Id ( ctx context . Context , in * Node Get Id Request , opts ... grpc . Call Option ) ( * Node Get Id Response , error ) { out := new ( Node Get Id } 
func New REST ( opts Getter generic . REST Options Getter ) ( * REST , * Status REST , * Finalize REST ) { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Namespace { } } , New List Func : func ( ) runtime . Object { return & api . Namespace List { } } , Predicate Func : namespace . Match Namespace , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : namespace . Strategy , Update Strategy : namespace . Strategy , Delete Strategy : namespace . Strategy , Return Deleted Object : true , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts Getter , Attr Func : namespace . Get if err := store . Complete With status status Store . Update Strategy = namespace . Status finalize finalize Store . Update Strategy = namespace . Finalize return & REST { store : store , status : & status Store } , & Status REST { store : & status Store } , & Finalize REST { store : & finalize } 
func ( r * REST ) Delete ( ctx context . Context , name string , options * metav1 . Delete Options ) ( runtime . Object , bool , error ) { ns Obj , err := r . Get ( ctx , name , & metav1 . Get namespace := ns // Ensure we have a UID precondition if options == nil { options = metav1 . New Delete } else if * options . Preconditions . UID != namespace . UID { err = apierrors . New if options . Preconditions . Resource Version != nil && * options . Preconditions . Resource Version != namespace . Resource Version { err = apierrors . New Conflict ( api . Resource ( " " ) , name , fmt . Errorf ( " " , * options . Preconditions . Resource Version , namespace . Resource // upon first request to delete, we switch the phase to start namespace termination // TODO: enhance graceful deletion's calls to Delete Strategy to allow phase change and finalizer patterns if namespace . Deletion Timestamp . Is Zero ( ) { key , err := r . store . Key preconditions := storage . Preconditions { UID : options . Preconditions . UID , Resource Version : options . Preconditions . Resource out := r . store . New err = r . store . Storage . Guaranteed Update ( ctx , key , out , false , & preconditions , storage . Simple Update ( func ( existing runtime . Object ) ( runtime . Object , error ) { existing // Set the deletion timestamp if needed if existing Namespace . Deletion Timestamp . Is existing Namespace . Deletion // Set the namespace phase to terminating, if needed if existing Namespace . Status . Phase != api . Namespace Terminating { existing Namespace . Status . Phase = api . Namespace // the current finalizers which are on namespace current for _ , f := range existing Namespace . Finalizers { current // the finalizers we should ensure on namespace should Have Finalizers := map [ string ] bool { metav1 . Finalizer Orphan Dependents : should Have Orphan Finalizer ( options , current Finalizers [ metav1 . Finalizer Orphan Dependents ] ) , metav1 . Finalizer Delete Dependents : should Have Delete Dependents Finalizer ( options , current Finalizers [ metav1 . Finalizer Delete // determine whether there are changes change for finalizer , should Have := range should Have Finalizers { change Needed = current Finalizers [ finalizer ] != should Have || change if should Have { current } else { delete ( current // make the changes if needed if change Needed { new for f := range current Finalizers { new Finalizers = append ( new existing Namespace . Finalizers = new return existing } ) , dryrun . Is Dry Run ( options . Dry if err != nil { err = storageerr . Interpret Get err = storageerr . Interpret Update if _ , ok := err . ( * apierrors . Status Error ) ; ! ok { err = apierrors . New Internal // prior to final deletion, we must ensure that finalizers is empty if len ( namespace . Spec . Finalizers ) != 0 { err = apierrors . New } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Limited Resource ) ( nil ) , ( * resourcequota . Limited Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Limited Resource_To_resourcequota_Limited Resource ( a . ( * Limited Resource ) , b . ( * resourcequota . Limited if err := s . Add Generated Conversion Func ( ( * resourcequota . Limited Resource ) ( nil ) , ( * Limited Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_resourcequota_Limited Resource_To_v1alpha1_Limited Resource ( a . ( * resourcequota . Limited Resource ) , b . ( * Limited } 
func Convert_v1alpha1_Configuration_To_resourcequota_Configuration ( in * Configuration , out * resourcequota . Configuration , s conversion . Scope ) error { return auto } 
func Convert_resourcequota_Configuration_To_v1alpha1_Configuration ( in * resourcequota . Configuration , out * Configuration , s conversion . Scope ) error { return auto } 
func Convert_v1alpha1_Limited Resource_To_resourcequota_Limited Resource ( in * Limited Resource , out * resourcequota . Limited Resource , s conversion . Scope ) error { return auto Convert_v1alpha1_Limited Resource_To_resourcequota_Limited } 
func Convert_resourcequota_Limited Resource_To_v1alpha1_Limited Resource ( in * resourcequota . Limited Resource , out * Limited Resource , s conversion . Scope ) error { return auto Convert_resourcequota_Limited Resource_To_v1alpha1_Limited } 
func ( s * limit Range Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Limit Range , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Limit } 
func ( s * limit Range Lister ) Limit Ranges ( namespace string ) Limit Range Namespace Lister { return limit Range Namespace } 
func ( s limit Range Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Limit Range , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Limit } 
func ( b * storageos Mounter ) Set Up ( fs Group * int64 ) error { // Need a namespace to find the volume, try pod's namespace if not set. if b . vol Namespace == " " { klog . V ( 2 ) . Infof ( " " , b . pod b . vol Namespace = b . pod target Path := make Global PD Name ( b . plugin . host , b . pv Name , b . vol Namespace , b . vol // Attach the device to the host. if err := b . manager . Attach Device ( b , target Path ) ; err != nil { klog . Errorf ( " " , target // Attach the Storage OS volume as a block device device Path , err := b . manager . Attach if err != nil { klog . Errorf ( " " , b . vol // Mount the loop device into the plugin's disk global mount dir. err = b . manager . Mount Volume ( b , device Path , target klog . V ( 4 ) . Infof ( " " , b . vol // Bind mount the volume into the pod return b . Set Up At ( b . Get Path ( ) , fs } 
func get Volume Info ( pv Name string , pod UID types . UID , host volume . Volume Host ) ( string , string , error ) { if vol Namespace , vol Name , err := get Volume From Ref ( pv Name ) ; err == nil { return vol Namespace , vol volume Dir := filepath . Dir ( host . Get Pod Volume Dir ( pod UID , utilstrings . Escape Qualified Name ( storageos Plugin Name ) , pv files , err := ioutil . Read Dir ( volume for _ , f := range files { if f . Mode ( ) . Is Dir ( ) && strings . Has Prefix ( f . Name ( ) , pv Name + " " ) { if vol Namespace , vol Name , err := get Volume From Ref ( f . Name ( ) ) ; err == nil { return vol Namespace , vol return " " , " " , fmt . Errorf ( " " , pv Name , volume } 
func get Volume From Ref ( ref string ) ( vol Namespace string , vol Name string , err error ) { ref switch len ( ref Parts ) { case 2 : return ref Parts [ 0 ] , ref case 3 : return ref Parts [ 1 ] , ref } 
func ( storageos Volume * storageos ) Get Path ( ) string { return get Path ( storageos Volume . pod UID , storageos Volume . vol Namespace , storageos Volume . vol Name , storageos Volume . pv Name , storageos } 
func ( b * storageos Unmounter ) Tear Down ( ) error { if len ( b . vol Namespace ) == 0 || len ( b . vol Name ) == 0 { klog . Warningf ( " " , b . vol Namespace , b . vol // Unmount from pod mount Path := b . Get err := b . Tear Down At ( mount // Find device name from global mount global PD Path := make Global PD Name ( b . plugin . host , b . pv Name , b . vol Namespace , b . vol device Path , _ , err := mount . Get Device Name From Mount ( b . mounter , global PD // Unmount from plugin's disk global mount dir. err = b . Tear Down At ( global PD // Detach loop device err = b . manager . Detach Volume ( b , device if err != nil { klog . Errorf ( " " , device Path , b . pv klog . V ( 4 ) . Infof ( " " , b . pv } 
func ( b * storageos Unmounter ) Tear Down At ( dir string ) error { if err := mount . Cleanup Mount Point ( dir , b . mounter , false ) ; err != nil { klog . V ( 4 ) . Infof ( " " , b . pv if err := b . manager . Unmount Volume ( b ) ; err != nil { klog . V ( 4 ) . Infof ( " " , b . pv } 
func get Volume Info From Spec ( spec * volume . Spec ) ( string , string , string , bool , error ) { if spec . Persistent Volume != nil { source , read Only , err := get Persistent Volume return source . Volume Name , source . Volume Namespace , source . FS Type , read if spec . Volume != nil { source , read Only , err := get Volume return source . Volume Name , source . Volume Namespace , source . FS Type , read } 
func get API Cfg ( spec * volume . Spec , pod * v1 . Pod , kube Client clientset . Interface ) ( * storageos API Config , error ) { if spec . Persistent Volume != nil { source , _ , err := get Persistent Volume if source . Secret return parse PV Secret ( source . Secret Ref . Namespace , source . Secret Ref . Name , kube if spec . Volume != nil { source , _ , err := get Volume if source . Secret return parse Pod Secret ( pod , source . Secret Ref . Name , kube } 
func parse PV Secret ( namespace , secret Name string , kube Client clientset . Interface ) ( * storageos API Config , error ) { secret , err := util . Get Secret For PV ( namespace , secret Name , storageos Plugin Name , kube if err != nil { klog . Errorf ( " " , namespace , secret return nil , fmt . Errorf ( " " , namespace , secret return parse API } 
func parse API Config ( params map [ string ] string ) ( * storageos API c := & storageos API for name , data := range params { switch strings . To Lower ( name ) { case " " : c . api case " " : c . api case " " : c . api case " " : c . api } 
func New Namespace Controller ( kube Client clientset . Interface , dynamic Client dynamic . Interface , discover Resources Fn func ( ) ( [ ] * metav1 . API Resource List , error ) , namespace Informer coreinformers . Namespace Informer , resync Period time . Duration , finalizer Token v1 . Finalizer Name ) * Namespace Controller { // create the controller so we can inject the enqueue function namespace Controller := & Namespace Controller { queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , namespaced Resources Deleter : deletion . New Namespaced Resources Deleter ( kube Client . Core V1 ( ) . Namespaces ( ) , dynamic Client , kube Client . Core V1 ( ) , discover Resources Fn , finalizer if kube Client != nil && kube Client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { metrics . Register Metric And Track Rate Limiter Usage ( " " , kube Client . Core V1 ( ) . REST Client ( ) . Get Rate // configure the namespace informer event handlers namespace Informer . Informer ( ) . Add Event Handler With Resync Period ( cache . Resource Event Handler Funcs { Add namespace Controller . enqueue } , Update Func : func ( old Obj , new Obj interface { } ) { namespace := new namespace Controller . enqueue } , } , resync namespace Controller . lister = namespace namespace Controller . lister Synced = namespace Informer . Informer ( ) . Has return namespace } 
func ( nm * Namespace Controller ) enqueue Namespace ( obj interface { } ) { key , err := controller . Key if err != nil { utilruntime . Handle // don't queue if we aren't deleted if namespace . Deletion Timestamp == nil || namespace . Deletion Timestamp . Is // delay processing namespace events to allow HA api servers to observe namespace deletion, // and HA etcd servers to observe last minute object creations inside the namespace nm . queue . Add After ( key , namespace Deletion Grace } 
func ( nm * Namespace Controller ) worker ( ) { work err := nm . sync Namespace From if estimate , ok := err . ( * deletion . Resources Remaining nm . queue . Add } else { // rather than wait for a full resync, re-add the namespace to the queue to be processed nm . queue . Add Rate utilruntime . Handle for { quit := work } 
func ( nm * Namespace Controller ) sync Namespace From Key ( key string ) ( err error ) { start defer func ( ) { klog . V ( 4 ) . Infof ( " " , key , time . Since ( start if errors . Is Not if err != nil { utilruntime . Handle return nm . namespaced Resources } 
func ( nm * Namespace Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer nm . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , nm . lister for i := 0 ; i < workers ; i ++ { go wait . Until ( nm . worker , time . Second , stop <- stop } 
func ( kl * Kubelet ) List Volumes For Pod ( pod UID types . UID ) ( map [ string ] volume . Volume , bool ) { volumes To pod Volumes := kl . volume Manager . Get Mounted Volumes For Pod ( volumetypes . Unique Pod Name ( pod for outer Volume Spec Name , volume := range pod volumes To Return [ outer Volume Spec return volumes To Return , len ( volumes To } 
func ( kl * Kubelet ) pod Volumes Exist ( pod UID types . UID ) bool { if mounted Volumes := kl . volume Manager . Get Mounted Volumes For Pod ( volumetypes . Unique Pod Name ( pod UID ) ) ; len ( mounted // TODO: This checks pod volume paths and whether they are mounted. If checking returns error, pod Volumes Exist will return true // which means we consider volumes might exist and requires further checking. // There are some volume plugins such as flexvolume might not have mounts. See issue #61229 volume Paths , err := kl . get Mounted Volume Path List From Disk ( pod if err != nil { klog . Errorf ( " " , pod if len ( volume Paths ) > 0 { klog . V ( 4 ) . Infof ( " " , pod UID , volume } 
func ( kl * Kubelet ) new Volume Mounter From Plugins ( spec * volume . Spec , pod * v1 . Pod , opts volume . Volume Options ) ( volume . Mounter , error ) { plugin , err := kl . volume Plugin Mgr . Find Plugin By physical Mounter , err := plugin . New if err != nil { return nil , fmt . Errorf ( " " , spec . Name ( ) , plugin . Get Plugin klog . V ( 10 ) . Infof ( " " , plugin . Get Plugin return physical } 
func ( kl * Kubelet ) cleanup Orphaned Pod Dirs ( pods [ ] * v1 . Pod , running Pods [ ] * kubecontainer . Pod ) error { all Pods := sets . New for _ , pod := range pods { all for _ , pod := range running Pods { all found , err := kl . list Pods From orphan Removal orphan Volume for _ , uid := range found { if all // If volumes have not been unmounted/detached, do not delete directory. // Doing so may result in corruption of data. // TODO: get Mounted Volume Path List From Disk() call may be redundant with // kl.get Pod Volume Path List From Disk(). Can this be cleaned up? if pod Volumes Exist := kl . pod Volumes Exist ( uid ) ; pod Volumes // If there are still volume directories, do not delete directory volume Paths , err := kl . get Pod Volume Path List From if err != nil { orphan Volume Errors = append ( orphan Volume if len ( volume Paths ) > 0 { orphan Volume Errors = append ( orphan Volume // If there are any volume-subpaths, do not cleanup directories volume Subpath Exists , err := kl . pod Volume Subpaths Dir if err != nil { orphan Volume Errors = append ( orphan Volume if volume Subpath Exists { orphan Volume Errors = append ( orphan Volume if err := removeall . Remove All One Filesystem ( kl . mounter , kl . get Pod orphan Removal Errors = append ( orphan Removal log log Spew ( orphan Volume log Spew ( orphan Removal return utilerrors . New Aggregate ( orphan Removal } 
func ( t TTY ) Get Size ( ) * remotecommand . Terminal Size { out Fd , is Terminal := term . Get Fd if ! is return Get Size ( out } 
func Get Size ( fd uintptr ) * remotecommand . Terminal Size { winsize , err := term . Get if err != nil { runtime . Handle return & remotecommand . Terminal } 
func ( t * TTY ) Monitor Size ( initial Sizes ... * remotecommand . Terminal Size ) remotecommand . Terminal Size Queue { out Fd , is Terminal := term . Get Fd if ! is t . size Queue = & size Queue { t : * t , // make it buffered so we can send the initial terminal sizes without blocking, prior to starting // the streaming below resize Chan : make ( chan remotecommand . Terminal Size , len ( initial Sizes ) ) , stop t . size Queue . monitor Size ( out Fd , initial return t . size } 
func ( s * size Queue ) monitor Size ( out Fd uintptr , initial Sizes ... * remotecommand . Terminal Size ) { // send the initial sizes for i := range initial Sizes { if initial Sizes [ i ] != nil { s . resize Chan <- * initial resize Events := make ( chan remotecommand . Terminal monitor Resize Events ( out Fd , resize Events , s . stop // listen for resize events in the background go func ( ) { defer runtime . Handle for { select { case size , ok := <- resize select { // try to send the size to resize Chan, but don't block case s . resize case <- s . stop } 
func ( s * size Queue ) Next ( ) * remotecommand . Terminal Size { size , ok := <- s . resize } 
func New Kube Waiter ( client clientset . Interface , timeout time . Duration , writer io . Writer ) Waiter { return & Kube } 
func ( w * Kube Waiter ) Wait For return wait . Poll Immediate ( constants . API Call Retry Interval , w . timeout , func ( ) ( bool , error ) { health w . client . Discovery ( ) . REST Client ( ) . Get ( ) . Abs Path ( " " ) . Do ( ) . Status Code ( & health if health Status != http . Status } 
func ( w * Kube Waiter ) Wait For Pods With Label ( kv Label string ) error { last Known Pod return wait . Poll Immediate ( constants . API Call Retry Interval , w . timeout , func ( ) ( bool , error ) { list Opts := metav1 . List Options { Label Selector : kv pods , err := w . client . Core V1 ( ) . Pods ( metav1 . Namespace System ) . List ( list if err != nil { fmt . Fprintf ( w . writer , " \n " , kv if last Known Pod Number != len ( pods . Items ) { fmt . Fprintf ( w . writer , " \n " , len ( pods . Items ) , kv last Known Pod for _ , pod := range pods . Items { if pod . Status . Phase != v1 . Pod } 
func ( w * Kube Waiter ) Wait For Pod To Disappear ( pod Name string ) error { return wait . Poll Immediate ( constants . API Call Retry Interval , w . timeout , func ( ) ( bool , error ) { _ , err := w . client . Core V1 ( ) . Pods ( metav1 . Namespace System ) . Get ( pod Name , metav1 . Get if apierrors . Is Not Found ( err ) { fmt . Printf ( " \n " , pod } 
func ( w * Kube Waiter ) Wait For Healthy Kubelet ( inital Timeout time . Duration , healthz Endpoint string ) error { time . Sleep ( inital fmt . Printf ( " \n " , inital return Try Run Command ( func ( ) error { client := & http . Client { Transport : netutil . Set Old Transport resp , err := client . Get ( healthz fmt . Printf ( " \n " , healthz if resp . Status Code != http . Status fmt . Printf ( " \n " , healthz Endpoint , resp . Status } , 5 ) // a failure } 
func ( w * Kube Waiter ) Wait For Kubelet And Func ( f func ( ) error ) error { error go func ( err C chan error , waiter Waiter ) { if err := waiter . Wait For Healthy Kubelet ( 40 * time . Second , fmt . Sprintf ( " " , kubeadmconstants . Kubelet Healthz Port ) ) ; err != nil { err } ( error go func ( err C chan error , waiter Waiter ) { // This main goroutine sends whatever the f function returns (error or not) to the channel // This in order to continue on success (nil error), or just fail if the function returns an error err } ( error // This call is blocking until one of the goroutines sends to error Chan return <- error } 
func ( w * Kube Waiter ) Wait For Static Pod Control Plane Hashes ( node Name string ) ( map [ string ] string , error ) { component mirror Pod for _ , component := range constants . Control Plane Components { err = wait . Poll Immediate ( constants . API Call Retry Interval , w . timeout , func ( ) ( bool , error ) { component Hash , err = get Static Pod Single Hash ( w . client , node mirror Pod Hashes [ component ] = component return mirror Pod } 
func ( w * Kube Waiter ) Wait For Static Pod Single Hash ( node Name string , component string ) ( string , error ) { component Pod err = wait . Poll Immediate ( constants . API Call Retry Interval , w . timeout , func ( ) ( bool , error ) { component Pod Hash , err = get Static Pod Single Hash ( w . client , node return component Pod } 
func ( w * Kube Waiter ) Wait For Static Pod Hash Change ( node Name , component , previous Hash string ) error { return wait . Poll Immediate ( constants . API Call Retry Interval , w . timeout , func ( ) ( bool , error ) { hash , err := get Static Pod Single Hash ( w . client , node // We should continue polling until the UID changes if hash == previous } 
func get Static Pod Single Hash ( client clientset . Interface , node Name string , component string ) ( string , error ) { static Pod Name := fmt . Sprintf ( " " , component , node static Pod , err := client . Core V1 ( ) . Pods ( metav1 . Namespace System ) . Get ( static Pod Name , metav1 . Get static Pod Hash := static Pod . Annotations [ kubetypes . Config Hash Annotation fmt . Printf ( " \n " , static Pod Name , static Pod return static Pod } 
func Try Run Command ( f func ( ) error , failure Threshold int ) error { backoff := wait . Backoff { Duration : 5 * time . Second , Factor : 2 , // double the timeout for every failure Steps : failure return wait . Exponential } 
func ( c * Fake Stateful Sets ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Stateful Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( statefulsets Resource , c . ns , name ) , & v1beta1 . Stateful return obj . ( * v1beta1 . Stateful } 
func ( c * Fake Stateful Sets ) Create ( stateful Set * v1beta1 . Stateful Set ) ( result * v1beta1 . Stateful Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( statefulsets Resource , c . ns , stateful Set ) , & v1beta1 . Stateful return obj . ( * v1beta1 . Stateful } 
func ( c * Fake Stateful Sets ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( statefulsets Resource , c . ns , name ) , & v1beta1 . Stateful } 
func ( c * Fake Stateful Sets ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( statefulsets Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Stateful Set } 
func ( c * Fake Stateful Sets ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Stateful Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( statefulsets Resource , c . ns , name , pt , data , subresources ... ) , & v1beta1 . Stateful return obj . ( * v1beta1 . Stateful } 
func Parse Docker } 
func match Image Tag Or SHA ( inspected dockertypes . Image Inspect , image string ) bool { // The image string follows the grammar specified here // https://github.com/docker/distribution/blob/master/reference/reference.go#L4 named , err := dockerref . Parse Normalized _ , is digest , is if ! is Tagged && ! is if is Tagged { // Check the Repo Tags for a match. for _ , tag := range inspected . Repo Tags { // An image name (without the tag/digest) can be [hostname '/'] component ['/' component]* // Because either the Repo Tag or the name *may* contain the // hostname or not, we only check for the suffix match. if strings . Has Suffix ( image , tag ) || strings . Has } else { // TODO: We need to remove this hack when project atomic based // docker distro(s) like centos/fedora/rhel image fix problems on // their end. // Say the tag is "docker.io/busybox:latest" // and the image is "docker.io/library/busybox:latest" t , err := dockerref . Parse Normalized // the parsed/normalized tag will look like // reference.tagged Reference { // named // normalized tag would look like "docker.io/library/busybox:latest" // note the library get added in the string normalized if normalized if strings . Has Suffix ( image , normalized Tag ) || strings . Has Suffix ( normalized if is Digested { for _ , repo Digest := range inspected . Repo Digests { named , err := dockerref . Parse Normalized Named ( repo if err != nil { klog . V ( 4 ) . Infof ( " " , repo if d , is Digested := named . ( dockerref . Digested ) ; is } 
func match Image ID Only ( inspected dockertypes . Image digest , is if ! is } 
func Is No Routes switch err . ( type ) { case no Routes } 
func get I Pv4Default scanner := bufio . New for { line , err := scanner . Read //ignore the headers in the route info if strings . Has // Interested in fields: // 0 - interface name // 1 - destination address // 2 - gateway dest , err := parse IP ( fields [ 1 ] , family I gw , err := parse IP ( fields [ 2 ] , family I if ! dest . Equal ( net . I routes = append ( routes , Route { Interface : fields [ 0 ] , Destination : dest , Gateway : gw , Family : family I } 
func parse IP ( str string , family Address bytes , err := hex . Decode if family == family I Pv4 { if len ( bytes ) != net . I // Must be I Pv6 if len ( bytes ) != net . I } 
func get Matching Global IP ( addrs [ ] net . Addr , family Address ip , _ , err := net . Parse if member Of ( ip , family ) { if ip . Is Global } 
func get IP From Interface ( intf Name string , for Family Address Family , nw network Interfacer ) ( net . IP , error ) { intf , err := nw . Interface By Name ( intf if is Interface klog . V ( 4 ) . Infof ( " " , intf matching IP , err := get Matching Global IP ( addrs , for if matching IP != nil { klog . V ( 4 ) . Infof ( " " , int ( for Family ) , matching IP , intf return matching } 
func member Of ( ip net . IP , family Address Family ) bool { if ip . To4 ( ) != nil { return family == family I } else { return family == family I } 
func choose IP From Host Interfaces ( nw network for _ , family := range [ ] Address Family { family I Pv4 , family I for _ , intf := range intfs { if ! is Interface if is Loopback Or Point To for _ , addr := range addrs { ip , _ , err := net . Parse if ! member // TODO: Decide if should open up to allow I Pv6 LL As in future. if ! ip . Is Global } 
func Choose Host Interface ( ) ( net . IP , error ) { var nw network Interfacer = network if _ , err := os . Stat ( ipv4Route File ) ; os . Is Not Exist ( err ) { return choose IP From Host routes , err := get All Default return choose Host Interface From } 
func get All Default if len ( routes ) == 0 { return nil , no Routes } 
func choose Host Interface From Route ( routes [ ] Route , nw network Interfacer ) ( net . IP , error ) { for _ , family := range [ ] Address Family { family I Pv4 , family I final IP , err := get IP From if final IP != nil { klog . V ( 4 ) . Infof ( " " , final return final } 
func Choose Bind Address ( bind Address net . IP ) ( net . IP , error ) { if bind Address == nil || bind Address . Is Unspecified ( ) || bind Address . Is Loopback ( ) { host IP , err := Choose Host bind Address = host return bind } 
func Resource Attributes From ( user user . Info , in authorizationapi . Resource Attributes ) authorizer . Attributes Record { return authorizer . Attributes Record { User : user , Verb : in . Verb , Namespace : in . Namespace , API Group : in . Group , API Version : in . Version , Resource : in . Resource , Subresource : in . Subresource , Name : in . Name , Resource } 
func Non Resource Attributes From ( user user . Info , in authorizationapi . Non Resource Attributes ) authorizer . Attributes Record { return authorizer . Attributes Record { User : user , Resource } 
func Authorization Attributes From ( spec authorizationapi . Subject Access Review Spec ) authorizer . Attributes Record { user To Check := & user . Default Info { Name : spec . User , Groups : spec . Groups , UID : spec . UID , Extra : convert To User Info var authorization Attributes authorizer . Attributes if spec . Resource Attributes != nil { authorization Attributes = Resource Attributes From ( user To Check , * spec . Resource } else { authorization Attributes = Non Resource Attributes From ( user To Check , * spec . Non Resource return authorization } 
func New Storage ( opts Getter generic . REST Options Getter , kubelet Client Config client . Kubelet Client Config , proxy Transport http . Round Tripper ) ( * Node Storage , error ) { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Node { } } , New List Func : func ( ) runtime . Object { return & api . Node List { } } , Predicate Func : node . Match Node , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : node . Strategy , Update Strategy : node . Strategy , Delete Strategy : node . Strategy , Export Strategy : node . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts Getter , Attr Func : node . Get Attrs , Trigger Func : node . Node Name Trigger if err := store . Complete With status status Store . Update Strategy = node . Status // Set up REST handlers node REST := & REST { Store : store , proxy Transport : proxy status REST := & Status REST { store : & status proxy REST := & noderest . Proxy REST { Store : store , Proxy Transport : proxy // Build a Node Getter that looks up nodes using the REST handler node Getter := client . Node Getter Func ( func ( ctx context . Context , node Name string , options metav1 . Get Options ) ( * v1 . Node , error ) { obj , err := node REST . Get ( ctx , node // TODO: Remove the conversion. Consider only return the Node Addresses external err = k8s_api_v1 . Convert_core_Node_To_v1_Node ( node , external return external connection Info Getter , err := client . New Node Connection Info Getter ( node Getter , kubelet Client node REST . connection = connection Info proxy REST . Connection = connection Info return & Node Storage { Node : node REST , Status : status REST , Proxy : proxy REST , Kubelet Connection Info : connection Info } 
func ( r * REST ) Resource Location ( ctx context . Context , id string ) ( * url . URL , http . Round Tripper , error ) { return node . Resource Location ( r , r . connection , r . proxy } 
func ( in * Network Policy Port ) Deep Copy Into ( out * Network Policy * out = new ( intstr . Int Or } 
func New Remote Config Source ( source * apiv1 . Node Config Source ) ( Remote Config Source , string , error ) { // NOTE: Even though the API server validates the config, we check whether all *known* fields are // nil here, so that if a new API server allows a new config source type, old clients can send // an error message rather than crashing due to a nil pointer dereference. // Exactly one reference subfield of the config source must be non-nil. // Currently Config Map is the only reference subfield. if source . Config Map == nil { return nil , status . All Nil Subfields Error , fmt . Errorf ( " " , status . All Nil Subfields return & remote Config } 
func Decode Remote Config Source ( data [ ] byte ) ( Remote Config Source , error ) { // decode the remote config source _ , codecs , err := scheme . New Scheme And obj , err := runtime . Decode ( codecs . Universal // for now we assume we are trying to load an kubeletconfigv1beta1.Serialized Node Config Source, // this may need to be extended if e.g. a new version of the api is born cs , ok := obj . ( * kubeletconfiginternal . Serialized Node Config // we use the v1.Node Config Source type on internal and external, so no need to convert to external here source , _ , err := New Remote Config } 
func Equal Remote Config Sources ( a , b Remote Config Source ) bool { if a != nil && b != nil { return apiequality . Semantic . Deep Equal ( a . Node Config Source ( ) , b . Node Config } 
func ( s * dynamic Lister } 
func ( s * dynamic Lister } 
func ( ns * dynamic Namespace Lister Shim ) List ( selector labels . Selector ) ( ret [ ] runtime . Object , err error ) { objs , err := ns . namespace } 
func ( ns * dynamic Namespace Lister Shim ) Get ( name string ) ( runtime . Object , error ) { return ns . namespace } 
func Drop Disabled Fields ( pvc Spec , old PVC Spec * core . Persistent Volume Claim Spec ) { if ! utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) && ! volume Mode In Use ( old PVC Spec ) { pvc Spec . Volume if ! utilfeature . Default Feature Gate . Enabled ( features . Volume Snapshot Data Source ) && ! volume Snapshot Data Source In Use ( old PVC Spec ) { pvc Spec . Data } 
func ( ipuc Is Privileged User Check ) Check ( ) ( warnings , error List [ ] error ) { error if os . Getuid ( ) != 0 { error List = append ( error return nil , error } 
func Split Environment From Resources ( args [ ] string ) ( resources , env for _ , s := range args { // this method also has to understand env removal syntax, i.e. KEY- is Env := Is Environment Argument ( s ) || strings . Has switch { case first && is case ! first && is Env : env Args = append ( env case first && ! is case ! first && ! is return resources , env } 
func parse Into Env Var ( spec [ ] string , default Reader io . Reader , env Var Type string ) ( [ ] v1 . Env Var , [ ] string , error ) { env := [ ] v1 . Env exists := sets . New for _ , env Spec := range spec { switch { case ! Is Valid Environment Argument ( env Spec ) && ! strings . Has Suffix ( env Spec , " " ) : return nil , nil , fmt . Errorf ( " " , env Var case env Spec == " " : if default file Env , err := read Env ( default Reader , env Var env = append ( env , file case strings . Index ( env Spec , " " ) != - 1 : parts := strings . Split N ( env if len ( parts ) != 2 { return nil , nil , fmt . Errorf ( " " , env Var Type , env env = append ( env , v1 . Env case strings . Has Suffix ( env Spec , " " ) : remove = append ( remove , env Spec [ : len ( env default : return nil , nil , fmt . Errorf ( " " , env Var Type , env for _ , remove Label := range remove { if _ , found := exists [ remove Label ] ; found { return nil , nil , fmt . Errorf ( " " , env Var } 
func Parse Env ( spec [ ] string , default Reader io . Reader ) ( [ ] v1 . Env Var , [ ] string , error ) { return parse Into Env Var ( spec , default } 
func New Cmd Init ( out io . Writer , init Options * init Options ) * cobra . Command { if init Options == nil { init Options = new Init init Runner := workflow . New cmd := & cobra . Command { Use : " " , Short : " " , Run : func ( cmd * cobra . Command , args [ ] string ) { c , err := init Runner . Init kubeadmutil . Check data := c . ( * init fmt . Printf ( " \n " , data . cfg . Kubernetes err = init kubeadmutil . Check err = show Join kubeadmutil . Check } , Args : cobra . No // adds flags to the init command // init command local flags could be eventually inherited by the sub-commands automatically generated for phases Add Init Config Flags ( cmd . Flags ( ) , init Options . externalcfg , & init Options . feature Gates Add Init Other Flags ( cmd . Flags ( ) , init init Options . bto . Add Token init Options . bto . Add TTL options . Add Image Meta Flags ( cmd . Flags ( ) , & init Options . externalcfg . Image // defines additional flag that are not used by the init command but that could be eventually used // by the sub-commands automatically generated for phases init Runner . Set Additional Flags ( func ( flags * flag . Flag Set ) { options . Add Kube Config Flag ( flags , & init Options . kubeconfig options . Add Kube Config Dir Flag ( flags , & init Options . kubeconfig options . Add Control Plan Extra Args Flags ( flags , & init Options . externalcfg . API Server . Extra Args , & init Options . externalcfg . Controller Manager . Extra Args , & init Options . externalcfg . Scheduler . Extra // initialize the workflow runner with the list of phases init Runner . Append Phase ( phases . New Preflight init Runner . Append Phase ( phases . New Kubelet Start init Runner . Append Phase ( phases . New Certs init Runner . Append Phase ( phases . New Kube Config init Runner . Append Phase ( phases . New Control Plane init Runner . Append Phase ( phases . New Etcd init Runner . Append Phase ( phases . New Wait Control Plane init Runner . Append Phase ( phases . New Upload Config init Runner . Append Phase ( phases . New Upload Certs init Runner . Append Phase ( phases . New Mark Control Plane init Runner . Append Phase ( phases . New Bootstrap Token init Runner . Append Phase ( phases . New Addon // sets the data builder function, that will be used by the runner // both when running the entire workflow or single phases init Runner . Set Data Initializer ( func ( cmd * cobra . Command , args [ ] string ) ( workflow . Run Data , error ) { return new Init Data ( cmd , args , init // binds the Runner to kubeadm init command by altering // command help, adding --skip-phases flag and by adding phases subcommands init Runner . Bind To } 
func Add Init Config Flags ( flag Set * flag . Flag Set , cfg * kubeadmapiv1beta2 . Init Configuration , feature Gates String * string ) { flag Set . String Var ( & cfg . Local API Endpoint . Advertise Address , options . API Server Advertise Address , cfg . Local API Endpoint . Advertise flag Set . Int32Var ( & cfg . Local API Endpoint . Bind Port , options . API Server Bind Port , cfg . Local API Endpoint . Bind flag Set . String Var ( & cfg . Networking . Service Subnet , options . Networking Service Subnet , cfg . Networking . Service flag Set . String Var ( & cfg . Networking . Pod Subnet , options . Networking Pod Subnet , cfg . Networking . Pod flag Set . String Var ( & cfg . Networking . DNS Domain , options . Networking DNS Domain , cfg . Networking . DNS options . Add Kubernetes Version Flag ( flag Set , & cfg . Kubernetes flag Set . String Var ( & cfg . Certificates Dir , options . Certificates Dir , cfg . Certificates flag Set . String Slice Var ( & cfg . API Server . Cert SA Ns , options . API Server Cert SA Ns , cfg . API Server . Cert SA Ns , `Optional extra Subject Alternative Names (SA flag Set . String Var ( & cfg . Node Registration . Name , options . Node Name , cfg . Node cmdutil . Add CRI Socket Flag ( flag Set , & cfg . Node Registration . CRI options . Add Feature Gates String Flag ( flag Set , feature Gates } 
func Add Init Other Flags ( flag Set * flag . Flag Set , init Options * init Options ) { options . Add Config Flag ( flag Set , & init Options . cfg flag Set . String Slice Var ( & init Options . ignore Preflight Errors , options . Ignore Preflight Errors , init Options . ignore Preflight flag Set . Bool Var ( & init Options . skip Token Print , options . Skip Token Print , init Options . skip Token flag Set . Bool Var ( & init Options . dry Run , options . Dry Run , init Options . dry flag Set . Bool Var ( & init Options . upload Certs , options . Upload Certs , init Options . upload flag Set . String Var ( & init Options . certificate Key , options . Certificate flag Set . Bool Var ( & init Options . skip Certificate Key Print , options . Skip Certificate Key Print , init Options . skip Certificate Key } 
func new Init Options ( ) * init Options { // initialize the public kubeadm config API by applying defaults externalcfg := & kubeadmapiv1beta2 . Init // Create the options object for the bootstrap token-related flags, and override the default value for .Description bto := options . New Bootstrap Token return & init Options { externalcfg : externalcfg , bto : bto , kubeconfig Dir : kubeadmconstants . Kubernetes Dir , kubeconfig Path : kubeadmconstants . Get Admin Kube Config Path ( ) , upload } 
func new Init Data ( cmd * cobra . Command , args [ ] string , options * init Options , out io . Writer ) ( * init if options . externalcfg . Feature Gates , err = features . New Feature Gate ( & features . Init Feature Gates , options . feature Gates ignore Preflight Errors Set , err := validation . Validate Ignore Preflight Errors ( options . ignore Preflight if err = validation . Validate Mixed if err = options . bto . Apply // Either use the config file if specified, or convert public kubeadm API to the internal Init Configuration // and validates Init Configuration cfg , err := configutil . Load Or Default Init Configuration ( options . cfg // override node name and CRI socket from the command line options if options . externalcfg . Node Registration . Name != " " { cfg . Node Registration . Name = options . externalcfg . Node if options . externalcfg . Node Registration . CRI Socket != " " { cfg . Node Registration . CRI Socket = options . externalcfg . Node Registration . CRI if err := configutil . Verify API Server Bind Address ( cfg . Local API Endpoint . Advertise if err := features . Validate Version ( features . Init Feature Gates , cfg . Feature Gates , cfg . Kubernetes // if dry running creates a temporary folder for saving kubeadm generated files dry Run if options . dry Run { if dry Run Dir , err = kubeadmconstants . Create Temp Dir For // Checks if an external CA is provided by the user (when the CA Cert is present but the CA Key is not) external CA , err := certsphase . Using External CA ( & cfg . Cluster if external // Validate that also the required kubeconfig files exists and are invalid, because // kubeadm can't regenerate them without the CA Key kubeconfig Dir := options . kubeconfig if options . dry Run { kubeconfig Dir = dry Run if err := kubeconfigphase . Validate Kubeconfigs For External CA ( kubeconfig // Checks if an external Front-Proxy CA is provided by the user (when the Front-Proxy CA Cert is present but the Front-Proxy CA Key is not) external Front Proxy CA , err := certsphase . Using External Front Proxy CA ( & cfg . Cluster if external Front Proxy if options . upload Certs && ( external CA || external Front Proxy return & init Data { cfg : cfg , certificates Dir : cfg . Certificates Dir , skip Token Print : options . skip Token Print , dry Run : options . dry Run , dry Run Dir : dry Run Dir , kubeconfig Dir : options . kubeconfig Dir , kubeconfig Path : options . kubeconfig Path , ignore Preflight Errors : ignore Preflight Errors Set , external CA : external CA , output Writer : out , upload Certs : options . upload Certs , certificate Key : options . certificate Key , skip Certificate Key Print : options . skip Certificate Key } 
func ( d * init Data ) Certificate Write Dir ( ) string { if d . dry Run { return d . dry Run return d . certificates } 
func ( d * init Data ) Kube Config Dir ( ) string { if d . dry Run { return d . dry Run return d . kubeconfig } 
func ( d * init Data ) Kube Config Path ( ) string { if d . dry Run { d . kubeconfig Path = filepath . Join ( d . dry Run Dir , kubeadmconstants . Admin Kube Config File return d . kubeconfig } 
func ( d * init Data ) Manifest Dir ( ) string { if d . dry Run { return d . dry Run return kubeadmconstants . Get Static Pod } 
func ( d * init Data ) Kubelet Dir ( ) string { if d . dry Run { return d . dry Run return kubeadmconstants . Kubelet Run } 
func ( d * init Data ) Client ( ) ( clientset . Interface , error ) { if d . client == nil { if d . dry Run { // If we're dry-running, we should create a faked client that answers some GE Ts in order to be able to do the full init flow and just logs the rest of requests dry Run Getter := apiclient . New Init Dry Run Getter ( d . cfg . Node Registration . Name , d . cfg . Networking . Service d . client = apiclient . New Dry Run Client ( dry Run d . client , err = kubeconfigutil . Client Set From File ( d . Kube Config } 
func ( d * init for _ , bt := range d . cfg . Bootstrap } 
func show Join Command ( i * init Data , out io . Writer ) error { admin Kube Config Path := i . Kube Config // Prints the join command, multiple times in case the user has multiple tokens for _ , token := range i . Tokens ( ) { if err := print Join Command ( out , admin Kube Config } 
func New Control Plane Prepare Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Short : " " , Example : control Plane Prepare Example , Phases : [ ] workflow . Phase { { Name : " " , Short : " " , Inherit Flags : get Control Plane Prepare Phase Flags ( " " ) , Run All Siblings : true , } , new Control Plane Prepare Download Certs Subphase ( ) , new Control Plane Prepare Certs Subphase ( ) , new Control Plane Prepare Kubeconfig Subphase ( ) , new Control Plane Prepare Control Plane } 
func ( job Strategy ) Prepare For job . Status = batch . Job if ! utilfeature . Default Feature Gate . Enabled ( features . TTL After Finished ) { job . Spec . TTL Seconds After pod . Drop Disabled Template } 
func ( job Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new old new Job . Status = old if ! utilfeature . Default Feature Gate . Enabled ( features . TTL After Finished ) && old Job . Spec . TTL Seconds After Finished == nil { new Job . Spec . TTL Seconds After pod . Drop Disabled Template Fields ( & new Job . Spec . Template , & old } 
func ( job Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error // TODO: move UID generation earlier and do this in defaulting logic? if job . Spec . Manual Selector == nil || * job . Spec . Manual Selector == false { generate all Errs := validation . Validate all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & job . Spec . Template , nil , field . New return all } 
func generate if found { // User asked us to not automatically generate a selector and labels, // but set a possibly conflicting value. If there is a conflict, // we will reject in validation. } else { obj . Spec . Template . Labels [ " " ] = string ( obj . Object if found { // User asked us to automatically generate a selector and labels, // but set a possibly conflicting value. If there is a conflict, // we will reject in validation. } else { obj . Spec . Template . Labels [ " " ] = string ( obj . Object // Select the controller-uid label. This is sufficient for uniqueness. if obj . Spec . Selector == nil { obj . Spec . Selector = & metav1 . Label if obj . Spec . Selector . Match Labels == nil { obj . Spec . Selector . Match if _ , found := obj . Spec . Selector . Match Labels [ " " ] ; ! found { obj . Spec . Selector . Match Labels [ " " ] = string ( obj . Object // If the user specified match Label controller-uid=$WRONGUID, then it should fail // in validation, either because the selector does not match the pod template // (controller-uid=$WRONGUID does not match controller-uid=$UID, which we applied // above, or we will reject in validation because the template has the wrong // labels. } 
func ( job Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error old validation Error List := validation . Validate update Error List := validation . Validate Job Update ( job , old update Error List = append ( update Error List , corevalidation . Validate Conditional Pod Template ( & job . Spec . Template , & old Job . Spec . Template , field . New return append ( validation Error List , update Error } 
func Job To Selectable Fields ( job * batch . Job ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & job . Object specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , specific Fields } 
func Get return labels . Set ( job . Object Meta . Labels ) , Job To Selectable } 
func New Replication Controller Condition ( cond Type v1 . Replication Controller Condition Type , status v1 . Condition Status , reason , msg string ) v1 . Replication Controller Condition { return v1 . Replication Controller Condition { Type : cond Type , Status : status , Last Transition } 
func Get Condition ( status v1 . Replication Controller Status , cond Type v1 . Replication Controller Condition Type ) * v1 . Replication Controller if c . Type == cond } 
func Set Condition ( status * v1 . Replication Controller Status , condition v1 . Replication Controller Condition ) { current Cond := Get if current Cond != nil && current Cond . Status == condition . Status && current new Conditions := filter Out status . Conditions = append ( new } 
func Remove Condition ( status * v1 . Replication Controller Status , cond Type v1 . Replication Controller Condition Type ) { status . Conditions = filter Out Condition ( status . Conditions , cond } 
func filter Out Condition ( conditions [ ] v1 . Replication Controller Condition , cond Type v1 . Replication Controller Condition Type ) [ ] v1 . Replication Controller Condition { var new Conditions [ ] v1 . Replication Controller for _ , c := range conditions { if c . Type == cond new Conditions = append ( new return new } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Fischer List ) ( nil ) , ( * wardle . Fischer List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Fischer List_To_wardle_Fischer List ( a . ( * Fischer List ) , b . ( * wardle . Fischer if err := s . Add Generated Conversion Func ( ( * wardle . Fischer List ) ( nil ) , ( * Fischer List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_wardle_Fischer List_To_v1alpha1_Fischer List ( a . ( * wardle . Fischer List ) , b . ( * Fischer if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * Flunder List ) ( nil ) , ( * wardle . Flunder List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Flunder List_To_wardle_Flunder List ( a . ( * Flunder List ) , b . ( * wardle . Flunder if err := s . Add Generated Conversion Func ( ( * wardle . Flunder List ) ( nil ) , ( * Flunder List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_wardle_Flunder List_To_v1alpha1_Flunder List ( a . ( * wardle . Flunder List ) , b . ( * Flunder if err := s . Add Generated Conversion Func ( ( * Flunder Spec ) ( nil ) , ( * wardle . Flunder Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Flunder Spec_To_wardle_Flunder Spec ( a . ( * Flunder Spec ) , b . ( * wardle . Flunder if err := s . Add Generated Conversion Func ( ( * wardle . Flunder Spec ) ( nil ) , ( * Flunder Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_wardle_Flunder Spec_To_v1alpha1_Flunder Spec ( a . ( * wardle . Flunder Spec ) , b . ( * Flunder if err := s . Add Generated Conversion Func ( ( * Flunder Status ) ( nil ) , ( * wardle . Flunder Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Flunder Status_To_wardle_Flunder Status ( a . ( * Flunder Status ) , b . ( * wardle . Flunder if err := s . Add Generated Conversion Func ( ( * wardle . Flunder Status ) ( nil ) , ( * Flunder Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_wardle_Flunder Status_To_v1alpha1_Flunder Status ( a . ( * wardle . Flunder Status ) , b . ( * Flunder if err := s . Add Conversion Func ( ( * Flunder Spec ) ( nil ) , ( * wardle . Flunder Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Flunder Spec_To_wardle_Flunder Spec ( a . ( * Flunder Spec ) , b . ( * wardle . Flunder if err := s . Add Conversion Func ( ( * wardle . Flunder Spec ) ( nil ) , ( * Flunder Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_wardle_Flunder Spec_To_v1alpha1_Flunder Spec ( a . ( * wardle . Flunder Spec ) , b . ( * Flunder } 
func Convert_v1alpha1_Fischer_To_wardle_Fischer ( in * Fischer , out * wardle . Fischer , s conversion . Scope ) error { return auto } 
func Convert_wardle_Fischer_To_v1alpha1_Fischer ( in * wardle . Fischer , out * Fischer , s conversion . Scope ) error { return auto } 
func Convert_v1alpha1_Fischer List_To_wardle_Fischer List ( in * Fischer List , out * wardle . Fischer List , s conversion . Scope ) error { return auto Convert_v1alpha1_Fischer List_To_wardle_Fischer } 
func Convert_wardle_Fischer List_To_v1alpha1_Fischer List ( in * wardle . Fischer List , out * Fischer List , s conversion . Scope ) error { return auto Convert_wardle_Fischer List_To_v1alpha1_Fischer } 
func Convert_v1alpha1_Flunder_To_wardle_Flunder ( in * Flunder , out * wardle . Flunder , s conversion . Scope ) error { return auto } 
func Convert_wardle_Flunder_To_v1alpha1_Flunder ( in * wardle . Flunder , out * Flunder , s conversion . Scope ) error { return auto } 
func Convert_v1alpha1_Flunder List_To_wardle_Flunder List ( in * Flunder List , out * wardle . Flunder List , s conversion . Scope ) error { return auto Convert_v1alpha1_Flunder List_To_wardle_Flunder } 
func Convert_wardle_Flunder List_To_v1alpha1_Flunder List ( in * wardle . Flunder List , out * Flunder List , s conversion . Scope ) error { return auto Convert_wardle_Flunder List_To_v1alpha1_Flunder } 
func Convert_v1alpha1_Flunder Status_To_wardle_Flunder Status ( in * Flunder Status , out * wardle . Flunder Status , s conversion . Scope ) error { return auto Convert_v1alpha1_Flunder Status_To_wardle_Flunder } 
func Convert_wardle_Flunder Status_To_v1alpha1_Flunder Status ( in * wardle . Flunder Status , out * Flunder Status , s conversion . Scope ) error { return auto Convert_wardle_Flunder Status_To_v1alpha1_Flunder } 
func New Aggregation Controller ( downloader * aggregator . Downloader , open API Aggregation Manager aggregator . Spec Aggregator ) * Aggregation Controller { c := & Aggregation Controller { open API Aggregation Manager : open API Aggregation Manager , queue : workqueue . New Named Rate Limiting Queue ( workqueue . New Item Exponential Failure Rate Limiter ( successful Update Delay , failed Update Max Exp c . sync // update each service at least once, also those which are not coming from API Services, namely local services for _ , name := range open API Aggregation Manager . Get API Service Names ( ) { c . queue . Add } 
func ( c * Aggregation Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle defer c . queue . Shut go wait . Until ( c . run Worker , time . Second , stop <- stop } 
func ( c * Aggregation Controller ) process Next Work if aggregator . Is Local API action , err := c . sync } else { utilruntime . Handle switch action { case sync Requeue : if aggregator . Is Local API Service ( key . ( string ) ) { klog . V ( 7 ) . Infof ( " " , key , successful Update Delay c . queue . Add After ( key , successful Update Delay c . queue . Add After ( key , successful Update case sync Requeue Rate c . queue . Add Rate case sync } 
func ( c * Aggregation Controller ) Add API Service ( handler http . Handler , api Service * apiregistration . API Service ) { if api if err := c . open API Aggregation Manager . Add Update API Service ( handler , api Service ) ; err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , api c . queue . Add After ( api } 
func ( c * Aggregation Controller ) Remove API Service ( api Service Name string ) { if err := c . open API Aggregation Manager . Remove API Service Spec ( api Service Name ) ; err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , api Service // This will only remove it if it was failing before. If it was successful, process Next Work Item will figure it out // and will not add it again to the queue. c . queue . Forget ( api Service } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( node . Add To utilruntime . Must ( v1alpha1 . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1beta1 . Scheme Group } 
func New Node Label Priority ( label string , presence bool ) ( Priority Map Function , Priority Reduce Function ) { label Prioritizer := & Node Label return label Prioritizer . Calculate Node Label Priority } 
func ( n * Node Label Prioritizer ) Calculate Node Label Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { node := node if node == nil { return schedulerapi . Host if ( exists && n . presence ) || ( ! exists && ! n . presence ) { score = schedulerapi . Max return schedulerapi . Host } 
func ( connection * V Sphere client defer client if connection . Client == nil { connection . Client , err = connection . New m := session . New user Session , err := m . User if user connection . Client , err = connection . New } 
func ( connection * V Sphere Connection ) Signer ( ctx context . Context , client * vim25 . Client ) ( * sts . Signer , error ) { // TODO: Add separate fields for certificate and private-key. // For now we can leave the config structs and validation as-is and // decide to use Login By cert , err := tls . X509Key tokens , err := sts . New req := sts . Token } 
func ( connection * V Sphere Connection ) login ( ctx context . Context , client * vim25 . Client ) error { m := session . New connection . credentials defer connection . credentials return m . Login ( ctx , neturl . User return m . Login By Token ( client . With } 
func ( connection * V Sphere Connection ) Logout ( ctx context . Context ) { client client m := session . New has Active Session , err := m . Session Is if ! has Active } 
func ( connection * V Sphere Connection ) New Client ( ctx context . Context ) ( * vim25 . Client , error ) { url , err := soap . Parse URL ( net . Join Host sc := soap . New if ca := connection . CA Cert ; ca != " " { if err := sc . Set Root C tp sc . Set Thumbprint ( tp client , err := vim25 . New k8s Version := version . Get ( ) . Git client . User Agent = fmt . Sprintf ( " " , k8s if klog . V ( 3 ) { s , err := session . New Manager ( client ) . User if err == nil { klog . Infof ( " " , s . User if connection . Round Tripper Count == 0 { connection . Round Tripper Count = Round Tripper Default client . Round Tripper = vim25 . Retry ( client . Round Tripper , vim25 . Temporary Network Error ( int ( connection . Round Tripper } 
func ( connection * V Sphere Connection ) Update Credentials ( username string , password string ) { connection . credentials defer connection . credentials } 
func init ( ) { // Register external types for Scheme metav1 . Add To Group Version ( Scheme , schema . Group utilruntime . Must ( metav1beta1 . Add To utilruntime . Must ( scheme . Add To utilruntime . Must ( Scheme . Set Version Priority ( corev1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( admissionv1alpha1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( admissionregistrationv1beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( appsv1beta1 . Scheme Group Version , appsv1beta2 . Scheme Group Version , appsv1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( authenticationv1 . Scheme Group Version , authenticationv1beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( authorizationv1 . Scheme Group Version , authorizationv1beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( autoscalingv1 . Scheme Group Version , autoscalingv2beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( batchv1 . Scheme Group Version , batchv1beta1 . Scheme Group Version , batchv2alpha1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( certificatesv1beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( extensionsv1beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( imagepolicyv1alpha1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( networkingv1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( policyv1beta1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( rbacv1 . Scheme Group Version , rbacv1beta1 . Scheme Group Version , rbacv1alpha1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( schedulingv1alpha1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( settingsv1alpha1 . Scheme Group utilruntime . Must ( Scheme . Set Version Priority ( storagev1 . Scheme Group Version , storagev1beta1 . Scheme Group } 
func Add To Scheme ( scheme * runtime . Scheme ) { utilruntime . Must ( config . Add To utilruntime . Must ( v1alpha1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1alpha1 . Scheme Group } 
func Log Impersonated User ( ae * auditinternal . Event , user user . Info ) { if ae == nil || ae . Level . Less ( auditinternal . Level ae . Impersonated User = & auditinternal . User Info { Username : user . Get ae . Impersonated User . Groups = user . Get ae . Impersonated User . UID = user . Get ae . Impersonated User . Extra = map [ string ] auditinternal . Extra for k , v := range user . Get Extra ( ) { ae . Impersonated User . Extra [ k ] = auditinternal . Extra } 
func Log Request Object ( ae * auditinternal . Event , obj runtime . Object , gvr schema . Group Version Resource , subresource string , s runtime . Negotiated Serializer ) { if ae == nil || ae . Level . Less ( auditinternal . Level // complete Object Ref if ae . Object Ref == nil { ae . Object Ref = & auditinternal . Object // meta.Accessor is more general than Object Meta Accessor, but if it fails, we can just skip setting these bits if meta , err := meta . Accessor ( obj ) ; err == nil { if len ( ae . Object Ref . Namespace ) == 0 { ae . Object Ref . Namespace = meta . Get if len ( ae . Object Ref . Name ) == 0 { ae . Object Ref . Name = meta . Get if len ( ae . Object Ref . UID ) == 0 { ae . Object Ref . UID = meta . Get if len ( ae . Object Ref . Resource Version ) == 0 { ae . Object Ref . Resource Version = meta . Get Resource if len ( ae . Object Ref . API Version ) == 0 { ae . Object Ref . API ae . Object Ref . API if len ( ae . Object Ref . Resource ) == 0 { ae . Object if len ( ae . Object Ref . Subresource ) == 0 { ae . Object if ae . Level . Less ( auditinternal . Level ae . Request Object , err = encode Object ( obj , gvr . Group if err != nil { // TODO(audit): add error slice to audit event struct klog . Warningf ( " " , reflect . Type } 
func Log Request Patch ( ae * auditinternal . Event , patch [ ] byte ) { if ae == nil || ae . Level . Less ( auditinternal . Level ae . Request Object = & runtime . Unknown { Raw : patch , Content Type : runtime . Content Type } 
func Log Response Object ( ae * auditinternal . Event , obj runtime . Object , gv schema . Group Version , s runtime . Negotiated Serializer ) { if ae == nil || ae . Level . Less ( auditinternal . Level if status , ok := obj . ( * metav1 . Status ) ; ok { // selectively copy the bounded fields. ae . Response if ae . Level . Less ( auditinternal . Level Request ae . Response Object , err = encode if err != nil { klog . Warningf ( " " , reflect . Type } 
func Log Annotation ( ae * auditinternal . Event , key , value string ) { if ae == nil || ae . Level . Less ( auditinternal . Level if v , ok := ae . Annotations [ key ] ; ok && v != value { klog . Warningf ( " " , key , value , ae . Audit } 
func Log Annotations ( ae * auditinternal . Event , annotations map [ string ] string ) { if ae == nil || ae . Level . Less ( auditinternal . Level for key , value := range annotations { Log } 
func maybe Truncate User Agent ( req * http . Request ) string { ua := req . User if len ( ua ) > max User Agent Length { ua = ua [ : max User Agent Length ] + user Agent Truncate } 
func New Cmd Config ( out io . Writer ) * cobra . Command { var kube Config There is a Config cluster. kubeadm CLI v1.8.0+ automatically creates this Config Config ` ) , metav1 . Namespace System , constants . Kubeadm Config Config Map ) , // Without this callback, if a user runs just the "upload" // command without a subcommand, or with an invalid subcommand, // cobra will print usage information, but still exit cleanly. // We want to return an error code in these cases so that the // user knows that their command was invalid. Run E : cmdutil . Sub Cmd Run options . Add Kube Config Flag ( cmd . Persistent Flags ( ) , & kube Config kube Config File = cmdutil . Get Kube Config Path ( kube Config cmd . Add Command ( New Cmd Config cmd . Add Command ( New Cmd Config cmd . Add Command ( New Cmd Config Upload ( out , & kube Config cmd . Add Command ( New Cmd Config View ( out , & kube Config cmd . Add Command ( New Cmd Config } 
func New Cmd Config Print ( out io . Writer ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Long : " " , Run E : cmdutil . Sub Cmd Run cmd . Add Command ( New Cmd Config Print Init cmd . Add Command ( New Cmd Config Print Join } 
func get Supported Component Config API for component Type := range componentconfigs . Known { objects = append ( objects , string ( component } 
func New Cmd Config Migrate ( out io . Writer ) * cobra . Command { var old Cfg Path , new Cfg ` ) , kubeadmapiv1beta2 . Scheme Group Version , kubeadmapiv1beta2 . Scheme Group Version ) , Run : func ( cmd * cobra . Command , args [ ] string ) { if len ( old Cfg Path ) == 0 { kubeadmutil . Check old Cfg Bytes , err := ioutil . Read File ( old Cfg kubeadmutil . Check output Bytes , err := configutil . Migrate Old Config ( old Cfg kubeadmutil . Check if new Cfg Path == " " { fmt . Fprint ( out , string ( output } else { if err := ioutil . Write File ( new Cfg Path , output Bytes , 0644 ) ; err != nil { kubeadmutil . Check Err ( errors . Wrapf ( err , " " , new Cfg cmd . Flags ( ) . String Var ( & old Cfg cmd . Flags ( ) . String Var ( & new Cfg } 
func New Cmd Config Upload ( out io . Writer , kube Config File * string ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Run E : cmdutil . Sub Cmd Run cmd . Add Command ( New Cmd Config Upload From File ( out , kube Config cmd . Add Command ( New Cmd Config Upload From Flags ( out , kube Config } 
func New Cmd Config View ( out io . Writer , kube Config Using this command, you can view the Config The configuration is located in the %q namespace in the %q Config ` ) , metav1 . Namespace System , constants . Kubeadm Config Config client , err := kubeconfigutil . Client Set From File ( * kube Config kubeadmutil . Check err = Run Config kubeadmutil . Check } 
func New Cmd Config Upload From File ( out io . Writer , kube Config File * string ) * cobra . Command { var cfg Using this command, you can upload configuration to the Config The configuration is located in the %q namespace in the %q Config ` ) , metav1 . Namespace System , constants . Kubeadm Config Config Map ) , Run : func ( cmd * cobra . Command , args [ ] string ) { if len ( cfg Path ) == 0 { kubeadmutil . Check client , err := kubeconfigutil . Client Set From File ( * kube Config kubeadmutil . Check // Default both statically and dynamically, convert to internal API type, and validate everything internalcfg , err := configutil . Load Init Configuration From File ( cfg kubeadmutil . Check err = uploadconfig . Upload kubeadmutil . Check options . Add Config Flag ( cmd . Flags ( ) , & cfg } 
func New Cmd Config Upload From Flags ( out io . Writer , kube Config File * string ) * cobra . Command { cfg := & kubeadmapiv1beta2 . Init var feature Gates Using this command, you can upload configuration to the Config The configuration is located in the %q namespace in the %q Config ` ) , metav1 . Namespace System , constants . Kubeadm Config Config if cfg . Feature Gates , err = features . New Feature Gate ( & features . Init Feature Gates , feature Gates String ) ; err != nil { kubeadmutil . Check client , err := kubeconfigutil . Client Set From File ( * kube Config kubeadmutil . Check // Kubernetes Version is not used, but we set it explicitly to avoid the lookup // of the version from the internet when executing Defaulted Init Configuration phaseutil . Set Kubernetes Version ( & cfg . Cluster internalcfg , err := configutil . Defaulted Init kubeadmutil . Check err = uploadconfig . Upload kubeadmutil . Check Add Init Config Flags ( cmd . Persistent Flags ( ) , cfg , & feature Gates } 
func Run Config cfg Config Map , err := client . Core V1 ( ) . Config Maps ( metav1 . Namespace System ) . Get ( constants . Kubeadm Config Config Map , metav1 . Get // No need to append \n as that already exists in the Config Map fmt . Fprintf ( out , " " , cfg Config Map . Data [ constants . Cluster Configuration Config Map } 
func New Cmd Config Images ( out io . Writer ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Run E : cmdutil . Sub Cmd Run cmd . Add Command ( New Cmd Config Images cmd . Add Command ( New Cmd Config Images } 
func New Cmd Config Images Pull ( ) * cobra . Command { externalcfg := & kubeadmapiv1beta2 . Init var cfg Path , feature Gates cmd := & cobra . Command { Use : " " , Short : " " , Run : func ( _ * cobra . Command , _ [ ] string ) { externalcfg . Cluster Configuration . Feature Gates , err = features . New Feature Gate ( & features . Init Feature Gates , feature Gates kubeadmutil . Check internalcfg , err := configutil . Load Or Default Init Configuration ( cfg kubeadmutil . Check container Runtime , err := utilruntime . New Container Runtime ( utilsexec . New ( ) , internalcfg . Node Registration . CRI kubeadmutil . Check Pull Control Plane Images ( container Runtime , & internalcfg . Cluster Add Images Common Config Flags ( cmd . Persistent Flags ( ) , externalcfg , & cfg Path , & feature Gates cmdutil . Add CRI Socket Flag ( cmd . Persistent Flags ( ) , & externalcfg . Node Registration . CRI } 
func New Images Pull ( runtime utilruntime . Container Runtime , images [ ] string ) * Images Pull { return & Images } 
func Pull Control Plane Images ( runtime utilruntime . Container Runtime , cfg * kubeadmapi . Cluster Configuration ) error { images := images . Get Control Plane for _ , image := range images { if err := runtime . Pull } 
func New Cmd Config Images List ( out io . Writer , mock K8s Version * string ) * cobra . Command { externalcfg := & kubeadmapiv1beta2 . Init var cfg Path , feature Gates // This just sets the Kubernetes version for unit testing so kubeadm won't try to // lookup the latest release from the internet. if mock K8s Version != nil { externalcfg . Kubernetes Version = * mock K8s cmd := & cobra . Command { Use : " " , Short : " " , Run : func ( _ * cobra . Command , _ [ ] string ) { externalcfg . Cluster Configuration . Feature Gates , err = features . New Feature Gate ( & features . Init Feature Gates , feature Gates kubeadmutil . Check images List , err := New Images List ( cfg kubeadmutil . Check kubeadmutil . Check Err ( images Add Images Common Config Flags ( cmd . Persistent Flags ( ) , externalcfg , & cfg Path , & feature Gates } 
func New Images List ( cfg Path string , cfg * kubeadmapiv1beta2 . Init Configuration ) ( * Images List , error ) { initcfg , err := configutil . Load Or Default Init Configuration ( cfg return & Images } 
func ( i * Images List ) Run ( out io . Writer ) error { imgs := images . Get Control Plane Images ( & i . cfg . Cluster } 
func Add Images Common Config Flags ( flag Set * flag . Flag Set , cfg * kubeadmapiv1beta2 . Init Configuration , cfg Path * string , feature Gates String * string ) { options . Add Kubernetes Version Flag ( flag Set , & cfg . Cluster Configuration . Kubernetes options . Add Feature Gates String Flag ( flag Set , feature Gates options . Add Image Meta Flags ( flag Set , & cfg . Image flag Set . String Var ( cfg Path , " " , * cfg } 
func ( in * JSON Schema Props ) Deep Copy ( ) * JSON Schema out := new ( JSON Schema if in . Max Length != nil { in , out := & in . Max Length , & out . Max if in . Min Length != nil { in , out := & in . Min Length , & out . Min if in . Max Items != nil { in , out := & in . Max Items , & out . Max if in . Min Items != nil { in , out := & in . Min Items , & out . Min if in . Multiple Of != nil { in , out := & in . Multiple Of , & out . Multiple if in . Max Properties != nil { in , out := & in . Max Properties , & out . Max if in . Min Properties != nil { in , out := & in . Min Properties , & out . Min } else { * out = new ( JSON Schema Props Or ( * in ) . Deep Copy if in . All Of != nil { in , out := & in . All Of , & out . All * out = make ( [ ] JSON Schema for i := range * in { ( * in ) [ i ] . Deep Copy if in . One Of != nil { in , out := & in . One Of , & out . One * out = make ( [ ] JSON Schema for i := range * in { ( * in ) [ i ] . Deep Copy if in . Any Of != nil { in , out := & in . Any Of , & out . Any * out = make ( [ ] JSON Schema for i := range * in { ( * in ) [ i ] . Deep Copy } else { * out = new ( JSON Schema ( * in ) . Deep Copy * out = make ( map [ string ] JSON Schema for key , val := range * in { ( * out ) [ key ] = * val . Deep if in . Additional Properties != nil { in , out := & in . Additional Properties , & out . Additional } else { * out = new ( JSON Schema Props Or ( * in ) . Deep Copy if in . Pattern Properties != nil { in , out := & in . Pattern Properties , & out . Pattern * out = make ( map [ string ] JSON Schema for key , val := range * in { ( * out ) [ key ] = * val . Deep * out = make ( JSON Schema for key , val := range * in { ( * out ) [ key ] = * val . Deep if in . Additional Items != nil { in , out := & in . Additional Items , & out . Additional } else { * out = new ( JSON Schema Props Or ( * in ) . Deep Copy * out = make ( JSON Schema for key , val := range * in { ( * out ) [ key ] = * val . Deep if in . External Docs != nil { in , out := & in . External Docs , & out . External } else { * out = new ( External ( * in ) . Deep Copy } 
func logs For Object With Client ( clientset corev1client . Core V1Interface , object , options runtime . Object , timeout time . Duration , all Containers bool ) ( [ ] rest . Response Wrapper , error ) { opts , ok := options . ( * corev1 . Pod Log switch t := object . ( type ) { case * corev1 . Pod List : ret := [ ] rest . Response for i := range t . Items { curr Ret , err := logs For Object With Client ( clientset , & t . Items [ i ] , options , timeout , all ret = append ( ret , curr case * corev1 . Pod : // if all Containers is true, then we're going to locate all containers and then iterate through them. At that point, "all Containers" is false if ! all Containers { return [ ] rest . Response Wrapper { clientset . Pods ( t . Namespace ) . Get ret := [ ] rest . Response for _ , c := range t . Spec . Init Containers { curr Opts := opts . Deep curr curr Ret , err := logs For Object With Client ( clientset , t , curr ret = append ( ret , curr for _ , c := range t . Spec . Containers { curr Opts := opts . Deep curr curr Ret , err := logs For Object With Client ( clientset , t , curr ret = append ( ret , curr namespace , selector , err := Selectors For sort By := func ( pods [ ] * v1 . Pod ) sort . Interface { return podutils . By pod , num Pods , err := Get First Pod ( clientset , namespace , selector . String ( ) , timeout , sort if num Pods > 1 { fmt . Fprintf ( os . Stderr , " \n " , num return logs For Object With Client ( clientset , pod , options , timeout , all } 
func new Cmd Kubelet Utility ( ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Long : cmdutil . Macro Command Long cmd . Add Command ( new Cmd Kubelet } 
func new Cmd Kubelet Config ( ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : " " , Long : cmdutil . Macro Command Long cmd . Add Command ( new Cmd Kubelet Config cmd . Add Command ( new Cmd Kubelet Config Enable } 
func new Cmd Kubelet Config Download ( ) * cobra . Command { var kubelet Version // TODO: Be smarter about this and be able to load multiple kubeconfig files in different orders of precedence kube Config File := constants . Get Kubelet Kube Config cmd := & cobra . Command { Use : " " , Short : " " , Long : kubelet Config Download Long Desc , Example : kubelet Config Download Example , Run : func ( cmd * cobra . Command , args [ ] string ) { kubelet Version , err := get Kubelet Version ( kubelet Version kubeadmutil . Check client , err := kubeconfigutil . Client Set From File ( kube Config kubeadmutil . Check err = kubeletphase . Download Config ( client , kubelet Version , constants . Kubelet Run kubeadmutil . Check options . Add Kube Config Flag ( cmd . Flags ( ) , & kube Config cmd . Flags ( ) . String Var ( & kubelet Version Str , " " , kubelet Version } 
func new Cmd Kubelet Config Enable Dynamic ( ) * cobra . Command { var node Name , kubelet Version var kube Config cmd := & cobra . Command { Use : " " , Short : " " , Long : kubelet Config Enable Dynamic Long Desc , Example : kubelet Config Enable Dynamic Example , Run : func ( cmd * cobra . Command , args [ ] string ) { if len ( node Name ) == 0 { kubeadmutil . Check if len ( kubelet Version Str ) == 0 { kubeadmutil . Check kubelet Version , err := version . Parse Semantic ( kubelet Version kubeadmutil . Check kube Config File = cmdutil . Get Kube Config Path ( kube Config client , err := kubeconfigutil . Client Set From File ( kube Config kubeadmutil . Check err = kubeletphase . Enable Dynamic Config For Node ( client , node Name , kubelet kubeadmutil . Check options . Add Kube Config Flag ( cmd . Flags ( ) , & kube Config cmd . Flags ( ) . String Var ( & node Name , options . Node Name , node cmd . Flags ( ) . String Var ( & kubelet Version Str , " " , kubelet Version } 
seenerrs := sets . New } 
func Filter if agg , ok := err . ( Aggregate ) ; ok { return New Aggregate ( filter if ! matches } 
func matches } 
func filter for _ , err := range list { r := Filter } 
return New } 
func Create Aggregate From Message Count Map ( m Message Count for err Str , count := range m { var count if count > 1 { count result = append ( result , fmt . Errorf ( " " , err Str , count return New } 
} 
func Aggregate Goroutines ( funcs ... func ( ) error ) Aggregate { err for _ , f := range funcs { go func ( f func ( ) error ) { err for i := 0 ; i < cap ( err Chan ) ; i ++ { if err := <- err return New } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Attachment ) ( nil ) , ( * storage . Volume Attachment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Attachment_To_storage_Volume Attachment ( a . ( * v1alpha1 . Volume Attachment ) , b . ( * storage . Volume if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment ) ( nil ) , ( * v1alpha1 . Volume Attachment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment_To_v1alpha1_Volume Attachment ( a . ( * storage . Volume Attachment ) , b . ( * v1alpha1 . Volume if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Attachment List ) ( nil ) , ( * storage . Volume Attachment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Attachment List_To_storage_Volume Attachment List ( a . ( * v1alpha1 . Volume Attachment List ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment List ) ( nil ) , ( * v1alpha1 . Volume Attachment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment List_To_v1alpha1_Volume Attachment List ( a . ( * storage . Volume Attachment List ) , b . ( * v1alpha1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Attachment Source ) ( nil ) , ( * storage . Volume Attachment Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Attachment Source_To_storage_Volume Attachment Source ( a . ( * v1alpha1 . Volume Attachment Source ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment Source ) ( nil ) , ( * v1alpha1 . Volume Attachment Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment Source_To_v1alpha1_Volume Attachment Source ( a . ( * storage . Volume Attachment Source ) , b . ( * v1alpha1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Attachment Spec ) ( nil ) , ( * storage . Volume Attachment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Attachment Spec_To_storage_Volume Attachment Spec ( a . ( * v1alpha1 . Volume Attachment Spec ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment Spec ) ( nil ) , ( * v1alpha1 . Volume Attachment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment Spec_To_v1alpha1_Volume Attachment Spec ( a . ( * storage . Volume Attachment Spec ) , b . ( * v1alpha1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Attachment Status ) ( nil ) , ( * storage . Volume Attachment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Attachment Status_To_storage_Volume Attachment Status ( a . ( * v1alpha1 . Volume Attachment Status ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment Status ) ( nil ) , ( * v1alpha1 . Volume Attachment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment Status_To_v1alpha1_Volume Attachment Status ( a . ( * storage . Volume Attachment Status ) , b . ( * v1alpha1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Volume Error ) ( nil ) , ( * storage . Volume Error ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Volume Error_To_storage_Volume Error ( a . ( * v1alpha1 . Volume Error ) , b . ( * storage . Volume if err := s . Add Generated Conversion Func ( ( * storage . Volume Error ) ( nil ) , ( * v1alpha1 . Volume Error ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Error_To_v1alpha1_Volume Error ( a . ( * storage . Volume Error ) , b . ( * v1alpha1 . Volume } 
func Convert_v1alpha1_Volume Attachment_To_storage_Volume Attachment ( in * v1alpha1 . Volume Attachment , out * storage . Volume Attachment , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Attachment_To_storage_Volume } 
func Convert_storage_Volume Attachment_To_v1alpha1_Volume Attachment ( in * storage . Volume Attachment , out * v1alpha1 . Volume Attachment , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment_To_v1alpha1_Volume } 
func Convert_v1alpha1_Volume Attachment List_To_storage_Volume Attachment List ( in * v1alpha1 . Volume Attachment List , out * storage . Volume Attachment List , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Attachment List_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment List_To_v1alpha1_Volume Attachment List ( in * storage . Volume Attachment List , out * v1alpha1 . Volume Attachment List , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment List_To_v1alpha1_Volume Attachment } 
func Convert_v1alpha1_Volume Attachment Source_To_storage_Volume Attachment Source ( in * v1alpha1 . Volume Attachment Source , out * storage . Volume Attachment Source , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Attachment Source_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment Source_To_v1alpha1_Volume Attachment Source ( in * storage . Volume Attachment Source , out * v1alpha1 . Volume Attachment Source , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment Source_To_v1alpha1_Volume Attachment } 
func Convert_v1alpha1_Volume Attachment Spec_To_storage_Volume Attachment Spec ( in * v1alpha1 . Volume Attachment Spec , out * storage . Volume Attachment Spec , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Attachment Spec_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment Spec_To_v1alpha1_Volume Attachment Spec ( in * storage . Volume Attachment Spec , out * v1alpha1 . Volume Attachment Spec , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment Spec_To_v1alpha1_Volume Attachment } 
func Convert_v1alpha1_Volume Attachment Status_To_storage_Volume Attachment Status ( in * v1alpha1 . Volume Attachment Status , out * storage . Volume Attachment Status , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Attachment Status_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment Status_To_v1alpha1_Volume Attachment Status ( in * storage . Volume Attachment Status , out * v1alpha1 . Volume Attachment Status , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment Status_To_v1alpha1_Volume Attachment } 
func Convert_v1alpha1_Volume Error_To_storage_Volume Error ( in * v1alpha1 . Volume Error , out * storage . Volume Error , s conversion . Scope ) error { return auto Convert_v1alpha1_Volume Error_To_storage_Volume } 
func Convert_storage_Volume Error_To_v1alpha1_Volume Error ( in * storage . Volume Error , out * v1alpha1 . Volume Error , s conversion . Scope ) error { return auto Convert_storage_Volume Error_To_v1alpha1_Volume } 
func ( g * gen Clientset ) Filter ( c * generator . Context , t * types . Type ) bool { ret := ! g . clientset g . clientset } 
func safe Open Sub Path ( mounter mount . Interface , subpath Subpath ) ( int , error ) { if ! mount . Path Within Base ( subpath . Path , subpath . Volume Path ) { return - 1 , fmt . Errorf ( " " , subpath . Path , subpath . Volume fd , err := do Safe Open ( subpath . Path , subpath . Volume } 
func prepare Subpath Target ( mounter mount . Interface , subpath Subpath ) ( bool , string , error ) { // Early check for already bind-mounted subpath. bind Path Target := get Subpath Bind not Mount , err := mount . Is Not Mount Point ( mounter , bind Path if err != nil { if ! os . Is Not Exist ( err ) { return false , " " , fmt . Errorf ( " " , bind Path // Ignore Error Not Exist: the file/directory will be created below if it does not exist yet. not if ! not Mount { // It's already mounted klog . V ( 5 ) . Infof ( " " , bind Path return true , bind Path // bind Path Target is in /var/lib/kubelet and thus reachable without any // translation even to containerized kubelet. bind Parent := filepath . Dir ( bind Path err = os . Mkdir All ( bind if err != nil && ! os . Is Exist ( err ) { return false , " " , fmt . Errorf ( " " , bind if t . Mode ( ) & os . Mode Dir > 0 { if err = os . Mkdir ( bind Path Target , 0750 ) ; err != nil && ! os . Is Exist ( err ) { return false , " " , fmt . Errorf ( " " , bind Path } else { // "/bin/touch <bind Path Target>". // A file is enough for all possible targets (symlink, device, pipe, // socket, ...), bind-mounting them into a file correctly changes type // of the target file. if err = ioutil . Write File ( bind Path Target , [ ] byte { } , 0640 ) ; err != nil { return false , " " , fmt . Errorf ( " " , bind Path return false , bind Path } 
func do Clean Sub Paths ( mounter mount . Interface , pod Dir string , volume Name string ) error { // scan /var/lib/kubelet/pods/<uid>/volume-subpaths/<volume>/* sub Path Dir := filepath . Join ( pod Dir , container Sub Path Directory Name , volume klog . V ( 4 ) . Infof ( " " , sub Path container Dirs , err := ioutil . Read Dir ( sub Path if err != nil { if os . Is Not return fmt . Errorf ( " " , sub Path for _ , container Dir := range container Dirs { if ! container Dir . Is Dir ( ) { klog . V ( 4 ) . Infof ( " " , container klog . V ( 4 ) . Infof ( " " , container // scan /var/lib/kubelet/pods/<uid>/volume-subpaths/<volume>/<container name>/* full Container Dir Path := filepath . Join ( sub Path Dir , container err = filepath . Walk ( full Container Dir Path , func ( path string , info os . File Info , err error ) error { if path == full Container Dir // pass through errors and let do Clean Sub Path handle them if err = do Clean Sub Path ( mounter , full Container Dir if err != nil { return fmt . Errorf ( " " , full Container Dir // Whole container has been processed, remove its directory. if err := os . Remove ( full Container Dir Path ) ; err != nil { return fmt . Errorf ( " " , full Container Dir klog . V ( 5 ) . Infof ( " " , full Container Dir // Whole pod volume subpaths have been cleaned up, remove its subpath directory. if err := os . Remove ( sub Path Dir ) ; err != nil { return fmt . Errorf ( " " , sub Path klog . V ( 5 ) . Infof ( " " , sub Path // Remove entire subpath directory if it's the last one pod Sub Path Dir := filepath . Join ( pod Dir , container Sub Path Directory if err := os . Remove ( pod Sub Path Dir ) ; err != nil && ! os . Is Exist ( err ) { return fmt . Errorf ( " " , pod Sub Path klog . V ( 5 ) . Infof ( " " , pod Sub Path } 
func do Clean Sub Path ( mounter mount . Interface , full Container Dir Path , sub Path Index string ) error { // process /var/lib/kubelet/pods/<uid>/volume-subpaths/<volume>/<container name>/<sub Path Name> klog . V ( 4 ) . Infof ( " " , sub Path full Sub Path := filepath . Join ( full Container Dir Path , sub Path if err := mount . Cleanup Mount Point ( full Sub Path , mounter , true ) ; err != nil { return fmt . Errorf ( " " , full Sub klog . V ( 4 ) . Infof ( " " , full Sub } 
func clean Sub Path ( mounter mount . Interface , subpath Subpath ) error { container Dir := filepath . Join ( subpath . Pod Dir , container Sub Path Directory Name , subpath . Volume Name , subpath . Container // Clean subdir bindmount if err := do Clean Sub Path ( mounter , container Dir , strconv . Itoa ( subpath . Volume Mount Index ) ) ; err != nil && ! os . Is Not // Recusively remove directories if empty if err := remove Empty Dirs ( subpath . Pod Dir , container } 
func remove Empty Dirs ( base Dir , end Dir string ) error { if ! mount . Path Within Base ( end Dir , base Dir ) { return fmt . Errorf ( " " , end Dir , base for cur Dir := end Dir ; cur Dir != base Dir ; cur Dir = filepath . Dir ( cur Dir ) { s , err := os . Stat ( cur if err != nil { if os . Is Not Exist ( err ) { klog . V ( 5 ) . Infof ( " " , cur return fmt . Errorf ( " " , cur if ! s . Is Dir ( ) { return fmt . Errorf ( " " , cur err = os . Remove ( cur if os . Is Exist ( err ) { klog . V ( 5 ) . Infof ( " " , cur } else if err != nil { return fmt . Errorf ( " " , cur klog . V ( 5 ) . Infof ( " " , cur } 
func do Safe Make Dir ( pathname string , base string , perm os . File if ! mount . Path Within if err == nil { // Path exists if s . Is return & os . Path // Find all existing directories existing Path , to Create , err := find Existing // Ensure the existing directory is inside allowed base full Existing Path , err := filepath . Eval Symlinks ( existing if err != nil { return fmt . Errorf ( " " , existing if ! mount . Path Within Base ( full Existing Path , base ) { return fmt . Errorf ( " " , full Existing klog . V ( 4 ) . Infof ( " " , full Existing Path , filepath . Join ( to parent FD , err := do Safe Open ( full Existing if err != nil { return fmt . Errorf ( " " , existing child defer func ( ) { if parent FD != - 1 { if err = syscall . Close ( parent FD ) ; err != nil { klog . V ( 4 ) . Infof ( " " , parent if child FD != - 1 { if err = syscall . Close ( child FD ) ; err != nil { klog . V ( 4 ) . Infof ( " " , child current Path := full Existing // create the directories one by one, making sure nobody can change // created directory into symlink. for _ , dir := range to Create { current Path = filepath . Join ( current err = syscall . Mkdirat ( parent FD , current if err != nil { return fmt . Errorf ( " " , current // Dive into the created directory child FD , err := syscall . Openat ( parent FD , dir , nofollow if err != nil { return fmt . Errorf ( " " , current // We can be sure that child FD is safe to use. It could be changed // by user after Mkdirat() and before Openat(), however: // - it could not be changed to symlink - we use nofollow Flags // - it could be changed to a file (or device, pipe, socket, ...) // but either subsequent Mkdirat() fails or we mount this file // to user's container. Security is no violated in both cases // and user either gets error or the file that it can already access. if err = syscall . Close ( parent FD ) ; err != nil { klog . V ( 4 ) . Infof ( " " , parent parent FD = child child // Everything was created. mkdirat(..., perm) above was affected by current // umask and we must apply the right permissions to the last directory // (that's the one that will be available to the container as subpath) // so user can read/write it. This is the behavior of previous code. // TODO: chmod all created directories, not just the last one. // parent FD is the last created directory. // Translate perm (os.File Mode) to uint32 that fchmod() expects kernel Perm := uint32 ( perm & os . Mode if perm & os . Mode Setgid > 0 { kernel if perm & os . Mode Setuid > 0 { kernel if perm & os . Mode Sticky > 0 { kernel if err = syscall . Fchmod ( parent FD , kernel Perm ) ; err != nil { return fmt . Errorf ( " " , current } 
func find Existing // Do Open At in a loop to find the first non-existing dir. Resolve symlinks. // This should be faster than looping through all dirs and calling os.Stat() // on each of them, as the symlinks are resolved only once with Open At(). current fd , err := syscall . Open ( current if err != nil { return pathname , nil , fmt . Errorf ( " " , current for i , dir := range dirs { // Using O_PATH here will prevent hangs in case user replaces directory with // fifo child if err != nil { if os . Is Not Exist ( err ) { return current fd = child current Path = filepath . Join ( current } 
func do Safe // Assumption: base is the only directory that we have under control. // Base dir is not allowed to be a symlink. parent FD , err := syscall . Open ( base , nofollow defer func ( ) { if parent FD != - 1 { if err = syscall . Close ( parent FD ) ; err != nil { klog . V ( 4 ) . Infof ( " " , parent child defer func ( ) { if child FD != - 1 { if err = syscall . Close ( child FD ) ; err != nil { klog . V ( 4 ) . Infof ( " " , child current // Follow the segments one by one using openat() to make // sure the user cannot change already existing directories into symlinks. for _ , seg := range segments { current Path = filepath . Join ( current if ! mount . Path Within Base ( current Path , base ) { return - 1 , fmt . Errorf ( " " , current klog . V ( 5 ) . Infof ( " " , current child FD , err = syscall . Openat ( parent FD , seg , open FD if err != nil { return - 1 , fmt . Errorf ( " " , current var device err := unix . Fstat ( child FD , & device if err != nil { return - 1 , fmt . Errorf ( " " , current file Fmt := device if file Fmt == syscall . S_IFLNK { return - 1 , fmt . Errorf ( " " , current // Close parent FD if err = syscall . Close ( parent FD ) ; err != nil { return - 1 , fmt . Errorf ( " " , filepath . Dir ( current // Set child to new parent parent FD = child child // We made it to the end, return this fd, don't close it final FD := parent parent return final } 
func Word Sep Normalize Func ( f * pflag . Flag Set , name string ) pflag . Normalized Name { if strings . Contains ( name , " " ) { return pflag . Normalized return pflag . Normalized } 
func Init Flags ( ) { pflag . Command Line . Set Normalize Func ( Word Sep Normalize pflag . Command Line . Add Go Flag Set ( goflag . Command pflag . Visit } 
func ( c * Fake Volume Attachments ) Update Status ( volume Attachment * v1alpha1 . Volume Attachment ) ( * v1alpha1 . Volume Attachment , error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Subresource Action ( volumeattachments Resource , " " , volume Attachment ) , & v1alpha1 . Volume return obj . ( * v1alpha1 . Volume } 
func ( c * Fake Volume Attachments ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1alpha1 . Volume Attachment , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( volumeattachments Resource , name , pt , data , subresources ... ) , & v1alpha1 . Volume return obj . ( * v1alpha1 . Volume } 
func get Containers To Delete In Pod ( filter Container ID string , pod Status * kubecontainer . Pod Status , containers To Keep int ) container Statusby Created List { matched Container := func ( filter Container Id string , pod Status * kubecontainer . Pod Status ) * kubecontainer . Container Status { if filter Container for _ , container Status := range pod Status . Container Statuses { if container Status . ID . ID == filter Container Id { return container } ( filter Container ID , pod if filter Container ID != " " && matched Container == nil { klog . Warningf ( " " , filter Container return container Statusby Created // Find the exited containers whose name matches the name of the container with id being filter Container Id var candidates container Statusby Created for _ , container Status := range pod Status . Container Statuses { if container Status . State != kubecontainer . Container State if matched Container == nil || matched Container . Name == container Status . Name { candidates = append ( candidates , container if len ( candidates ) <= containers To Keep { return container Statusby Created return candidates [ containers To } 
func ( p * pod Container Deletor ) delete Containers In Pod ( filter Container ID string , pod Status * kubecontainer . Pod Status , remove All bool ) { containers To Keep := p . containers To if remove All { containers To filter Container for _ , candidate := range get Containers To Delete In Pod ( filter Container ID , pod Status , containers To } 
func ( in * CSR Signing Controller Configuration ) Deep Copy Into ( out * CSR Signing Controller out . Cluster Signing Duration = in . Cluster Signing } 
func ( in * CSR Signing Controller Configuration ) Deep Copy ( ) * CSR Signing Controller out := new ( CSR Signing Controller in . Deep Copy } 
func Print And Exit If Requested ( ) { if * version Flag == Version } else if * version Flag == Version } 
func ( ds * docker Service ) get Network Ready ( pod Sandbox ID string ) ( bool , bool ) { ds . network Ready defer ds . network Ready ready , ok := ds . network Ready [ pod Sandbox } 
func ( ds * docker Service ) Run Pod Sandbox ( ctx context . Context , r * runtimeapi . Run Pod Sandbox Request ) ( * runtimeapi . Run Pod Sandbox Response , error ) { config := r . Get // Step 1: Pull the image for the sandbox. image := default Sandbox pod Sandbox Image := ds . pod Sandbox if len ( pod Sandbox Image ) != 0 { image = pod Sandbox // NOTE: To use a custom sandbox image in a private repository, users need to configure the nodes with credentials properly. // see: http://kubernetes.io/docs/user-guide/images/#configuring-nodes-to-authenticate-to-a-private-repository // Only pull sandbox image when it's not present - v1.Pull If Not Present. if err := ensure Sandbox Image // Step 2: Create the sandbox container. if r . Get Runtime Handler ( ) != " " { return nil , fmt . Errorf ( " " , r . Get Runtime create Config , err := ds . make Sandbox Docker create Resp , err := ds . client . Create Container ( * create if err != nil { create Resp , err = recover From Creation Conflict If Needed ( ds . client , * create if err != nil || create resp := & runtimeapi . Run Pod Sandbox Response { Pod Sandbox Id : create ds . set Network Ready ( create defer func ( e * error ) { // Set networking ready depending on the error return of // the parent function if * e == nil { ds . set Network Ready ( create // Step 3: Create Sandbox Checkpoint. if err = ds . checkpoint Manager . Create Checkpoint ( create Resp . ID , construct Pod Sandbox // Step 4: Start the sandbox container. // Assume kubelet's garbage collector would remove the sandbox later, if // start Container failed. err = ds . client . Start Container ( create // Rewrite resolv.conf file generated by docker. // NOTE: cluster dns settings aren't passed anymore to docker api in all cases, // not only for pods with host network: the resolver conf will be overwritten // after sandbox creation to override docker's behaviour. This resolv.conf // file is shared by all containers of the same pod, and needs to be modified // only once per pod. if dns Config := config . Get Dns Config ( ) ; dns Config != nil { container Info , err := ds . client . Inspect Container ( create if err := rewrite Resolv File ( container Info . Resolv Conf Path , dns Config . Servers , dns Config . Searches , dns // Do not invoke network plugins if in host Network mode. if config . Get Linux ( ) . Get Security Context ( ) . Get Namespace Options ( ) . Get Network ( ) == runtimeapi . Namespace // Step 5: Setup networking for the sandbox. // All pod networking is setup by a CNI plugin discovered at startup time. // This plugin assigns the pod ip, sets up routes inside the sandbox, // creates interfaces etc. In theory, its jurisdiction ends with pod // sandbox networking, but it might insert iptables rules or open ports // on the host as well, to satisfy parts of the pod spec that aren't // recognized by the CNI standard yet. c ID := kubecontainer . Build Container ID ( runtime Name , create network if dns Config := config . Get Dns Config ( ) ; dns Config != nil { // Build DNS options. dns Option , err := json . Marshal ( dns network Options [ " " ] = string ( dns err = ds . network . Set Up Pod ( config . Get Metadata ( ) . Namespace , config . Get Metadata ( ) . Name , c ID , config . Annotations , network if err != nil { err List := [ ] error { fmt . Errorf ( " " , create // Ensure network resources are cleaned up even if the plugin // succeeded but an error happened between that success and here. err = ds . network . Tear Down Pod ( config . Get Metadata ( ) . Namespace , config . Get Metadata ( ) . Name , c if err != nil { err List = append ( err List , fmt . Errorf ( " " , create err = ds . client . Stop Container ( create Resp . ID , default Sandbox Grace if err != nil { err List = append ( err List , fmt . Errorf ( " " , create return resp , utilerrors . New Aggregate ( err } 
func ( ds * docker Service ) Stop Pod Sandbox ( ctx context . Context , r * runtimeapi . Stop Pod Sandbox Request ) ( * runtimeapi . Stop Pod Sandbox var host pod Sandbox ID := r . Pod Sandbox resp := & runtimeapi . Stop Pod Sandbox // Try to retrieve minimal sandbox information from docker daemon or sandbox checkpoint. inspect Result , metadata , status Err := ds . get Pod Sandbox Details ( pod Sandbox if status host Network = ( network Namespace Mode ( inspect Result ) == runtimeapi . Namespace } else { checkpoint := New Pod Sandbox Checkpoint ( " " , " " , & Checkpoint checkpoint Err := ds . checkpoint Manager . Get Checkpoint ( pod Sandbox // Proceed if both sandbox container and checkpoint could not be found. This means that following // actions will only have sandbox ID and not have pod namespace and name information. // Return error if encounter any unexpected error. if checkpoint Err != nil { if checkpoint Err != errors . Err Checkpoint Not Found { err := ds . checkpoint Manager . Remove Checkpoint ( pod Sandbox if err != nil { klog . Errorf ( " " , pod Sandbox if libdocker . Is Container Not Found Error ( status Err ) { klog . Warningf ( " " + " " , pod Sandbox } else { return nil , utilerrors . New Aggregate ( [ ] error { fmt . Errorf ( " " , pod Sandbox ID , checkpoint Err ) , fmt . Errorf ( " " , status } else { _ , name , namespace , _ , host Network = checkpoint . Get // WARNING: The following operations made the following assumption: // 1. kubelet will retry on any error returned by Stop Pod Sandbox. // 2. tearing down network and stopping sandbox container can succeed in any sequence. // This depends on the implementation detail of network plugin and proper error handling. // For kubenet, if tearing down network failed and sandbox container is stopped, kubelet // will retry. On retry, kubenet will not be able to retrieve network namespace of the sandbox // since it is stopped. With empty network namespcae, CNI bridge plugin will conduct best // effort clean up and will not return error. err ready , ok := ds . get Network Ready ( pod Sandbox if ! host Network && ( ready || ! ok ) { // Only tear down the pod network if we haven't done so already c ID := kubecontainer . Build Container ID ( runtime Name , pod Sandbox err := ds . network . Tear Down Pod ( namespace , name , c if err == nil { ds . set Network Ready ( pod Sandbox } else { err List = append ( err if err := ds . client . Stop Container ( pod Sandbox ID , default Sandbox Grace Period ) ; err != nil { // Do not return error if the container does not exist if ! libdocker . Is Container Not Found Error ( err ) { klog . Errorf ( " " , pod Sandbox err List = append ( err } else { // remove the checkpoint for any sandbox that is not found in the runtime ds . checkpoint Manager . Remove Checkpoint ( pod Sandbox if len ( err // TODO: Stop all running containers in the sandbox. return nil , utilerrors . New Aggregate ( err } 
func ( ds * docker Service ) Remove Pod Sandbox ( ctx context . Context , r * runtimeapi . Remove Pod Sandbox Request ) ( * runtimeapi . Remove Pod Sandbox Response , error ) { pod Sandbox ID := r . Pod Sandbox opts := dockertypes . Container List opts . Filters = dockerfilters . New f := new Docker f . Add Label ( sandbox ID Label Key , pod Sandbox containers , err := ds . client . List // Remove all containers in the sandbox. for i := range containers { if _ , err := ds . Remove Container ( ctx , & runtimeapi . Remove Container Request { Container Id : containers [ i ] . ID } ) ; err != nil && ! libdocker . Is Container Not Found // Remove the sandbox container. err = ds . client . Remove Container ( pod Sandbox ID , dockertypes . Container Remove Options { Remove if err == nil || libdocker . Is Container Not Found Error ( err ) { // Only clear network ready when the sandbox has actually been // removed from docker or doesn't exist ds . clear Network Ready ( pod Sandbox // Remove the checkpoint of the sandbox. if err := ds . checkpoint Manager . Remove Checkpoint ( pod Sandbox if len ( errs ) == 0 { return & runtimeapi . Remove Pod Sandbox return nil , utilerrors . New } 
func ( ds * docker Service ) get IP From Plugin ( sandbox * dockertypes . Container JSON ) ( string , error ) { metadata , err := parse Sandbox c ID := kubecontainer . Build Container ID ( runtime network Status , err := ds . network . Get Pod Network Status ( metadata . Namespace , metadata . Name , c if network return network } 
func ( ds * docker Service ) get IP ( pod Sandbox ID string , sandbox * dockertypes . Container JSON ) string { if sandbox . Network if network Namespace Mode ( sandbox ) == runtimeapi . Namespace // Don't bother getting IP if the pod is known and networking isn't ready ready , ok := ds . get Network Ready ( pod Sandbox ip , err := ds . get IP From // TODO: trusting the docker ip is not a great idea. However docker uses // eth0 by default and so does CNI, so if we find a docker IP here, we // conclude that the plugin must have failed setup, or forgotten its ip. // This is not a sensible assumption for plugins across the board, but if // a plugin doesn't want this behavior, it can throw an error. if sandbox . Network Settings . IP Address != " " { return sandbox . Network Settings . IP if sandbox . Network Settings . Global I Pv6Address != " " { return sandbox . Network Settings . Global I } 
func ( ds * docker Service ) get Pod Sandbox Details ( pod Sandbox ID string ) ( * dockertypes . Container JSON , * runtimeapi . Pod Sandbox Metadata , error ) { resp , err := ds . client . Inspect Container ( pod Sandbox metadata , err := parse Sandbox } 
func ( ds * docker Service ) Pod Sandbox Status ( ctx context . Context , req * runtimeapi . Pod Sandbox Status Request ) ( * runtimeapi . Pod Sandbox Status Response , error ) { pod Sandbox ID := req . Pod Sandbox r , metadata , err := ds . get Pod Sandbox Details ( pod Sandbox // Parse the timestamps. created At , _ , _ , err := get Container if err != nil { return nil , fmt . Errorf ( " " , pod Sandbox ct := created At . Unix // Translate container to sandbox state. state := runtimeapi . Pod Sandbox if r . State . Running { state = runtimeapi . Pod Sandbox // TODO: Remove this when sandbox is available on windows // This is a workaround for windows, where sandbox is not in use, and pod IP is determined through containers belonging to the Pod. if IP = ds . determine Pod IP By Sandbox ID ( pod Sandbox ID ) ; IP == " " { IP = ds . get IP ( pod Sandbox labels , annotations := extract status := & runtimeapi . Pod Sandbox Status { Id : r . ID , State : state , Created At : ct , Metadata : metadata , Labels : labels , Annotations : annotations , Network : & runtimeapi . Pod Sandbox Network Status { Ip : IP , } , Linux : & runtimeapi . Linux Pod Sandbox Status { Namespaces : & runtimeapi . Namespace { Options : & runtimeapi . Namespace Option { Network : network Namespace Mode ( r ) , Pid : pid Namespace Mode ( r ) , Ipc : ipc Namespace return & runtimeapi . Pod Sandbox Status } 
func ( ds * docker Service ) List Pod Sandbox ( _ context . Context , r * runtimeapi . List Pod Sandbox Request ) ( * runtimeapi . List Pod Sandbox Response , error ) { filter := r . Get // By default, list all containers whether they are running or not. opts := dockertypes . Container List filter Out Ready opts . Filters = dockerfilters . New f := new Docker // Add filter to select only sandbox containers. f . Add Label ( container Type Label Key , container Type Label if filter . State != nil { if filter . Get State ( ) . State == runtimeapi . Pod Sandbox } else { // runtimeapi.Pod Sandbox State_SANDBOX_NOTREADY can mean the // container is in any of the non-running state (e.g., created, // exited). We can't tell docker to filter out running // containers directly, so we'll need to filter them out // ourselves after getting the results. filter Out Ready if filter . Label Selector != nil { for k , v := range filter . Label Selector { f . Add // Make sure we get the list of checkpoints first so that we don't include // new Pod if filter == nil { checkpoints , err = ds . checkpoint Manager . List containers , err := ds . client . List // Convert docker containers to runtime api sandboxes. result := [ ] * runtimeapi . Pod // using map as set sandbox I converted , err := container To Runtime API if filter Out Ready Sandboxes && converted . State == runtimeapi . Pod Sandbox sandbox I // Include sandbox that could only be found with its checkpoint if no filter is applied // These Pod Sandbox will only include Pod Sandbox ID, Name, Namespace. // These Pod Sandbox will be in Pod Sandbox State_SANDBOX_NOTREADY state. for _ , id := range checkpoints { if _ , ok := sandbox I checkpoint := New Pod Sandbox Checkpoint ( " " , " " , & Checkpoint err := ds . checkpoint Manager . Get if err == errors . Err Corrupt Checkpoint { err = ds . checkpoint Manager . Remove result = append ( result , checkpoint To Runtime API return & runtimeapi . List Pod Sandbox } 
func ( ds * docker Service ) apply Sandbox Linux Options ( hc * dockercontainer . Host Config , lc * runtimeapi . Linux Pod Sandbox Config , create Config * dockertypes . Container Create // Apply security context. if err := apply Sandbox Security Context ( lc , create } 
func ( ds * docker Service ) make Sandbox Docker Config ( c * runtimeapi . Pod Sandbox Config , image string ) ( * dockertypes . Container Create Config , error ) { // Merge annotations and labels because docker supports only labels. labels := make Labels ( c . Get Labels ( ) , c . Get // Apply a label to distinguish sandboxes from regular containers. labels [ container Type Label Key ] = container Type Label // Apply a container name label for infra container. This is used in summary v1. // TODO(random-liu): Deprecate this label once container metrics is directly got from CRI. labels [ types . Kubernetes Container Name Label ] = sandbox Container hc := & dockercontainer . Host Config { Ipc Mode : dockercontainer . Ipc create Config := & dockertypes . Container Create Config { Name : make Sandbox Name ( c ) , Config : & dockercontainer . Config { Hostname : c . Hostname , // TODO: Handle environment variables. Image : image , Labels : labels , } , Host // Apply linux-specific options. if err := ds . apply Sandbox Linux Options ( hc , c . Get Linux ( ) , create Config , image , security Opt // Set port mappings. exposed Ports , port Bindings := make Ports And Bindings ( c . Get Port create Config . Config . Exposed Ports = exposed hc . Port Bindings = port // TODO: Get rid of the dependency on kubelet internal package. hc . Oom Score Adj = qos . Pod Infra OOM // Apply resource options. if err := ds . apply Sandbox Resources ( hc , c . Get // Set security options. security Opts , err := ds . get Security Opts ( c . Get Linux ( ) . Get Security Context ( ) . Get Seccomp Profile Path ( ) , security Opt hc . Security Opt = append ( hc . Security Opt , security apply Experimental Create Config ( create return create } 
func network Namespace Mode ( container * dockertypes . Container JSON ) runtimeapi . Namespace Mode { if container != nil && container . Host Config != nil && string ( container . Host Config . Network Mode ) == namespace Mode Host { return runtimeapi . Namespace return runtimeapi . Namespace } 
func pid Namespace Mode ( container * dockertypes . Container JSON ) runtimeapi . Namespace Mode { if container != nil && container . Host Config != nil && string ( container . Host Config . Pid Mode ) == namespace Mode Host { return runtimeapi . Namespace return runtimeapi . Namespace } 
func ipc Namespace Mode ( container * dockertypes . Container JSON ) runtimeapi . Namespace Mode { if container != nil && container . Host Config != nil && string ( container . Host Config . Ipc Mode ) == namespace Mode Host { return runtimeapi . Namespace return runtimeapi . Namespace } 
func rewrite Resolv File ( resolv File Path string , dns [ ] string , dns Search [ ] string , dns Options [ ] string ) error { if len ( resolv File if _ , err := os . Stat ( resolv File Path ) ; os . Is Not Exist ( err ) { return fmt . Errorf ( " " , resolv File var resolv File for _ , srv := range dns { resolv File Content = append ( resolv File if len ( dns Search ) > 0 { resolv File Content = append ( resolv File Content , " " + strings . Join ( dns if len ( dns Options ) > 0 { resolv File Content = append ( resolv File Content , " " + strings . Join ( dns if len ( resolv File Content ) > 0 { resolv File Content Str := strings . Join ( resolv File resolv File Content klog . V ( 4 ) . Infof ( " \n " , resolv File Path , resolv File if err := rewrite File ( resolv File Path , resolv File Content } 
func ( dsc * Daemon Sets Controller ) rolling Update ( ds * apps . Daemon Set , node List [ ] * v1 . Node , hash string ) error { node To Daemon Pods , err := dsc . get Nodes To Daemon _ , old Pods := dsc . get All Daemon Set Pods ( ds , node To Daemon max Unavailable , num Unavailable , err := dsc . get Unavailable Numbers ( ds , node List , node To Daemon old Available Pods , old Unavailable Pods := util . Split By Available Pods ( ds . Spec . Min Ready Seconds , old // for old Pods delete all not running pods var old Pods To for _ , pod := range old Unavailable Pods { // Skip terminating pods. We won't delete them again if pod . Deletion old Pods To Delete = append ( old Pods To for _ , pod := range old Available Pods { if num Unavailable >= max Unavailable { klog . V ( 4 ) . Infof ( " " , num Unavailable , max old Pods To Delete = append ( old Pods To num return dsc . sync Nodes ( ds , old Pods To } 
func ( dsc * Daemon Sets Controller ) construct History ( ds * apps . Daemon Set ) ( cur * apps . Controller Revision , old [ ] * apps . Controller Revision , err error ) { var histories [ ] * apps . Controller var current Histories [ ] * apps . Controller histories , err = dsc . controlled for _ , history := range histories { // Add the unique label if it's not already added to the history // We use history name instead of computing hash, so that we don't need to worry about hash collision if _ , ok := history . Labels [ apps . Default Daemon Set Unique Label Key ] ; ! ok { to Update := history . Deep to Update . Labels [ apps . Default Daemon Set Unique Label Key ] = to history , err = dsc . kube Client . Apps V1 ( ) . Controller Revisions ( ds . Namespace ) . Update ( to if found { current Histories = append ( current curr Revision := max switch len ( current Histories ) { case 0 : // Create a new history if the current one isn't found cur , err = dsc . snapshot ( ds , curr default : cur , err = dsc . dedup Cur Histories ( ds , current // Update revision number if necessary if cur . Revision < curr Revision { to Update := cur . Deep to Update . Revision = curr _ , err = dsc . kube Client . Apps V1 ( ) . Controller Revisions ( ds . Namespace ) . Update ( to } 
func max Revision ( histories [ ] * apps . Controller } 
func ( dsc * Daemon Sets Controller ) controlled Histories ( ds * apps . Daemon Set ) ( [ ] * apps . Controller Revision , error ) { selector , err := metav1 . Label Selector As // List all histories to include those that don't match the selector anymore // but have a Controller Ref pointing to the controller. histories , err := dsc . history // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing Pods (see #42639). can Adopt Func := controller . Recheck Deletion Timestamp ( func ( ) ( metav1 . Object , error ) { fresh , err := dsc . kube Client . Apps V1 ( ) . Daemon Sets ( ds . Namespace ) . Get ( ds . Name , metav1 . Get // Use Controller Ref Manager to adopt/orphan as needed. cm := controller . New Controller Revision Controller Ref Manager ( dsc . cr Control , ds , selector , controller Kind , can Adopt return cm . Claim Controller } 
func Match ( ds * apps . Daemon Set , history * apps . Controller Revision ) ( bool , error ) { patch , err := get } 
func ( c * Wardle V1beta1Client ) REST return c . rest } 
func All Stages ( ) sets . String { return sets . New String ( audit . Stage Request Received , audit . Stage Response Started , audit . Stage Response Complete , audit . Stage } 
func All Levels ( ) sets . String { return sets . New String ( string ( audit . Level None ) , string ( audit . Level Metadata ) , string ( audit . Level Request ) , string ( audit . Level Request } 
func Invert Stages ( stages [ ] audit . Stage ) [ ] audit . Stage { s := Convert Stages To a := All return Convert String Set To } 
func Convert Stages To } 
func Convert String Set To } 
func ( gc * Garbage Collector ) api Resource ( api Version , kind string ) ( schema . Group Version Resource , bool , error ) { fq Kind := schema . From API Version And Kind ( api mapping , err := gc . rest Mapper . REST Mapping ( fq Kind . Group Kind ( ) , fq if err != nil { return schema . Group Version Resource { } , false , new REST Mapping Error ( kind , api return mapping . Resource , mapping . Scope == meta . REST Scope } 
func Equals ( a corev1 . Resource List , b corev1 . Resource } 
func Less Than Or Equal ( a corev1 . Resource List , b corev1 . Resource List ) ( bool , [ ] corev1 . Resource resource Names := [ ] corev1 . Resource resource Names = append ( resource return result , resource } 
func Max ( a corev1 . Resource List , b corev1 . Resource List ) corev1 . Resource List { result := corev1 . Resource } 
func Add ( a corev1 . Resource List , b corev1 . Resource List ) corev1 . Resource List { result := corev1 . Resource } 
func Subtract With Non Negative Result ( a corev1 . Resource List , b corev1 . Resource List ) corev1 . Resource List { zero := resource . Must result := corev1 . Resource } 
func Subtract ( a corev1 . Resource List , b corev1 . Resource List ) corev1 . Resource List { result := corev1 . Resource } 
func Mask ( resources corev1 . Resource List , names [ ] corev1 . Resource Name ) corev1 . Resource List { name Set := To result := corev1 . Resource for key , value := range resources { if name } 
func Resource Names ( resources corev1 . Resource List ) [ ] corev1 . Resource Name { result := [ ] corev1 . Resource for resource Name := range resources { result = append ( result , resource } 
func Contains ( items [ ] corev1 . Resource Name , item corev1 . Resource } 
func Contains Prefix ( prefix Set [ ] string , item corev1 . Resource Name ) bool { for _ , prefix := range prefix Set { if strings . Has } 
func Intersection ( a [ ] corev1 . Resource Name , b [ ] corev1 . Resource Name ) [ ] corev1 . Resource Name { result := make ( [ ] corev1 . Resource } 
func Is Zero ( a corev1 . Resource List ) bool { zero := resource . Must } 
func Is Negative ( a corev1 . Resource List ) [ ] corev1 . Resource Name { results := [ ] corev1 . Resource zero := resource . Must } 
func To Set ( resource Names [ ] corev1 . Resource Name ) sets . String { result := sets . New for _ , resource Name := range resource Names { result . Insert ( string ( resource } 
func Calculate Usage ( namespace Name string , scopes [ ] corev1 . Resource Quota Scope , hard Limits corev1 . Resource List , registry Registry , scope Selector * corev1 . Scope Selector ) ( corev1 . Resource List , error ) { // find the intersection between the hard resources on the quota // and the resources this controller can track to know what we can // look to measure updated usage stats for hard Resources := Resource Names ( hard potential Resources := [ ] corev1 . Resource for _ , evaluator := range evaluators { potential Resources = append ( potential Resources , evaluator . Matching Resources ( hard // NOTE: the intersection just removes duplicates since the evaluator match intersects with hard matched Resources := Intersection ( hard Resources , potential // sum the observed usage from each evaluator new Usage := corev1 . Resource for _ , evaluator := range evaluators { // only trigger the evaluator if it matches a resource in the quota, otherwise, skip calculating anything intersection := evaluator . Matching Resources ( matched usage Stats Options := Usage Stats Options { Namespace : namespace Name , Scopes : scopes , Resources : intersection , Scope Selector : scope stats , err := evaluator . Usage Stats ( usage Stats // exclude resources which encountered calculation errors matched Resources = Difference ( matched new Usage = Add ( new // mask the observed usage to only the set of resources tracked by this quota // merge our observed usage with the quota usage status // if the new usage is different than the last usage, we will need to do an update new Usage = Mask ( new Usage , matched return new Usage , utilerrors . New } 
func New Options ( ) ( * Options , error ) { cfg , err := new Default Component hhost , hport , err := split Host Int Port ( cfg . Healthz Bind o := & Options { Component Config : * cfg , Secure Serving : apiserveroptions . New Secure Serving Options ( ) . With Loopback ( ) , Combined Insecure Serving : & Combined Insecure Serving Options { Healthz : ( & apiserveroptions . Deprecated Insecure Serving Options { Bind Network : " " , } ) . With Loopback ( ) , Metrics : ( & apiserveroptions . Deprecated Insecure Serving Options { Bind Network : " " , } ) . With Loopback ( ) , Bind Port : hport , Bind Address : hhost , } , Authentication : apiserveroptions . New Delegating Authentication Options ( ) , Authorization : apiserveroptions . New Delegating Authorization Options ( ) , Deprecated : & Deprecated Options { Use Legacy Policy Config : false , Policy Config Map Namespace : metav1 . Namespace o . Authentication . Tolerate In Cluster Lookup o . Authentication . Remote Kube Config File o . Authorization . Remote Kube Config File o . Authorization . Always Allow // Set the Pair Name but leave certificate directory blank to generate in-memory by default o . Secure Serving . Server Cert . Cert o . Secure Serving . Server Cert . Pair o . Secure Serving . Bind Port = ports . Kube Scheduler } 
func ( o * Options ) Flags ( ) ( nfs cliflag . Named Flag Sets ) { fs := nfs . Flag fs . String Var ( & o . Config File , " " , o . Config fs . String Var ( & o . Write Config To , " " , o . Write Config fs . String o . Secure Serving . Add Flags ( nfs . Flag o . Combined Insecure Serving . Add Flags ( nfs . Flag o . Authentication . Add Flags ( nfs . Flag o . Authorization . Add Flags ( nfs . Flag o . Deprecated . Add Flags ( nfs . Flag Set ( " " ) , & o . Component leaderelectionconfig . Bind Flags ( & o . Component Config . Leader Election . Leader Election Configuration , nfs . Flag utilfeature . Default Mutable Feature Gate . Add Flag ( nfs . Flag } 
func ( o * Options ) Apply To ( c * schedulerappconfig . Config ) error { if len ( o . Config File ) == 0 { c . Component Config = o . Component // only apply deprecated flags if no config file is loaded (this is the old behaviour). if err := o . Deprecated . Apply To ( & c . Component if err := o . Combined Insecure Serving . Apply To ( c , & c . Component } else { cfg , err := load Config From File ( o . Config // use the loaded config file only, with the exception of --address and --port. This means that // none of the deprecated flags in o.Deprecated are taken into consideration. This is the old // behaviour of the flags we have to keep. c . Component if err := o . Combined Insecure Serving . Apply To From Loaded Config ( c , & c . Component if err := o . Secure Serving . Apply To ( & c . Secure Serving , & c . Loopback Client if o . Secure Serving != nil && ( o . Secure Serving . Bind Port != 0 || o . Secure Serving . Listener != nil ) { if err := o . Authentication . Apply To ( & c . Authentication , c . Secure if err := o . Authorization . Apply } 
if err := validation . Validate Kube Scheduler Configuration ( & o . Component Config ) . To errs = append ( errs , o . Secure errs = append ( errs , o . Combined Insecure } 
func ( o * Options ) Config ( ) ( * schedulerappconfig . Config , error ) { if o . Secure Serving != nil { if err := o . Secure Serving . Maybe Default With Self Signed Certs ( " " , nil , [ ] net . IP { net . Parse if err := o . Apply // Prepare kube clients. client , leader Election Client , event Client , err := create Clients ( c . Component Config . Client Connection , o . Master , c . Component Config . Leader Election . Renew // Prepare event clients. event Broadcaster := record . New recorder := event Broadcaster . New Recorder ( legacyscheme . Scheme , corev1 . Event Source { Component : c . Component Config . Scheduler // Set up leader election if enabled. var leader Election Config * leaderelection . Leader Election if c . Component Config . Leader Election . Leader Elect { leader Election Config , err = make Leader Election Config ( c . Component Config . Leader Election , leader Election c . Informer Factory = informers . New Shared Informer c . Pod Informer = factory . New Pod c . Event Client = event c . Broadcaster = event c . Leader Election = leader Election } 
func make Leader Election Config ( config kubeschedulerconfig . Kube Scheduler Leader Election Configuration , client clientset . Interface , recorder record . Event Recorder ) ( * leaderelection . Leader Election // add a uniquifier so that two processes on the same host don't accidentally both become active id := hostname + " " + string ( uuid . New rl , err := resourcelock . New ( config . Resource Lock , config . Lock Object Namespace , config . Lock Object Name , client . Core V1 ( ) , client . Coordination V1 ( ) , resourcelock . Resource Lock Config { Identity : id , Event return & leaderelection . Leader Election Config { Lock : rl , Lease Duration : config . Lease Duration . Duration , Renew Deadline : config . Renew Deadline . Duration , Retry Period : config . Retry Period . Duration , Watch Dog : leaderelection . New Leader Healthz } 
func create Clients ( config componentbaseconfig . Client Connection Configuration , master Override string , timeout time . Duration ) ( clientset . Interface , clientset . Interface , v1core . Events Getter , error ) { if len ( config . Kubeconfig ) == 0 && len ( master // This creates a client, first loading any specified kubeconfig // file, and then overriding the Master flag, if non-empty. kube Config , err := clientcmd . New Non Interactive Deferred Loading Client Config ( & clientcmd . Client Config Loading Rules { Explicit Path : config . Kubeconfig } , & clientcmd . Config Overrides { Cluster Info : clientcmdapi . Cluster { Server : master Override } } ) . Client kube Config . Accept Content Types = config . Accept Content kube Config . Content Type = config . Content kube //TODO make config struct use int instead of int32? kube client , err := clientset . New For Config ( restclient . Add User Agent ( kube // shallow copy, do not modify the kube Config.Timeout. rest Config := * kube rest leader Election Client , err := clientset . New For Config ( restclient . Add User Agent ( & rest event Client , err := clientset . New For Config ( kube return client , leader Election Client , event Client . Core } 
func New REST ( opts Getter generic . REST Options Getter ) ( * REST , * Status REST ) { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & networking . Ingress { } } , New List Func : func ( ) runtime . Object { return & networking . Ingress List { } } , Default Qualified Resource : networking . Resource ( " " ) , Create Strategy : ingress . Strategy , Update Strategy : ingress . Strategy , Delete Strategy : ingress . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts if err := store . Complete With status status Store . Update Strategy = ingress . Status return & REST { store } , & Status REST { store : & status } 
func ( in Extra Value ) Deep Copy Into ( out * Extra * out = make ( Extra } 
func ( in Extra Value ) Deep Copy ( ) Extra out := new ( Extra in . Deep Copy } 
func ( in * Group Resources ) Deep Copy Into ( out * Group if in . Resource Names != nil { in , out := & in . Resource Names , & out . Resource } 
func ( in * Group Resources ) Deep Copy ( ) * Group out := new ( Group in . Deep Copy } 
func ( in * Policy ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object * out = make ( [ ] Policy for i := range * in { ( * in ) [ i ] . Deep Copy if in . Omit Stages != nil { in , out := & in . Omit Stages , & out . Omit } 
func ( in * Policy ) Deep in . Deep Copy } 
func ( in * Policy ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Policy List ) Deep Copy Into ( out * Policy out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Policy List ) Deep Copy ( ) * Policy out := new ( Policy in . Deep Copy } 
func ( in * Policy List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Policy Rule ) Deep Copy Into ( out * Policy if in . User Groups != nil { in , out := & in . User Groups , & out . User * out = make ( [ ] Group for i := range * in { ( * in ) [ i ] . Deep Copy if in . Non Resource UR Ls != nil { in , out := & in . Non Resource UR Ls , & out . Non Resource UR if in . Omit Stages != nil { in , out := & in . Omit Stages , & out . Omit } 
func ( in * Policy Rule ) Deep Copy ( ) * Policy out := new ( Policy in . Deep Copy } 
func ( in * User Info ) Deep Copy ( ) * User out := new ( User in . Deep Copy } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( custom_metrics . Add To utilruntime . Must ( v1beta2 . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1beta1 . Scheme Group Version , v1beta2 . Scheme Group } 
func Is Corrupted var underlying case * os . Path Error : underlying case * os . Link Error : underlying case * os . Syscall Error : underlying if ee , ok := underlying Error . ( syscall . Errno ) ; ok { for _ , errno := range error No } 
func New Actual State Of World ( node Name types . Node Name , volume Plugin Mgr * volume . Volume Plugin Mgr ) Actual State Of World { return & actual State Of World { node Name : node Name , attached Volumes : make ( map [ v1 . Unique Volume Name ] attached Volume ) , volume Plugin Mgr : volume Plugin } 
func ( asw * actual State Of World ) add Volume ( volume Name v1 . Unique Volume Name , volume Spec * volume . Spec , device volume Plugin , err := asw . volume Plugin Mgr . Find Plugin By Spec ( volume if err != nil || volume Plugin == nil { return fmt . Errorf ( " " , volume if len ( volume Name ) == 0 { volume Name , err = util . Get Unique Volume Name From Spec ( volume Plugin , volume if err != nil { return fmt . Errorf ( " " , volume Spec . Name ( ) , volume Plugin . Get Plugin plugin Is if _ , ok := volume Plugin . ( volume . Attachable Volume Plugin ) ; ok { plugin Is volume Obj , volume Exists := asw . attached Volumes [ volume if ! volume Exists { volume Obj = attached Volume { volume Name : volume Name , spec : volume Spec , mounted Pods : make ( map [ volumetypes . Unique Pod Name ] mounted Pod ) , plugin Name : volume Plugin . Get Plugin Name ( ) , plugin Is Attachable : plugin Is Attachable , globally Mounted : false , device Path : device } else { // If volume object already exists, update the fields such as device path volume Obj . device Path = device klog . V ( 2 ) . Infof ( " " , volume Name , device asw . attached Volumes [ volume Name ] = volume } 
func get Mounted Volume ( mounted Pod * mounted Pod , attached Volume * attached Volume ) Mounted Volume { return Mounted Volume { Mounted Volume : operationexecutor . Mounted Volume { Pod Name : mounted Pod . pod Name , Volume Name : attached Volume . volume Name , Inner Volume Spec Name : mounted Pod . volume Spec . Name ( ) , Outer Volume Spec Name : mounted Pod . outer Volume Spec Name , Plugin Name : attached Volume . plugin Name , Pod UID : mounted Pod . pod UID , Mounter : mounted Pod . mounter , Block Volume Mapper : mounted Pod . block Volume Mapper , Volume Gid Value : mounted Pod . volume Gid Value , Volume Spec : mounted Pod . volume Spec , Device Mount Path : attached Volume . device Mount } 
func New Configurer ( recorder record . Event Recorder , node Ref * v1 . Object Reference , node IP net . IP , cluster DNS [ ] net . IP , cluster Domain , resolver Config string ) * Configurer { return & Configurer { recorder : recorder , node Ref : node Ref , node IP : node IP , cluster DNS : cluster DNS , Cluster Domain : cluster Domain , Resolver Config : resolver } 
func ( c * Configurer ) Check Limits For Resolv Conf ( ) { f , err := os . Open ( c . Resolver if err != nil { c . recorder . Event ( c . node Ref , v1 . Event Type _ , host Search , _ , err := parse Resolv if err != nil { c . recorder . Event ( c . node Ref , v1 . Event Type domain Count Limit := validation . Max DNS Search if c . Cluster Domain != " " { domain Count if len ( host Search ) > domain Count Limit { log := fmt . Sprintf ( " " , c . Resolver Config , domain Count c . recorder . Event ( c . node Ref , v1 . Event Type if len ( strings . Join ( host Search , " " ) ) > validation . Max DNS Search List Chars { log := fmt . Sprintf ( " " , c . Resolver Config , validation . Max DNS Search List c . recorder . Event ( c . node Ref , v1 . Event Type } 
func parse Resolv Conf ( reader io . Reader ) ( nameservers [ ] string , searches [ ] string , options [ ] string , err error ) { file , err := ioutil . Read var all for l := range lines { trimmed := strings . Trim if strings . Has } else { all Errors = append ( all return nameservers , searches , options , utilerrors . New Aggregate ( all } 
func merge DNS Options ( existing DNS Config Options [ ] string , dns Config Options [ ] v1 . Pod DNS Config Option ) [ ] string { options for _ , op := range existing DNS Config Options { if index := strings . Index ( op , " " ) ; index != - 1 { options } else { options for _ , op := range dns Config Options { if op . Value != nil { options } else { options for op Name , op Value := range options Map { op := op if op Value != " " { op = op + " " + op } 
func append DNS Config ( existing DNS Config * runtimeapi . DNS Config , dns Config * v1 . Pod DNS Config ) * runtimeapi . DNS Config { existing DNS Config . Servers = omit Duplicates ( append ( existing DNS Config . Servers , dns existing DNS Config . Searches = omit Duplicates ( append ( existing DNS Config . Searches , dns existing DNS Config . Options = merge DNS Options ( existing DNS Config . Options , dns return existing DNS } 
func ( c * Configurer ) Get Pod DNS ( pod * v1 . Pod ) ( * runtimeapi . DNS Config , error ) { dns Config , err := c . get Host DNS dns Type , err := get Pod DNS dns Type = pod DNS switch dns Type { case pod DNS None : // DNS None should use empty DNS settings as the base. dns Config = & runtimeapi . DNS case pod DNS Cluster : if len ( c . cluster DNS ) != 0 { // For a pod with DNS Cluster First policy, the cluster DNS server is // the only nameserver configured for the pod. The cluster DNS server // itself will forward queries to other nameservers that is configured // to use, in case the cluster DNS server cannot resolve the DNS query // itself. dns for _ , ip := range c . cluster DNS { dns Config . Servers = append ( dns dns Config . Searches = c . generate Searches For DNS Cluster First ( dns dns Config . Options = default DNS // cluster DNS is not known. Pod with Cluster DNS First Policy cannot be created. node Error Msg := fmt . Sprintf ( " " , v1 . DNS Cluster First , v1 . DNS c . recorder . Eventf ( c . node Ref , v1 . Event Type Warning , " " , node Error c . recorder . Eventf ( pod , v1 . Event Type Warning , " " , " " , format . Pod ( pod ) , node Error // Fallback to DNS case pod DNS Host : // When the kubelet --resolv-conf flag is set to the empty string, use // DNS settings that override the docker default (which is to use // /etc/resolv.conf) and effectively disable DNS lookups. According to // the bind documentation, the behavior of the DNS client library when // "nameservers" are not specified is to "use the nameserver on the // local machine". A nameserver setting of localhost is equivalent to // this documented behavior. if c . Resolver Config == " " { switch { case c . node IP == nil || c . node IP . To4 ( ) != nil : dns case c . node IP . To16 ( ) != nil : dns dns if pod . Spec . DNS Config != nil { dns Config = append DNS Config ( dns Config , pod . Spec . DNS return c . form DNS Config Fits Limits ( dns } 
func ( c * Configurer ) Setup DN Sin Containerized Mounter ( mounter Path string ) { resolve Path := filepath . Join ( strings . Trim Suffix ( mounter dns for _ , dns := range c . cluster DNS { dns String = dns if c . Resolver Config != " " { f , err := os . Open ( c . Resolver } else { _ , host Search , _ , err := parse Resolv } else { dns String = dns for _ , search := range host Search { dns String = dns dns String = dns if err := ioutil . Write File ( resolve Path , [ ] byte ( dns String ) , 0600 ) ; err != nil { klog . Errorf ( " " , resolve } 
func ( admission Handler chain Admission Handler ) Admit ( a Attributes , o Object Interfaces ) error { for _ , handler := range admission Handler { if ! handler . Handles ( a . Get if mutator , ok := handler . ( Mutation } 
func ( admission Handler chain Admission Handler ) Validate ( a Attributes , o Object Interfaces ) error { for _ , handler := range admission Handler { if ! handler . Handles ( a . Get if validator , ok := handler . ( Validation } 
func ( admission Handler chain Admission Handler ) Handles ( operation Operation ) bool { for _ , handler := range admission } 
func ( c * Fake Discovery ) Server Resources For Group Version ( group Version string ) ( * metav1 . API Resource List , error ) { action := testing . Action Impl { Verb : " " , Resource : schema . Group Version for _ , resource List := range c . Resources { if resource List . Group Version == group Version { return resource return nil , fmt . Errorf ( " " , group } 
func ( c * Fake Discovery ) Server Resources ( ) ( [ ] * metav1 . API Resource List , error ) { _ , rs , err := c . Server Groups And } 
func ( c * Fake Discovery ) Server Groups And Resources ( ) ( [ ] * metav1 . API Group , [ ] * metav1 . API Resource List , error ) { sgs , err := c . Server result Groups := [ ] * metav1 . API for i := range sgs . Groups { result Groups = append ( result action := testing . Action Impl { Verb : " " , Resource : schema . Group Version return result } 
func ( c * Fake Discovery ) Server Groups ( ) ( * metav1 . API Group List , error ) { action := testing . Action Impl { Verb : " " , Resource : schema . Group Version groups := map [ string ] * metav1 . API for _ , res := range c . Resources { gv , err := schema . Parse Group Version ( res . Group if group == nil { group = & metav1 . API Group { Name : gv . Group , Preferred Version : metav1 . Group Version For Discovery { Group Version : res . Group group . Versions = append ( group . Versions , metav1 . Group Version For Discovery { Group Version : res . Group list := & metav1 . API Group for _ , api Group := range groups { list . Groups = append ( list . Groups , * api } 
func ( c * Fake Discovery ) Server Version ( ) ( * version . Info , error ) { action := testing . Action action . Resource = schema . Group Version if c . Faked Server Version != nil { return c . Faked Server version return & version } 
func ( f * field Lookup ) Save Leaf } 
func ( f * field Lookup ) Visit Array ( a * proto . Array ) { if f . Save Leaf // Passthrough arrays. a . Sub } 
func ( f * field Lookup ) Visit Map ( m * proto . Map ) { if f . Save Leaf // Passthrough maps. m . Sub } 
func ( f * field Lookup ) Visit Kind ( k * proto . Kind ) { if f . Save Leaf sub sub } 
func ( f * field Lookup ) Visit Reference ( r proto . Reference ) { if f . Save Leaf // Passthrough references. r . Sub } 
func Lookup Schema For Field ( schema proto . Schema , path [ ] string ) ( proto . Schema , error ) { f := & field } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( apps . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( v1beta2 . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1 . Scheme Group Version , v1beta2 . Scheme Group Version , v1beta1 . Scheme Group } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Job { } , & Job List { } , & Job Template { } , & Cron Job { } , & Cron Job } 
func Convert To Timestamp ( time String string ) * Timestamp { parsed , _ := time . Parse ( time . RFC3339Nano , time } 
func Sort Init Container Statuses ( p * v1 . Pod , statuses [ ] v1 . Container Status ) { containers := p . Spec . Init } 
} 
func New CPU Set ( cpus ... int ) CPU Set { b := New } 
func ( s CPU } 
func ( s CPU Set ) Equals ( s2 CPU Set ) bool { return reflect . Deep } 
func ( s CPU Set ) Filter ( predicate func ( int ) bool ) CPU Set { b := New } 
func ( s CPU Set ) Is Subset Of ( s2 CPU } 
func ( s CPU Set ) Union ( s2 CPU Set ) CPU Set { b := New } 
func ( s CPU Set ) Intersection ( s2 CPU Set ) CPU } 
func ( s CPU Set ) Difference ( s2 CPU Set ) CPU Set { return s . Filter } 
func ( s CPU Set ) To } 
func ( s CPU Set ) String ( ) string { if s . Is elems := s . To for i := 1 ; i < len ( elems ) ; i ++ { last // if this element is adjacent to the high end of the last range if elems [ i ] == last Range . end + 1 { // then extend the last range to include this element last for _ , r := range ranges { if r . start == r . end { result . Write } else { result . Write result . Write return strings . Trim } 
func Must Parse ( s string ) CPU } 
func Parse ( s string ) ( CPU Set , error ) { b := New if err != nil { return New CPU if err != nil { return New CPU if err != nil { return New CPU } 
func ( s CPU Set ) Clone ( ) CPU Set { b := New } 
func new Persistent Volume Claims ( c * Core V1Client , namespace string ) * persistent Volume Claims { return & persistent Volume Claims { client : c . REST } 
func ( c * persistent Volume Claims ) Create ( persistent Volume Claim * v1 . Persistent Volume Claim ) ( result * v1 . Persistent Volume Claim , err error ) { result = & v1 . Persistent Volume err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( persistent Volume } 
func ( c * Naming Condition Controller ) process Next Work err := c . sync utilruntime . Handle c . queue . Add Rate } 
func ( t * Taint ) Match Taint ( taint To Match * Taint ) bool { return t . Key == taint To Match . Key && t . Effect == taint To } 
func ( s * persistent Volume Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Persistent Volume , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Persistent } 
func ( s * persistent Volume Lister ) Get ( name string ) ( * v1 . Persistent Volume , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1 . Persistent } 
func New Undirected Graph ( self , absent float64 ) * Undirected Graph { return & Undirected Graph { nodes : make ( map [ int ] graph . Node ) , edges : make ( map [ int ] edge } 
func ( g * Undirected Graph ) New Node if len ( g . nodes ) == max if g . free I Ds . Len ( ) != 0 && g . free I Ds . Take if id = g . used I Ds . Max ( ) ; id < max for id = 0 ; id < max Int ; id ++ { if ! g . used I } 
func ( g * Undirected Graph ) Add g . edges [ n . ID ( ) ] = & slice Edge g . free I g . used I } 
func ( g * Undirected Graph ) Remove g . free I g . used I } 
func ( g * Undirected Graph ) Set if ! g . Has ( from ) { g . Add if ! g . Has ( to ) { g . Add } 
func ( g * Undirected Graph ) Remove } 
func ( g * Undirected } 
func ( g * Undirected } 
func ( g * Undirected } 
func ( g * Undirected } 
func ( g * Undirected } 
func ( g * Undirected Graph ) Has Edge } 
func ( g * Undirected Graph ) Edge ( u , v graph . Node ) graph . Edge { return g . Edge } 
func ( g * Undirected Graph ) Edge } 
func ( g * Undirected } 
func ( g * Undirected } 
func new Secrets ( c * Core V1Client , namespace string ) * secrets { return & secrets { client : c . REST } 
} 
func ( s Config Map Generator V1 ) Generate ( generic Params map [ string ] interface { } ) ( runtime . Object , error ) { err := generate . Validate Params ( s . Param Names ( ) , generic delegate := & Config Map Generator from File Strings , found := generic if found { from File Array , is Array := from File if ! is Array { return nil , fmt . Errorf ( " " , from File delegate . File Sources = from File delete ( generic from Literal Strings , found := generic if found { from Literal Array , is Array := from Literal if ! is Array { return nil , fmt . Errorf ( " " , from Literal delegate . Literal Sources = from Literal delete ( generic from Env File String , found := generic if found { from Env File , is String := from Env File if ! is String { return nil , fmt . Errorf ( " " , from Env File delegate . Env File Source = from Env delete ( generic hash Param , found := generic if found { hash Bool , is Bool := hash if ! is Bool { return nil , fmt . Errorf ( " " , hash delegate . Append Hash = hash delete ( generic for key , value := range generic Params { str Val , is if ! is params [ key ] = str return delegate . Structured } 
func ( s Config Map Generator V1 ) Structured config Map := & v1 . Config config config config Map . Binary if len ( s . File Sources ) > 0 { if err := handle Config Map From File Sources ( config Map , s . File if len ( s . Literal Sources ) > 0 { if err := handle Config Map From Literal Sources ( config Map , s . Literal if len ( s . Env File Source ) > 0 { if err := handle Config Map From Env File Source ( config Map , s . Env File if s . Append Hash { h , err := hash . Config Map Hash ( config config Map . Name = fmt . Sprintf ( " " , config return config } 
func ( s Config Map Generator if len ( s . Env File Source ) > 0 && ( len ( s . File Sources ) > 0 || len ( s . Literal } 
func handle Config Map From Literal Sources ( config Map * v1 . Config Map , literal Sources [ ] string ) error { for _ , literal Source := range literal Sources { key Name , value , err := util . Parse Literal Source ( literal err = add Key From Literal To Config Map ( config Map , key } 
func handle Config Map From Env File Source ( config Map * v1 . Config Map , env File Source string ) error { info , err := os . Stat ( env File if err != nil { switch err := err . ( type ) { case * os . Path Error : return fmt . Errorf ( " " , env File default : return fmt . Errorf ( " " , env File if info . Is return add From Env File ( env File Source , func ( key , value string ) error { return add Key From Literal To Config Map ( config } 
func add Key From File To Config Map ( config Map * v1 . Config Map , key Name , file Path string ) error { data , err := ioutil . Read File ( file if utf8 . Valid ( data ) { return add Key From Literal To Config Map ( config Map , key err = validate New Config Map ( config Map , key config Map . Binary Data [ key } 
func add Key From Literal To Config Map ( config Map * v1 . Config Map , key Name , data string ) error { err := validate New Config Map ( config Map , key config Map . Data [ key } 
func ( in * Aggregation Rule ) Deep Copy ( ) * Aggregation out := new ( Aggregation in . Deep Copy } 
func ( in * Cluster Role ) Deep Copy ( ) * Cluster out := new ( Cluster in . Deep Copy } 
func ( in * Cluster Role ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Cluster Role Binding ) Deep Copy ( ) * Cluster Role out := new ( Cluster Role in . Deep Copy } 
func ( in * Cluster Role Binding ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Cluster Role Binding List ) Deep Copy Into ( out * Cluster Role Binding out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Cluster Role for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Cluster Role Binding List ) Deep Copy ( ) * Cluster Role Binding out := new ( Cluster Role Binding in . Deep Copy } 
func ( in * Cluster Role Binding List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Cluster Role List ) Deep Copy Into ( out * Cluster Role out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Cluster for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Cluster Role List ) Deep Copy ( ) * Cluster Role out := new ( Cluster Role in . Deep Copy } 
func ( in * Cluster Role List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Policy Rule ) Deep Copy Into ( out * Policy if in . API Groups != nil { in , out := & in . API Groups , & out . API if in . Resource Names != nil { in , out := & in . Resource Names , & out . Resource if in . Non Resource UR Ls != nil { in , out := & in . Non Resource UR Ls , & out . Non Resource UR } 
func ( in * Role ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object * out = make ( [ ] Policy for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Role ) Deep in . Deep Copy } 
func ( in * Role ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Role Binding ) Deep Copy Into ( out * Role out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object out . Role Ref = in . Role } 
func ( in * Role Binding ) Deep Copy ( ) * Role out := new ( Role in . Deep Copy } 
func ( in * Role Binding ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Role Binding List ) Deep Copy Into ( out * Role Binding out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Role for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Role Binding List ) Deep Copy ( ) * Role Binding out := new ( Role Binding in . Deep Copy } 
func ( in * Role Binding List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Role List ) Deep Copy Into ( out * Role out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Role List ) Deep Copy ( ) * Role out := new ( Role in . Deep Copy } 
func ( in * Role List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Role Ref ) Deep Copy ( ) * Role out := new ( Role in . Deep Copy } 
func ( in * Subject ) Deep in . Deep Copy } 
func New REST ( opts Getter generic . REST Options Getter ) * REST { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Limit Range { } } , New List Func : func ( ) runtime . Object { return & api . Limit Range List { } } , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : limitrange . Strategy , Update Strategy : limitrange . Strategy , Delete Strategy : limitrange . Strategy , Export options := & generic . Store Options { REST Options : opts if err := store . Complete With } 
func aws Handler Logger ( req * request . Request ) { service , name := aws Service And } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion } 
func Convert_v1alpha1_Configuration_To_podtolerationrestriction_Configuration ( in * Configuration , out * podtolerationrestriction . Configuration , s conversion . Scope ) error { return auto } 
func Convert_podtolerationrestriction_Configuration_To_v1alpha1_Configuration ( in * podtolerationrestriction . Configuration , out * Configuration , s conversion . Scope ) error { return auto } 
func new Pod Templates ( c * Core V1Client , namespace string ) * pod Templates { return & pod Templates { client : c . REST } 
func ( o * CSR Signing Controller Options ) Add Flags ( fs * pflag . Flag fs . String Var ( & o . Cluster Signing Cert File , " " , o . Cluster Signing Cert fs . String Var ( & o . Cluster Signing Key File , " " , o . Cluster Signing Key fs . Duration Var ( & o . Cluster Signing Duration . Duration , " " , o . Cluster Signing } 
func ( o * CSR Signing Controller Options ) Apply To ( cfg * csrsigningconfig . CSR Signing Controller cfg . Cluster Signing Cert File = o . Cluster Signing Cert cfg . Cluster Signing Key File = o . Cluster Signing Key cfg . Cluster Signing Duration = o . Cluster Signing } 
func ( o * CSR Signing Controller } 
func ( f * Record Flags ) To Recorder ( ) ( Recorder , error ) { if f == nil { return Noop should if f . Record != nil { should // if flag was explicitly set to false by the user, // do not record if ! should Record { return Noop return & Change Cause Recorder { change Cause : f . change } 
func ( f * Record f . change Cause = parse Command } 
func ( f * Record Flags ) Add if f . Record != nil { cmd . Flags ( ) . Bool } 
func ( r Noop Recorder ) Make Record Merge } 
func ( r * Change Cause annotations := accessor . Get annotations [ Change Cause Annotation ] = r . change accessor . Set } 
func ( r * Change Cause Recorder ) Make Record Merge Patch ( obj runtime . Object ) ( [ ] byte , error ) { // copy so we don't mess with the original obj Copy := obj . Deep Copy if err := r . Record ( obj old new Data , err := json . Marshal ( obj return jsonpatch . Create Merge Patch ( old Data , new } 
func parse Command parse err = cmd . Flags ( ) . Parse All ( os . Args [ 1 : ] , parse } 
func New Ingress Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Ingress Informer ( client , namespace , resync } 
func New Filtered Ingress Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Networking } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Networking } , } , & networkingv1beta1 . Ingress { } , resync } 
func Is Delay Binding Mode ( claim * v1 . Persistent Volume Claim , class Lister storagelisters . Storage Class Lister ) ( bool , error ) { class Name := v1helper . Get Persistent Volume Claim if class class , err := class Lister . Get ( class if class . Volume Binding Mode == nil { return false , fmt . Errorf ( " " , class return * class . Volume Binding Mode == storage . Volume Binding Wait For First } 
func Get Bind Volume To Claim ( volume * v1 . Persistent Volume , claim * v1 . Persistent Volume Claim ) ( * v1 . Persistent // Check if the volume was already bound (either by user or by controller) should Set Bound By if ! Is Volume Bound To Claim ( volume , claim ) { should Set Bound By // The volume from method args can be pointing to watcher cache. We must not // modify these, therefore create a copy. volume Clone := volume . Deep // Bind the volume to the claim if it is not bound yet if volume . Spec . Claim Ref == nil || volume . Spec . Claim Ref . Name != claim . Name || volume . Spec . Claim Ref . Namespace != claim . Namespace || volume . Spec . Claim Ref . UID != claim . UID { claim Ref , err := reference . Get volume Clone . Spec . Claim Ref = claim // Set ann Bound By Controller if it is not set yet if should Set Bound By Controller && ! metav1 . Has Annotation ( volume Clone . Object Meta , ann Bound By Controller ) { metav1 . Set Meta Data Annotation ( & volume Clone . Object Meta , ann Bound By return volume } 
func Is Volume Bound To Claim ( volume * v1 . Persistent Volume , claim * v1 . Persistent Volume Claim ) bool { if volume . Spec . Claim if claim . Name != volume . Spec . Claim Ref . Name || claim . Namespace != volume . Spec . Claim if volume . Spec . Claim Ref . UID != " " && claim . UID != volume . Spec . Claim } 
func New Atomic Writer ( target Dir string , log Context string ) ( * Atomic Writer , error ) { _ , err := os . Stat ( target if os . Is Not return & Atomic Writer { target Dir : target Dir , log Context : log } 
func ( w * Atomic Writer ) Write ( payload map [ string ] File Projection ) error { // (1) clean Payload , err := validate if err != nil { klog . Errorf ( " " , w . log // (2) data Dir Path := filepath . Join ( w . target Dir , data Dir old Ts Dir , err := os . Readlink ( data Dir if err != nil { if ! os . Is Not Exist ( err ) { klog . Errorf ( " " , w . log // although Readlink() returns "" on err, don't be fragile by relying on it (since it's not specified in docs) // empty old Ts Dir indicates that it didn't exist old Ts old Ts Path := filepath . Join ( w . target Dir , old Ts var paths To // if there was no old version, there's nothing to remove if len ( old Ts Dir ) != 0 { // (3) paths To Remove , err = w . paths To Remove ( clean Payload , old Ts if err != nil { klog . Errorf ( " " , w . log // (4) if should , err := should Write Payload ( clean Payload , old Ts Path ) ; err != nil { klog . Errorf ( " " , w . log } else if ! should && len ( paths To Remove ) == 0 { klog . V ( 4 ) . Infof ( " " , w . log Context , w . target } else { klog . V ( 4 ) . Infof ( " " , w . log Context , w . target // (5) ts Dir , err := w . new Timestamp if err != nil { klog . V ( 4 ) . Infof ( " " , w . log ts Dir Name := filepath . Base ( ts // (6) if err = w . write Payload To Dir ( clean Payload , ts Dir ) ; err != nil { klog . Errorf ( " " , w . log Context , ts klog . V ( 4 ) . Infof ( " " , w . log Context , ts // (7) if err = w . create User Visible Files ( clean Payload ) ; err != nil { klog . Errorf ( " " , w . log Context , w . target // (8) new Data Dir Path := filepath . Join ( w . target Dir , new Data Dir if err = os . Symlink ( ts Dir Name , new Data Dir Path ) ; err != nil { os . Remove All ( ts klog . Errorf ( " " , w . log // (9) if runtime . GOOS == " " { os . Remove ( data Dir err = os . Symlink ( ts Dir Name , data Dir os . Remove ( new Data Dir } else { err = os . Rename ( new Data Dir Path , data Dir if err != nil { os . Remove ( new Data Dir os . Remove All ( ts klog . Errorf ( " " , w . log Context , new Data Dir // (10) if err = w . remove User Visible Paths ( paths To Remove ) ; err != nil { klog . Errorf ( " " , w . log // (11) if len ( old Ts Dir ) > 0 { if err = os . Remove All ( old Ts Path ) ; err != nil { klog . Errorf ( " " , w . log Context , old Ts } 
func validate Payload ( payload map [ string ] File Projection ) ( map [ string ] File Projection , error ) { clean Payload := make ( map [ string ] File for k , content := range payload { if err := validate clean return clean } 
func validate Path ( target Path string ) error { // TODO: somehow unify this with the similar api validation, // validate Volume Source Path; the error semantics are just different enough // from this that it was time-prohibitive trying to find the right // refactoring to re-use. if target Path == " " { return fmt . Errorf ( " " , target if path . Is Abs ( target Path ) { return fmt . Errorf ( " " , target if len ( target Path ) > max Path Length { return fmt . Errorf ( " " , max Path items := strings . Split ( target Path , string ( os . Path for _ , item := range items { if item == " " { return fmt . Errorf ( " " , target if len ( item ) > max File Name Length { return fmt . Errorf ( " " , max File Name if strings . Has Prefix ( items [ 0 ] , " " ) && len ( items [ 0 ] ) > 2 { return fmt . Errorf ( " " , target } 
func should Write Payload ( payload map [ string ] File Projection , old Ts Dir string ) ( bool , error ) { for user Visible Path , file Projection := range payload { should Write , err := should Write File ( filepath . Join ( old Ts Dir , user Visible Path ) , file if should } 
func should Write if os . Is Not content On Fs , err := ioutil . Read return ( bytes . Compare ( content , content On } 
func ( w * Atomic Writer ) paths To Remove ( payload map [ string ] File Projection , old Ts Dir string ) ( sets . String , error ) { paths := sets . New visitor := func ( path string , info os . File Info , err error ) error { relative Path := strings . Trim Prefix ( path , old Ts relative Path = strings . Trim Prefix ( relative Path , string ( os . Path if relative paths . Insert ( relative err := filepath . Walk ( old Ts if os . Is Not klog . V ( 5 ) . Infof ( " " , w . target new Paths := sets . New for file := range payload { // add all subpaths for the payload to the set of new paths // to avoid attempting to remove non-empty dirs for sub Path := file ; sub Path != " " ; { new Paths . Insert ( sub sub Path , _ = filepath . Split ( sub sub Path = strings . Trim Suffix ( sub Path , string ( os . Path klog . V ( 5 ) . Infof ( " " , w . target Dir , new result := paths . Difference ( new klog . V ( 5 ) . Infof ( " " , w . target } 
func ( w * Atomic Writer ) new Timestamp Dir ( ) ( string , error ) { ts Dir , err := ioutil . Temp Dir ( w . target if err != nil { klog . Errorf ( " " , w . log // 0755 permissions are needed to allow 'group' and 'other' to recurse the // directory tree. do a chmod here to ensure that permissions are set correctly // regardless of the process' umask. err = os . Chmod ( ts if err != nil { klog . Errorf ( " " , w . log return ts } 
func ( w * Atomic Writer ) write Payload To Dir ( payload map [ string ] File Projection , dir string ) error { for user Visible Path , file Projection := range payload { content := file mode := os . File Mode ( file full Path := filepath . Join ( dir , user Visible base Dir , _ := filepath . Split ( full err := os . Mkdir All ( base Dir , os . Mode if err != nil { klog . Errorf ( " " , w . log Context , base err = ioutil . Write File ( full if err != nil { klog . Errorf ( " " , w . log Context , full // Chmod is needed because ioutil.Write File() ends up calling // open(2) to create the file, so the final mode used is "mode & // ~umask". But we want to make sure the specified mode is used // in the file no matter what the umask is. err = os . Chmod ( full if err != nil { klog . Errorf ( " " , w . log Context , full } 
func ( w * Atomic Writer ) create User Visible Files ( payload map [ string ] File Projection ) error { for user Visible Path := range payload { slashpos := strings . Index ( user Visible Path , string ( os . Path if slashpos == - 1 { slashpos = len ( user Visible linkname := user Visible _ , err := os . Readlink ( filepath . Join ( w . target if err != nil && os . Is Not Exist ( err ) { // The link into the data directory for this path doesn't exist; create it visible File := filepath . Join ( w . target data Dir File := filepath . Join ( data Dir err = os . Symlink ( data Dir File , visible } 
func ( w * Atomic Writer ) remove User Visible Paths ( paths sets . String ) error { ps := string ( os . Path if err := os . Remove ( filepath . Join ( w . target Dir , p ) ) ; err != nil { klog . Errorf ( " " , w . log } 
func New Memory return & state Memory { assignments : Container CPU Assignments { } , default CPU Set : cpuset . New CPU } 
func ( c * Conflict Error ) Error ( ) string { return fmt . Sprintf ( " " , c . element . Get Recorded ( ) , c . element . Get } 
func ( c * Fake CSI Nodes ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . CSI Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( csinodes Resource , name ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func ( c * Fake CSI Nodes ) List ( opts v1 . List Options ) ( result * v1beta1 . CSI Node List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( csinodes Resource , csinodes Kind , opts ) , & v1beta1 . CSI Node label , _ , _ := testing . Extract From List list := & v1beta1 . CSI Node List { List Meta : obj . ( * v1beta1 . CSI Node List ) . List for _ , item := range obj . ( * v1beta1 . CSI Node } 
func ( c * Fake CSI Nodes ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( csinodes } 
func ( c * Fake CSI Nodes ) Create ( c SI Node * v1beta1 . CSI Node ) ( result * v1beta1 . CSI Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( csinodes Resource , c SI Node ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func ( c * Fake CSI Nodes ) Update ( c SI Node * v1beta1 . CSI Node ) ( result * v1beta1 . CSI Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( csinodes Resource , c SI Node ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func ( c * Fake CSI Nodes ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( csinodes Resource , name ) , & v1beta1 . CSI } 
func ( c * Fake CSI Nodes ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( csinodes Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . CSI Node } 
func ( c * Fake CSI Nodes ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . CSI Node , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( csinodes Resource , name , pt , data , subresources ... ) , & v1beta1 . CSI return obj . ( * v1beta1 . CSI } 
func record Operation ( operation string , start time . Time ) { metrics . Docker Operations . With Label metrics . Deprecated Docker Operations . With Label metrics . Docker Operations Latency . With Label Values ( operation ) . Observe ( metrics . Since In metrics . Deprecated Docker Operations Latency . With Label Values ( operation ) . Observe ( metrics . Since In } 
func record Error ( operation string , err error ) { if err != nil { if _ , ok := err . ( operation Timeout ) ; ok { metrics . Docker Operations Timeout . With Label metrics . Deprecated Docker Operations Timeout . With Label // Docker operation timeout error is also a docker error, so we don't add else here. metrics . Docker Operations Errors . With Label metrics . Deprecated Docker Operations Errors . With Label } 
func Cleanup Mount Point ( mount Path string , mounter Interface , extensive Mount Point Check bool ) error { // mounter.Exists Path cannot be used because for containerized kubelet, we need to check // the path in the kubelet container, not on the host. path Exists , path Err := Path Exists ( mount if ! path Exists { klog . Warningf ( " " , mount corrupted Mnt := Is Corrupted Mnt ( path if path Err != nil && ! corrupted Mnt { return fmt . Errorf ( " " , path return do Cleanup Mount Point ( mount Path , mounter , extensive Mount Point Check , corrupted } 
func do Cleanup Mount Point ( mount Path string , mounter Interface , extensive Mount Point Check bool , corrupted Mnt bool ) error { if ! corrupted Mnt { var not if extensive Mount Point Check { not Mnt , err = Is Not Mount Point ( mounter , mount } else { not Mnt , err = mounter . Is Likely Not Mount Point ( mount if not Mnt { klog . Warningf ( " " , mount return os . Remove ( mount // Unmount the mount path klog . V ( 4 ) . Infof ( " " , mount if err := mounter . Unmount ( mount not Mnt , mnt Err := mounter . Is Likely Not Mount Point ( mount if mnt Err != nil { return mnt if not Mnt { klog . V ( 4 ) . Infof ( " " , mount return os . Remove ( mount return fmt . Errorf ( " " , mount } 
func Until Without Retry ( ctx context . Context , watcher watch . Interface , conditions ... Condition Func ) ( * watch . Event , error ) { ch := watcher . Result var last for _ , condition := range conditions { // check the next condition against the previous event and short circuit waiting for the next watch if last Event != nil { done , err := condition ( * last if err != nil { return last Condition Succeeded : for { select { case event , ok := <- ch : if ! ok { return last Event , Err Watch last if err != nil { return last if done { break Condition case <- ctx . Done ( ) : return last Event , wait . Err Wait return last } 
func Until ( ctx context . Context , initial Resource Version string , watcher Client cache . Watcher , conditions ... Condition Func ) ( * watch . Event , error ) { w , err := New Retry Watcher ( initial Resource Version , watcher return Until Without } 
func Until With Sync ( ctx context . Context , lw cache . Lister Watcher , obj Type runtime . Object , precondition Precondition Func , conditions ... Condition Func ) ( * watch . Event , error ) { indexer , informer , watcher , done := New Indexer Informer Watcher ( lw , obj // Proxy watcher can be stopped multiple times so it's fine to use defer here to cover alternative branches and // let Until Without if precondition != nil { if ! cache . Wait For Cache Sync ( ctx . Done ( ) , informer . Has return Until Without } 
func Context With Optional Timeout ( parent context . Context , timeout time . Duration ) ( context . Context , context . Cancel if timeout == 0 { return context . With return context . With } 
func List Watch Until ( ctx context . Context , lw cache . Lister Watcher , conditions ... Condition list , err := lw . List ( metav1 . List initial Items , err := meta . Extract // use the initial items as simulated "adds" var last curr passed for _ , condition := range conditions { // check the next condition against the previous event and short circuit waiting for the next watch if last Event != nil { done , err := condition ( * last if err != nil { return last if done { passed Conditions = passed Condition Succeeded : for curr Index < len ( initial Items ) { last Event = & watch . Event { Type : watch . Added , Object : initial Items [ curr curr done , err := condition ( * last if err != nil { return last if done { passed Conditions = passed break Condition if passed Conditions == len ( conditions ) { return last remaining Conditions := conditions [ passed meta Obj , err := meta . List curr Resource Version := meta Obj . Get Resource return Until ( ctx , curr Resource Version , lw , remaining } 
func Error To API Status ( err error ) * metav1 . Status { switch t := err . ( type ) { case status if len ( status . Status ) == 0 { status . Status = metav1 . Status switch status . Status { case metav1 . Status Success : if status . Code == 0 { status . Code = http . Status case metav1 . Status Failure : if status . Code == 0 { status . Code = http . Status Internal Server default : runtime . Handle if status . Code == 0 { status . Code = http . Status Internal Server status . API default : status := http . Status Internal Server switch { //TODO: replace me with New Conflict Err case storage . Is Conflict ( err ) : status = http . Status // Log errors that were not converted to an error status // by REST storage - these typically indicate programmer // error by not using pkg/api/errors, or unexpected failure // cases. runtime . Handle return & metav1 . Status { Type Meta : metav1 . Type Meta { Kind : " " , API Version : " " , } , Status : metav1 . Status Failure , Code : int32 ( status ) , Reason : metav1 . Status Reason } 
func Add Custom Global Flags ( fs * pflag . Flag } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . Cron Job { } , func ( obj interface { } ) { Set Object Defaults_Cron Job ( obj . ( * v1beta1 . Cron scheme . Add Type Defaulting Func ( & v1beta1 . Cron Job List { } , func ( obj interface { } ) { Set Object Defaults_Cron Job List ( obj . ( * v1beta1 . Cron Job scheme . Add Type Defaulting Func ( & v1beta1 . Job Template { } , func ( obj interface { } ) { Set Object Defaults_Job Template ( obj . ( * v1beta1 . Job } 
func ( a * azure Disk Attacher ) Attach ( spec * volume . Spec , node Name types . Node Name ) ( string , error ) { volume Source , _ , err := get Volume disk Controller , err := get Disk lun , err := disk Controller . Get Disk Lun ( volume Source . Disk Name , volume Source . Data Disk URI , node if err == cloudprovider . Instance Not Found { // Log error and continue with attach klog . Warningf ( " " , node if err == nil { // Volume is already attached to node. klog . V ( 2 ) . Infof ( " " , volume Source . Disk Name , node } else { klog . V ( 2 ) . Infof ( " " , err , volume Source . Data Disk URI , node is Managed Disk := ( * volume Source . Kind == v1 . Azure Managed err = disk Controller . Attach Disk ( is Managed Disk , volume Source . Disk Name , volume Source . Data Disk URI , node Name , compute . Caching Types ( * volume Source . Caching if err == nil { klog . V ( 2 ) . Infof ( " " , volume Source . Data Disk URI , node } else { klog . V ( 2 ) . Infof ( " " , volume Source . Data Disk URI , node return " " , fmt . Errorf ( " " , volume Source . Disk Name , node } 
func ( a * azure Disk Attacher ) Get Device Mount Path ( spec * volume . Spec ) ( string , error ) { volume Source , _ , err := get Volume if volume Source . Kind == nil { // this spec was constructed from info on the node pd Path := filepath . Join ( a . plugin . host . Get Plugin Dir ( azure Data Disk Plugin Name ) , util . Mounts In Global PD Path , volume Source . Data Disk return pd is Managed Disk := ( * volume Source . Kind == v1 . Azure Managed return make Global PD Path ( a . plugin . host , volume Source . Data Disk URI , is Managed } 
func ( d * azure Disk Detacher ) Detach ( disk URI string , node Name types . Node Name ) error { if disk URI == " " { return fmt . Errorf ( " " , disk disk Controller , err := get Disk err = disk Controller . Detach Disk ( " " , disk URI , node if err != nil { klog . Errorf ( " " , disk klog . V ( 2 ) . Infof ( " " , disk URI , node } 
func ( detacher * azure Disk Detacher ) Unmount Device ( device Mount Path string ) error { err := mount . Cleanup Mount Point ( device Mount Path , detacher . plugin . host . Get Mounter ( detacher . plugin . Get Plugin if err == nil { klog . V ( 2 ) . Infof ( " " , device Mount } else { klog . Warningf ( " " , device Mount } 
func New } 
func Int32Key Set ( the Map interface { } ) Int32 { v := reflect . Value Of ( the for _ , key Value := range v . Map Keys ( ) { ret . Insert ( key } 
} 
} 
} 
func ( s Int32 ) Has } 
func ( s Int32 ) Difference ( s2 Int32 ) Int32 { result := New } 
func ( s1 Int32 ) Union ( s2 Int32 ) Int32 { result := New } 
result := New } 
func ( s1 Int32 ) Equal ( s2 Int32 ) bool { return len ( s1 ) == len ( s2 ) && s1 . Is } 
func ( s Int32 ) List ( ) [ ] int32 { res := make ( sortable Slice Of } 
func ( s Int32 ) Unsorted } 
func ( s Int32 ) Pop var zero return zero } 
func New Schema Validator ( custom Resource Validation * apiextensions . Custom Resource Validation ) ( * validate . Schema Validator , * spec . Schema , error ) { // Convert CRD schema to openapi schema openapi if custom Resource Validation != nil { if err := Convert JSON Schema Props ( custom Resource Validation . Open APIV3Schema , openapi return validate . New Schema Validator ( openapi Schema , nil , " " , strfmt . Default ) , openapi } 
func Validate Custom Resource ( custom Resource interface { } , validator * validate . Schema result := validator . Validate ( custom if result . As Error ( ) != nil { return result . As } 
func Convert JSON Schema Props ( in * apiextensions . JSON Schema Props , out * spec . Schema ) error { return Convert JSON Schema Props With Post } 
func Convert JSON Schema Props With Post Process ( in * apiextensions . JSON Schema Props , out * spec . Schema , post Process Post Process out . Schema = spec . Schema if in . Type != " " { out . Type = spec . String Or out . Exclusive Maximum = in . Exclusive out . Exclusive Minimum = in . Exclusive out . Max Length = in . Max out . Min Length = in . Min out . Max Items = in . Max out . Min Items = in . Min out . Unique Items = in . Unique out . Multiple Of = in . Multiple out . Max Properties = in . Max out . Min Properties = in . Min if err := convert Slice Of JSON Schema Props ( & in . All Of , & out . All Of , post if err := convert Slice Of JSON Schema Props ( & in . One Of , & out . One Of , post if err := convert Slice Of JSON Schema Props ( & in . Any Of , & out . Any Of , post if err := Convert JSON Schema Props With Post Process ( * in , * out , post out . Properties , err = convert Map Of JSON Schema Props ( in . Properties , post out . Pattern Properties , err = convert Map Of JSON Schema Props ( in . Pattern Properties , post out . Definitions , err = convert Map Of JSON Schema Props ( in . Definitions , post if in . Ref != nil { out . Ref , err = spec . New if in . Additional Properties != nil { in , out := & in . Additional Properties , & out . Additional * out = new ( spec . Schema Or if err := convert JSON Schema Propsor Bool ( * in , * out , post if in . Additional Items != nil { in , out := & in . Additional Items , & out . Additional * out = new ( spec . Schema Or if err := convert JSON Schema Propsor Bool ( * in , * out , post * out = new ( spec . Schema Or if err := convert JSON Schema Props Or Array ( * in , * out , post for key , val := range * in { new Val := new ( spec . Schema Or String if err := convert JSON Schema Props Or String Array ( & val , new Val , post ( * out ) [ key ] = * new if in . External Docs != nil { out . External Docs = & spec . External out . External Docs . Description = in . External out . External Docs . URL = in . External if post Process != nil { if err := post } 
func ( mounter * Mounter ) Mount ( source string , target string , fstype string , options [ ] string ) error { target = normalize Windows return os . Mkdir parent if err := os . Mkdir All ( parent bind // tell it's going to mount azure disk or azure file according to options if bind , _ , _ := Is Bind ( options ) ; bind { // mount azure disk bind Source = normalize Windows // currently only cifs mount is supported if strings . To // lock smb mount for the same source get SMB Mount Mutex . Lock defer get SMB Mount Mutex . Unlock if output , err := new SMB Mapping ( options [ 0 ] , options [ 1 ] , source ) ; err != nil { if is SMB Mapping if output , err := remove SMB if output , err := new SMB if output , err := exec . Command ( " " , " " , " " , " " , target , bind Source ) . Combined Output ( ) ; err != nil { klog . Errorf ( " " , err , bind } 
func new SMB // use Power Shell Environment Variables to store user input string to prevent command line injection // https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_environment_variables?view=powershell-5.1 cmd Line := `$P Word = Convert To-Secure String -String $Env:smbpassword -As Plain Text -Force` + `;$Credential = New-Object -Type Name System.Management.Automation.PS Credential -Argument List $Env:smbuser, $P Word` + `;New-Smb Global Mapping -Remote cmd := exec . Command ( " " , " " , cmd output , err := cmd . Combined } 
func is SMB Mapping Exist ( remotepath string ) bool { cmd := exec . Command ( " " , " " , `Get-Smb Global Mapping -Remote _ , err := cmd . Combined } 
func remove SMB Mapping ( remotepath string ) ( string , error ) { cmd := exec . Command ( " " , " " , `Remove-Smb Global Mapping -Remote output , err := cmd . Combined } 
func ( mounter * Mounter ) Is Mount Point Match ( mp Mount } 
func ( mounter * Mounter ) Is Likely Not Mount // If current file is a symlink, then it is a mountpoint. if stat . Mode ( ) & os . Mode exists , err := mounter . Exists } 
func ( mounter * Mounter ) Get Device Name From Mount ( mount Path , plugin Mount Dir string ) ( string , error ) { return get Device Name From Mount ( mounter , mount Path , plugin Mount } 
func ( mounter * Mounter ) Make Dir ( pathname string ) error { err := os . Mkdir All ( pathname , os . File if err != nil { if ! os . Is } 
func ( mounter * Mounter ) Make File ( pathname string ) error { f , err := os . Open File ( pathname , os . O_CREATE , os . File if err != nil { if ! os . Is } 
func ( mounter * Mounter ) Exists Path ( pathname string ) ( bool , error ) { return utilpath . Exists ( utilpath . Check Follow } 
func ( mounter * Mounter ) Eval Host Symlinks ( pathname string ) ( string , error ) { return filepath . Eval } 
func Validate Disk Number ( disk string ) error { disk if disk Num < 0 || disk } 
func get Drive Letter By Disk Number ( disk Num string , exec Exec ) ( string , error ) { cmd := fmt . Sprintf ( " " , disk } 
func get All Parent Links ( path string ) ( [ ] string , error ) { const max if len ( links ) > max if fi . Mode ( ) & os . Mode } 
func ( mounter * Mounter ) Get Mount Refs ( pathname string ) ( [ ] string , error ) { windows Path := normalize Windows path Exists , path Err := Path Exists ( windows if ! path } else if Is Corrupted Mnt ( path Err ) { klog . Warningf ( " " , windows } else if path Err != nil { return nil , fmt . Errorf ( " " , windows Path , path } 
func ( c * Settings V1alpha1Client ) REST return c . rest } 
func new Internal ( exec utilexec . Interface , dbus utildbus . Interface , protocol Protocol , lockfile Path string ) Interface { vstring , err := get IP Tables Version if err != nil { klog . Warningf ( " " , Min Check vstring = Min Check if lockfile Path == " " { lockfile Path = Lockfile runner := & runner { exec : exec , dbus : dbus , protocol : protocol , has Check : get IP Tables Has Check Command ( vstring ) , has Listener : false , wait Flag : get IP Tables Wait Flag ( vstring ) , restore Wait Flag : get IP Tables Restore Wait Flag ( exec , protocol ) , lockfile Path : lockfile } 
func New ( exec utilexec . Interface , dbus utildbus . Interface , protocol Protocol ) Interface { return new } 
func ( runner * runner ) connect To Firewall D ( ) { bus , err := runner . dbus . System runner . has rule := fmt . Sprintf ( " " , firewalld Name , firewalld Path , firewalld bus . Bus rule = fmt . Sprintf ( " " , firewalld bus . Bus go runner . dbus Signal } 
func ( runner * runner ) Get Version ( ) ( string , error ) { return get IP Tables Version } 
func ( runner * runner ) Ensure Chain ( table Table , chain Chain ) ( bool , error ) { full Args := make Full out , err := runner . run ( op Create Chain , full if err != nil { if ee , ok := err . ( utilexec . Exit Error ) ; ok { if ee . Exited ( ) && ee . Exit } 
func ( runner * runner ) Flush Chain ( table Table , chain Chain ) error { full Args := make Full out , err := runner . run ( op Flush Chain , full } 
func ( runner * runner ) Ensure Rule ( position Rule Position , table Table , chain Chain , args ... string ) ( bool , error ) { full Args := make Full exists , err := runner . check out , err := runner . run ( operation ( position ) , full } 
func ( runner * runner ) Delete Rule ( table Table , chain Chain , args ... string ) error { full Args := make Full exists , err := runner . check out , err := runner . run ( op Delete Rule , full } 
func ( runner * runner ) Save defer trace . Log If // run and return iptables Save Cmd := iptables Save klog . V ( 4 ) . Infof ( " " , iptables Save cmd := runner . exec . Command ( iptables Save // Since Combined Output() doesn't support redirecting it to a buffer, // we need to workaround it by redirecting stdout and stderr to buffer // and explicitly calling Run() [Combined Output() underneath itself // creates a new buffer, redirects stdout and stderr to it and also // calls Run()]. cmd . Set cmd . Set } 
func ( runner * runner ) Restore ( table Table , data [ ] byte , flush Flush Flag , counters Restore Counters return runner . restore } 
func ( runner * runner ) Restore All ( data [ ] byte , flush Flush Flag , counters Restore Counters return runner . restore } 
func ( runner * runner ) restore Internal ( args [ ] string , data [ ] byte , flush Flush Flag , counters Restore Counters defer trace . Log If // Grab the iptables lock to prevent iptables-restore and iptables // from stepping on each other. iptables-restore 1.6.2 will have // a --wait option like iptables itself, but that's not widely deployed. if len ( runner . restore Wait Flag ) == 0 { locker , err := grab Iptables Locks ( runner . lockfile defer func ( locker iptables // run the command and return the output or an error including the output and error full Args := append ( runner . restore Wait iptables Restore Cmd := iptables Restore klog . V ( 4 ) . Infof ( " " , iptables Restore Cmd , full cmd := runner . exec . Command ( iptables Restore Cmd , full cmd . Set Stdin ( bytes . New b , err := cmd . Combined } 
func ( runner * runner ) check Rule Without Check ( table Table , chain Chain , args ... string ) ( bool , error ) { iptables Save Cmd := iptables Save klog . V ( 1 ) . Infof ( " " , iptables Save out , err := runner . exec . Command ( iptables Save Cmd , " " , string ( table ) ) . Combined // Sadly, iptables has inconsistent quoting rules for comments. Just remove all quotes. // Also, quoted multi-word comments (which are counted as a single arg) // will be unpacked into multiple args, // in order to compare against iptables-save output (which will be split at whitespace boundary) // e.g. a single arg('"this must be before the Node Port rules"') will be unquoted and unpacked into 7 args. var args for i := range args { tmp tmp Field = trimhex ( tmp args Copy = append ( args Copy , strings . Fields ( tmp argset := sets . New String ( args // Check that this is a rule for the correct chain, and that it has // the correct number of argument (+2 for "-A <chain name>") if ! strings . Has Prefix ( line , fmt . Sprintf ( " " , string ( chain ) ) ) || len ( fields ) != len ( args // TODO: This misses reorderings e.g. "-x foo ! -y bar" will match "! -x foo -y bar" if sets . New String ( fields ... ) . Is } 
func ( runner * runner ) check Rule Using Check ( args [ ] string ) ( bool , error ) { ctx , cancel := context . With out , err := runner . run Context ( ctx , op Check if ctx . Err ( ) == context . Deadline if ee , ok := err . ( utilexec . Exit Error ) ; ok { // iptables uses exit(1) to indicate a failure of the operation, // as compared to a malformed commandline, for example. if ee . Exited ( ) && ee . Exit } 
func get IP Tables Has Check Command ( vstring string ) bool { min Version , err := utilversion . Parse Generic ( Min Check if err != nil { klog . Errorf ( " " , Min Check version , err := utilversion . Parse return version . At Least ( min } 
func get IP Tables Wait Flag ( vstring string ) [ ] string { version , err := utilversion . Parse min Version , err := utilversion . Parse Generic ( Wait Min if err != nil { klog . Errorf ( " " , Wait Min if version . Less Than ( min min Version , err = utilversion . Parse Generic ( Wait Seconds Min if err != nil { klog . Errorf ( " " , Wait Seconds Min if version . Less Than ( min Version ) { return [ ] string { Wait return [ ] string { Wait String , Wait Seconds } 
func get IP Tables Version String ( exec utilexec . Interface , protocol Protocol ) ( string , error ) { // this doesn't access mutable state so we don't need to use the interface / runner iptables Cmd := iptables bytes , err := exec . Command ( iptables Cmd , " " ) . Combined version Matcher := regexp . Must match := version Matcher . Find String } 
func get IP Tables Restore Wait Flag ( exec utilexec . Interface , protocol Protocol ) [ ] string { vstring , err := get IP Tables Restore Version if _ , err := utilversion . Parse return [ ] string { Wait String , Wait Seconds } 
func get IP Tables Restore Version String ( exec utilexec . Interface , protocol Protocol ) ( string , error ) { // this doesn't access mutable state so we don't need to use the interface / runner // iptables-restore hasn't always had --version, and worse complains // about unrecognized commands but doesn't exit when it gets them. // Work around that by setting stdin to nothing so it exits immediately. iptables Restore Cmd := iptables Restore cmd := exec . Command ( iptables Restore cmd . Set Stdin ( bytes . New bytes , err := cmd . Combined version Matcher := regexp . Must match := version Matcher . Find String } 
func ( runner * runner ) dbus Signal Handler ( bus utildbus . Connection ) { firewalld := bus . Object ( firewalld Name , firewalld new if name != firewalld Name || len ( new // Firewall D startup (specifically the part where it deletes // all existing iptables rules) may not yet be complete when // we get this signal, so make a dummy request to it to // synchronize. firewalld . Call ( firewalld case firewalld } 
func ( runner * runner ) Add Reload Func ( reload // We only need to listen to firewalld if there are Reload functions, so lazy // initialize the listener. if ! runner . has Listener { runner . connect To Firewall runner . reload Funcs = append ( runner . reload Funcs , reload } 
for _ , f := range runner . reload } 
func Is Not Found for _ , str := range iptables Not Found } 
func Validate Conditional Service ( service , old Service * api . Service ) field . Error List { var errs field . Error // If the SCTP Support feature is disabled, and the old object isn't using the SCTP feature, prevent the new object from using it if ! utilfeature . Default Feature Gate . Enabled ( features . SCTP Support ) && len ( service SCTP Fields ( old Service ) ) == 0 { for _ , f := range service SCTP Fields ( service ) { errs = append ( errs , field . Not Supported ( f , api . Protocol SCTP , [ ] string { string ( api . Protocol TCP ) , string ( api . Protocol } 
func Validate Conditional Endpoints ( endpoints , old Endpoints * api . Endpoints ) field . Error List { var errs field . Error // If the SCTP Support feature is disabled, and the old object isn't using the SCTP feature, prevent the new object from using it if ! utilfeature . Default Feature Gate . Enabled ( features . SCTP Support ) && len ( endpoints SCTP Fields ( old Endpoints ) ) == 0 { for _ , f := range endpoints SCTP Fields ( endpoints ) { errs = append ( errs , field . Not Supported ( f , api . Protocol SCTP , [ ] string { string ( api . Protocol TCP ) , string ( api . Protocol } 
func Validate Conditional Pod Template ( pod Template , old Pod Template * api . Pod Template Spec , fld Path * field . Path ) field . Error List { var ( pod Spec * api . Pod old Pod Spec * api . Pod if pod Template != nil { pod Spec = & pod if old Pod Template != nil { old Pod Spec = & old Pod return validate Conditional Pod Spec ( pod Spec , old Pod Spec , fld } 
func Validate Conditional Pod ( pod , old Pod * api . Pod , fld Path * field . Path ) field . Error List { var ( pod Spec * api . Pod old Pod Spec * api . Pod if pod != nil { pod if old Pod != nil { old Pod Spec = & old return validate Conditional Pod Spec ( pod Spec , old Pod Spec , fld } 
func ( f Debug Flags ) Install ( c * mux . Path Recorder Mux , flag string , handler func ( http . Response Writer , * http . Request ) ) { c . Unlisted Handle ( " " , http . Handler c . Unlisted Handle Prefix ( " " , http . Handler c . Unlisted Handle f . add } 
func ( f Debug Flags ) Index ( w http . Response Writer , r * http . Request ) { lock . R defer lock . R if err := index Tmpl . Execute ( w , registered } 
func String Flag Put Handler ( setter String Flag Setter Func ) http . Handler Func { return http . Handler Func ( func ( w http . Response Writer , req * http . Request ) { switch { case req . Method == " " : body , err := ioutil . Read if err != nil { write Plain Text ( http . Status Bad if err != nil { write Plain Text ( http . Status Bad write Plain Text ( http . Status default : write Plain Text ( http . Status Not } 
func write Plain Text ( status Code int , text string , w http . Response w . Write Header ( status } 
func ( c * Scheduling V1Client ) REST return c . rest } 
func ( v * version ) Certificate Signing Requests ( ) Certificate Signing Request Informer { return & certificate Signing Request Informer { factory : v . factory , tweak List Options : v . tweak List } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1alpha1 . Kube Scheduler Configuration { } , func ( obj interface { } ) { Set Object Defaults_Kube Scheduler Configuration ( obj . ( * v1alpha1 . Kube Scheduler } 
func New Caching Config Map Manager ( kube Client clientset . Interface , get TTL manager . Get Object TTL Func ) Manager { get Config Map := func ( namespace , name string , opts metav1 . Get Options ) ( runtime . Object , error ) { return kube Client . Core V1 ( ) . Config config Map Store := manager . New Object Store ( get Config Map , clock . Real Clock { } , get TTL , default return & config Map Manager { manager : manager . New Cache Based Manager ( config Map Store , get Config Map } 
func New Watching Config Map Manager ( kube Client clientset . Interface ) Manager { list Config Map := func ( namespace string , opts metav1 . List Options ) ( runtime . Object , error ) { return kube Client . Core V1 ( ) . Config watch Config Map := func ( namespace string , opts metav1 . List Options ) ( watch . Interface , error ) { return kube Client . Core V1 ( ) . Config new Config Map := func ( ) runtime . Object { return & v1 . Config return & config Map Manager { manager : manager . New Watch Based Manager ( list Config Map , watch Config Map , new Config Map , gr , get Config Map } 
func New Publisher ( cm Informer coreinformers . Config Map Informer , ns Informer coreinformers . Namespace Informer , cl clientset . Interface , root CA [ ] byte ) ( * Publisher , error ) { e := & Publisher { client : cl , root CA : root CA , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate if cl . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { if err := metrics . Register Metric And Track Rate Limiter Usage ( " " , cl . Core V1 ( ) . REST Client ( ) . Get Rate cm Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Delete Func : e . config Map Deleted , Update Func : e . config Map e . cm Lister = cm e . cm Lister Synced = cm Informer . Informer ( ) . Has ns Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : e . namespace Added , Update Func : e . namespace e . ns Lister Synced = ns Informer . Informer ( ) . Has e . sync Handler = e . sync } 
func add Conversion Funcs ( scheme * runtime . Scheme ) error { err := scheme . Add Conversion Funcs ( Convert_scheme_Scale Status_To_v1beta2_Scale Status , Convert_v1beta2_Scale Status_To_scheme_Scale } 
func New For Config ( c * rest . Config ) ( * Clientset , error ) { config Shallow if config Shallow Copy . Rate Limiter == nil && config Shallow Copy . QPS > 0 { config Shallow Copy . Rate Limiter = flowcontrol . New Token Bucket Rate Limiter ( config Shallow Copy . QPS , config Shallow cs . wardle V1alpha1 , err = wardlev1alpha1 . New For Config ( & config Shallow cs . wardle V1beta1 , err = wardlev1beta1 . New For Config ( & config Shallow cs . Discovery Client , err = discovery . New Discovery Client For Config ( & config Shallow } 
func New For Config Or cs . wardle V1alpha1 = wardlev1alpha1 . New For Config Or cs . wardle V1beta1 = wardlev1beta1 . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . wardle cs . wardle cs . Discovery Client = discovery . New Discovery } 
func Terminal Size ( w io . Writer ) ( int , int , error ) { out Fd , is Terminal := term . Get Fd if ! is winsize , err := term . Get Winsize ( out } 
func ( s * int } 
func ( s * int Set ) mark ( i int ) { s . members [ i ] = s . current } 
func ( s * int Set ) sweep ( ) { for k , v := range s . members { if v != s . current } 
func new Controller Revisions ( c * Apps V1Client , namespace string ) * controller Revisions { return & controller Revisions { client : c . REST } 
func ( c * controller Revisions ) Update ( controller Revision * v1 . Controller Revision ) ( result * v1 . Controller Revision , err error ) { result = & v1 . Controller err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( controller Revision . Name ) . Body ( controller } 
func ( in * External Metric Status ) Deep Copy Into ( out * External Metric if in . Metric Selector != nil { in , out := & in . Metric Selector , & out . Metric * out = new ( v1 . Label ( * in ) . Deep Copy out . Current Value = in . Current Value . Deep if in . Current Average Value != nil { in , out := & in . Current Average Value , & out . Current Average x := ( * in ) . Deep } 
func ( in * Horizontal Pod Autoscaler Spec ) Deep Copy Into ( out * Horizontal Pod Autoscaler out . Scale Target Ref = in . Scale Target if in . Min Replicas != nil { in , out := & in . Min Replicas , & out . Min * out = make ( [ ] Metric for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Horizontal Pod Autoscaler Status ) Deep Copy Into ( out * Horizontal Pod Autoscaler if in . Observed Generation != nil { in , out := & in . Observed Generation , & out . Observed if in . Last Scale Time != nil { in , out := & in . Last Scale Time , & out . Last Scale * out = ( * in ) . Deep if in . Current Metrics != nil { in , out := & in . Current Metrics , & out . Current * out = make ( [ ] Metric for i := range * in { ( * in ) [ i ] . Deep Copy * out = make ( [ ] Horizontal Pod Autoscaler for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Object Metric Source ) Deep Copy Into ( out * Object Metric out . Target Value = in . Target Value . Deep * out = new ( v1 . Label ( * in ) . Deep Copy if in . Average Value != nil { in , out := & in . Average Value , & out . Average x := ( * in ) . Deep } 
func ( in * Object Metric Status ) Deep Copy Into ( out * Object Metric out . Current Value = in . Current Value . Deep * out = new ( v1 . Label ( * in ) . Deep Copy if in . Average Value != nil { in , out := & in . Average Value , & out . Average x := ( * in ) . Deep } 
func ( in * Pods Metric Source ) Deep Copy Into ( out * Pods Metric out . Target Average Value = in . Target Average Value . Deep * out = new ( v1 . Label ( * in ) . Deep Copy } 
func ( in * Pods Metric Status ) Deep Copy Into ( out * Pods Metric out . Current Average Value = in . Current Average Value . Deep * out = new ( v1 . Label ( * in ) . Deep Copy } 
func ( resourcequota Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { resourcequota := obj . ( * api . Resource resourcequota . Status = api . Resource Quota resourcequotautil . Drop Disabled } 
func ( resourcequota Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Resourcequota := obj . ( * api . Resource old Resourcequota := old . ( * api . Resource new Resourcequota . Status = old resourcequotautil . Drop Disabled Fields ( & new Resourcequota . Spec , & old } 
func ( resourcequota Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { resourcequota := obj . ( * api . Resource return validation . Validate Resource } 
func ( resourcequota Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { error List := validation . Validate Resource Quota ( obj . ( * api . Resource return append ( error List , validation . Validate Resource Quota Update ( obj . ( * api . Resource Quota ) , old . ( * api . Resource } 
func ( in * Raw Extension ) Deep Copy Into ( out * Raw if in . Object != nil { out . Object = in . Object . Deep Copy } 
func ( in * Raw Extension ) Deep Copy ( ) * Raw out := new ( Raw in . Deep Copy } 
func ( in * Unknown ) Deep Copy out . Type Meta = in . Type } 
func ( in * Unknown ) Deep in . Deep Copy } 
func ( in * Unknown ) Deep Copy Object ( ) Object { if c := in . Deep } 
func ( in * Versioned Objects ) Deep Copy Into ( out * Versioned for i := range * in { if ( * in ) [ i ] != nil { ( * out ) [ i ] = ( * in ) [ i ] . Deep Copy } 
func ( in * Versioned Objects ) Deep Copy ( ) * Versioned out := new ( Versioned in . Deep Copy } 
func ( in * Versioned Objects ) Deep Copy Object ( ) Object { if c := in . Deep } 
func ( endpoints Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { all Errs := validation . Validate all Errs = append ( all Errs , validation . Validate Conditional return all } 
func ( endpoints endpoints . Subsets = endptspkg . Repack } 
func ( endpoints Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { error List := validation . Validate error List = append ( error List , validation . Validate Endpoints error List = append ( error List , validation . Validate Conditional return error } 
func ( always Admit ) Validate ( a admission . Attributes , o admission . Object } 
func With Max In Flight Limit ( handler http . Handler , non Mutating Limit int , mutating Limit int , long Running Request Check apirequest . Long Running Request Check , ) http . Handler { start Once . Do ( start Recording if non Mutating Limit == 0 && mutating var non Mutating var mutating if non Mutating Limit != 0 { non Mutating Chan = make ( chan bool , non Mutating if mutating Limit != 0 { mutating Chan = make ( chan bool , mutating return http . Handler Func ( func ( w http . Response request Info , ok := apirequest . Request Info if ! ok { handle // Skip tracking long running events. if long Running Request Check != nil && long Running Request Check ( r , request Info ) { handler . Serve is Mutating Request := ! non Mutating Request Verbs . Has ( request if is Mutating Request { c = mutating } else { c = non Mutating if c == nil { handler . Serve } else { select { case c <- true : var mutating Len , read Only if is Mutating Request { mutating Len = len ( mutating } else { read Only Len = len ( non Mutating if is Mutating Request { watermark . record Mutating ( mutating } else { watermark . record Read Only ( read Only handler . Serve default : // We need to split this data between buckets used for throttling. if is Mutating Request { metrics . Dropped Requests . With Label Values ( metrics . Mutating metrics . Deprecated Dropped Requests . With Label Values ( metrics . Mutating } else { metrics . Dropped Requests . With Label Values ( metrics . Read Only metrics . Deprecated Dropped Requests . With Label Values ( metrics . Read Only // at this point we're about to return a 429, BUT not all actors should be rate limited. A system:master is so powerful // that they should always get an answer. It's a super-admin or a loopback connection. if curr User , ok := apirequest . User From ( ctx ) ; ok { for _ , group := range curr User . Get Groups ( ) { if group == user . System Privileged Group { handler . Serve metrics . Record ( r , request Info , metrics . API Server Component , " " , http . Status Too Many too Many } 
func has Object } 
func ( g * gen Fake For Type ) Generate Type ( c * generator . Context , t * types . Type , w io . Writer ) error { sw := generator . New Snippet tags , err := util . Parse Client Gen Tags ( append ( t . Second Closest Comment Lines , t . Comment canonical if canonical Group == " " { canonical group if g . group == " " { group // allow user to define a group name that's different from the one parsed from the directory. p := c . Universe . Package ( path . Vendorless ( g . input if override := types . Extract Comment Tags ( " " , p . Comments ) [ " " ] ; override != nil { group const pkg Client Go m := map [ string ] interface { } { " " : t , " " : t , " " : t , " " : " " , " " : pkg , " " : namer . IC ( pkg ) , " " : ! tags . Non Namespaced , " " : namer . IC ( g . group ) , " " : g . group Go Name , " " : namer . IC ( g . version ) , " " : canonical Group , " " : group Name , " " : g . version , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Type ( types . Name { Package : " " , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go Testing , Name : " " } ) , " " : c . Universe . Function ( types . Name { Package : pkg Client Go if tags . Non Namespaced { sw . Do ( struct Non } else { sw . Do ( struct if tags . No if tags . Has Verb ( " " ) { sw . Do ( get if tags . Has Verb ( " " ) { if has Object Meta ( t ) { sw . Do ( list Using Options } else { sw . Do ( list if tags . Has Verb ( " " ) { sw . Do ( watch if tags . Has Verb ( " " ) { sw . Do ( create if tags . Has Verb ( " " ) { sw . Do ( update if tags . Has Verb ( " " ) && gen Status ( t ) { sw . Do ( update Status if tags . Has Verb ( " " ) { sw . Do ( delete if tags . Has Verb ( " " ) { sw . Do ( delete Collection if tags . Has Verb ( " " ) { sw . Do ( patch // generate extended client methods for _ , e := range tags . Extensions { input result if len ( e . Input Type Override ) > 0 { if name , pkg := e . Input ( ) ; len ( pkg ) > 0 { new input Type = * new } else { input Type . Name . Name = e . Input Type if len ( e . Result Type Override ) > 0 { if name , pkg := e . Result ( ) ; len ( pkg ) > 0 { new result Type = * new } else { result Type . Name . Name = e . Result Type m [ " " ] = & input m [ " " ] = & result m [ " " ] = e . Sub Resource if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , get Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , get if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , list // TODO: Figure out schemantic for watching a sub-resource. if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , watch if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , create Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , create if e . Has Verb ( " " ) { if e . Is Subresource ( ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , update Subresource } else { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , update // TODO: Figure out schemantic for deleting a sub-resource (what arguments // are passed, does it need two names? etc. if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , delete if e . Has Verb ( " " ) { sw . Do ( adjust Template ( e . Verb Name , e . Verb Type , patch } 
func ( s * Downloader ) Download ( handler http . Handler , etag string ) ( return Spec * spec . Swagger , new Etag string , http Status int , err error ) { handler = s . handler With User ( handler , & user . Default Info { Name : aggregator handler = http . Timeout Handler ( handler , spec Download req , err := http . New // Only pass e Tag if it is not generated locally if len ( etag ) > 0 && ! strings . Has Prefix ( etag , locally Generated Etag writer := new In Memory Response handler . Serve switch writer . resp Code { case http . Status Not Modified : if len ( etag ) == 0 { return nil , etag , http . Status Not return nil , etag , http . Status Not case http . Status Not Found : // Gracefully skip 404, assuming the server won't provide any spec return nil , " " , http . Status Not case http . Status OK : open API if err := json . Unmarshal ( writer . data , open API new if len ( new Etag ) == 0 { new Etag = etag if len ( etag ) > 0 && strings . Has Prefix ( etag , locally Generated Etag Prefix ) { // The function call with an etag and server does not report an etag. // That means this server does not support etag and the etag that passed // to the function generated previously by us. Just compare etags and // return Status Not Modified if they are the same. if etag == new Etag { return nil , etag , http . Status Not return open API Spec , new Etag , http . Status } 
func New Fs Store ( fs utilfs . Filesystem , dir string ) Store { return & fs } 
func ( o * Options ) Add Flags ( fs * pflag . Flag Set ) { o . add OS fs . String Var ( & o . Config File , " " , o . Config fs . String Var ( & o . Write Config To , " " , o . Write Config fs . Bool Var ( & o . Cleanup And Exit , " " , o . Cleanup And fs . Mark fs . Bool Var ( & o . Cleanup And Exit , " " , o . Cleanup And fs . Bool Var ( & o . Cleanup IPVS , " " , o . Cleanup // All flags below here are deprecated and will eventually be removed. fs . Var ( utilflag . IP Var { Val : & o . config . Bind fs . String fs . Int32Var ( & o . healthz Port , " " , o . healthz fs . Var ( utilflag . IP Var { Val : & o . config . Healthz Bind fs . Int32Var ( & o . metrics Port , " " , o . metrics fs . Var ( utilflag . IP Var { Val : & o . config . Metrics Bind fs . Int32Var ( o . config . OOM Score Adj , " " , utilpointer . Int32Ptr Deref Or ( o . config . OOM Score Adj , int32 ( qos . Kube Proxy OOM Score fs . String Var ( & o . config . Resource Container , " " , o . config . Resource fs . Mark fs . String Var ( & o . config . Client Connection . Kubeconfig , " " , o . config . Client fs . Var ( utilflag . Port Range Var { Val : & o . config . Port fs . String Var ( & o . hostname Override , " " , o . hostname fs . Int32Var ( o . config . IP Tables . Masquerade Bit , " " , utilpointer . Int32Ptr Deref Or ( o . config . IP Tables . Masquerade fs . Duration Var ( & o . config . IP Tables . Sync Period . Duration , " " , o . config . IP Tables . Sync fs . Duration Var ( & o . config . IP Tables . Min Sync Period . Duration , " " , o . config . IP Tables . Min Sync fs . Duration Var ( & o . config . IPVS . Sync Period . Duration , " " , o . config . IPVS . Sync fs . Duration Var ( & o . config . IPVS . Min Sync Period . Duration , " " , o . config . IPVS . Min Sync fs . String Slice Var ( & o . config . IPVS . Exclude CID Rs , " " , o . config . IPVS . Exclude CID fs . Bool Var ( & o . config . IPVS . Strict ARP , " " , o . config . IPVS . Strict fs . Duration Var ( & o . config . Config Sync Period . Duration , " " , o . config . Config Sync fs . Bool Var ( & o . config . IP Tables . Masquerade All , " " , o . config . IP Tables . Masquerade fs . String Var ( & o . config . Cluster CIDR , " " , o . config . Cluster fs . String Var ( & o . config . Client Connection . Content Type , " " , o . config . Client Connection . Content fs . Float32Var ( & o . config . Client Connection . QPS , " " , o . config . Client fs . Int32Var ( & o . config . Client Connection . Burst , " " , o . config . Client fs . Duration Var ( & o . config . UDP Idle Timeout . Duration , " " , o . config . UDP Idle fs . Mark fs . Int32Var ( o . config . Conntrack . Max Per Core , " " , * o . config . Conntrack . Max Per fs . Duration Var ( & o . config . Conntrack . TCP Established Timeout . Duration , " " , o . config . Conntrack . TCP Established fs . Duration Var ( & o . config . Conntrack . TCP Close Wait Timeout . Duration , " " , o . config . Conntrack . TCP Close Wait fs . Bool Var ( & o . config . Enable Profiling , " " , o . config . Enable fs . String fs . String Slice Var ( & o . config . Node Port Addresses , " " , o . config . Node Port fs . Var ( cliflag . New Map String Bool ( & o . config . Feature Gates ) , " " , " " + " \n " + strings . Join ( utilfeature . Default Feature Gate . Known } 
func New Options ( ) * Options { return & Options { config : new ( kubeproxyconfig . Kube Proxy Configuration ) , healthz Port : ports . Proxy Healthz Port , metrics Port : ports . Proxy Status Port , scheme : scheme . Scheme , codecs : scheme . Codecs , Cleanup IPVS : true , err } 
func ( o * Options ) Complete ( ) error { if len ( o . Config File ) == 0 && len ( o . Write Config o . config . Healthz Bind Address = address From Deprecated Flags ( o . config . Healthz Bind Address , o . healthz o . config . Metrics Bind Address = address From Deprecated Flags ( o . config . Metrics Bind Address , o . metrics // Load the config file here in Complete, so that Validate validates the fully-resolved config. if len ( o . Config File ) > 0 { c , err := o . load Config From File ( o . Config if err := o . init if err := o . process Hostname Override if err := utilfeature . Default Mutable Feature Gate . Set From Map ( o . config . Feature } 
func ( o * Options ) init Watcher ( ) error { fswatcher := filesystem . New Fsnotify err := fswatcher . Init ( o . event Handler , o . error err = fswatcher . Add Watch ( o . Config } 
func ( o * Options ) process Hostname Override Flag ( ) error { // Check if hostname-override flag is set and use value since config File always overrides if len ( o . hostname Override ) > 0 { host Name := strings . Trim Space ( o . hostname if len ( host o . config . Hostname Override = strings . To Lower ( host } 
if errs := validation . Validate ( o . config ) ; len ( errs ) != 0 { return errs . To } 
func ( o * Options ) Run ( ) error { defer close ( o . err if len ( o . Write Config To ) > 0 { return o . write Config proxy Server , err := New Proxy if o . Cleanup And Exit { return proxy Server . Cleanup And o . proxy Server = proxy return o . run } 
func ( o * Options ) run // run the proxy in goroutine go func ( ) { err := o . proxy o . err for { select { case err := <- o . err } 
func address From Deprecated return proxyutil . Append Port If } 
func ( o * Options ) load Config From File ( file string ) ( * kubeproxyconfig . Kube Proxy Configuration , error ) { data , err := ioutil . Read return o . load } 
func ( o * Options ) load Config ( data [ ] byte ) ( * kubeproxyconfig . Kube Proxy Configuration , error ) { config Obj , gvk , err := o . codecs . Universal proxy Config , ok := config Obj . ( * kubeproxyconfig . Kube Proxy return proxy } 
func ( o * Options ) Apply Defaults ( in * kubeproxyconfig . Kube Proxy Configuration ) ( * kubeproxyconfig . Kube Proxy Configuration , error ) { external , err := o . scheme . Convert To Version ( in , v1alpha1 . Scheme Group internal , err := o . scheme . Convert To Version ( external , kubeproxyconfig . Scheme Group out := internal . ( * kubeproxyconfig . Kube Proxy } 
func New Proxy Command ( ) * cobra . Command { opts := New Service cluster I addon that provides cluster DNS for these cluster I with the apiserver API to configure the proxy.` , Run : func ( cmd * cobra . Command , args [ ] string ) { verflag . Print And Exit If utilflag . Print if err := init For OS ( opts . Windows opts . config , err = opts . Apply opts . Add // TODO handle error cmd . Mark Flag } 
func create Clients ( config componentbaseconfig . Client Connection Configuration , master Override string ) ( clientset . Interface , v1core . Events Getter , error ) { var kube if len ( config . Kubeconfig ) == 0 && len ( master kube Config , err = rest . In Cluster } else { // This creates a client, first loading any specified kubeconfig // file, and then overriding the Master flag, if non-empty. kube Config , err = clientcmd . New Non Interactive Deferred Loading Client Config ( & clientcmd . Client Config Loading Rules { Explicit Path : config . Kubeconfig } , & clientcmd . Config Overrides { Cluster Info : clientcmdapi . Cluster { Server : master Override } } ) . Client kube Config . Accept Content Types = config . Accept Content kube Config . Content Type = config . Content kube kube client , err := clientset . New For Config ( kube event Client , err := clientset . New For Config ( kube return client , event Client . Core } 
func ( s * Proxy // TODO(vmarmol): Use container config for this. var oom Adjuster * oom . OOM if s . OOM Score Adj != nil { oom Adjuster = oom . New OOM if err := oom Adjuster . Apply OOM Score Adj ( 0 , int ( * s . OOM Score if len ( s . Resource Container ) != 0 { // Run in its own container. if err := resourcecontainer . Run In Resource Container ( s . Resource Container ) ; err != nil { klog . Warningf ( " " , s . Resource } else { klog . V ( 2 ) . Infof ( " " , s . Resource if s . Broadcaster != nil && s . Event Client != nil { s . Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : s . Event // Start up a healthz server if requested if s . Healthz Server != nil { s . Healthz // Start up a metrics server if requested if len ( s . Metrics Bind Address ) > 0 { proxy Mux := mux . New Path Recorder healthz . Install Handler ( proxy proxy Mux . Handle Func ( " " , func ( w http . Response Writer , r * http . Request ) { fmt . Fprintf ( w , " " , s . Proxy proxy if s . Enable Profiling { routes . Profiling { } . Install ( proxy configz . Install Handler ( proxy go wait . Until ( func ( ) { err := http . Listen And Serve ( s . Metrics Bind Address , proxy if err != nil { utilruntime . Handle } , 5 * time . Second , wait . Never // Tune conntrack, if requested // Conntracker is always nil for windows if s . Conntracker != nil { max , err := get Conntrack Max ( s . Conntrack if max > 0 { err := s . Conntracker . Set if err != nil { if err != err Read Only Sys // err Read Only Sys s . Recorder . Eventf ( s . Node Ref , api . Event Type if s . Conntrack Configuration . TCP Established Timeout != nil && s . Conntrack Configuration . TCP Established Timeout . Duration > 0 { timeout := int ( s . Conntrack Configuration . TCP Established if err := s . Conntracker . Set TCP Established if s . Conntrack Configuration . TCP Close Wait Timeout != nil && s . Conntrack Configuration . TCP Close Wait Timeout . Duration > 0 { timeout := int ( s . Conntrack Configuration . TCP Close Wait if err := s . Conntracker . Set TCP Close Wait informer Factory := informers . New Shared Informer Factory With Options ( s . Client , s . Config Sync Period , informers . With Tweak List Options ( func ( options * v1meta . List Options ) { options . Label Selector = " " + apis . Label Service Proxy // Create configs (i.e. Watches for Services and Endpoints) // Note: Register Handler() calls need to happen before creation of Sources because sources // only notify on changes, and the initial update (on process start) may be lost if no handlers // are registered yet. service Config := config . New Service Config ( informer Factory . Core ( ) . V1 ( ) . Services ( ) , s . Config Sync service Config . Register Event go service Config . Run ( wait . Never endpoints Config := config . New Endpoints Config ( informer Factory . Core ( ) . V1 ( ) . Endpoints ( ) , s . Config Sync endpoints Config . Register Event go endpoints Config . Run ( wait . Never // This has to start after the calls to New Service Config and New Endpoints Config because those // functions must configure their shared informer event handlers first. go informer Factory . Start ( wait . Never // Birth Cry after the birth is successful s . birth // Just loop forever for now... s . Proxier . Sync } 
func ( s * Proxy Server ) Cleanup And Exit ( ) error { encountered Error := userspace . Cleanup Leftovers ( s . Ipt encountered Error = iptables . Cleanup Leftovers ( s . Ipt Interface ) || encountered encountered Error = ipvs . Cleanup Leftovers ( s . Ipvs Interface , s . Ipt Interface , s . Ipset Interface , s . Cleanup IPVS ) || encountered if encountered } 
func New Port Allocator Custom ( pr net . Port Range , allocator Factory allocator . Allocator Factory ) * Port range a := & Port Allocator { port a . alloc = allocator Factory ( max , range } 
func New Port Allocator ( pr net . Port Range ) * Port Allocator { return New Port Allocator Custom ( pr , func ( max int , range Spec string ) allocator . Interface { return allocator . New Allocation Map ( max , range } 
func New From Snapshot ( snap * api . Range Allocation ) ( * Port Allocator , error ) { pr , err := net . Parse Port r := New Port } 
func ( r * Port Allocator ) Used ( ) int { return r . port } 
func ( r * Port if ! ok { // include valid port range in error valid Ports := r . port return & Err Not In Range { valid if ! allocated { return Err } 
func ( r * Port Allocator ) Allocate Next ( ) ( int , error ) { offset , ok , err := r . alloc . Allocate if ! ok { return 0 , Err return r . port } 
func ( r * Port Allocator ) For Each ( fn func ( int ) ) { r . alloc . For Each ( func ( offset int ) { fn ( r . port } 
func ( r * Port } 
func ( r * Port } 
func ( r * Port Allocator ) Snapshot ( dst * api . Range range dst . Range = range } 
func ( r * Port Allocator ) Restore ( pr net . Port Range , data [ ] byte ) error { if pr . String ( ) != r . port Range . String ( ) { return Err Mismatched } 
func ( r * Port Allocator ) contains ( port int ) ( bool , int ) { if ! r . port offset := port - r . port } 
func Unmarshal ( data [ ] byte , v interface { } ) error { switch v := v . ( type ) { case * map [ string ] interface { } : // Build a decoder from the given data decoder := json . New Decoder ( bytes . New // Preserve numbers, rather than casting to float64 automatically decoder . Use // If the decode succeeds, post-process the map to convert json.Number objects to int64 or float64 return convert Map case * [ ] interface { } : // Build a decoder from the given data decoder := json . New Decoder ( bytes . New // Preserve numbers, rather than casting to float64 automatically decoder . Use // If the decode succeeds, post-process the map to convert json.Number objects to int64 or float64 return convert Slice } 
func convert Map for k , v := range m { switch v := v . ( type ) { case json . Number : m [ k ] , err = convert case map [ string ] interface { } : err = convert Map case [ ] interface { } : err = convert Slice } 
func convert Slice for i , v := range s { switch v := v . ( type ) { case json . Number : s [ i ] , err = convert case map [ string ] interface { } : err = convert Map case [ ] interface { } : err = convert Slice } 
func convert } 
func disk Set Up ( manager disk Manager , b iscsi Disk Mounter , vol Path string , mounter mount . Interface , fs Group * int64 ) error { not Mnt , err := mounter . Is Likely Not Mount Point ( vol if err != nil && ! os . Is Not Exist ( err ) { klog . Errorf ( " " , vol if ! not if err := os . Mkdir All ( vol Path , 0750 ) ; err != nil { klog . Errorf ( " " , vol if b . read if b . iscsi Disk . Initiator Name != " " { // new iface name is <target portal>:<volume name> b . iscsi Disk . Iface = b . iscsi Disk . Portals [ 0 ] + " " + b . iscsi Disk . Vol global PD Path := manager . Make Global PD Name ( * b . iscsi mount Options := util . Join Mount Options ( b . mount err = mounter . Mount ( global PD Path , vol Path , " " , mount if err != nil { klog . Errorf ( " " , global PD Path , vol no Mnt , mnt Err := b . mounter . Is Likely Not Mount Point ( vol if mnt Err != nil { klog . Errorf ( " " , mnt if ! no Mnt { if mnt Err = b . mounter . Unmount ( vol Path ) ; mnt Err != nil { klog . Errorf ( " " , mnt no Mnt , mnt Err = b . mounter . Is Likely Not Mount Point ( vol if mnt Err != nil { klog . Errorf ( " " , mnt if ! no Mnt { // will most likely retry on next sync loop. klog . Errorf ( " " , vol os . Remove ( vol if ! b . read Only { volume . Set Volume Ownership ( & b , fs } 
func New Resource Builder Flags ( ) * Resource Builder return & Resource Builder Flags { File Name Flags : & File Name Flags { Usage : " " , Filenames : & filenames , Recursive : bool } 
func ( o * Resource Builder Flags ) Add Flags ( flagset * pflag . Flag Set ) { o . File Name Flags . Add if o . Label Selector != nil { flagset . String Var P ( o . Label Selector , " " , " " , * o . Label if o . Field Selector != nil { flagset . String Var ( o . Field Selector , " " , * o . Field if o . All Namespaces != nil { flagset . Bool Var P ( o . All Namespaces , " " , " " , * o . All if o . All != nil { flagset . Bool if o . Local != nil { flagset . Bool } 
func ( o * Resource Builder Flags ) To Builder ( rest Client Getter REST Client Getter , resources [ ] string ) Resource Finder { namespace , enforce Namespace , namespace Err := rest Client Getter . To Raw Kube Config builder := resource . New Builder ( rest Client Getter ) . Namespace Param ( namespace ) . Default if o . Scheme != nil { builder . With Scheme ( o . Scheme , o . Scheme . Prioritized Versions All if o . File Name Flags != nil { opts := o . File Name Flags . To builder . Filename Param ( enforce if o . Local == nil || ! * o . Local { // resource type/name tuples only work non-local if o . All != nil { builder . Resource Type Or Name } else { builder . Resource Type Or Name // label selectors only work non-local (for now) if o . Label Selector != nil { builder . Label Selector Param ( * o . Label // field selectors only work non-local (forever) if o . Field Selector != nil { builder . Field Selector Param ( * o . Field if len ( resources ) > 0 { builder . Add Error ( resource . Local Resource if ! o . Stop On First Error { builder . Continue On return & Resource Find Builder Wrapper { builder : builder . Flatten ( ) . // I think we're going to recommend this everywhere Add Error ( namespace } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Scheduler Configuration ) ( nil ) , ( * config . Kube Scheduler Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Scheduler Configuration_To_config_Kube Scheduler Configuration ( a . ( * v1alpha1 . Kube Scheduler Configuration ) , b . ( * config . Kube Scheduler if err := s . Add Generated Conversion Func ( ( * config . Kube Scheduler Configuration ) ( nil ) , ( * v1alpha1 . Kube Scheduler Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Scheduler Configuration_To_v1alpha1_Kube Scheduler Configuration ( a . ( * config . Kube Scheduler Configuration ) , b . ( * v1alpha1 . Kube Scheduler if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Kube Scheduler Leader Election Configuration ) ( nil ) , ( * config . Kube Scheduler Leader Election Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Kube Scheduler Leader Election Configuration_To_config_Kube Scheduler Leader Election Configuration ( a . ( * v1alpha1 . Kube Scheduler Leader Election Configuration ) , b . ( * config . Kube Scheduler Leader Election if err := s . Add Generated Conversion Func ( ( * config . Kube Scheduler Leader Election Configuration ) ( nil ) , ( * v1alpha1 . Kube Scheduler Leader Election Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Kube Scheduler Leader Election Configuration_To_v1alpha1_Kube Scheduler Leader Election Configuration ( a . ( * config . Kube Scheduler Leader Election Configuration ) , b . ( * v1alpha1 . Kube Scheduler Leader Election if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Scheduler Algorithm Source ) ( nil ) , ( * config . Scheduler Algorithm Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Scheduler Algorithm Source_To_config_Scheduler Algorithm Source ( a . ( * v1alpha1 . Scheduler Algorithm Source ) , b . ( * config . Scheduler Algorithm if err := s . Add Generated Conversion Func ( ( * config . Scheduler Algorithm Source ) ( nil ) , ( * v1alpha1 . Scheduler Algorithm Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Scheduler Algorithm Source_To_v1alpha1_Scheduler Algorithm Source ( a . ( * config . Scheduler Algorithm Source ) , b . ( * v1alpha1 . Scheduler Algorithm if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Scheduler Policy Config Map Source ) ( nil ) , ( * config . Scheduler Policy Config Map Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Scheduler Policy Config Map Source_To_config_Scheduler Policy Config Map Source ( a . ( * v1alpha1 . Scheduler Policy Config Map Source ) , b . ( * config . Scheduler Policy Config Map if err := s . Add Generated Conversion Func ( ( * config . Scheduler Policy Config Map Source ) ( nil ) , ( * v1alpha1 . Scheduler Policy Config Map Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Scheduler Policy Config Map Source_To_v1alpha1_Scheduler Policy Config Map Source ( a . ( * config . Scheduler Policy Config Map Source ) , b . ( * v1alpha1 . Scheduler Policy Config Map if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Scheduler Policy File Source ) ( nil ) , ( * config . Scheduler Policy File Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Scheduler Policy File Source_To_config_Scheduler Policy File Source ( a . ( * v1alpha1 . Scheduler Policy File Source ) , b . ( * config . Scheduler Policy File if err := s . Add Generated Conversion Func ( ( * config . Scheduler Policy File Source ) ( nil ) , ( * v1alpha1 . Scheduler Policy File Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Scheduler Policy File Source_To_v1alpha1_Scheduler Policy File Source ( a . ( * config . Scheduler Policy File Source ) , b . ( * v1alpha1 . Scheduler Policy File if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Scheduler Policy Source ) ( nil ) , ( * config . Scheduler Policy Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Scheduler Policy Source_To_config_Scheduler Policy Source ( a . ( * v1alpha1 . Scheduler Policy Source ) , b . ( * config . Scheduler Policy if err := s . Add Generated Conversion Func ( ( * config . Scheduler Policy Source ) ( nil ) , ( * v1alpha1 . Scheduler Policy Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Scheduler Policy Source_To_v1alpha1_Scheduler Policy Source ( a . ( * config . Scheduler Policy Source ) , b . ( * v1alpha1 . Scheduler Policy } 
func Convert_v1alpha1_Kube Scheduler Configuration_To_config_Kube Scheduler Configuration ( in * v1alpha1 . Kube Scheduler Configuration , out * config . Kube Scheduler Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Scheduler Configuration_To_config_Kube Scheduler } 
func Convert_config_Kube Scheduler Configuration_To_v1alpha1_Kube Scheduler Configuration ( in * config . Kube Scheduler Configuration , out * v1alpha1 . Kube Scheduler Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Scheduler Configuration_To_v1alpha1_Kube Scheduler } 
func Convert_v1alpha1_Kube Scheduler Leader Election Configuration_To_config_Kube Scheduler Leader Election Configuration ( in * v1alpha1 . Kube Scheduler Leader Election Configuration , out * config . Kube Scheduler Leader Election Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Kube Scheduler Leader Election Configuration_To_config_Kube Scheduler Leader Election } 
func Convert_config_Kube Scheduler Leader Election Configuration_To_v1alpha1_Kube Scheduler Leader Election Configuration ( in * config . Kube Scheduler Leader Election Configuration , out * v1alpha1 . Kube Scheduler Leader Election Configuration , s conversion . Scope ) error { return auto Convert_config_Kube Scheduler Leader Election Configuration_To_v1alpha1_Kube Scheduler Leader Election } 
func Convert_v1alpha1_Scheduler Algorithm Source_To_config_Scheduler Algorithm Source ( in * v1alpha1 . Scheduler Algorithm Source , out * config . Scheduler Algorithm Source , s conversion . Scope ) error { return auto Convert_v1alpha1_Scheduler Algorithm Source_To_config_Scheduler Algorithm } 
func Convert_config_Scheduler Algorithm Source_To_v1alpha1_Scheduler Algorithm Source ( in * config . Scheduler Algorithm Source , out * v1alpha1 . Scheduler Algorithm Source , s conversion . Scope ) error { return auto Convert_config_Scheduler Algorithm Source_To_v1alpha1_Scheduler Algorithm } 
func Convert_v1alpha1_Scheduler Policy Config Map Source_To_config_Scheduler Policy Config Map Source ( in * v1alpha1 . Scheduler Policy Config Map Source , out * config . Scheduler Policy Config Map Source , s conversion . Scope ) error { return auto Convert_v1alpha1_Scheduler Policy Config Map Source_To_config_Scheduler Policy Config Map } 
func Convert_config_Scheduler Policy Config Map Source_To_v1alpha1_Scheduler Policy Config Map Source ( in * config . Scheduler Policy Config Map Source , out * v1alpha1 . Scheduler Policy Config Map Source , s conversion . Scope ) error { return auto Convert_config_Scheduler Policy Config Map Source_To_v1alpha1_Scheduler Policy Config Map } 
func Convert_v1alpha1_Scheduler Policy File Source_To_config_Scheduler Policy File Source ( in * v1alpha1 . Scheduler Policy File Source , out * config . Scheduler Policy File Source , s conversion . Scope ) error { return auto Convert_v1alpha1_Scheduler Policy File Source_To_config_Scheduler Policy File } 
func Convert_config_Scheduler Policy File Source_To_v1alpha1_Scheduler Policy File Source ( in * config . Scheduler Policy File Source , out * v1alpha1 . Scheduler Policy File Source , s conversion . Scope ) error { return auto Convert_config_Scheduler Policy File Source_To_v1alpha1_Scheduler Policy File } 
func Convert_v1alpha1_Scheduler Policy Source_To_config_Scheduler Policy Source ( in * v1alpha1 . Scheduler Policy Source , out * config . Scheduler Policy Source , s conversion . Scope ) error { return auto Convert_v1alpha1_Scheduler Policy Source_To_config_Scheduler Policy } 
func Convert_config_Scheduler Policy Source_To_v1alpha1_Scheduler Policy Source ( in * config . Scheduler Policy Source , out * v1alpha1 . Scheduler Policy Source , s conversion . Scope ) error { return auto Convert_config_Scheduler Policy Source_To_v1alpha1_Scheduler Policy } 
func Default Marshal Func ( obj runtime . Object , gv schema . Group Version ) ( [ ] byte , error ) { return kubeadmutil . Marshal To } 
func Get Default Dry Run Client Options ( drg Dry Run Getter , w io . Writer ) Dry Run Client Options { return Dry Run Client Options { Writer : w , Getter : drg , Prepend Reactors : [ ] core . Reactor { } , Append Reactors : [ ] core . Reactor { } , Marshal Func : Default Marshal Func , Print GET And } 
func New Dry Run Client ( drg Dry Run Getter , w io . Writer ) clientset . Interface { return New Dry Run Client With Opts ( Get Default Dry Run Client } 
func New Dry Run Client With Opts ( opts Dry Run Client Options ) clientset . Interface { // Build a chain of reactors to act like a normal clientset; but log everything that is happening and don't change any state client := fakeclientset . New Simple // Build the chain of reactors. Order matters; first item here will be invoked first on match, then the second one will be evaluated, etc. default Reactor Chain := [ ] core . Reactor { // Log everything that happens. Default the object if it's about to be created/updated so that the logged object is representative. & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : func ( action core . Action ) ( bool , runtime . Object , error ) { log Dry Run Action ( action , opts . Writer , opts . Marshal } , } , // Let the Dry Run Getter implementation take care of all GET requests. // The Dry Run Getter implementation may call a real API Server behind the scenes or just fake everything & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : func ( action core . Action ) ( bool , runtime . Object , error ) { get Action , ok := action . ( core . Get handled , obj , err := opts . Getter . Handle Get Action ( get if opts . Print GET And LIST { // Print the marshalled object format with one tab indentation obj Bytes , err := opts . Marshal Func ( obj , action . Get Resource ( ) . Group Print Bytes With Line Prefix ( opts . Writer , obj } , } , // Let the Dry Run Getter implementation take care of all GET requests. // The Dry Run Getter implementation may call a real API Server behind the scenes or just fake everything & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : func ( action core . Action ) ( bool , runtime . Object , error ) { list Action , ok := action . ( core . List handled , objs , err := opts . Getter . Handle List Action ( list if opts . Print GET And LIST { // Print the marshalled object format with one tab indentation obj Bytes , err := opts . Marshal Func ( objs , action . Get Resource ( ) . Group Print Bytes With Line Prefix ( opts . Writer , obj } , } , // For the verbs that modify anything on the server; just return the object if present and exit successfully & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : successful Modification Reactor Func , } , & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : successful Modification Reactor Func , } , & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : successful Modification Reactor Func , } , & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : successful Modification Reactor Func , } , & core . Simple Reactor { Verb : " " , Resource : " " , Reaction : successful Modification Reactor // The chain of reactors will look like this: // opts.Prepend Reactors | default Reactor Chain | opts.Append Reactors | client.Fake.Reaction Chain (default reactors for the fake clientset) full Reactor Chain := append ( opts . Prepend Reactors , default Reactor full Reactor Chain = append ( full Reactor Chain , opts . Append // Prepend the reaction chain with our reactors. Important, these MUST be prepended; not appended due to how the fake clientset works by default client . Fake . Reaction Chain = append ( full Reactor Chain , client . Fake . Reaction } 
func successful Modification Reactor Func ( action core . Action ) ( bool , runtime . Object , error ) { obj Action , ok := action . ( action With if ok { return true , obj Action . Get } 
func log Dry Run Action ( action core . Action , w io . Writer , marshal Func Marshal Func ) { group := action . Get fmt . Fprintf ( w , " \" \" \n " , strings . To Upper ( action . Get Verb ( ) ) , action . Get Resource ( ) . Resource , group , action . Get named Action , ok := action . ( action With if ok { fmt . Fprintf ( w , " \n " , named Action . Get obj Action , ok := action . ( action With if ok && obj Action . Get Object ( ) != nil { // Print the marshalled object with a tab indentation obj Bytes , err := marshal Func ( obj Action . Get Object ( ) , action . Get Resource ( ) . Group Print Bytes With Line Prefix ( w , obj patch Action , ok := action . ( core . Patch if ok { // Replace all occurrences of \" with a simple " when printing fmt . Fprintf ( w , " \n \t \n " , strings . Replace ( string ( patch Action . Get } 
func Print Bytes With Line Prefix ( w io . Writer , obj Bytes [ ] byte , line Prefix string ) { scanner := bufio . New Scanner ( bytes . New Reader ( obj for scanner . Scan ( ) { fmt . Fprintf ( w , " \n " , line } 
func Equalities Or if err := e . Add } 
func ( e Equalities ) Add Funcs ( funcs ... interface { } ) error { for _ , f := range funcs { if err := e . Add } 
func ( e Equalities ) Add Func ( eq Func interface { } ) error { fv := reflect . Value Of ( eq if ft . Num if ft . Num var for Return bool Type := reflect . Type Of ( for Return if ft . Out ( 0 ) != bool } 
func ( e Equalities ) deep Value Equal ( v1 , v2 reflect . Value , visited map [ visit ] bool , depth int ) bool { defer make Useful if ! v1 . Is Valid ( ) || ! v2 . Is Valid ( ) { return v1 . Is Valid ( ) == v2 . Is if v1 . Can Addr ( ) && v2 . Can Addr ( ) && hard ( v1 . Kind ( ) ) { addr1 := v1 . Unsafe addr2 := v2 . Unsafe switch v1 . Kind ( ) { case reflect . Array : // We don't need to check length here because length is part of // an array's type, which has already been filtered for. for i := 0 ; i < v1 . Len ( ) ; i ++ { if ! e . deep Value case reflect . Slice : if ( v1 . Is Nil ( ) || v1 . Len ( ) == 0 ) != ( v2 . Is if v1 . Is for i := 0 ; i < v1 . Len ( ) ; i ++ { if ! e . deep Value case reflect . Interface : if v1 . Is Nil ( ) || v2 . Is Nil ( ) { return v1 . Is Nil ( ) == v2 . Is return e . deep Value case reflect . Ptr : return e . deep Value case reflect . Struct : for i , n := 0 , v1 . Num Field ( ) ; i < n ; i ++ { if ! e . deep Value case reflect . Map : if ( v1 . Is Nil ( ) || v1 . Len ( ) == 0 ) != ( v2 . Is if v1 . Is for _ , k := range v1 . Map Keys ( ) { if ! e . deep Value Equal ( v1 . Map Index ( k ) , v2 . Map case reflect . Func : if v1 . Is Nil ( ) && v2 . Is default : // Normal equality suffices if ! v1 . Can Interface ( ) || ! v2 . Can Interface ( ) { panic ( unexported Type } 
func ( e Equalities ) Deep v1 := reflect . Value v2 := reflect . Value return e . deep Value } 
func ( e Equalities ) Deep v1 := reflect . Value v2 := reflect . Value return e . deep Value } 
func ( v * sio Volume ) Get Path ( ) string { return v . plugin . host . Get Pod Volume Dir ( v . pod UID , utilstrings . Escape Qualified Name ( sio Plugin Name ) , v . vol Spec } 
func ( v * sio Volume ) Set Up At ( dir string , fs Group * int64 ) error { v . plugin . volume Mtx . Lock Key ( v . vol Spec defer v . plugin . volume Mtx . Unlock Key ( v . vol Spec klog . V ( 4 ) . Info ( log ( " " , v . vol Spec if err := v . set Sio mounter := v . plugin . host . Get Mounter ( v . plugin . Get Plugin not Dev Mnt , err := mounter . Is Likely Not Mount if err != nil && ! os . Is Not if ! not Dev Mnt { klog . V ( 4 ) . Info ( log ( " " , v . vol // should multiple-mapping be enabled enable Multi is if v . spec . Persistent Volume != nil { ams := v . spec . Persistent Volume . Spec . Access for _ , am := range ams { if am == api . Read Only Many { enable Multi is klog . V ( 4 ) . Info ( log ( " " , enable Multi vol Name := v . vol device Path , err := v . sio Mgr . Attach Volume ( vol Name , enable Multi if err != nil { klog . Error ( log ( " " , v . vol Spec case is ROM && ! v . read case is case v . read klog . V ( 4 ) . Info ( log ( " " , device if err := os . Mkdir disk Mounter := util . New Safe Format And Mount From Host ( v . plugin . Get Plugin err = disk Mounter . Format And Mount ( device Path , dir , v . fs if err := os . Remove ( dir ) ; err != nil && ! os . Is Not if ! v . read Only && fs volume . Set Volume Ownership ( v , fs klog . V ( 4 ) . Info ( log ( " " , v . vol Spec Name , v . vol Name , device } 
func ( v * sio Volume ) Tear Down At ( dir string ) error { v . plugin . volume Mtx . Lock Key ( v . vol Spec defer v . plugin . volume Mtx . Unlock Key ( v . vol Spec mounter := v . plugin . host . Get Mounter ( v . plugin . Get Plugin dev , _ , err := mount . Get Device Name From if err := mount . Cleanup Mount // detach/unmap device Busy , err := mounter . Device // Detach volume from node: // use "last attempt wins" strategy to detach volume from node // only allow volume to detach when it is not busy (not being used by other pods) if ! device Busy { klog . V ( 4 ) . Info ( log ( " " , v . vol Spec if err := v . reset Sio vol Name := v . vol if err := v . sio Mgr . Detach Volume ( vol Name ) ; err != nil { klog . Warning ( log ( " " , vol klog . V ( 4 ) . Infof ( log ( " " , vol } 
func ( v * sio Volume ) set Sio Mgr ( ) error { klog . V ( 4 ) . Info ( log ( " " , v . vol Spec pod Dir := v . plugin . host . Get Pod Plugin Dir ( v . pod UID , sio Plugin config Name := filepath . Join ( pod Dir , sio Config File if v . sio Mgr == nil { config Data , err := load Config ( config if err != nil { if ! os . Is Not Exist ( err ) { klog . Error ( log ( " " , config // prepare config data config map Volume Spec ( config // additional config data config Data [ conf Key . secret Namespace ] = v . secret config Data [ conf Key . secret Name ] = v . secret config Data [ conf Key . vol Spec Name ] = v . vol Spec if err := validate Configs ( config // persist config if err := save Config ( config Name , config // merge in secret if err := attach Secret ( v . plugin , v . secret Namespace , config // merge in Sdc Guid label value if err := attach Sdc GUID ( v . plugin , config mgr , err := new Sio Mgr ( config Data , v . plugin . host . Get Exec ( v . plugin . Get Plugin v . sio } 
func ( v * sio Volume ) reset Sio Mgr ( ) error { pod Dir := v . plugin . host . Get Pod Plugin Dir ( v . pod UID , sio Plugin config Name := filepath . Join ( pod Dir , sio Config File if v . sio Mgr == nil { // load config data from disk config Data , err := load Config ( config v . secret Name = config Data [ conf Key . secret v . secret Namespace = config Data [ conf Key . secret v . vol Name = config Data [ conf Key . volume v . vol Spec Name = config Data [ conf Key . vol Spec // attach secret if err := attach Secret ( v . plugin , v . secret Namespace , config // merge in Sdc Guid label value if err := attach Sdc GUID ( v . plugin , config mgr , err := new Sio Mgr ( config Data , v . plugin . host . Get Exec ( v . plugin . Get Plugin v . sio } 
func ( v * sio Volume ) set Sio Mgr From if v . sio Mgr == nil { apply Config Defaults ( v . config v . config Data [ conf Key . vol Spec Name ] = v . vol Spec if err := validate Configs ( v . config for k , v := range v . config if err := attach Secret ( v . plugin , v . secret mgr , err := new Sio Mgr ( data , v . plugin . host . Get Exec ( v . plugin . Get Plugin v . sio } 
func ( v * sio Volume ) set Sio Mgr From if v . sio Mgr == nil { // get config data form spec volume source config map Volume Spec ( config // additional config config Data [ conf Key . secret Namespace ] = v . secret config Data [ conf Key . secret Name ] = v . secret config Data [ conf Key . vol Spec Name ] = v . vol Spec if err := validate Configs ( config // attach secret object to config data if err := attach Secret ( v . plugin , v . secret Namespace , config mgr , err := new Sio Mgr ( config Data , v . plugin . host . Get Exec ( v . plugin . Get Plugin v . sio } 
func New Fischer Informer ( client versioned . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Fischer Informer ( client , resync } 
func New Filtered Fischer Informer ( client versioned . Interface , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options v1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Wardle } , Watch Func : func ( options v1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Wardle } , } , & wardlev1alpha1 . Fischer { } , resync } 
func Get return labels . Set ( event . Labels ) , Event To Selectable } 
func Event To Selectable Fields ( event * api . Event ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & event . Object specific Fields Set := fields . Set { " " : event . Involved Object . Kind , " " : event . Involved Object . Namespace , " " : event . Involved Object . Name , " " : string ( event . Involved Object . UID ) , " " : event . Involved Object . API Version , " " : event . Involved Object . Resource Version , " " : event . Involved Object . Field return generic . Merge Fields Sets ( object Meta Fields Set , specific Fields } 
func New Namespace Options ( streams genericclioptions . IO Streams ) * Namespace Options { return & Namespace Options { config Flags : genericclioptions . New Config Flags ( true ) , IO } 
func New Cmd Namespace ( streams genericclioptions . IO Streams ) * cobra . Command { o := New Namespace cmd := & cobra . Command { Use : " " , Short : " " , Example : fmt . Sprintf ( namespace Example , " " ) , Silence Usage : true , Run cmd . Flags ( ) . Bool Var ( & o . list Namespaces , " " , o . list o . config Flags . Add } 
func ( o * Namespace o . raw Config , err = o . config Flags . To Raw Kube Config Loader ( ) . Raw o . user Specified Namespace , err = cmd . Flags ( ) . Get if len ( args ) > 0 { if len ( o . user Specified o . user Specified // if no namespace argument or flag value was specified, then there // is no need to generate a resulting context if len ( o . user Specified o . user Specified Context , err = cmd . Flags ( ) . Get o . user Specified Cluster , err = cmd . Flags ( ) . Get o . user Specified Auth Info , err = cmd . Flags ( ) . Get current Context , exists := o . raw Config . Contexts [ o . raw Config . Current if ! exists { return err No o . resulting Context = api . New o . resulting Context . Cluster = current o . resulting Context . Auth Info = current Context . Auth // if a target context is explicitly provided by the user, // use that as our reference for the final, resulting context if len ( o . user Specified Context ) > 0 { o . resulting Context Name = o . user Specified if user Ctx , exists := o . raw Config . Contexts [ o . user Specified Context ] ; exists { o . resulting Context = user Ctx . Deep // override context info with user provided values o . resulting Context . Namespace = o . user Specified if len ( o . user Specified Cluster ) > 0 { o . resulting Context . Cluster = o . user Specified if len ( o . user Specified Auth Info ) > 0 { o . resulting Context . Auth Info = o . user Specified Auth // generate a unique context name based on its new values if // user did not explicitly request a context by name if len ( o . user Specified Context ) == 0 { o . resulting Context Name = generate Context Name ( o . resulting } 
func ( o * Namespace Options ) Validate ( ) error { if len ( o . raw Config . Current Context ) == 0 { return err No } 
func ( o * Namespace Options ) Run ( ) error { if len ( o . user Specified Namespace ) > 0 && o . resulting Context != nil { return o . set Namespace ( o . resulting Context , o . resulting Context for name , c := range o . raw Config . Contexts { if ! o . list Namespaces && name == o . raw Config . Current if ! o . list } 
func ( o * Namespace Options ) set Namespace ( from Context * api . Context , with Context Name string ) error { if len ( from config Access := clientcmd . New Default Path // determine if we have already saved this context to the user's KUBECONFIG before // if so, simply switch the current context to the existing one. if existing Resulting Ctx , exists := o . raw Config . Contexts [ with Context Name ] ; ! exists || ! is Context Equal ( from Context , existing Resulting Ctx ) { o . raw Config . Contexts [ with Context Name ] = from o . raw Config . Current Context = with Context fmt . Fprintf ( o . Out , " \n " , from return clientcmd . Modify Config ( config Access , o . raw } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Certificate Signing Request { } , & Certificate Signing Request // Add the watch version that applies metav1 . Add To Group Version ( scheme , Scheme Group } 
func New } 
func ( b * Broadcaster ) Add ( listener Listener ) { b . listener defer b . listener } 
func ( b * Broadcaster ) Notify ( instance interface { } ) { b . listener Lock . R b . listener Lock . R for _ , listener := range listeners { listener . On } 
func ( c * Apiextensions Client ) REST return c . rest } 
func New Manager ( cpu Policy Name string , reconcile Period time . Duration , machine Info * cadvisorapi . Machine Info , node Allocatable Reservation v1 . Resource List , state File switch policy Name ( cpu Policy Name ) { case Policy None : policy = New None case Policy Static : topo , err := topology . Discover ( machine reserved CP Us , ok := node Allocatable Reservation [ v1 . Resource if reserved CP Us . Is Zero ( ) { // The static policy requires this to be nonzero. Zero CPU reservation // would allow the shared pool to be completely exhausted. At that point // either we would violate our guarantee of exclusivity or need to evict // any pod that has at least one container that requires zero CP // Take the ceiling of the reservation, since fractional CP Us cannot be // exclusively allocated. reserved CP Us Float := float64 ( reserved CP Us . Milli num Reserved CP Us := int ( math . Ceil ( reserved CP Us policy = New Static Policy ( topo , num Reserved CP default : klog . Errorf ( " \" \" \" \" " , cpu Policy Name , Policy policy = New None state Impl , err := state . New Checkpoint State ( state File Directory , cpu Manager State File manager := & manager { policy : policy , reconcile Period : reconcile Period , state : state Impl , machine Info : machine Info , node Allocatable Reservation : node Allocatable } 
func ( s * pod Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Pod , err error ) { err = cache . List } 
func ( s * pod Lister ) Pods ( namespace string ) Pod Namespace Lister { return pod Namespace } 
func ( s pod Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Pod , err error ) { err = cache . List All By } 
func Serve Exec ( w http . Response Writer , req * http . Request , executor Executor , pod Name string , uid types . UID , container string , cmd [ ] string , stream Opts * Options , idle Timeout , stream Creation Timeout time . Duration , supported Protocols [ ] string ) { ctx , ok := create Streams ( req , w , stream Opts , supported Protocols , idle Timeout , stream Creation if ! ok { // error is handled by create err := executor . Exec In Container ( pod Name , uid , container , cmd , ctx . stdin Stream , ctx . stdout Stream , ctx . stderr Stream , ctx . tty , ctx . resize if err != nil { if exit Err , ok := err . ( utilexec . Exit Error ) ; ok && exit Err . Exited ( ) { rc := exit Err . Exit ctx . write Status ( & apierrors . Status Error { Err Status : metav1 . Status { Status : metav1 . Status Failure , Reason : remotecommandconsts . Non Zero Exit Code Reason , Details : & metav1 . Status Details { Causes : [ ] metav1 . Status Cause { { Type : remotecommandconsts . Exit Code Cause Type , Message : fmt . Sprintf ( " " , rc ) , } , } , } , Message : fmt . Sprintf ( " " , exit runtime . Handle ctx . write Status ( apierrors . New Internal } else { ctx . write Status ( & apierrors . Status Error { Err Status : metav1 . Status { Status : metav1 . Status } 
func is Sys FS Writable ( ) ( bool , error ) { const perm const sysfs mount for _ , mount Point := range mount Points { if mount Point . Type != sysfs // Check whether sysfs is 'rw' if len ( mount Point . Opts ) > 0 && mount Point . Opts [ 0 ] == perm klog . Errorf ( " " , mount Point , mount return false , err Read Only Sys } 
func ( p * JSON Printer ) Print Obj ( obj runtime . Object , w io . Writer ) error { // we use reflect.Indirect here in order to obtain the actual value from a pointer. // we need an actual value in order to retrieve the package path for an object. // using reflect.Indirect indiscriminately is valid here, as all runtime.Objects are supposed to be pointers. if Internal Object Preventer . Is Forbidden ( reflect . Indirect ( reflect . Value Of ( obj ) ) . Type ( ) . Pkg Path ( ) ) { return fmt . Errorf ( Internal Object Printer buf . Write _ , err = buf . Write if obj . Get Object Kind ( ) . Group Version data , err := json . Marshal } 
func ( p * YAML Printer ) Print Obj ( obj runtime . Object , w io . Writer ) error { // we use reflect.Indirect here in order to obtain the actual value from a pointer. // we need an actual value in order to retrieve the package path for an object. // using reflect.Indirect indiscriminately is valid here, as all runtime.Objects are supposed to be pointers. if Internal Object Preventer . Is Forbidden ( reflect . Indirect ( reflect . Value Of ( obj ) ) . Type ( ) . Pkg Path ( ) ) { return fmt . Errorf ( Internal Object Printer switch obj := obj . ( type ) { case * runtime . Unknown : data , err := yaml . JSON To if obj . Get Object Kind ( ) . Group Version } 
func ( i * Info ) Get ( ) ( err error ) { obj , err := New if err != nil { if errors . Is Not Found ( err ) && len ( i . Namespace ) > 0 && i . Namespace != metav1 . Namespace Default && i . Namespace != metav1 . Namespace All { err2 := i . Client . Get ( ) . Abs if err2 != nil && errors . Is Not i . Resource Version , _ = metadata Accessor . Resource } 
func ( i * Info ) Refresh ( obj runtime . Object , ignore Error bool ) error { name , err := metadata if err != nil { if ! ignore namespace , err := metadata if err != nil { if ! ignore version , err := metadata Accessor . Resource if err != nil { if ! ignore } else { i . Resource } 
func ( i * Info ) Object gvk := i . Object . Get Object Kind ( ) . Group Version if len ( gvk . Group ) == 0 { return fmt . Sprintf ( " " , strings . To return fmt . Sprintf ( " \n " , strings . To } 
func ( i * Info ) String ( ) string { basic if i . Mapping != nil { mapping Info := fmt . Sprintf ( " " , i . Mapping . Resource . String ( ) , i . Mapping . Group Version return fmt . Sprint ( mapping Info , " \n " , basic return basic } 
func ( i * Info ) Watch ( resource Version string ) ( watch . Interface , error ) { return New Helper ( i . Client , i . Mapping ) . Watch Single ( i . Namespace , i . Name , resource } 
func ( l Visitor List ) Visit ( fn Visitor } 
func ( l Eager Visitor List ) Visit ( fn Visitor return utilerrors . New } 
func read Http With Retries ( get httpget , duration time . Duration , u string , attempts int ) ( io . Read var body io . Read for i := 0 ; i < attempts ; i ++ { var status // Try to get the URL status // Error - Set the error condition from the Status Code if status Code != http . Status OK { err = fmt . Errorf ( " " , u , status , status if status Code >= 500 && status } else { // Don't retry other Status } 
func httpget Impl ( url string ) ( int , string , io . Read return resp . Status } 
func New Decorated Visitor ( v Visitor , fn ... Visitor return Decorated } 
func ( v Decorated Visitor ) Visit ( fn Visitor } 
func ( v Continue On Error Visitor ) Visit ( fn Visitor return utilerrors . New } 
func New Flatten List Visitor ( v Visitor , typer runtime . Object Typer , mapper * mapper ) Visitor { return Flatten List } 
func File Visitor For STDIN ( mapper * mapper , schema Content Validator ) Visitor { return & File Visitor { Path : const STDI Nstr , Stream Visitor : New Stream Visitor ( nil , mapper , const STDI } 
func ( v * File Visitor ) Visit ( fn Visitor if v . Path == const STDI // TODO: Consider adding a flag to force to UTF16, apparently some // Windows tools don't write the BOM utf16bom := unicode . BOM Override ( unicode . UTF8 . New v . Stream Visitor . Reader = transform . New return v . Stream } 
func ( v * Kustomize Visitor ) Visit ( fn Visitor Func ) error { f Sys := fs . Make Real err := kustomize . Run Kustomize Build ( & out , f v . Stream Visitor . Reader = bytes . New return v . Stream } 
func New Stream Visitor ( r io . Reader , mapper * mapper , source string , schema Content Validator ) * Stream Visitor { return & Stream } 
func ( v * Stream Visitor ) Visit ( fn Visitor Func ) error { d := yaml . New YAML Or JSON for { ext := runtime . Raw // TODO: This needs to be able to handle object in other encodings and schemas. ext . Raw = bytes . Trim if err := Validate info , err := v . info For if err != nil { if fn Err := fn ( info , err ) ; fn Err != nil { return fn } 
func Filter Update Object } 
func Set Namespace ( namespace string ) Visitor Update Object } 
func Require Namespace ( namespace string ) Visitor Update Object } 
func Retrieve } 
func Create And Refresh ( info * Info ) error { obj , err := New } 
func ( c * Fake Events ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( events } 
func ( c * Fake Events ) Create ( event * v1beta1 . Event ) ( result * v1beta1 . Event , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( events } 
func ( c * Fake Events ) Update ( event * v1beta1 . Event ) ( result * v1beta1 . Event , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( events } 
func ( c * Fake Events ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( events } 
func ( c * Fake Events ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( events Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Event } 
func ( c * Fake Events ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Event , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( events } 
func New Controller ( crd Informer informers . Custom Resource Definition Informer ) * Controller { c := & Controller { crd Lister : crd Informer . Lister ( ) , crds Synced : crd Informer . Informer ( ) . Has Synced , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , crd crd Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : c . add Custom Resource Definition , Update Func : c . update Custom Resource Definition , Delete Func : c . delete Custom Resource c . sync } 
func ( c * Controller ) Run ( static Spec * spec . Swagger , open API Service * handler . Open API Service , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer c . queue . Shut c . static Spec = static c . open API Service = open API if ! cache . Wait For Cache Sync ( stop Ch , c . crds Synced ) { utilruntime . Handle // create initial spec to avoid merging once per CRD on startup crds , err := c . crd if err != nil { utilruntime . Handle for _ , crd := range crds { if ! apiextensions . Is CRD Condition new Specs , changed , err := build Version if err != nil { utilruntime . Handle c . crd Specs [ crd . Name ] = new if err := c . update Spec Locked ( ) ; err != nil { utilruntime . Handle // only start one worker thread since its a slow moving API go wait . Until ( c . run Worker , time . Second , stop <- stop } 
func ( c * Controller ) update Spec Locked ( ) error { crd for _ , version Specs := range c . crd Specs { for _ , s := range version Specs { crd Specs = append ( crd return c . open API Service . Update Spec ( merge Specs ( c . static Spec , crd } 
func ( in * List ) Deep Copy out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] runtime . Raw for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Pod Affinity Term ) Deep Copy Into ( out * Pod Affinity if in . Label Selector != nil { in , out := & in . Label Selector , & out . Label * out = new ( metav1 . Label ( * in ) . Deep Copy } 
func ( in * Pod Signature ) Deep Copy Into ( out * Pod if in . Pod Controller != nil { in , out := & in . Pod Controller , & out . Pod * out = new ( metav1 . Owner ( * in ) . Deep Copy } 
func ( in * Replication Controller Spec ) Deep Copy Into ( out * Replication Controller * out = new ( Pod Template ( * in ) . Deep Copy } 
func ( in * Secret ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object for key , val := range * in { var out } else { in , out := & val , & out ( * out ) [ key ] = out if in . String Data != nil { in , out := & in . String Data , & out . String } 
func ( in * Service Account Token Projection ) Deep Copy Into ( out * Service Account Token if in . Expiration Seconds != nil { in , out := & in . Expiration Seconds , & out . Expiration } 
func ( c * Fake Cluster Role Bindings ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( clusterrolebindings } 
func ( c * Fake Cluster Role Bindings ) Update ( cluster Role Binding * v1alpha1 . Cluster Role Binding ) ( result * v1alpha1 . Cluster Role Binding , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( clusterrolebindings Resource , cluster Role Binding ) , & v1alpha1 . Cluster Role return obj . ( * v1alpha1 . Cluster Role } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Pod Preset { } , & Pod Preset metav1 . Add To Group Version ( scheme , Scheme Group } 
func ( c * Fake Mutating Webhook Configurations ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Mutating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( mutatingwebhookconfigurations Resource , name ) , & v1beta1 . Mutating Webhook return obj . ( * v1beta1 . Mutating Webhook } 
func ( c * Fake Mutating Webhook Configurations ) List ( opts v1 . List Options ) ( result * v1beta1 . Mutating Webhook Configuration List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( mutatingwebhookconfigurations Resource , mutatingwebhookconfigurations Kind , opts ) , & v1beta1 . Mutating Webhook Configuration label , _ , _ := testing . Extract From List list := & v1beta1 . Mutating Webhook Configuration List { List Meta : obj . ( * v1beta1 . Mutating Webhook Configuration List ) . List for _ , item := range obj . ( * v1beta1 . Mutating Webhook Configuration } 
func ( c * Fake Mutating Webhook Configurations ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( mutatingwebhookconfigurations } 
func ( c * Fake Mutating Webhook Configurations ) Create ( mutating Webhook Configuration * v1beta1 . Mutating Webhook Configuration ) ( result * v1beta1 . Mutating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( mutatingwebhookconfigurations Resource , mutating Webhook Configuration ) , & v1beta1 . Mutating Webhook return obj . ( * v1beta1 . Mutating Webhook } 
func ( c * Fake Mutating Webhook Configurations ) Update ( mutating Webhook Configuration * v1beta1 . Mutating Webhook Configuration ) ( result * v1beta1 . Mutating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( mutatingwebhookconfigurations Resource , mutating Webhook Configuration ) , & v1beta1 . Mutating Webhook return obj . ( * v1beta1 . Mutating Webhook } 
func ( c * Fake Mutating Webhook Configurations ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( mutatingwebhookconfigurations Resource , name ) , & v1beta1 . Mutating Webhook } 
func ( c * Fake Mutating Webhook Configurations ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( mutatingwebhookconfigurations Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Mutating Webhook Configuration } 
func ( c * Fake Mutating Webhook Configurations ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Mutating Webhook Configuration , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( mutatingwebhookconfigurations Resource , name , pt , data , subresources ... ) , & v1beta1 . Mutating Webhook return obj . ( * v1beta1 . Mutating Webhook } 
func generate Env List ( envs [ ] * runtimeapi . Key } 
func make for k , v := range annotations { // Assume there won't be conflict. merged [ fmt . Sprintf ( " " , annotation } 
func extract for _ , internal Key := range internal Label Keys { if k == internal // Delete the container name label for the sandbox. It is added in the shim, // should not be exposed via CRI. if k == types . Kubernetes Container Name Label && input [ container Type Label Key ] == container Type Label // Check if the label should be treated as an annotation. if strings . Has Prefix ( k , annotation Prefix ) { annotations [ strings . Trim Prefix ( k , annotation } 
func generate Mount for _ , m := range mounts { bind := fmt . Sprintf ( " " , m . Host Path , m . Container // Only request relabeling if the pod provides an SE Linux context. If the pod // does not provide an SE Linux context relabeling will label the volume with // the container's randomly allocated MCS label. This would restrict access // to the volume to the container which mounts it first. if m . Selinux switch m . Propagation { case runtimeapi . Mount Propagation_PROPAGATION_PRIVATE : // noop, private is default case runtimeapi . Mount case runtimeapi . Mount default : klog . Warningf ( " " , m . Host } 
func get Apparmor Security Opts ( sc * runtimeapi . Linux Container Security Context , separator rune ) ( [ ] string , error ) { if sc == nil || sc . Apparmor app Armor Opts , err := get App Armor Opts ( sc . Apparmor fmt Opts := fmt Docker Opts ( app Armor return fmt } 
func parse User From Image } 
func get User From Image User ( image User string ) ( * int64 , string ) { user := parse User From Image User ( image // user could be either uid or user name. Try to interpret as numeric uid. uid , err := strconv . Parse } 
func recover From Creation Conflict If Needed ( client libdocker . Interface , create Config dockertypes . Container Create Config , err error ) ( * dockercontainer . Container Create Created Body , error ) { matches := conflict RE . Find String if rm Err := client . Remove Container ( id , dockertypes . Container Remove Options { Remove Volumes : true } ) ; rm } else { klog . Errorf ( " " , id , rm // Return if the error is not container not found error. if ! libdocker . Is Container Not Found Error ( rm // randomize the name to avoid conflict. create Config . Name = randomize Name ( create klog . V ( 2 ) . Infof ( " " , create return client . Create Container ( create } 
func transform Start Container matches := start RE . Find String } 
func ensure Sandbox Image Exists ( client libdocker . Interface , image string ) error { _ , err := client . Inspect Image By if ! libdocker . Is Image Not Found repo To Pull , _ , _ , err := parsers . Parse Image keyring := credentialprovider . New Docker creds , with Credentials := keyring . Lookup ( repo To if ! with err := client . Pull Image ( image , dockertypes . Auth Config { } , dockertypes . Image Pull var pull for _ , current Creds := range creds { auth Config := dockertypes . Auth Config ( credentialprovider . Lazy Provide ( current Creds , repo To err := client . Pull Image ( image , auth Config , dockertypes . Image Pull pull Errs = append ( pull return utilerrors . New Aggregate ( pull } 
func fmt Docker Opts ( opts [ ] docker Opt , sep rune ) [ ] string { fmt for i , opt := range opts { fmt return fmt } 
func ( c * Metrics V1beta1Client ) REST return c . rest } 
func Is Dir if err := ioutil . Write File ( f , [ ] byte ( " " ) , private File } 
func Read } 
func Touch Dir All ( dir string ) error { err := os . Mkdir All ( dir , private Dir if err != nil && err != os . Err return Is Dir } 
} 
} 
func Decode if out != into { return fmt . Errorf ( " " , gvk , reflect . Type } 
func Encode Or } 
func Use Or Create Object ( t Object Typer , c Object Creater , gvk schema . Group Version Kind , obj Object ) ( Object , error ) { if obj != nil { kinds , _ , err := t . Object } 
func ( c * parameter Codec ) Decode Parameters ( parameters url . Values , from schema . Group target GV Ks , _ , err := c . typer . Object for i := range target GV Ks { if target GV Ks [ i ] . Group input , err := c . creator . New ( from . With Kind ( target GV } 
func ( c * parameter Codec ) Encode Parameters ( obj Object , to schema . Group Version ) ( url . Values , error ) { gvks , _ , err := c . typer . Object if to != gvk . Group Version ( ) { out , err := c . convertor . Convert To } 
func Serializer Info For Media Type ( types [ ] Serializer Info , media Type string ) ( Serializer Info , bool ) { for _ , info := range types { if info . Media Type == media for _ , info := range types { if len ( info . Media return Serializer } 
func ( internal Group Versioner ) Kind For Group Version Kinds ( kinds [ ] schema . Group Version Kind ) ( schema . Group Version Kind , bool ) { for _ , kind := range kinds { if kind . Version == API Version for _ , kind := range kinds { return schema . Group Version Kind { Group : kind . Group , Version : API Version return schema . Group Version } 
func ( disabled Group Versioner ) Kind For Group Version Kinds ( kinds [ ] schema . Group Version Kind ) ( schema . Group Version Kind , bool ) { return schema . Group Version } 
func ( gvs Group Versioners ) Kind For Group Version Kinds ( kinds [ ] schema . Group Version Kind ) ( schema . Group Version Kind , bool ) { for _ , gv := range gvs { target , ok := gv . Kind For Group Version return schema . Group Version } 
func New Multi Group Versioner ( gv schema . Group Version , group Kinds ... schema . Group Kind ) Group Versioner { if len ( group Kinds ) == 0 || ( len ( group Kinds ) == 1 && group return multi Group Versioner { target : gv , accepted Group Kinds : group } 
func New Coercing Multi Group Versioner ( gv schema . Group Version , group Kinds ... schema . Group Kind ) Group Versioner { return multi Group Versioner { target : gv , accepted Group Kinds : group } 
func ( v multi Group Versioner ) Kind For Group Version Kinds ( kinds [ ] schema . Group Version Kind ) ( schema . Group Version Kind , bool ) { for _ , src := range kinds { for _ , kind := range v . accepted Group return v . target . With if v . coerce && len ( kinds ) > 0 { return v . target . With return schema . Group Version } 
func For ( cfg * kubeadmapi . Join Configuration ) ( * clientcmdapi . Config , error ) { // TODO: Print summary info about the CA certificate, along with the checksum signature // we also need an ability for the user to configure the client to validate received CA cert against a checksum config , err := Discover Validated Kube if len ( cfg . Discovery . TLS Bootstrap clusterinfo := kubeconfigutil . Get Cluster From Kube return kubeconfigutil . Create With Token ( clusterinfo . Server , kubeadmapiv1beta2 . Default Cluster Name , Token User , clusterinfo . Certificate Authority Data , cfg . Discovery . TLS Bootstrap } 
func Discover Validated Kube Config ( cfg * kubeadmapi . Join Configuration ) ( * clientcmdapi . Config , error ) { switch { case cfg . Discovery . File != nil : kube Config Path := cfg . Discovery . File . Kube Config if is HTTPSURL ( kube Config Path ) { return https . Retrieve Validated Config Info ( kube Config Path , kubeadmapiv1beta2 . Default Cluster return file . Retrieve Validated Config Info ( kube Config Path , kubeadmapiv1beta2 . Default Cluster case cfg . Discovery . Bootstrap Token != nil : return token . Retrieve Validated Config } 
func is } 
func Long } 
} 
func New Must Match Patterns ( safe Whitelist , allowed Unsafe Sysctls , forbidden Sysctls [ ] string ) Sysctls Strategy { return & must Match Patterns { safe Whitelist : safe Whitelist , allowed Unsafe Sysctls : allowed Unsafe Sysctls , forbidden Sysctls : forbidden } 
func ( s * must Match Patterns ) Validate ( pod * api . Pod ) field . Error List { all Errs := field . Error if pod . Spec . Security Context != nil { sysctls = pod . Spec . Security field Path := field . New for i , sysctl := range sysctls { switch { case s . is Forbidden ( sysctl . Name ) : all Errs = append ( all Errs , field . Error List { field . Forbidden ( field case s . is case s . is Allowed default : all Errs = append ( all Errs , field . Error List { field . Forbidden ( field return all } 
func ( c * Config ) Complete ( ) Completed Config { cc := completed if c . Insecure Serving != nil { c . Insecure if c . Insecure Metrics Serving != nil { c . Insecure Metrics apiserver . Authorize Client Bearer Token ( c . Loopback Client return Completed } 
func ( in * Network Policy Peer ) Deep Copy Into ( out * Network Policy if in . Pod Selector != nil { in , out := & in . Pod Selector , & out . Pod * out = new ( metav1 . Label ( * in ) . Deep Copy if in . Namespace Selector != nil { in , out := & in . Namespace Selector , & out . Namespace * out = new ( metav1 . Label ( * in ) . Deep Copy if in . IP Block != nil { in , out := & in . IP Block , & out . IP * out = new ( IP ( * in ) . Deep Copy } 
func New CIDR Set ( cluster CIDR * net . IP Net , sub Net Mask Size int ) ( * Cidr Set , error ) { cluster Mask := cluster cluster Mask Size , _ := cluster var max CID if ( cluster CIDR . IP . To4 ( ) == nil ) && ( sub Net Mask Size - cluster Mask Size > cluster Subnet Max Diff ) { return nil , Err CIDR Set Sub Net Too max CID Rs = 1 << uint32 ( sub Net Mask Size - cluster Mask return & Cidr Set { cluster CIDR : cluster CIDR , cluster IP : cluster CIDR . IP , cluster Mask Size : cluster Mask Size , max CID Rs : max CID Rs , sub Net Mask Size : sub Net Mask } 
func ( s * Cidr Set ) Allocate Next ( ) ( * net . IP next for i := 0 ; i < s . max CID Rs ; i ++ { candidate := ( i + s . next Candidate ) % s . max CID if s . used . Bit ( candidate ) == 0 { next if next Unused == - 1 { return nil , Err CIDR Range No CID Rs s . next Candidate = ( next Unused + 1 ) % s . max CID s . used . Set Bit ( & s . used , next return s . index To CIDR Block ( next } 
func ( s * Cidr Set ) Release ( cidr * net . IP Net ) error { begin , end , err := s . get Begining And End for i := begin ; i <= end ; i ++ { s . used . Set } 
func add Known Types ( scheme * runtime . Scheme ) error { // TODO this gets cleaned up when the types are fixed scheme . Add Known Types ( Scheme Group Version , & Pod Disruption Budget { } , & Pod Disruption Budget List { } , & Pod Security Policy { } , & Pod Security Policy } 
func list Azure Disk Path ( io io Handler ) [ ] string { azure Disk var azure Disk if dirs , err := io . Read Dir ( azure Disk disk Path := azure Disk if link , link Err := io . Readlink ( disk Path ) ; link Err == nil { sd := link [ ( libstrings . Last azure Disk List = append ( azure Disk klog . V ( 12 ) . Infof ( " " , azure Disk return azure Disk } 
func get Disk Link By Dev Name ( io io Handler , dev Link Path , dev Name string ) ( string , error ) { dirs , err := io . Read Dir ( dev Link klog . V ( 12 ) . Infof ( " " , dev Name , dev Link if err == nil { for _ , f := range dirs { disk Path := dev Link klog . V ( 12 ) . Infof ( " " , disk link , link Err := io . Readlink ( disk if link Err != nil { klog . Warningf ( " " , disk Path , link if libstrings . Has Suffix ( link , dev Name ) { return disk return " " , fmt . Errorf ( " " , dev Name , dev Link return " " , fmt . Errorf ( " " , dev Link } 
func find Disk By Lun With Constraint ( lun int , io io Handler , azure if dirs , err := io . Read if len ( azure if lun == l { // find the matching LUN // read vendor and model to ensure it is a VHD disk vendor vendor Bytes , err := io . Read File ( vendor vendor := libstrings . Trim Space ( string ( vendor if libstrings . To model model Bytes , err := io . Read File ( model model := libstrings . Trim Space ( string ( model if libstrings . To if dev , err := io . Read dev for _ , disk Name := range azure Disks { klog . V ( 12 ) . Infof ( " " , dev Name , disk if dev Name == disk if ! found { dev Link for _ , dev Link Path := range dev Link Paths { disk Path , err := get Disk Link By Dev Name ( io , dev Link Path , dev if err == nil { klog . V ( 4 ) . Infof ( " " , disk Path , dev Name , dev Link return disk klog . Warningf ( " " , dev Name , dev Link return " " + dev } 
func new Node Tree ( nodes [ ] * v1 . Node ) * Node Tree { nt := & Node Tree { tree : make ( map [ string ] * node for _ , n := range nodes { nt . Add } 
func ( nt * Node Tree ) Add nt . add } 
func ( nt * Node Tree ) Remove return nt . remove } 
func ( nt * Node Tree ) remove } 
func ( nt * Node Tree ) Update Node ( old , new * v1 . Node ) { var old if old != nil { old Zone = utilnode . Get Zone new Zone := utilnode . Get Zone // If the zone ID of the node has not changed, we don't need to do anything. Name of the node // cannot be changed in an update. if old Zone == new nt . remove nt . add } 
func ( nt * Node num Exhausted for { if nt . zone Index >= len ( nt . zones ) { nt . zone zone := nt . zones [ nt . zone nt . zone // We do not check the exhausted zones before calling next() on the zone. This ensures // that if more nodes are added to a zone after it is exhausted, we iterate over the new nodes. node if exhausted { num Exhausted if num Exhausted Zones >= len ( nt . zones ) { // all zones are exhausted. we should reset. nt . reset } else { return node } 
func ( nt * Node Tree ) Num Nodes ( ) int { nt . mu . R defer nt . mu . R return nt . num } 
func New Resource Quota Controller ( options * Resource Quota Controller Options ) ( * Resource Quota Controller , error ) { // build the resource quota controller rq := & Resource Quota Controller { rq Client : options . Quota Client , rq Lister : options . Resource Quota Informer . Lister ( ) , informer Synced Funcs : [ ] cache . Informer Synced { options . Resource Quota Informer . Informer ( ) . Has Synced } , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , missing Usage Queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , resync Period : options . Resync // set the synchronization handler rq . sync Handler = rq . sync Resource Quota From options . Resource Quota Informer . Informer ( ) . Add Event Handler With Resync Period ( cache . Resource Event Handler Funcs { Add Func : rq . add Quota , Update Func : func ( old , cur interface { } ) { // We are only interested in observing updates to quota.spec to drive updates to quota.status. // We ignore all updates to quota.Status because they are all driven by this controller. // IMPORTANT: // We do not use this function to queue up a full quota recalculation. To do so, would require // us to enqueue all quota.Status updates, and since quota.Status updates involve additional queries // that cannot be backed by a cache and result in a full query of a namespace's content, we do not // want to pay the price on spurious status updates. As a result, we have a separate routine that is // responsible for enqueue of all resource quotas when doing a full resync (enqueue All) old Resource Quota := old . ( * v1 . Resource cur Resource Quota := cur . ( * v1 . Resource if quota . V1Equals ( old Resource Quota . Spec . Hard , cur Resource rq . add Quota ( cur Resource } , // This will enter the sync loop and no-op, because the controller has been deleted from the store. // Note that deleting a controller immediately after scaling it to 0 will not work. The recommended // way of achieving this is by performing a `stop` operation on the controller. Delete Func : rq . enqueue Resource Quota , } , rq . resync if options . Discovery Func != nil { qm := & Quota Monitor { informers Started : options . Informers Started , informer Factory : options . Informer Factory , ignored Resources : options . Ignored Resources Func ( ) , resource Changes : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate Limiter ( ) , " " ) , resync Period : options . Replenishment Resync Period , replenishment Func : rq . replenish rq . quota // do initial quota monitor setup. If we have a discovery failure here, it's ok. We'll discover more resources when a later sync happens. resources , err := Get Quotable Resources ( options . Discovery if discovery . Is Group Discovery Failed Error ( err ) { utilruntime . Handle if err = qm . Sync Monitors ( resources ) ; err != nil { utilruntime . Handle // only start quota once all informers synced rq . informer Synced Funcs = append ( rq . informer Synced Funcs , qm . Is } 
func ( rq * Resource Quota Controller ) enqueue rqs , err := rq . rq if err != nil { utilruntime . Handle for i := range rqs { key , err := controller . Key if err != nil { utilruntime . Handle } 
func ( rq * Resource Quota Controller ) enqueue Resource Quota ( obj interface { } ) { key , err := controller . Key } 
func ( rq * Resource Quota Controller ) worker ( queue workqueue . Rate Limiting Interface ) func ( ) { work rq . worker Lock . R defer rq . worker Lock . R err := rq . sync utilruntime . Handle queue . Add Rate return func ( ) { for { if quit := work } 
func ( rq * Resource Quota Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer rq . queue . Shut if rq . quota Monitor != nil { go rq . quota Monitor . Run ( stop if ! controller . Wait For Cache Sync ( " " , stop Ch , rq . informer Synced // the workers that chug through the quota calculation backlog for i := 0 ; i < workers ; i ++ { go wait . Until ( rq . worker ( rq . queue ) , time . Second , stop go wait . Until ( rq . worker ( rq . missing Usage Queue ) , time . Second , stop // the timer for how often we do a full recalculation across all quotas go wait . Until ( func ( ) { rq . enqueue All ( ) } , rq . resync Period ( ) , stop <- stop } 
func ( rq * Resource Quota Controller ) sync Resource Quota From Key ( key string ) ( err error ) { start defer func ( ) { klog . V ( 4 ) . Infof ( " " , key , time . Since ( start namespace , name , err := cache . Split Meta Namespace quota , err := rq . rq Lister . Resource if errors . Is Not return rq . sync Resource } 
func ( rq * Resource Quota Controller ) sync Resource Quota ( resource Quota * v1 . Resource Quota ) ( err error ) { // quota is dirty if any part of spec hard limits differs from the status hard limits status Limits Dirty := ! apiequality . Semantic . Deep Equal ( resource Quota . Spec . Hard , resource // dirty tracks if the usage status differs from the previous sync, // if so, we send a new usage with latest status // if this is our first sync, it will be dirty by default, since we need track usage dirty := status Limits Dirty || resource Quota . Status . Hard == nil || resource used := v1 . Resource if resource Quota . Status . Used != nil { used = quota . Add ( v1 . Resource List { } , resource hard Limits := quota . Add ( v1 . Resource List { } , resource new Usage , err := quota . Calculate Usage ( resource Quota . Namespace , resource Quota . Spec . Scopes , hard Limits , rq . registry , resource Quota . Spec . Scope if err != nil { // if err is non-nil, remember it to return, but continue updating status with any resources in new for key , value := range new // ensure set of used values match those that have hard constraints hard Resources := quota . Resource Names ( hard used = quota . Mask ( used , hard // Create a usage object that is based on the quota resource version that will handle updates // by default, we preserve the past usage observation, and set hard to the current spec usage := resource Quota . Deep usage . Status = v1 . Resource Quota Status { Hard : hard dirty = dirty || ! quota . Equals ( usage . Status . Used , resource // there was a change observed by this controller that requires we update quota if dirty { _ , err = rq . rq Client . Resource Quotas ( usage . Namespace ) . Update return utilerrors . New } 
func ( rq * Resource Quota Controller ) replenish Quota ( group Resource schema . Group Resource , namespace string ) { // check if the quota controller can evaluate this group Resource, if not, ignore it altogether... evaluator := rq . registry . Get ( group // check if this namespace even has a quota... resource Quotas , err := rq . rq Lister . Resource if errors . Is Not Found ( err ) { utilruntime . Handle Error ( fmt . Errorf ( " " , namespace , rq . resync if err != nil { utilruntime . Handle if len ( resource // only queue those quotas that are tracking a resource associated with this kind. for i := range resource Quotas { resource Quota := resource resource Quota Resources := quota . Resource Names ( resource if intersection := evaluator . Matching Resources ( resource Quota Resources ) ; len ( intersection ) > 0 { // TODO: make this support targeted replenishment to a specific kind, right now it does a full recalc on that quota. rq . enqueue Resource Quota ( resource } 
func ( rq * Resource Quota Controller ) resync Monitors ( resources map [ schema . Group Version Resource ] struct { } ) error { if rq . quota if err := rq . quota Monitor . Sync rq . quota Monitor . Start } 
func Get Quotable Resources ( discovery Func Namespaced Resources Func ) ( map [ schema . Group Version Resource ] struct { } , error ) { possible Resources , discovery Err := discovery if discovery Err != nil && len ( possible Resources ) == 0 { return nil , fmt . Errorf ( " " , discovery quotable Resources := discovery . Filtered By ( discovery . Supports All Verbs { Verbs : [ ] string { " " , " " , " " , " " } } , possible quotable Group Version Resources , err := discovery . Group Version Resources ( quotable // return the original discovery error (if any) in addition to the list return quotable Group Version Resources , discovery } 
func New Cordon Helper From Runtime Object ( node Object runtime . Object , scheme * runtime . Scheme , gvk schema . Group Version Kind ) ( * Cordon Helper , error ) { node Object , err := scheme . Convert To Version ( node Object , gvk . Group node , ok := node if ! ok { return nil , fmt . Errorf ( " " , node return New Cordon } 
func ( c * Cordon Helper ) Update If } 
func ( c * Cordon Helper ) Patch Or Replace ( clientset kubernetes . Interface ) ( error , error ) { client := clientset . Core old new patch Bytes , patch Err := strategicpatch . Create Two Way Merge Patch ( old Data , new if patch Err == nil { _ , err = client . Patch ( c . node . Name , types . Strategic Merge Patch Type , patch return err , patch } 
func ( s * pod Template Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Pod Template , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Pod } 
func ( s * pod Template Lister ) Pod Templates ( namespace string ) Pod Template Namespace Lister { return pod Template Namespace } 
func ( s pod Template Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Pod Template , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Pod } 
func Retrieve Validated Config Info ( cfg * kubeadmapi . Join Configuration ) ( * clientcmdapi . Config , error ) { token , err := kubeadmapi . New Bootstrap Token String ( cfg . Discovery . Bootstrap // Load the cfg.Discovery Token CA Cert Hashes into a pubkeypin.Set pub Key Pins := pubkeypin . New err = pub Key Pins . Allow ( cfg . Discovery . Bootstrap Token . CA Cert // The function below runs for every endpoint, and all endpoints races with each other. // The endpoint that wins the race and completes the task first gets its kubeconfig returned below base Kube Config , err := fetch Kube Config With Timeout ( cfg . Discovery . Bootstrap Token . API Server Endpoint , cfg . Discovery . Timeout . Duration , func ( endpoint string ) ( * clientcmdapi . Config , error ) { insecure Bootstrap Config := build Insecure Bootstrap Kube Config ( endpoint , kubeadmapiv1beta2 . Default Cluster cluster Name := insecure Bootstrap Config . Contexts [ insecure Bootstrap Config . Current insecure Client , err := kubeconfigutil . To Client Set ( insecure Bootstrap klog . V ( 1 ) . Infof ( " \n " , insecure Bootstrap Config . Clusters [ cluster // Make an initial insecure connection to get the cluster-info Config Map var insecure Cluster Info * v1 . Config wait . Poll Immediate Infinite ( constants . Discovery Retry insecure Cluster Info , err = insecure Client . Core V1 ( ) . Config Maps ( metav1 . Namespace Public ) . Get ( bootstrapapi . Config Map Cluster Info , metav1 . Get // Validate the MAC on the kubeconfig from the Config Map and load it insecure Kubeconfig String , ok := insecure Cluster Info . Data [ bootstrapapi . Kube Config if ! ok || len ( insecure Kubeconfig String ) == 0 { return nil , errors . Errorf ( " " , bootstrapapi . Kube Config Key , bootstrapapi . Config Map Cluster detached JWS Token , ok := insecure Cluster Info . Data [ bootstrapapi . JWS Signature Key if ! ok || len ( detached JWS if ! bootstrap . Detached Token Is Valid ( detached JWS Token , insecure Kubeconfig insecure Kubeconfig Bytes := [ ] byte ( insecure Kubeconfig insecure Config , err := clientcmd . Load ( insecure Kubeconfig if err != nil { return nil , errors . Wrapf ( err , " " , bootstrapapi . Config Map Cluster // If no TLS root CA pinning was specified, we're done if pub Key return insecure // Load the cluster CA from the Config if len ( insecure Config . Clusters ) != 1 { return nil , errors . Errorf ( " " , bootstrapapi . Config Map Cluster Info , len ( insecure var cluster CA for _ , cluster := range insecure Config . Clusters { cluster CA Bytes = cluster . Certificate Authority cluster C As , err := certutil . Parse Certs PEM ( cluster CA if err != nil { return nil , errors . Wrapf ( err , " " , bootstrapapi . Config Map Cluster // Validate the cluster CA public key against the pinned set err = pub Key Pins . Check Any ( cluster C if err != nil { return nil , errors . Wrapf ( err , " " , bootstrapapi . Config Map Cluster // Now that we know the proported cluster CA, connect back a second time validating with that CA secure Bootstrap Config := build Secure Bootstrap Kube Config ( endpoint , cluster CA Bytes , cluster secure Client , err := kubeconfigutil . To Client Set ( secure Bootstrap klog . V ( 1 ) . Infof ( " \n " , insecure Bootstrap Config . Clusters [ cluster var secure Cluster Info * v1 . Config wait . Poll Immediate Infinite ( constants . Discovery Retry secure Cluster Info , err = secure Client . Core V1 ( ) . Config Maps ( metav1 . Namespace Public ) . Get ( bootstrapapi . Config Map Cluster Info , metav1 . Get // Pull the kubeconfig from the securely-obtained Config Map and validate that it's the same as what we found the first time secure Kubeconfig Bytes := [ ] byte ( secure Cluster Info . Data [ bootstrapapi . Kube Config if ! bytes . Equal ( secure Kubeconfig Bytes , insecure Kubeconfig Bytes ) { return nil , errors . Errorf ( " " , bootstrapapi . Config Map Cluster secure Kubeconfig , err := clientcmd . Load ( secure Kubeconfig if err != nil { return nil , errors . Wrapf ( err , " " , bootstrapapi . Config Map Cluster return secure return base Kube } 
func build Insecure Bootstrap Kube Config ( endpoint , clustername string ) * clientcmdapi . Config { control Plane bootstrap Config := kubeconfigutil . Create Basic ( control Plane Endpoint , clustername , Bootstrap bootstrap Config . Clusters [ clustername ] . Insecure Skip TLS return bootstrap } 
func build Secure Bootstrap Kube Config ( endpoint string , ca Cert [ ] byte , clustername string ) * clientcmdapi . Config { control Plane bootstrap Config := kubeconfigutil . Create Basic ( control Plane Endpoint , clustername , Bootstrap User , ca return bootstrap } 
func fetch Kube Config With Timeout ( api Endpoint string , discovery Timeout time . Duration , fetch Kube Config Func func ( string ) ( * clientcmdapi . Config , error ) ) ( * clientcmdapi . Config , error ) { stop var resulting Kube var wg sync . Wait wait . Until ( func ( ) { klog . V ( 1 ) . Infof ( " \n " , api cfg , err := fetch Kube Config Func ( api if err != nil { klog . V ( 1 ) . Infof ( " \n " , api klog . V ( 1 ) . Infof ( " \n " , api once . Do ( func ( ) { resulting Kube close ( stop } , constants . Discovery Retry Interval , stop select { case <- time . After ( discovery Timeout ) : once . Do ( func ( ) { close ( stop err := errors . Errorf ( " " , discovery case <- stop return resulting Kube } 
func ( t * Extender Config ) Unmarshal JSON ( b [ ] byte ) error { return gojson . Unmarshal ( b , case Insensitive Extender } 
func New Rollout History Options ( streams genericclioptions . IO Streams ) * Rollout History Options { return & Rollout History Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , IO } 
func New Cmd Rollout History ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := New Rollout History valid cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : history Long , Example : history Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check } , Valid Args : valid cmdutil . Add Filename Option Flags ( cmd , & o . Filename o . Print Flags . Add } 
func ( o * Rollout History if o . Namespace , o . Enforce Namespace , err = f . To Raw Kube Config o . To Printer = func ( operation string ) ( printers . Resource Printer , error ) { o . Print Flags . Name Print return o . Print Flags . To o . History Viewer = polymorphichelpers . History Viewer o . REST Client o . Builder = f . New } 
func ( o * Rollout History Options ) Validate ( ) error { if len ( o . Resources ) == 0 && cmdutil . Is Filename Slice } 
func ( o * Rollout History Options ) Run ( ) error { r := o . Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . Filename Param ( o . Enforce Namespace , & o . Filename Options ) . Resource Type Or Name Args ( true , o . Resources ... ) . Continue On mapping := info . Resource history Viewer , err := o . History Viewer ( o . REST Client history Info , err := history Viewer . View with if o . Revision > 0 { with printer , err := o . To Printer ( fmt . Sprintf ( " \n " , with Revision , history return printer . Print } 
func New For Version ( client rest . Interface , mapper meta . REST Mapper , version schema . Group Version ) Custom Metrics Client { return & custom Metrics } 
func New For Version For Config ( c * rest . Config , mapper meta . REST Mapper , version schema . Group Version ) ( Custom Metrics Client , error ) { config Shallow if config Shallow Copy . Rate Limiter == nil && config Shallow Copy . QPS > 0 { config Shallow Copy . Rate Limiter = flowcontrol . New Token Bucket Rate Limiter ( config Shallow Copy . QPS , config Shallow config Shallow Copy . API if config Shallow Copy . User Agent == " " { config Shallow Copy . User Agent = rest . Default Kubernetes User config Shallow Copy . Group config Shallow Copy . Negotiated Serializer = scheme . Codecs . Without client , err := rest . REST Client For ( & config Shallow return New For } 
func ( c * custom Metrics Client ) qual Resource For Kind ( group Kind schema . Group Kind ) ( string , error ) { mapping , err := c . mapper . REST Mapping ( group if err != nil { return " " , fmt . Errorf ( " " , group gr := mapping . Resource . Group } 
func ( plugin * cni Network Plugin ) Get Pod Network Status ( namespace string , name string , id kubecontainer . Container ID ) ( * network . Pod Network Status , error ) { netns Path , err := plugin . host . Get Net result , err := plugin . add To Network ( plugin . get Default Network ( ) , name , namespace , id , netns // Parse the result and get the IP Address var result020 * cni result020 , err = cni Types020 . Get return & network . Pod Network } 
func build DNS Capabilities ( dns Config * runtimeapi . DNS Config ) * cni DNS Config { if dns Config != nil { return & cni DNS Config { Servers : dns Config . Servers , Searches : dns Config . Searches , Options : dns } 
func ( asw * actual State Of World ) get Node And Volume ( volume Name v1 . Unique Volume Name , node Name types . Node Name ) ( attached Volume , node Attached To , error ) { volume Obj , volume Exists := asw . attached Volumes [ volume if volume Exists { node Obj , node Exists := volume Obj . nodes Attached To [ node if node Exists { return volume Obj , node return attached Volume { } , node Attached To { } , fmt . Errorf ( " " , volume Name , node } 
func ( asw * actual State Of World ) remove Volume From Report As Attached ( volume Name v1 . Unique Volume Name , node Name types . Node Name ) error { node To Update , node To Update Exists := asw . nodes To Update Status For [ node if node To Update Exists { _ , node To Update Volume Exists := node To Update . volumes To Report As Attached [ volume if node To Update Volume Exists { node To Update . status Update delete ( node To Update . volumes To Report As Attached , volume asw . nodes To Update Status For [ node Name ] = node To return fmt . Errorf ( " " , volume Name , node } 
func ( asw * actual State Of World ) add Volume To Report As Attached ( volume Name v1 . Unique Volume Name , node Name types . Node Name ) { // In case the volume/node entry is no longer in attached Volume list, skip the rest if _ , _ , err := asw . get Node And Volume ( volume Name , node Name ) ; err != nil { klog . V ( 4 ) . Infof ( " " , volume Name , node node To Update , node To Update Exists := asw . nodes To Update Status For [ node if ! node To Update Exists { // Create object if it doesn't exist node To Update = node To Update Status For { node Name : node Name , status Update Needed : true , volumes To Report As Attached : make ( map [ v1 . Unique Volume Name ] v1 . Unique Volume asw . nodes To Update Status For [ node Name ] = node To klog . V ( 4 ) . Infof ( " " , node _ , node To Update Volume Exists := node To Update . volumes To Report As Attached [ volume if ! node To Update Volume Exists { node To Update . status Update node To Update . volumes To Report As Attached [ volume Name ] = volume asw . nodes To Update Status For [ node Name ] = node To klog . V ( 4 ) . Infof ( " " , volume Name , node } 
func ( asw * actual State Of World ) update Node Status Update Needed ( node Name types . Node Name , needed bool ) error { node To Update , node To Update Exists := asw . nodes To Update Status For [ node if ! node To Update Exists { // should not happen err Msg := fmt . Sprintf ( " " , needed , node return fmt . Errorf ( err node To Update . status Update asw . nodes To Update Status For [ node Name ] = node To } 
func With Audit ( handler http . Handler , sink audit . Sink , policy policy . Checker , long Running Check request . Long Running Request return http . Handler Func ( func ( w http . Response Writer , req * http . Request ) { req , ev , omit Stages , err := create Audit Event And Attach To if err != nil { utilruntime . Handle responsewriters . Internal if ev == nil || ctx == nil { handler . Serve ev . Stage = auditinternal . Stage Request if processed := process Audit Event ( sink , ev , omit Stages ) ; ! processed { audit . Apiserver Audit Dropped responsewriters . Internal // intercept the status code var long Running if long Running Check != nil { ri , _ := request . Request Info if long Running Check ( req , ri ) { long Running resp Writer := decorate Response Writer ( w , ev , long Running Sink , omit ev . Stage = auditinternal . Stage ev . Response Status = & metav1 . Status { Code : http . Status Internal Server Error , Status : metav1 . Status Failure , Reason : metav1 . Status Reason Internal process Audit Event ( sink , ev , omit // if no Stage Response Started event was sent b/c neither a status code nor a body was sent, fake it here // But Audit-Id http header will only be sent when http.Response Writer.Write Header is called. faked Success Status := & metav1 . Status { Code : http . Status OK , Status : metav1 . Status if ev . Response Status == nil && long Running Sink != nil { ev . Response Status = faked Success ev . Stage = auditinternal . Stage Response process Audit Event ( long Running Sink , ev , omit ev . Stage = auditinternal . Stage Response if ev . Response Status == nil { ev . Response Status = faked Success process Audit Event ( sink , ev , omit handler . Serve HTTP ( resp } 
func create Audit Event And Attach To attribs , err := Get Authorizer level , omit Stages := policy . Level And audit . Observe Policy if level == auditinternal . Level ev , err := audit . New Event From req = req . With Context ( request . With Audit return req , ev , omit } 
func ( k * Kubeadm Cert ) Get Config ( ic * kubeadmapi . Init Configuration ) ( * certutil . Config , error ) { for _ , f := range k . config } 
func ( k * Kubeadm Cert ) Create From CA ( ic * kubeadmapi . Init Configuration , ca Cert * x509 . Certificate , ca Key crypto . Signer ) error { cfg , err := k . Get cert , key , err := pkiutil . New Cert And Key ( ca Cert , ca err = write Certificate Files If Not Exist ( ic . Certificates Dir , k . Base Name , ca } 
func ( k * Kubeadm Cert ) Create As CA ( ic * kubeadmapi . Init Configuration ) ( * x509 . Certificate , crypto . Signer , error ) { cfg , err := k . Get ca Cert , ca Key , err := pkiutil . New Certificate err = write Certificate Authorithy Files If Not Exist ( ic . Certificates Dir , k . Base Name , ca Cert , ca return ca Cert , ca } 
func ( t Certificate Tree ) Create Tree ( ic * kubeadmapi . Init Configuration ) error { for ca , leaves := range t { cfg , err := ca . Get var ca ca Cert , err := pkiutil . Try Load Cert From Disk ( ic . Certificates Dir , ca . Base if err == nil { // Cert exists already, make sure it's valid if ! ca Cert . Is // Try and load a CA Key ca Key , err = pkiutil . Try Load Key From Disk ( ic . Certificates Dir , ca . Base if err != nil { // If there's no CA key, make sure every certificate exists. for _ , leaf := range leaves { cl := cert Key Location { pki Dir : ic . Certificates Dir , base Name : leaf . Base Name , ux if err := validate Signed Cert With CA ( cl , ca // CA key exists; just use that to create new certificates. } else { // CA Cert doesn't already exist, create a new cert and key. ca Cert , ca Key , err = pkiutil . New Certificate err = write Certificate Authorithy Files If Not Exist ( ic . Certificates Dir , ca . Base Name , ca Cert , ca for _ , leaf := range leaves { if err := leaf . Create From CA ( ic , ca Cert , ca } 
func ( m Certificate Map ) Cert Tree ( ) ( Certificate Tree , error ) { ca Map := make ( Certificate for _ , cert := range m { if cert . CA Name == " " { if _ , ok := ca Map [ cert ] ; ! ok { ca Map [ cert ] = [ ] * Kubeadm } else { ca , ok := m [ cert . CA if ! ok { return nil , errors . Errorf ( " " , cert . Name , cert . CA ca Map [ ca ] = append ( ca return ca } 
func ( c Certificates ) As Map ( ) Certificate Map { cert Map := make ( map [ string ] * Kubeadm for _ , cert := range c { cert return cert } 
func Get Default Cert List ( ) Certificates { return Certificates { & Kubeadm Cert Root CA , & Kubeadm Cert API Server , & Kubeadm Cert Kubelet Client , // Front Proxy certs & Kubeadm Cert Front Proxy CA , & Kubeadm Cert Front Proxy Client , // etcd certs & Kubeadm Cert Etcd CA , & Kubeadm Cert Etcd Server , & Kubeadm Cert Etcd Peer , & Kubeadm Cert Etcd Healthcheck , & Kubeadm Cert Etcd API } 
func Setup Certificate Authorithy ( t * testing . T ) ( * x509 . Certificate , crypto . Signer ) { ca Cert , ca Key , err := pkiutil . New Certificate Authority ( & certutil . Config { Common return ca Cert , ca } 
func Assert Certificate Is Signed By Ca ( t * testing . T , cert * x509 . Certificate , signing Ca * x509 . Certificate ) { if err := cert . Check Signature From ( signing } 
func Assert Certificate Has Common Name ( t * testing . T , cert * x509 . Certificate , common Name string ) { if cert . Subject . Common Name != common Name { t . Errorf ( " " , cert . Subject . Common Name , common } 
func Assert Certificate Has } 
func Assert Certificate Has Client Auth Usage ( t * testing . T , cert * x509 . Certificate ) { for i := range cert . Ext Key Usage { if cert . Ext Key Usage [ i ] == x509 . Ext Key Usage Client } 
func Assert Certificate Has Server Auth Usage ( t * testing . T , cert * x509 . Certificate ) { for i := range cert . Ext Key Usage { if cert . Ext Key Usage [ i ] == x509 . Ext Key Usage Server } 
func Assert Certificate Has DNS Names ( t * testing . T , cert * x509 . Certificate , DNS Names ... string ) { for _ , DNS Name := range DNS for _ , val := range cert . DNS Names { if val == DNS if ! found { t . Errorf ( " " , DNS } 
func Assert Certificate Has IP Addresses ( t * testing . T , cert * x509 . Certificate , IP Addresses ... net . IP ) { for _ , IP Address := range IP for _ , val := range cert . IP Addresses { if val . Equal ( IP if ! found { t . Errorf ( " " , IP } 
func Create CA Cert ( t * testing . T ) ( * x509 . Certificate , crypto . Signer ) { cert Cfg := & certutil . Config { Common cert , key , err := pkiutil . New Certificate Authority ( cert } 
func Write PKI Files ( t * testing . T , dir string , files PKI Files ) { for filename , body := range files { switch body := body . ( type ) { case * x509 . Certificate : if err := certutil . Write Cert ( path . Join ( dir , filename ) , pkiutil . Encode Cert case * rsa . Public Key : public Key Bytes , err := pkiutil . Encode Public Key if err := keyutil . Write Key ( path . Join ( dir , filename ) , public Key case * rsa . Private Key : private Key , err := keyutil . Marshal Private Key To if err := keyutil . Write Key ( path . Join ( dir , filename ) , private } 
func New ( c controller , cloud Alias cloud Alias , kube API kube API , mode Node Sync Mode , node Name string , set * cidrset . Cidr Set ) * Node Sync { return & Node Sync { c : c , cloud Alias : cloud Alias , kube API : kube API , mode : mode , node Name : node Name , op Chan : make ( chan sync } 
func ( sync * Node Sync ) Loop ( done chan struct { } ) { klog . V ( 2 ) . Infof ( " " , sync . node timeout := sync . c . Resync delay Timer := time . New klog . V ( 4 ) . Infof ( " " , sync . node for { select { case op , more := <- sync . op sync . c . Report if ! delay Timer . Stop ( ) { <- delay case <- delay Timer . C : klog . V ( 4 ) . Infof ( " " , sync . node sync . c . Report Result ( ( & update timeout := sync . c . Resync delay klog . V ( 4 ) . Infof ( " " , sync . node } 
func ( sync * Node Sync ) Update ( node * v1 . Node ) { sync . op Chan <- & update } 
func ( sync * Node Sync ) Delete ( node * v1 . Node ) { sync . op Chan <- & delete close ( sync . op } 
func ( op * update Op ) validate Range ( ctx context . Context , sync * Node Sync , node * v1 . Node , alias Range * net . IP Net ) error { if node . Spec . Pod CIDR != alias Range . String ( ) { klog . Errorf ( " " , node . Spec . Pod CIDR , alias sync . kube API . Emit Node Warning Event ( node . Name , Mismatch Event , " " , node . Spec . Pod CIDR , alias // User intervention is required in this case, as this is most likely due // to the user mucking around with their VM aliases on the side. } else { klog . V ( 4 ) . Infof ( " " , node . Name , node . Spec . Pod } 
func ( op * update Op ) update Node From Alias ( ctx context . Context , sync * Node Sync , node * v1 . Node , alias Range * net . IP Net ) error { if sync . mode != Sync From Cloud { sync . kube API . Emit Node Warning Event ( node . Name , Invalid Mode klog . V ( 2 ) . Infof ( " " , alias if err := sync . set . Occupy ( alias Range ) ; err != nil { klog . Errorf ( " " , alias Range , sync . node if err := sync . kube API . Update Node Pod CIDR ( ctx , node , alias Range ) ; err != nil { klog . Errorf ( " " , node . Name , alias klog . V ( 2 ) . Infof ( " " , node . Name , alias if err := sync . kube API . Update Node Network klog . V ( 2 ) . Infof ( " " , node . Name , alias } 
func ( op * update Op ) update Alias From Node ( ctx context . Context , sync * Node Sync , node * v1 . Node ) error { if sync . mode != Sync From Cluster { sync . kube API . Emit Node Warning Event ( node . Name , Invalid Mode _ , alias Range , err := net . Parse CIDR ( node . Spec . Pod if err != nil { klog . Errorf ( " " , node . Spec . Pod if err := sync . set . Occupy ( alias Range ) ; err != nil { klog . Errorf ( " " , alias Range , sync . node if err := sync . cloud Alias . Add Alias ( ctx , node . Name , alias Range ) ; err != nil { klog . Errorf ( " " , alias if err := sync . kube API . Update Node Network klog . V ( 2 ) . Infof ( " " , node . Name , node . Spec . Pod } 
func ( op * update Op ) allocate Range ( ctx context . Context , sync * Node Sync , node * v1 . Node ) error { if sync . mode != Sync From Cluster { sync . kube API . Emit Node Warning Event ( node . Name , Invalid Mode cidr Range , err := sync . set . Allocate // If add Alias returns a hard error, cidr Range will be leaked as there // is no durable record of the range. The missing space will be // recovered on the next restart of the controller. if err := sync . cloud Alias . Add Alias ( ctx , node . Name , cidr Range ) ; err != nil { klog . Errorf ( " " , cidr if err := sync . kube API . Update Node Pod CIDR ( ctx , node , cidr Range ) ; err != nil { klog . Errorf ( " " , node . Name , cidr if err := sync . kube API . Update Node Network klog . V ( 2 ) . Infof ( " " , cidr } 
func New Client Manager ( gv schema . Group Version , add To Schema Func func ( s * runtime . Scheme ) error ) ( Client Manager , error ) { cache , err := lru . New ( default Cache if err != nil { return Client hook Scheme := runtime . New if err := add To Schema Func ( hook Scheme ) ; err != nil { return Client return Client Manager { cache : cache , negotiated Serializer : serializer . Negotiated Serializer Wrapper ( runtime . Serializer Info { Serializer : serializer . New Codec Factory ( hook Scheme ) . Legacy } 
func ( cm * Client Manager ) Set Authentication Info Resolver Wrapper ( wrapper Authentication Info Resolver Wrapper ) { if wrapper != nil { cm . auth Info Resolver = wrapper ( cm . auth Info } 
func ( cm * Client Manager ) Set Service Resolver ( sr Service Resolver ) { if sr != nil { cm . service } 
func ( cm * Client if cm . negotiated if cm . service if cm . auth Info return utilerrors . New } 
func ( cm * Client Manager ) Hook Client ( cc Client Config ) ( * rest . REST Client , error ) { cc With No cc With No cache Key , err := json . Marshal ( cc With No if client , ok := cm . cache . Get ( string ( cache Key ) ) ; ok { return client . ( * rest . REST complete := func ( cfg * rest . Config ) ( * rest . REST Client , error ) { // Combine CA Data from the config with any existing CA bundle provided if len ( cfg . TLS Client Config . CA Data ) > 0 { cfg . TLS Client Config . CA Data = append ( cfg . TLS Client Config . CA cfg . TLS Client Config . CA Data = append ( cfg . TLS Client Config . CA Data , cc . CA cfg . Content Config . Negotiated Serializer = cm . negotiated cfg . Content Config . Content Type = runtime . Content Type client , err := rest . Unversioned REST Client if err == nil { cm . cache . Add ( string ( cache if cc . Service != nil { rest Config , err := cm . auth Info Resolver . Client Config For cfg := rest . Copy Config ( rest server host := server cfg . API // Set the server name if not already set if len ( cfg . TLS Client Config . Server Name ) == 0 { cfg . TLS Client Config . Server Name = server delegate if delegate delegate Dialer = d . Dial u , err := cm . service Resolver . Resolve return delegate if cc . URL == " " { return nil , & Err Calling Webhook { Webhook if err != nil { return nil , & Err Calling Webhook { Webhook rest Config , err := cm . auth Info Resolver . Client Config cfg := rest . Copy Config ( rest cfg . API } 
func TLS Config For ( config * Config ) ( * tls . Config , error ) { cfg , err := config . Transport return transport . TLS Config } 
func Transport For ( config * Config ) ( http . Round Tripper , error ) { cfg , err := config . Transport } 
func HTTP Wrappers For Config ( config * Config , rt http . Round Tripper ) ( http . Round Tripper , error ) { cfg , err := config . Transport return transport . HTTP Wrappers For } 
func ( c * Config ) Transport Config ( ) ( * transport . Config , error ) { conf := & transport . Config { User Agent : c . User Agent , Transport : c . Transport , Wrap Transport : c . Wrap Transport , TLS : transport . TLS Config { Insecure : c . Insecure , Server Name : c . Server Name , CA File : c . CA File , CA Data : c . CA Data , Cert File : c . Cert File , Cert Data : c . Cert Data , Key File : c . Key File , Key Data : c . Key Data , } , Username : c . Username , Password : c . Password , Bearer Token : c . Bearer Token , Impersonate : transport . Impersonation Config { User Name : c . Impersonate . User if c . Exec Provider != nil && c . Auth if c . Exec Provider != nil { provider , err := exec . Get Authenticator ( c . Exec if err := provider . Update Transport if c . Auth Provider != nil { provider , err := Get Auth Provider ( c . Host , c . Auth Provider , c . Auth Config conf . Wrap ( provider . Wrap } 
func Allow Bootstrap Tokens To Post CS return apiclient . Create Or Update Cluster Role Binding ( client , & rbac . Cluster Role Binding { Object Meta : metav1 . Object Meta { Name : Node Kubelet Bootstrap , } , Role Ref : rbac . Role Ref { API Group : rbac . Group Name , Kind : " " , Name : Node Bootstrapper Cluster Role Name , } , Subjects : [ ] rbac . Subject { { Kind : rbac . Group Kind , Name : constants . Node Bootstrap Token Auth } 
func Auto Approve Node Bootstrap // Always create this kubeadm-specific binding though return apiclient . Create Or Update Cluster Role Binding ( client , & rbac . Cluster Role Binding { Object Meta : metav1 . Object Meta { Name : Node Auto Approve Bootstrap Cluster Role Binding , } , Role Ref : rbac . Role Ref { API Group : rbac . Group Name , Kind : " " , Name : CSR Auto Approval Cluster Role Name , } , Subjects : [ ] rbac . Subject { { Kind : " " , Name : constants . Node Bootstrap Token Auth } 
func Auto Approve Node Certificate return apiclient . Create Or Update Cluster Role Binding ( client , & rbac . Cluster Role Binding { Object Meta : metav1 . Object Meta { Name : Node Auto Approve Certificate Rotation Cluster Role Binding , } , Role Ref : rbac . Role Ref { API Group : rbac . Group Name , Kind : " " , Name : Node Self CSR Auto Approval Cluster Role Name , } , Subjects : [ ] rbac . Subject { { Kind : " " , Name : constants . Nodes } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Local Subject Access Review ) ( nil ) , ( * authorization . Local Subject Access Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Local Subject Access Review_To_authorization_Local Subject Access Review ( a . ( * v1 . Local Subject Access Review ) , b . ( * authorization . Local Subject Access if err := s . Add Generated Conversion Func ( ( * authorization . Local Subject Access Review ) ( nil ) , ( * v1 . Local Subject Access Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Local Subject Access Review_To_v1_Local Subject Access Review ( a . ( * authorization . Local Subject Access Review ) , b . ( * v1 . Local Subject Access if err := s . Add Generated Conversion Func ( ( * v1 . Non Resource Attributes ) ( nil ) , ( * authorization . Non Resource Attributes ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Non Resource Attributes_To_authorization_Non Resource Attributes ( a . ( * v1 . Non Resource Attributes ) , b . ( * authorization . Non Resource if err := s . Add Generated Conversion Func ( ( * authorization . Non Resource Attributes ) ( nil ) , ( * v1 . Non Resource Attributes ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Non Resource Attributes_To_v1_Non Resource Attributes ( a . ( * authorization . Non Resource Attributes ) , b . ( * v1 . Non Resource if err := s . Add Generated Conversion Func ( ( * v1 . Non Resource Rule ) ( nil ) , ( * authorization . Non Resource Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Non Resource Rule_To_authorization_Non Resource Rule ( a . ( * v1 . Non Resource Rule ) , b . ( * authorization . Non Resource if err := s . Add Generated Conversion Func ( ( * authorization . Non Resource Rule ) ( nil ) , ( * v1 . Non Resource Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Non Resource Rule_To_v1_Non Resource Rule ( a . ( * authorization . Non Resource Rule ) , b . ( * v1 . Non Resource if err := s . Add Generated Conversion Func ( ( * v1 . Resource Attributes ) ( nil ) , ( * authorization . Resource Attributes ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Attributes_To_authorization_Resource Attributes ( a . ( * v1 . Resource Attributes ) , b . ( * authorization . Resource if err := s . Add Generated Conversion Func ( ( * authorization . Resource Attributes ) ( nil ) , ( * v1 . Resource Attributes ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Resource Attributes_To_v1_Resource Attributes ( a . ( * authorization . Resource Attributes ) , b . ( * v1 . Resource if err := s . Add Generated Conversion Func ( ( * v1 . Resource Rule ) ( nil ) , ( * authorization . Resource Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Resource Rule_To_authorization_Resource Rule ( a . ( * v1 . Resource Rule ) , b . ( * authorization . Resource if err := s . Add Generated Conversion Func ( ( * authorization . Resource Rule ) ( nil ) , ( * v1 . Resource Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Resource Rule_To_v1_Resource Rule ( a . ( * authorization . Resource Rule ) , b . ( * v1 . Resource if err := s . Add Generated Conversion Func ( ( * v1 . Self Subject Access Review ) ( nil ) , ( * authorization . Self Subject Access Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Self Subject Access Review_To_authorization_Self Subject Access Review ( a . ( * v1 . Self Subject Access Review ) , b . ( * authorization . Self Subject Access if err := s . Add Generated Conversion Func ( ( * authorization . Self Subject Access Review ) ( nil ) , ( * v1 . Self Subject Access Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Self Subject Access Review_To_v1_Self Subject Access Review ( a . ( * authorization . Self Subject Access Review ) , b . ( * v1 . Self Subject Access if err := s . Add Generated Conversion Func ( ( * v1 . Self Subject Access Review Spec ) ( nil ) , ( * authorization . Self Subject Access Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Self Subject Access Review Spec_To_authorization_Self Subject Access Review Spec ( a . ( * v1 . Self Subject Access Review Spec ) , b . ( * authorization . Self Subject Access Review if err := s . Add Generated Conversion Func ( ( * authorization . Self Subject Access Review Spec ) ( nil ) , ( * v1 . Self Subject Access Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Self Subject Access Review Spec_To_v1_Self Subject Access Review Spec ( a . ( * authorization . Self Subject Access Review Spec ) , b . ( * v1 . Self Subject Access Review if err := s . Add Generated Conversion Func ( ( * v1 . Self Subject Rules Review ) ( nil ) , ( * authorization . Self Subject Rules Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Self Subject Rules Review_To_authorization_Self Subject Rules Review ( a . ( * v1 . Self Subject Rules Review ) , b . ( * authorization . Self Subject Rules if err := s . Add Generated Conversion Func ( ( * authorization . Self Subject Rules Review ) ( nil ) , ( * v1 . Self Subject Rules Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Self Subject Rules Review_To_v1_Self Subject Rules Review ( a . ( * authorization . Self Subject Rules Review ) , b . ( * v1 . Self Subject Rules if err := s . Add Generated Conversion Func ( ( * v1 . Self Subject Rules Review Spec ) ( nil ) , ( * authorization . Self Subject Rules Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Self Subject Rules Review Spec_To_authorization_Self Subject Rules Review Spec ( a . ( * v1 . Self Subject Rules Review Spec ) , b . ( * authorization . Self Subject Rules Review if err := s . Add Generated Conversion Func ( ( * authorization . Self Subject Rules Review Spec ) ( nil ) , ( * v1 . Self Subject Rules Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Self Subject Rules Review Spec_To_v1_Self Subject Rules Review Spec ( a . ( * authorization . Self Subject Rules Review Spec ) , b . ( * v1 . Self Subject Rules Review if err := s . Add Generated Conversion Func ( ( * v1 . Subject Access Review ) ( nil ) , ( * authorization . Subject Access Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Subject Access Review_To_authorization_Subject Access Review ( a . ( * v1 . Subject Access Review ) , b . ( * authorization . Subject Access if err := s . Add Generated Conversion Func ( ( * authorization . Subject Access Review ) ( nil ) , ( * v1 . Subject Access Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Subject Access Review_To_v1_Subject Access Review ( a . ( * authorization . Subject Access Review ) , b . ( * v1 . Subject Access if err := s . Add Generated Conversion Func ( ( * v1 . Subject Access Review Spec ) ( nil ) , ( * authorization . Subject Access Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Subject Access Review Spec_To_authorization_Subject Access Review Spec ( a . ( * v1 . Subject Access Review Spec ) , b . ( * authorization . Subject Access Review if err := s . Add Generated Conversion Func ( ( * authorization . Subject Access Review Spec ) ( nil ) , ( * v1 . Subject Access Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Subject Access Review Spec_To_v1_Subject Access Review Spec ( a . ( * authorization . Subject Access Review Spec ) , b . ( * v1 . Subject Access Review if err := s . Add Generated Conversion Func ( ( * v1 . Subject Access Review Status ) ( nil ) , ( * authorization . Subject Access Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Subject Access Review Status_To_authorization_Subject Access Review Status ( a . ( * v1 . Subject Access Review Status ) , b . ( * authorization . Subject Access Review if err := s . Add Generated Conversion Func ( ( * authorization . Subject Access Review Status ) ( nil ) , ( * v1 . Subject Access Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Subject Access Review Status_To_v1_Subject Access Review Status ( a . ( * authorization . Subject Access Review Status ) , b . ( * v1 . Subject Access Review if err := s . Add Generated Conversion Func ( ( * v1 . Subject Rules Review Status ) ( nil ) , ( * authorization . Subject Rules Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Subject Rules Review Status_To_authorization_Subject Rules Review Status ( a . ( * v1 . Subject Rules Review Status ) , b . ( * authorization . Subject Rules Review if err := s . Add Generated Conversion Func ( ( * authorization . Subject Rules Review Status ) ( nil ) , ( * v1 . Subject Rules Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authorization_Subject Rules Review Status_To_v1_Subject Rules Review Status ( a . ( * authorization . Subject Rules Review Status ) , b . ( * v1 . Subject Rules Review } 
func Convert_v1_Local Subject Access Review_To_authorization_Local Subject Access Review ( in * v1 . Local Subject Access Review , out * authorization . Local Subject Access Review , s conversion . Scope ) error { return auto Convert_v1_Local Subject Access Review_To_authorization_Local Subject Access } 
func Convert_authorization_Local Subject Access Review_To_v1_Local Subject Access Review ( in * authorization . Local Subject Access Review , out * v1 . Local Subject Access Review , s conversion . Scope ) error { return auto Convert_authorization_Local Subject Access Review_To_v1_Local Subject Access } 
func Convert_v1_Non Resource Attributes_To_authorization_Non Resource Attributes ( in * v1 . Non Resource Attributes , out * authorization . Non Resource Attributes , s conversion . Scope ) error { return auto Convert_v1_Non Resource Attributes_To_authorization_Non Resource } 
func Convert_authorization_Non Resource Attributes_To_v1_Non Resource Attributes ( in * authorization . Non Resource Attributes , out * v1 . Non Resource Attributes , s conversion . Scope ) error { return auto Convert_authorization_Non Resource Attributes_To_v1_Non Resource } 
func Convert_v1_Non Resource Rule_To_authorization_Non Resource Rule ( in * v1 . Non Resource Rule , out * authorization . Non Resource Rule , s conversion . Scope ) error { return auto Convert_v1_Non Resource Rule_To_authorization_Non Resource } 
func Convert_authorization_Non Resource Rule_To_v1_Non Resource Rule ( in * authorization . Non Resource Rule , out * v1 . Non Resource Rule , s conversion . Scope ) error { return auto Convert_authorization_Non Resource Rule_To_v1_Non Resource } 
func Convert_v1_Resource Attributes_To_authorization_Resource Attributes ( in * v1 . Resource Attributes , out * authorization . Resource Attributes , s conversion . Scope ) error { return auto Convert_v1_Resource Attributes_To_authorization_Resource } 
func Convert_authorization_Resource Attributes_To_v1_Resource Attributes ( in * authorization . Resource Attributes , out * v1 . Resource Attributes , s conversion . Scope ) error { return auto Convert_authorization_Resource Attributes_To_v1_Resource } 
func Convert_v1_Resource Rule_To_authorization_Resource Rule ( in * v1 . Resource Rule , out * authorization . Resource Rule , s conversion . Scope ) error { return auto Convert_v1_Resource Rule_To_authorization_Resource } 
func Convert_authorization_Resource Rule_To_v1_Resource Rule ( in * authorization . Resource Rule , out * v1 . Resource Rule , s conversion . Scope ) error { return auto Convert_authorization_Resource Rule_To_v1_Resource } 
func Convert_v1_Self Subject Access Review_To_authorization_Self Subject Access Review ( in * v1 . Self Subject Access Review , out * authorization . Self Subject Access Review , s conversion . Scope ) error { return auto Convert_v1_Self Subject Access Review_To_authorization_Self Subject Access } 
func Convert_authorization_Self Subject Access Review_To_v1_Self Subject Access Review ( in * authorization . Self Subject Access Review , out * v1 . Self Subject Access Review , s conversion . Scope ) error { return auto Convert_authorization_Self Subject Access Review_To_v1_Self Subject Access } 
func Convert_v1_Self Subject Access Review Spec_To_authorization_Self Subject Access Review Spec ( in * v1 . Self Subject Access Review Spec , out * authorization . Self Subject Access Review Spec , s conversion . Scope ) error { return auto Convert_v1_Self Subject Access Review Spec_To_authorization_Self Subject Access Review } 
func Convert_authorization_Self Subject Access Review Spec_To_v1_Self Subject Access Review Spec ( in * authorization . Self Subject Access Review Spec , out * v1 . Self Subject Access Review Spec , s conversion . Scope ) error { return auto Convert_authorization_Self Subject Access Review Spec_To_v1_Self Subject Access Review } 
func Convert_v1_Self Subject Rules Review_To_authorization_Self Subject Rules Review ( in * v1 . Self Subject Rules Review , out * authorization . Self Subject Rules Review , s conversion . Scope ) error { return auto Convert_v1_Self Subject Rules Review_To_authorization_Self Subject Rules } 
func Convert_authorization_Self Subject Rules Review_To_v1_Self Subject Rules Review ( in * authorization . Self Subject Rules Review , out * v1 . Self Subject Rules Review , s conversion . Scope ) error { return auto Convert_authorization_Self Subject Rules Review_To_v1_Self Subject Rules } 
func Convert_v1_Self Subject Rules Review Spec_To_authorization_Self Subject Rules Review Spec ( in * v1 . Self Subject Rules Review Spec , out * authorization . Self Subject Rules Review Spec , s conversion . Scope ) error { return auto Convert_v1_Self Subject Rules Review Spec_To_authorization_Self Subject Rules Review } 
func Convert_authorization_Self Subject Rules Review Spec_To_v1_Self Subject Rules Review Spec ( in * authorization . Self Subject Rules Review Spec , out * v1 . Self Subject Rules Review Spec , s conversion . Scope ) error { return auto Convert_authorization_Self Subject Rules Review Spec_To_v1_Self Subject Rules Review } 
func Convert_v1_Subject Access Review_To_authorization_Subject Access Review ( in * v1 . Subject Access Review , out * authorization . Subject Access Review , s conversion . Scope ) error { return auto Convert_v1_Subject Access Review_To_authorization_Subject Access } 
func Convert_authorization_Subject Access Review_To_v1_Subject Access Review ( in * authorization . Subject Access Review , out * v1 . Subject Access Review , s conversion . Scope ) error { return auto Convert_authorization_Subject Access Review_To_v1_Subject Access } 
func Convert_v1_Subject Access Review Spec_To_authorization_Subject Access Review Spec ( in * v1 . Subject Access Review Spec , out * authorization . Subject Access Review Spec , s conversion . Scope ) error { return auto Convert_v1_Subject Access Review Spec_To_authorization_Subject Access Review } 
func Convert_authorization_Subject Access Review Spec_To_v1_Subject Access Review Spec ( in * authorization . Subject Access Review Spec , out * v1 . Subject Access Review Spec , s conversion . Scope ) error { return auto Convert_authorization_Subject Access Review Spec_To_v1_Subject Access Review } 
func Convert_v1_Subject Access Review Status_To_authorization_Subject Access Review Status ( in * v1 . Subject Access Review Status , out * authorization . Subject Access Review Status , s conversion . Scope ) error { return auto Convert_v1_Subject Access Review Status_To_authorization_Subject Access Review } 
func Convert_authorization_Subject Access Review Status_To_v1_Subject Access Review Status ( in * authorization . Subject Access Review Status , out * v1 . Subject Access Review Status , s conversion . Scope ) error { return auto Convert_authorization_Subject Access Review Status_To_v1_Subject Access Review } 
func Convert_v1_Subject Rules Review Status_To_authorization_Subject Rules Review Status ( in * v1 . Subject Rules Review Status , out * authorization . Subject Rules Review Status , s conversion . Scope ) error { return auto Convert_v1_Subject Rules Review Status_To_authorization_Subject Rules Review } 
func Convert_authorization_Subject Rules Review Status_To_v1_Subject Rules Review Status ( in * authorization . Subject Rules Review Status , out * v1 . Subject Rules Review Status , s conversion . Scope ) error { return auto Convert_authorization_Subject Rules Review Status_To_v1_Subject Rules Review } 
func New Endpoint Service Resolver ( services listersv1 . Service Lister , endpoints listersv1 . Endpoints Lister ) Service Resolver { return & aggregator Endpoint } 
func New Loopback Service Resolver ( delegate Service Resolver , host * url . URL ) Service Resolver { return & loopback } 
func new Prober ( runner kubecontainer . Container Command Runner , ref Manager * kubecontainer . Ref Manager , recorder record . Event Recorder ) * prober { const follow Non Local return & prober { exec : execprobe . New ( ) , readiness Http : httprobe . New ( follow Non Local Redirects ) , liveness Http : httprobe . New ( follow Non Local Redirects ) , tcp : tcprobe . New ( ) , runner : runner , ref Manager : ref } 
func ( pb * prober ) probe ( probe Type probe Type , pod * v1 . Pod , status v1 . Pod Status , container v1 . Container , container ID kubecontainer . Container ID ) ( results . Result , error ) { var probe switch probe Type { case readiness : probe Spec = container . Readiness case liveness : probe Spec = container . Liveness default : return results . Failure , fmt . Errorf ( " " , probe ctr if probe Spec == nil { klog . Warningf ( " " , probe Type , ctr result , output , err := pb . run Probe With Retries ( probe Type , probe Spec , pod , status , container , container ID , max Probe if err != nil || ( result != probe . Success && result != probe . Warning ) { // Probe failed in one way or another. ref , has Ref := pb . ref Manager . Get Ref ( container if ! has Ref { klog . Warningf ( " " , container ID . String ( ) , ctr if err != nil { klog . V ( 1 ) . Infof ( " " , probe Type , ctr if has Ref { pb . recorder . Eventf ( ref , v1 . Event Type Warning , events . Container Unhealthy , " " , probe } else { // result != probe.Success klog . V ( 1 ) . Infof ( " " , probe Type , ctr if has Ref { pb . recorder . Eventf ( ref , v1 . Event Type Warning , events . Container Unhealthy , " " , probe if result == probe . Warning { if ref , has Ref := pb . ref Manager . Get Ref ( container ID ) ; has Ref { pb . recorder . Eventf ( ref , v1 . Event Type Warning , events . Container Probe Warning , " " , probe klog . V ( 3 ) . Infof ( " " , probe Type , ctr } else { klog . V ( 3 ) . Infof ( " " , probe Type , ctr } 
func ( pb * prober ) run Probe With Retries ( probe Type probe Type , p * v1 . Probe , pod * v1 . Pod , status v1 . Pod Status , container v1 . Container , container ID kubecontainer . Container for i := 0 ; i < retries ; i ++ { result , output , err = pb . run Probe ( probe Type , p , pod , status , container , container } 
func build Header ( header List [ ] v1 . HTTP for _ , header := range header } 
func find Port By Name ( container v1 . Container , port Name string ) ( int , error ) { for _ , port := range container . Ports { if port . Name == port Name { return int ( port . Container return 0 , fmt . Errorf ( " " , port } 
func format u . Host = net . Join Host } 
func ( b * cinder Volume Mounter ) Set Up At ( dir string , fs Group * int64 ) error { klog . V ( 5 ) . Infof ( " " , b . pd b . plugin . volume Locks . Lock Key ( b . pd defer b . plugin . volume Locks . Unlock Key ( b . pd notmnt , err := b . mounter . Is Likely Not Mount if err != nil && ! os . Is Not global PD Path := make Global PD Name ( b . plugin . host , b . pd if b . read if err := os . Mkdir mount Options := util . Join Mount Options ( options , b . mount // Perform a bind mount to the full path to allow duplicate mounts of the same PD. klog . V ( 4 ) . Infof ( " " , b . pd Name , dir , mount err = b . mounter . Mount ( global PD notmnt , mnt Err := b . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! notmnt { if mnt Err = b . mounter . Unmount ( dir ) ; mnt Err != nil { klog . Errorf ( " " , mnt notmnt , mnt Err := b . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! notmnt { // This is very odd, we don't expect it. We'll try again next sync loop. klog . Errorf ( " " , b . Get if ! b . read Only { volume . Set Volume Ownership ( b , fs klog . V ( 3 ) . Infof ( " " , b . pd } 
func ( c * cinder Volume Unmounter ) Tear Down At ( dir string ) error { if path Exists , path Err := mount . Path Exists ( dir ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path notmnt , err := c . mounter . Is Likely Not Mount // Find Cinder volume ID to lock the right volume // TODO: refactor Volume Plugin.New Unmounter to get full volume.Spec just like // New Mounter. We could then find volume ID there without probing Mount Refs. refs , err := c . mounter . Get Mount c . pd klog . V ( 4 ) . Infof ( " " , c . pd // lock the volume (and thus wait for any concurrrent Set Up At to finish) c . plugin . volume Locks . Lock Key ( c . pd defer c . plugin . volume Locks . Unlock Key ( c . pd // Reload list of references, there might be Set Up At finished in the meantime refs , err = c . mounter . Get Mount notmnt , mnt Err := c . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt } 
func get Rbd Dev From Image And if dirs , err := ioutil . Read // First match pool, then match name. pool pool Bytes , err := ioutil . Read File ( pool if err != nil { klog . V ( 4 ) . Infof ( " " , pool if strings . Trim Space ( string ( pool Bytes ) ) != pool { klog . V ( 4 ) . Infof ( " " , name , pool , string ( pool img img Bytes , err := ioutil . Read File ( img if err != nil { klog . V ( 4 ) . Infof ( " " , img if strings . Trim Space ( string ( img Bytes ) ) != image { klog . V ( 4 ) . Infof ( " " , name , image , string ( img // Found a match, check if device exists. device if _ , err := os . Lstat ( device Path ) ; err == nil { return device } 
func get Nbd Dev From Image And Pool ( pool string , image string ) ( string , bool ) { // nbd module exports the pid of serving process in sysfs base // Do not change img Path format - some tools like rbd-nbd are strict about it. img max Nbds , max Nbds Err := get Max if max Nbds Err != nil { klog . V ( 4 ) . Infof ( " " , max Nbds for i := 0 ; i < max Nbds ; i ++ { nbd Path := base _ , err := os . Lstat ( nbd if err != nil { klog . V ( 4 ) . Infof ( " " , nbd pid Bytes , err := ioutil . Read File ( filepath . Join ( nbd if err != nil { klog . V ( 5 ) . Infof ( " " , nbd cmdline File Name := filepath . Join ( " " , strings . Trim Space ( string ( pid raw Cmdline , err := ioutil . Read File ( cmdline File if err != nil { klog . V ( 4 ) . Infof ( " " , cmdline File cmdline Args := strings . Fields Func ( string ( raw // Check if this process is mapping a rbd device. // Only accepted pattern of cmdline is from exec Rbd Map: // rbd-nbd map pool/image ... if len ( cmdline Args ) < 3 || cmdline Args [ 0 ] != " " || cmdline Args [ 1 ] != " " { klog . V ( 4 ) . Infof ( " " , nbd if cmdline Args [ 2 ] != img Path { klog . V ( 4 ) . Infof ( " " , nbd Path , img Path , cmdline device if _ , err := os . Lstat ( device Path ) ; err != nil { klog . Warningf ( " " , device Path , img return device } 
func wait For Path ( pool , image string , max Retries int , use Nbd Driver bool ) ( string , bool ) { for i := 0 ; i < max if use Nbd Driver { if device Path , found := get Nbd Dev From Image And Pool ( pool , image ) ; found { return device } else { if device Path , found := get Rbd Dev From Image And Pool ( pool , image ) ; found { return device } 
func exec Rbd Map ( b rbd Mounter , rbd Cmd string , mon string ) ( [ ] byte , error ) { // Commandline: rbd Cmd map img Path ... // do not change this format - some tools like rbd-nbd are strict about it. img if b . Secret != " " { return b . exec . Run ( rbd Cmd , " " , img } else { return b . exec . Run ( rbd Cmd , " " , img } 
func check Rbd Nbd } 
func make PD Name Internal ( host volume . Volume Host , pool string , image string ) string { // Backward compatibility for the deprecated format: /var/lib/kubelet/plugins/kubernetes.io/rbd/rbd/pool-image-image. deprecated Dir := filepath . Join ( host . Get Plugin Dir ( rbd Plugin info , err := os . Stat ( deprecated if err == nil && info . Is Dir ( ) { // The device mount path has already been created with the deprecated format, return it. klog . V ( 5 ) . Infof ( " " , deprecated return deprecated // Return the canonical format path. return filepath . Join ( host . Get Plugin Dir ( rbd Plugin Name ) , volutil . Mounts In Global PD } 
func make VDPD Name Internal ( host volume . Volume Host , pool string , image string ) string { return filepath . Join ( host . Get Volume Device Plugin Dir ( rbd Plugin } 
func ( util * RBD Util ) rbd Unlock ( b rbd if len ( b . admin Id ) == 0 { b . admin if len ( b . admin Secret ) == 0 { b . admin // Construct lock id using host name and a magic prefix. host Name , err := node . Get lock_id := kube Lock Magic + host mon := util . kernel RBD Monitors ind := strings . Last } 
func ( util * RBD Util ) Attach Disk ( b rbd global PD Path := util . Make Global PD if path Exists , path Err := mount . Path Exists ( global PD Path ) ; path Err != nil { return " " , fmt . Errorf ( " " , path } else if ! path Exists { if err := os . Mkdir All ( global PD // Evalute whether this device was mapped with rbd. device Path , mapped := wait For Path ( b . Pool , b . Image , 1 /*max Retries*/ , false /*use Nbd // If rbd-nbd tools are found, we will fallback to it should the default krbd driver fail. nbd Tools if ! mapped { nbd Tools Found = check Rbd Nbd if nbd Tools Found { device Path , mapped = wait For Path ( b . Pool , b . Image , 1 /*max Retries*/ , true /*use Nbd if ! mapped { // Currently, we don't acquire advisory lock on image, but for backward // compatibility, we need to check if the image is being used by nodes running old kubelet. // osd_client_watch_timeout defaults to 30 seconds, if the watcher stays active longer than 30 seconds, // rbd image does not get mounted and failure message gets generated. backoff := wait . Backoff { Duration : rbd Image Watcher Init Delay , Factor : rbd Image Watcher Factor , Steps : rbd Image Watcher need Valid if b . access Modes != nil { // If access Modes only contains Read Only Many, we don't need check rbd status of being used. if len ( b . access Modes ) == 1 && b . access Modes [ 0 ] == v1 . Read Only Many { need Valid // If access Modes is nil, the volume is referenced by in-line volume. // We can assume the Access Modes to be {"RWO" and "ROX"}, which is what the volume plugin supports. // We do not need to consider Read Only here, because it is used for Volume Mounts. if need Valid Used { err := wait . Exponential Backoff ( backoff , func ( ) ( bool , error ) { used , rbd Output , err := util . rbd if err != nil { return false , fmt . Errorf ( " " , err , rbd // Return error if rbd image has not become available for the specified timeout. if err == wait . Err Wait mon := util . kernel RBD Monitors output , err = exec Rbd if err != nil { if ! nbd Tools err output output , err = exec Rbd if err != nil { err List = append ( err output List = append ( output return " " , fmt . Errorf ( " " , errors . New Aggregate ( err List ) , string ( output device Path , mapped = wait For Path ( b . Pool , b . Image , 10 /*max Retries*/ , true /*use Nbd } else { device Path , mapped = wait For Path ( b . Pool , b . Image , 10 /*max Retries*/ , false /*use Nbd return device } 
func ( util * RBD Util ) Detach Disk ( plugin * rbd Plugin , device Mount exec := plugin . host . Get Exec ( plugin . Get Plugin var rbd // Unlike map, we cannot fallthrough for unmap // the tool to unmap is based on device type if strings . Has Prefix ( device , " " ) { rbd } else { rbd // rbd unmap output , err := exec . Run ( rbd if err != nil { return rbd // Currently, we don't persist rbd info on the disk, but for backward // compatbility, we need to clean it if found. rbd File := filepath . Join ( device Mount exists , err := utilpath . Exists ( utilpath . Check Follow Symlink , rbd if exists { klog . V ( 3 ) . Infof ( " " , device Mount err = util . clean Old RBD File ( plugin , rbd if err != nil { klog . Errorf ( " " , rbd klog . V ( 3 ) . Infof ( " " , rbd } 
func ( util * RBD Util ) Detach Block Disk ( disk rbd Disk Unmapper , map Path string ) error { if path Exists , path Err := mount . Path Exists ( map Path ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path Exists { klog . Warningf ( " " , map // If we arrive here, device is no longer used, see if we need to logout of the target device , err := get Block Volume Device ( map exec := disk . plugin . host . Get Exec ( disk . plugin . Get Plugin var rbd // Unlike map, we cannot fallthrough here. // Any nbd device must be unmapped by rbd-nbd if strings . Has Prefix ( device , " " ) { rbd } else { rbd // rbd unmap output , err := exec . Run ( rbd if err != nil { return rbd } 
func ( util * RBD Util ) clean Old RBD File ( plugin * rbd Plugin , rbd File string ) error { mounter := & rbd Mounter { // util.rbd Unlock needs it to run command. rbd : new fp , err := os . Open ( rbd if err != nil { return fmt . Errorf ( " " , rbd decoder := json . New if err != nil { klog . Errorf ( " " , rbd // Remove rbd lock if found. // The disk is not attached to this node anymore, so the lock on image // for this node can be removed safely. err = util . rbd if err == nil { os . Remove ( rbd } 
func ( util * RBD Util ) Expand Image ( rbd Expander * rbd Volume Expander , old Size resource . Quantity , new // Convert to MB that rbd defaults on. sz := int ( volumehelpers . Round Up To Mi B ( new new Vol new Size Quant := resource . Must // Check the current size of rbd image, if equals to or greater that the new request size, do nothing. cur Size , info Err := util . rbd Info ( rbd Expander . rbd if info Err != nil { return old Size , fmt . Errorf ( " " , info if cur Size >= sz { return new Size // rbd resize. mon := util . kernel RBD Monitors Opt ( rbd Expander . rbd klog . V ( 4 ) . Infof ( " " , rbd Expander . rbd Mounter . Image , mon , rbd Expander . rbd Mounter . Pool , rbd Expander . rbd Mounter . admin Id , rbd Expander . rbd Mounter . admin output , err = rbd Expander . exec . Run ( " " , " " , rbd Expander . rbd Mounter . Image , " " , new Vol Sz , " " , rbd Expander . rbd Mounter . Pool , " " , rbd Expander . rbd Mounter . admin Id , " " , mon , " " + rbd Expander . rbd Mounter . admin if err == nil { return new Size return old } 
func ( util * RBD Util ) rbd Info ( b * rbd // If we don't have admin id/secret (e.g. attaching), fallback to user id/secret. id := b . admin secret := b . admin mon := util . kernel RBD Monitors // cmd "rbd info" get the image info with the following output: // // # image exists (exit=0) // rbd info volume-4a5bcc8b-2b55-46da-ba04-0d3dc5227f08 // size 1024 MB in 256 objects // order 22 (4096 k if err , ok := err . ( * exec . Error ) ; ok { if err . Err == exec . Err Not return get Rbd Image } 
func ( util * RBD Util ) rbd Status ( b * rbd // If we don't have admin id/secret (e.g. attaching), fallback to user id/secret. id := b . admin secret := b . admin mon := util . kernel RBD Monitors if err , ok := err . ( * exec . Error ) ; ok { if err . Err == exec . Err Not if strings . Contains ( output , image Watcher } 
} 
} 
func ( c * roles ) Delete ( name string , options * v1 . Delete } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * API Service ) ( nil ) , ( * apiregistration . API Service ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_API Service_To_apiregistration_API Service ( a . ( * API Service ) , b . ( * apiregistration . API if err := s . Add Generated Conversion Func ( ( * apiregistration . API Service ) ( nil ) , ( * API Service ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiregistration_API Service_To_v1_API Service ( a . ( * apiregistration . API Service ) , b . ( * API if err := s . Add Generated Conversion Func ( ( * API Service Condition ) ( nil ) , ( * apiregistration . API Service Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_API Service Condition_To_apiregistration_API Service Condition ( a . ( * API Service Condition ) , b . ( * apiregistration . API Service if err := s . Add Generated Conversion Func ( ( * apiregistration . API Service Condition ) ( nil ) , ( * API Service Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiregistration_API Service Condition_To_v1_API Service Condition ( a . ( * apiregistration . API Service Condition ) , b . ( * API Service if err := s . Add Generated Conversion Func ( ( * API Service List ) ( nil ) , ( * apiregistration . API Service List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_API Service List_To_apiregistration_API Service List ( a . ( * API Service List ) , b . ( * apiregistration . API Service if err := s . Add Generated Conversion Func ( ( * apiregistration . API Service List ) ( nil ) , ( * API Service List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiregistration_API Service List_To_v1_API Service List ( a . ( * apiregistration . API Service List ) , b . ( * API Service if err := s . Add Generated Conversion Func ( ( * API Service Spec ) ( nil ) , ( * apiregistration . API Service Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_API Service Spec_To_apiregistration_API Service Spec ( a . ( * API Service Spec ) , b . ( * apiregistration . API Service if err := s . Add Generated Conversion Func ( ( * apiregistration . API Service Spec ) ( nil ) , ( * API Service Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiregistration_API Service Spec_To_v1_API Service Spec ( a . ( * apiregistration . API Service Spec ) , b . ( * API Service if err := s . Add Generated Conversion Func ( ( * API Service Status ) ( nil ) , ( * apiregistration . API Service Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_API Service Status_To_apiregistration_API Service Status ( a . ( * API Service Status ) , b . ( * apiregistration . API Service if err := s . Add Generated Conversion Func ( ( * apiregistration . API Service Status ) ( nil ) , ( * API Service Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiregistration_API Service Status_To_v1_API Service Status ( a . ( * apiregistration . API Service Status ) , b . ( * API Service if err := s . Add Generated Conversion Func ( ( * Service Reference ) ( nil ) , ( * apiregistration . Service Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Service Reference_To_apiregistration_Service Reference ( a . ( * Service Reference ) , b . ( * apiregistration . Service if err := s . Add Generated Conversion Func ( ( * apiregistration . Service Reference ) ( nil ) , ( * Service Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apiregistration_Service Reference_To_v1_Service Reference ( a . ( * apiregistration . Service Reference ) , b . ( * Service } 
func Convert_v1_API Service_To_apiregistration_API Service ( in * API Service , out * apiregistration . API Service , s conversion . Scope ) error { return auto Convert_v1_API Service_To_apiregistration_API } 
func Convert_apiregistration_API Service_To_v1_API Service ( in * apiregistration . API Service , out * API Service , s conversion . Scope ) error { return auto Convert_apiregistration_API Service_To_v1_API } 
func Convert_v1_API Service Condition_To_apiregistration_API Service Condition ( in * API Service Condition , out * apiregistration . API Service Condition , s conversion . Scope ) error { return auto Convert_v1_API Service Condition_To_apiregistration_API Service } 
func Convert_apiregistration_API Service Condition_To_v1_API Service Condition ( in * apiregistration . API Service Condition , out * API Service Condition , s conversion . Scope ) error { return auto Convert_apiregistration_API Service Condition_To_v1_API Service } 
func Convert_v1_API Service List_To_apiregistration_API Service List ( in * API Service List , out * apiregistration . API Service List , s conversion . Scope ) error { return auto Convert_v1_API Service List_To_apiregistration_API Service } 
func Convert_apiregistration_API Service List_To_v1_API Service List ( in * apiregistration . API Service List , out * API Service List , s conversion . Scope ) error { return auto Convert_apiregistration_API Service List_To_v1_API Service } 
func Convert_v1_API Service Spec_To_apiregistration_API Service Spec ( in * API Service Spec , out * apiregistration . API Service Spec , s conversion . Scope ) error { return auto Convert_v1_API Service Spec_To_apiregistration_API Service } 
func Convert_apiregistration_API Service Spec_To_v1_API Service Spec ( in * apiregistration . API Service Spec , out * API Service Spec , s conversion . Scope ) error { return auto Convert_apiregistration_API Service Spec_To_v1_API Service } 
func Convert_v1_API Service Status_To_apiregistration_API Service Status ( in * API Service Status , out * apiregistration . API Service Status , s conversion . Scope ) error { return auto Convert_v1_API Service Status_To_apiregistration_API Service } 
func Convert_apiregistration_API Service Status_To_v1_API Service Status ( in * apiregistration . API Service Status , out * API Service Status , s conversion . Scope ) error { return auto Convert_apiregistration_API Service Status_To_v1_API Service } 
func Convert_v1_Service Reference_To_apiregistration_Service Reference ( in * Service Reference , out * apiregistration . Service Reference , s conversion . Scope ) error { return auto Convert_v1_Service Reference_To_apiregistration_Service } 
func Convert_apiregistration_Service Reference_To_v1_Service Reference ( in * apiregistration . Service Reference , out * Service Reference , s conversion . Scope ) error { return auto Convert_apiregistration_Service Reference_To_v1_Service } 
func ( plugin * Pod Security Policy Plugin ) Validate Initialization ( ) error { if plugin . authz == nil { return fmt . Errorf ( " " , Plugin if plugin . lister == nil { return fmt . Errorf ( " " , Plugin } 
func new Plugin ( strategy Factory psp . Strategy Factory , fail On No Policies bool ) * Pod Security Policy Plugin { return & Pod Security Policy Plugin { Handler : admission . New Handler ( admission . Create , admission . Update ) , strategy Factory : strategy Factory , fail On No Policies : fail On No } 
func ( c * Pod Security Policy Plugin ) Admit ( a admission . Attributes , o admission . Object Interfaces ) error { if ignore , err := should // only mutate if this is a CREATE request. On updates we only validate. // TODO(liggitt): allow spec mutation during initializing updates? if a . Get pod := a . Get // compute the context. Mutation is allowed. Validated PSP Annotation is not taken into account. allowed Pod , psp Name , validation Errs , err := c . compute Security if err != nil { return admission . New if allowed Pod != nil { * pod = * allowed // annotate and accept the pod klog . V ( 4 ) . Infof ( " " , pod . Name , pod . Generate Name , a . Get Namespace ( ) , psp if pod . Object Meta . Annotations == nil { pod . Object pod . Object Meta . Annotations [ psputil . Validated PSP Annotation ] = psp key := audit Key if err := a . Add Annotation ( key , psp Name ) ; err != nil { klog . Warningf ( " " , key , psp // we didn't validate against any provider, reject the pod and give the errors for each attempt klog . V ( 4 ) . Infof ( " " , pod . Name , pod . Generate Name , a . Get Namespace ( ) , validation return admission . New Forbidden ( a , fmt . Errorf ( " " , validation } 
func ( c * Pod Security Policy Plugin ) compute Security Context ( a admission . Attributes , pod * api . Pod , spec Mutation Allowed bool , validated PSP Hint string ) ( * api . Pod , string , field . Error List , error ) { // get all constraints that are usable by the user klog . V ( 4 ) . Infof ( " " , pod . Name , pod . Generate var sa if len ( pod . Spec . Service Account Name ) > 0 { sa Info = serviceaccount . User Info ( a . Get Namespace ( ) , pod . Spec . Service Account // if we have no policies and want to succeed then return. Otherwise we'll end up with no // providers and fail with "unable to validate against any pod security policy" below. if len ( policies ) == 0 && ! c . fail On No // sort policies by name to make order deterministic // If mutation is not allowed and validated PSP Hint is provided, check the validated policy first. // TODO(liggitt): add priority field to allow admins to bucket differently sort . Slice Stable ( policies , func ( i , j int ) bool { if ! spec Mutation Allowed { if policies [ i ] . Name == validated PSP if policies [ j ] . Name == validated PSP providers , errs := c . create Providers From var ( allowed Mutated allowing Mutating // Map of PSP name to associated validation errors. validation Errs = map [ string ] field . Error for _ , provider := range providers { pod Copy := pod . Deep if errs := assign Security Context ( provider , pod Copy ) ; len ( errs ) > 0 { validation Errs [ provider . Get PSP // the entire pod validated mutated := ! apiequality . Semantic . Deep Equal ( pod , pod if mutated && ! spec Mutation if ! is Authorized For Policy ( a . Get User Info ( ) , sa Info , a . Get Namespace ( ) , provider . Get PSP switch { case ! mutated : // if it validated without mutating anything, use this result return pod Copy , provider . Get PSP case spec Mutation Allowed && allowed Mutated Pod == nil : // if mutation is allowed and this is the first PSP to allow the pod, remember it, // but continue to see if another PSP allows without mutating allowed Mutated Pod = pod allowing Mutating PSP = provider . Get PSP if allowed Mutated Pod != nil { return allowed Mutated Pod , allowing Mutating // Pod is rejected. Filter the validation errors to only include errors from authorized PS Ps. aggregate := field . Error for psp , errs := range validation Errs { if is Authorized For Policy ( a . Get User Info ( ) , sa Info , a . Get } 
func assign Security Context ( provider psp . Provider , pod * api . Pod ) field . Error List { errs := field . Error if err := provider . Mutate Pod ( pod ) ; err != nil { // TODO(tallclair): Mutate Pod should return a field.Error List errs = append ( errs , field . Invalid ( field . New errs = append ( errs , provider . Validate } 
func ( c * Pod Security Policy Plugin ) create Providers From Policies ( psps [ ] * policyv1beta1 . Pod Security for _ , constraint := range psps { provider , err := psp . New Simple Provider ( constraint , namespace , c . strategy } 
func authorized For Policy ( info user . Info , namespace string , policy Name string , authz authorizer . Authorizer ) bool { // Check against extensions API group for backward compatibility return authorized For Policy In API Group ( info , namespace , policy Name , policy . Group Name , authz ) || authorized For Policy In API Group ( info , namespace , policy Name , extensions . Group } 
func authorized For Policy In API Group ( info user . Info , namespace , policy Name , api Group attr := build Attributes ( info , namespace , policy Name , api Group return ( decision == authorizer . Decision } 
func build Attributes ( info user . Info , namespace , policy Name , api Group Name string ) authorizer . Attributes { // check against the namespace that the pod is being created in to allow per-namespace PSP grants. attr := authorizer . Attributes Record { User : info , Verb : " " , Namespace : namespace , Name : policy Name , API Group : api Group Name , Resource : " " , Resource } 
func ( in * Event ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Event Time . Deep Copy Into ( & out . Event * out = new ( Event ( * in ) . Deep Copy * out = new ( v1 . Object out . Deprecated Source = in . Deprecated in . Deprecated First Timestamp . Deep Copy Into ( & out . Deprecated First in . Deprecated Last Timestamp . Deep Copy Into ( & out . Deprecated Last } 
func Register ( ) { // Register the metrics. register Metrics . Do ( func ( ) { for _ , metric := range metrics List { prometheus . Must persistentvolume . Register Volume Scheduling } 
func ( c * Networking V1beta1Client ) REST return c . rest } 
func ( e Primitive Element ) Merge ( v Strategy ) ( Result , error ) { return v . Merge } 
func ( e Primitive Element ) Has Conflict ( ) error { if e . Has Recorded ( ) && e . Has Remote ( ) { if ! reflect . Deep Equal ( e . Get Recorded ( ) , e . Get Remote ( ) ) { return New Conflict if e . Has Recorded ( ) && ! e . Has Remote ( ) { return New Conflict } 
func New Path Recorder Mux ( name string ) * Path Recorder Mux { ret := & Path Recorder Mux { name : name , path To Handler : map [ string ] http . Handler { } , prefix To Handler : map [ string ] http . Handler { } , mux : atomic . Value { } , exposed Paths : [ ] string { } , path ret . mux . Store ( & path Handler { not Found Handler : http . Not Found } 
func ( m * Path Recorder Mux ) Listed Paths ( ) [ ] string { handled Paths := append ( [ ] string { } , m . exposed sort . Strings ( handled return handled } 
func ( m * Path Recorder Mux ) refresh Mux Locked ( ) { new Mux := & path Handler { mux Name : m . name , path To Handler : map [ string ] http . Handler { } , prefix Handlers : [ ] prefix Handler { } , not Found Handler : http . Not Found if m . not Found Handler != nil { new Mux . not Found Handler = m . not Found for path , handler := range m . path To Handler { new Mux . path To keys := sets . String Key Set ( m . prefix To sort . Sort ( sort . Reverse ( by Prefix for _ , prefix := range keys { new Mux . prefix Handlers = append ( new Mux . prefix Handlers , prefix Handler { prefix : prefix , handler : m . prefix To m . mux . Store ( new } 
func ( m * Path Recorder Mux ) Not Found Handler ( not Found m . not Found Handler = not Found m . refresh Mux } 
func ( m * Path Recorder delete ( m . path To delete ( m . prefix To delete ( m . path for i := range m . exposed Paths { if m . exposed Paths [ i ] == path { m . exposed Paths = append ( m . exposed Paths [ : i ] , m . exposed m . refresh Mux } 
func ( m * Path Recorder m . track m . exposed Paths = append ( m . exposed m . path To m . refresh Mux } 
func ( m * Path Recorder Mux ) Handle Func ( path string , handler func ( http . Response Writer , * http . Request ) ) { m . Handle ( path , http . Handler } 
func ( m * Path Recorder Mux ) Unlisted m . track m . path To m . refresh Mux } 
func ( m * Path Recorder Mux ) Unlisted Handle Func ( path string , handler func ( http . Response Writer , * http . Request ) ) { m . Unlisted Handle ( path , http . Handler } 
func ( m * Path Recorder Mux ) Unlisted Handle Prefix ( path string , handler http . Handler ) { if ! strings . Has m . track m . prefix To m . refresh Mux } 
func ( m * Path Recorder Mux ) Serve HTTP ( w http . Response Writer , r * http . Request ) { m . mux . Load ( ) . ( * path Handler ) . Serve } 
func ( h * path Handler ) Serve HTTP ( w http . Response Writer , r * http . Request ) { if exact Handler , ok := h . path To Handler [ r . URL . Path ] ; ok { klog . V ( 5 ) . Infof ( " " , h . mux exact Handler . Serve for _ , prefix Handler := range h . prefix Handlers { if strings . Has Prefix ( r . URL . Path , prefix Handler . prefix ) { klog . V ( 5 ) . Infof ( " " , h . mux Name , r . URL . Path , prefix prefix Handler . handler . Serve klog . V ( 5 ) . Infof ( " " , h . mux h . not Found Handler . Serve } 
func ( c * Clientset ) Node V1alpha1 ( ) nodev1alpha1 . Node V1alpha1Interface { return & fakenodev1alpha1 . Fake Node } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Storage Class ) ( nil ) , ( * storage . Storage Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Storage Class_To_storage_Storage Class ( a . ( * v1 . Storage Class ) , b . ( * storage . Storage if err := s . Add Generated Conversion Func ( ( * storage . Storage Class ) ( nil ) , ( * v1 . Storage Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Storage Class_To_v1_Storage Class ( a . ( * storage . Storage Class ) , b . ( * v1 . Storage if err := s . Add Generated Conversion Func ( ( * v1 . Storage Class List ) ( nil ) , ( * storage . Storage Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Storage Class List_To_storage_Storage Class List ( a . ( * v1 . Storage Class List ) , b . ( * storage . Storage Class if err := s . Add Generated Conversion Func ( ( * storage . Storage Class List ) ( nil ) , ( * v1 . Storage Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Storage Class List_To_v1_Storage Class List ( a . ( * storage . Storage Class List ) , b . ( * v1 . Storage Class if err := s . Add Generated Conversion Func ( ( * v1 . Volume Attachment ) ( nil ) , ( * storage . Volume Attachment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Attachment_To_storage_Volume Attachment ( a . ( * v1 . Volume Attachment ) , b . ( * storage . Volume if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment ) ( nil ) , ( * v1 . Volume Attachment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment_To_v1_Volume Attachment ( a . ( * storage . Volume Attachment ) , b . ( * v1 . Volume if err := s . Add Generated Conversion Func ( ( * v1 . Volume Attachment List ) ( nil ) , ( * storage . Volume Attachment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Attachment List_To_storage_Volume Attachment List ( a . ( * v1 . Volume Attachment List ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment List ) ( nil ) , ( * v1 . Volume Attachment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment List_To_v1_Volume Attachment List ( a . ( * storage . Volume Attachment List ) , b . ( * v1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1 . Volume Attachment Source ) ( nil ) , ( * storage . Volume Attachment Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Attachment Source_To_storage_Volume Attachment Source ( a . ( * v1 . Volume Attachment Source ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment Source ) ( nil ) , ( * v1 . Volume Attachment Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment Source_To_v1_Volume Attachment Source ( a . ( * storage . Volume Attachment Source ) , b . ( * v1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1 . Volume Attachment Spec ) ( nil ) , ( * storage . Volume Attachment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Attachment Spec_To_storage_Volume Attachment Spec ( a . ( * v1 . Volume Attachment Spec ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment Spec ) ( nil ) , ( * v1 . Volume Attachment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment Spec_To_v1_Volume Attachment Spec ( a . ( * storage . Volume Attachment Spec ) , b . ( * v1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1 . Volume Attachment Status ) ( nil ) , ( * storage . Volume Attachment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Attachment Status_To_storage_Volume Attachment Status ( a . ( * v1 . Volume Attachment Status ) , b . ( * storage . Volume Attachment if err := s . Add Generated Conversion Func ( ( * storage . Volume Attachment Status ) ( nil ) , ( * v1 . Volume Attachment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Attachment Status_To_v1_Volume Attachment Status ( a . ( * storage . Volume Attachment Status ) , b . ( * v1 . Volume Attachment if err := s . Add Generated Conversion Func ( ( * v1 . Volume Error ) ( nil ) , ( * storage . Volume Error ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Volume Error_To_storage_Volume Error ( a . ( * v1 . Volume Error ) , b . ( * storage . Volume if err := s . Add Generated Conversion Func ( ( * storage . Volume Error ) ( nil ) , ( * v1 . Volume Error ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_storage_Volume Error_To_v1_Volume Error ( a . ( * storage . Volume Error ) , b . ( * v1 . Volume } 
func Convert_v1_Storage Class_To_storage_Storage Class ( in * v1 . Storage Class , out * storage . Storage Class , s conversion . Scope ) error { return auto Convert_v1_Storage Class_To_storage_Storage } 
func Convert_storage_Storage Class_To_v1_Storage Class ( in * storage . Storage Class , out * v1 . Storage Class , s conversion . Scope ) error { return auto Convert_storage_Storage Class_To_v1_Storage } 
func Convert_v1_Storage Class List_To_storage_Storage Class List ( in * v1 . Storage Class List , out * storage . Storage Class List , s conversion . Scope ) error { return auto Convert_v1_Storage Class List_To_storage_Storage Class } 
func Convert_storage_Storage Class List_To_v1_Storage Class List ( in * storage . Storage Class List , out * v1 . Storage Class List , s conversion . Scope ) error { return auto Convert_storage_Storage Class List_To_v1_Storage Class } 
func Convert_v1_Volume Attachment_To_storage_Volume Attachment ( in * v1 . Volume Attachment , out * storage . Volume Attachment , s conversion . Scope ) error { return auto Convert_v1_Volume Attachment_To_storage_Volume } 
func Convert_storage_Volume Attachment_To_v1_Volume Attachment ( in * storage . Volume Attachment , out * v1 . Volume Attachment , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment_To_v1_Volume } 
func Convert_v1_Volume Attachment List_To_storage_Volume Attachment List ( in * v1 . Volume Attachment List , out * storage . Volume Attachment List , s conversion . Scope ) error { return auto Convert_v1_Volume Attachment List_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment List_To_v1_Volume Attachment List ( in * storage . Volume Attachment List , out * v1 . Volume Attachment List , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment List_To_v1_Volume Attachment } 
func Convert_v1_Volume Attachment Source_To_storage_Volume Attachment Source ( in * v1 . Volume Attachment Source , out * storage . Volume Attachment Source , s conversion . Scope ) error { return auto Convert_v1_Volume Attachment Source_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment Source_To_v1_Volume Attachment Source ( in * storage . Volume Attachment Source , out * v1 . Volume Attachment Source , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment Source_To_v1_Volume Attachment } 
func Convert_v1_Volume Attachment Spec_To_storage_Volume Attachment Spec ( in * v1 . Volume Attachment Spec , out * storage . Volume Attachment Spec , s conversion . Scope ) error { return auto Convert_v1_Volume Attachment Spec_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment Spec_To_v1_Volume Attachment Spec ( in * storage . Volume Attachment Spec , out * v1 . Volume Attachment Spec , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment Spec_To_v1_Volume Attachment } 
func Convert_v1_Volume Attachment Status_To_storage_Volume Attachment Status ( in * v1 . Volume Attachment Status , out * storage . Volume Attachment Status , s conversion . Scope ) error { return auto Convert_v1_Volume Attachment Status_To_storage_Volume Attachment } 
func Convert_storage_Volume Attachment Status_To_v1_Volume Attachment Status ( in * storage . Volume Attachment Status , out * v1 . Volume Attachment Status , s conversion . Scope ) error { return auto Convert_storage_Volume Attachment Status_To_v1_Volume Attachment } 
func Convert_v1_Volume Error_To_storage_Volume Error ( in * v1 . Volume Error , out * storage . Volume Error , s conversion . Scope ) error { return auto Convert_v1_Volume Error_To_storage_Volume } 
func Convert_storage_Volume Error_To_v1_Volume Error ( in * storage . Volume Error , out * v1 . Volume Error , s conversion . Scope ) error { return auto Convert_storage_Volume Error_To_v1_Volume } 
func Update Resource ( r rest . Updater , scope * Request Scope , admit admission . Interface ) http . Handler Func { return func ( w http . Response defer trace . Log If if is Dry Run ( req . URL ) && ! utilfeature . Default Feature Gate . Enabled ( features . Dry Run ) { scope . err ( errors . New Bad // TODO: we either want to remove timeout or document it (if we document, move timeout out of this function and declare it in api_installer) timeout := parse ctx = request . With output Media Type , _ , err := negotiation . Negotiate Output Media body , err := limited Read Body ( req , scope . Max Request Body options := & metav1 . Update if err := metainternalversion . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , scope . Meta Group Version , options ) ; err != nil { err = errors . New Bad if errs := validation . Validate Update Options ( options ) ; len ( errs ) > 0 { err := errors . New Invalid ( schema . Group Kind { Group : metav1 . Group s , err := negotiation . Negotiate Input default decoder := scope . Serializer . Decoder To Version ( s . Serializer , scope . Hub Group obj , gvk , err := decoder . Decode ( body , & default if err != nil { err = transform Decode if gvk . Group Version ( ) != default GVK . Group Version ( ) { err = errors . New Bad Request ( fmt . Sprintf ( " " , gvk . Group Version ( ) , default GVK . Group ae := request . Audit Event audit . Log Request admit = admission . With if err := check user Info , _ := request . User transformers := [ ] rest . Transform if scope . Field Manager != nil { transformers = append ( transformers , func ( _ context . Context , new Obj , live Obj runtime . Object ) ( runtime . Object , error ) { obj , err := scope . Field Manager . Update ( live Obj , new Obj , manager Or User Agent ( options . Field Manager , req . User if mutating Admission , ok := admit . ( admission . Mutation Interface ) ; ok { transformers = append ( transformers , func ( ctx context . Context , new Obj , old Obj runtime . Object ) ( runtime . Object , error ) { is Not Zero Object , err := has UID ( old } else if ! is Not Zero Object { if mutating Admission . Handles ( admission . Create ) { return new Obj , mutating Admission . Admit ( admission . New Attributes Record ( new Obj , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Create , dryrun . Is Dry Run ( options . Dry Run ) , user } else { if mutating Admission . Handles ( admission . Update ) { return new Obj , mutating Admission . Admit ( admission . New Attributes Record ( new Obj , old Obj , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Update , dryrun . Is Dry Run ( options . Dry Run ) , user return new create Authorizer Attributes := authorizer . Attributes Record { User : user Info , Resource Request : true , Path : req . URL . Path , Verb : " " , API Group : scope . Resource . Group , API was result , err := finish Request ( timeout , func ( ) ( runtime . Object , error ) { obj , created , err := r . Update ( ctx , name , rest . Default Updated Object Info ( obj , transformers ... ) , with Authorization ( rest . Admission To Validate Object Func ( admit , admission . New Attributes Record ( nil , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Create , dryrun . Is Dry Run ( options . Dry Run ) , user Info ) , scope ) , scope . Authorizer , create Authorizer Attributes ) , rest . Admission To Validate Object Update Func ( admit , admission . New Attributes Record ( nil , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Update , dryrun . Is Dry Run ( options . Dry Run ) , user was status := http . Status if was Created { status = http . Status transform Response Object ( ctx , scope , trace , req , w , status , output Media } 
func ( attacher * vsphere VMDK Attacher ) Attach ( spec * volume . Spec , node Name types . Node Name ) ( string , error ) { volume Source , _ , err := get Volume klog . V ( 4 ) . Infof ( " " , node // Keeps concurrent attach operations to same host atomic attachdetach Mutex . Lock Key ( string ( node defer attachdetach Mutex . Unlock Key ( string ( node // vsphere Cloud.Attach Disk checks if disk is already attached to host and // succeeds in that case, so no need to do that separately. disk UUID , err := attacher . vsphere Volumes . Attach Disk ( volume Source . Volume Path , volume Source . Storage Policy Name , node if err != nil { klog . Errorf ( " " , volume Source . Volume Path , node return filepath . Join ( disk By ID Path , disk SCSI Prefix + disk } 
func ( attacher * vsphere VMDK Attacher ) Get Device Mount Path ( spec * volume . Spec ) ( string , error ) { volume Source , _ , err := get Volume return make Global PD Path ( attacher . host , volume Source . Volume } 
func ( plugin * vsphere Volume Plugin ) Get Device Mount Refs ( device Mount Path string ) ( [ ] string , error ) { mounter := plugin . host . Get Mounter ( plugin . Get Plugin return mounter . Get Mount Refs ( device Mount } 
func ( detacher * vsphere VMDK Detacher ) Detach ( volume Name string , node Name types . Node Name ) error { vol Path := get Vol Pathfrom Volume Name ( volume attached , err := detacher . vsphere Volumes . Disk Is Attached ( vol Path , node if err != nil { // Log error and continue with detach klog . Errorf ( " " , vol Path , node if err == nil && ! attached { // Volume is already detached from node. klog . Infof ( " " , vol Path , node attachdetach Mutex . Lock Key ( string ( node defer attachdetach Mutex . Unlock Key ( string ( node if err := detacher . vsphere Volumes . Detach Disk ( vol Path , node Name ) ; err != nil { klog . Errorf ( " " , vol } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta2 . Daemon Set { } , func ( obj interface { } ) { Set Object Defaults_Daemon Set ( obj . ( * v1beta2 . Daemon scheme . Add Type Defaulting Func ( & v1beta2 . Daemon Set List { } , func ( obj interface { } ) { Set Object Defaults_Daemon Set List ( obj . ( * v1beta2 . Daemon Set scheme . Add Type Defaulting Func ( & v1beta2 . Deployment { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1beta2 . Deployment List { } , func ( obj interface { } ) { Set Object Defaults_Deployment List ( obj . ( * v1beta2 . Deployment scheme . Add Type Defaulting Func ( & v1beta2 . Replica Set { } , func ( obj interface { } ) { Set Object Defaults_Replica Set ( obj . ( * v1beta2 . Replica scheme . Add Type Defaulting Func ( & v1beta2 . Replica Set List { } , func ( obj interface { } ) { Set Object Defaults_Replica Set List ( obj . ( * v1beta2 . Replica Set scheme . Add Type Defaulting Func ( & v1beta2 . Stateful Set { } , func ( obj interface { } ) { Set Object Defaults_Stateful Set ( obj . ( * v1beta2 . Stateful scheme . Add Type Defaulting Func ( & v1beta2 . Stateful Set List { } , func ( obj interface { } ) { Set Object Defaults_Stateful Set List ( obj . ( * v1beta2 . Stateful Set } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta2 . Scale Spec ) ( nil ) , ( * scheme . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Scale Spec_To_scheme_Scale Spec ( a . ( * v1beta2 . Scale Spec ) , b . ( * scheme . Scale if err := s . Add Generated Conversion Func ( ( * scheme . Scale Spec ) ( nil ) , ( * v1beta2 . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheme_Scale Spec_To_v1beta2_Scale Spec ( a . ( * scheme . Scale Spec ) , b . ( * v1beta2 . Scale if err := s . Add Generated Conversion Func ( ( * v1beta2 . Scale Status ) ( nil ) , ( * scheme . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Scale Status_To_scheme_Scale Status ( a . ( * v1beta2 . Scale Status ) , b . ( * scheme . Scale if err := s . Add Generated Conversion Func ( ( * scheme . Scale Status ) ( nil ) , ( * v1beta2 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheme_Scale Status_To_v1beta2_Scale Status ( a . ( * scheme . Scale Status ) , b . ( * v1beta2 . Scale if err := s . Add Conversion Func ( ( * scheme . Scale Status ) ( nil ) , ( * v1beta2 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheme_Scale Status_To_v1beta2_Scale Status ( a . ( * scheme . Scale Status ) , b . ( * v1beta2 . Scale if err := s . Add Conversion Func ( ( * v1beta2 . Scale Status ) ( nil ) , ( * scheme . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta2_Scale Status_To_scheme_Scale Status ( a . ( * v1beta2 . Scale Status ) , b . ( * scheme . Scale } 
func Convert_v1beta2_Scale_To_scheme_Scale ( in * v1beta2 . Scale , out * scheme . Scale , s conversion . Scope ) error { return auto } 
func Convert_scheme_Scale_To_v1beta2_Scale ( in * scheme . Scale , out * v1beta2 . Scale , s conversion . Scope ) error { return auto } 
func Convert_v1beta2_Scale Spec_To_scheme_Scale Spec ( in * v1beta2 . Scale Spec , out * scheme . Scale Spec , s conversion . Scope ) error { return auto Convert_v1beta2_Scale Spec_To_scheme_Scale } 
func Convert_scheme_Scale Spec_To_v1beta2_Scale Spec ( in * scheme . Scale Spec , out * v1beta2 . Scale Spec , s conversion . Scope ) error { return auto Convert_scheme_Scale Spec_To_v1beta2_Scale } 
func ( r * Rolling Updater ) scale Up ( new Rc , old Rc * corev1 . Replication Controller , desired , max Surge , max Unavailable int32 , scale Retry Params * Retry Params , config * Rolling Updater Config ) ( * corev1 . Replication Controller , error ) { // If we're already at the desired, do nothing. if val Or Zero ( new Rc . Spec . Replicas ) == desired { return new // Scale up as far as we can based on the surge limit. increment := ( desired + max Surge ) - ( val Or Zero ( old Rc . Spec . Replicas ) + val Or Zero ( new // If the old is already scaled down, go ahead and scale all the way up. if val Or Zero ( old Rc . Spec . Replicas ) == 0 { increment = desired - val Or Zero ( new // We can't scale up without violating the surge limit, so do nothing. if increment <= 0 { return new // Increase the replica count, and deal with fenceposts. next Val := val Or Zero ( new new Rc . Spec . Replicas = & next if val Or Zero ( new Rc . Spec . Replicas ) > desired { new // Perform the scale-up. fmt . Fprintf ( config . Out , " \n " , new Rc . Name , val Or Zero ( new scaled Rc , err := r . scale And Wait ( new Rc , scale Retry Params , scale Retry return scaled } 
func ( r * Rolling Updater ) get Or Create Target Controller With Client ( controller * corev1 . Replication Controller , source ID string ) ( * corev1 . Replication Controller , bool , error ) { existing Rc , err := r . existing if err != nil { if ! errors . Is Not if val Or Zero ( controller . Spec . Replicas ) <= 0 { return nil , false , fmt . Errorf ( " " , controller . Name , val Or controller . Annotations [ desired Replicas Annotation ] = fmt . Sprintf ( " " , val Or controller . Annotations [ source ID Annotation ] = source new Rc , err := r . rc Client . Replication return new // Validate and use the existing controller. annotations := existing source := annotations [ source ID _ , ok := annotations [ desired Replicas if source != source ID || ! ok { return nil , false , fmt . Errorf ( " " , controller . Name , source return existing } 
func ( r * Rolling Updater ) existing Controller ( controller * corev1 . Replication Controller ) ( * corev1 . Replication Controller , error ) { // without rc name but generate name, there's no existing rc if len ( controller . Name ) == 0 && len ( controller . Generate Name ) > 0 { return nil , errors . New Not // controller name is required to get rc back return r . rc Client . Replication Controllers ( controller . Namespace ) . Get ( controller . Name , metav1 . Get } 
func update Rc With Retries ( rc Client corev1client . Replication Controllers Getter , namespace string , rc * corev1 . Replication Controller , apply Update update Rc Func ) ( * corev1 . Replication Controller , error ) { // Deep copy the rc in case we failed on Get during retry loop old Rc := rc . Deep err := retry . Retry On Conflict ( retry . Default Backoff , func ( ) ( e error ) { // Apply the update, then attempt to push it to the apiserver. apply if rc , e = rc Client . Replication update // Update the controller with the latest resource version, if the update failed we // can't trust rc so use old Rc.Name. if rc , e = rc Client . Replication Controllers ( namespace ) . Get ( old Rc . Name , metav1 . Get Options { } ) ; e != nil { // The Get failed: Value in rc cannot be trusted. rc = old // Only return the error from update return update } 
func update Pod With Retries ( pod Client corev1client . Pods Getter , namespace string , pod * corev1 . Pod , apply Update update Pod Func ) ( * corev1 . Pod , error ) { // Deep copy the pod in case we failed on Get during retry loop old Pod := pod . Deep err := retry . Retry On Conflict ( retry . Default Backoff , func ( ) ( e error ) { // Apply the update, then attempt to push it to the apiserver. apply if pod , e = pod update if pod , e = pod Client . Pods ( namespace ) . Get ( old Pod . Name , metav1 . Get Options { } ) ; e != nil { pod = old // Only return the error from update return update } 
func ( c * Batch V2alpha1Client ) REST return c . rest } 
func Serve Port Forward ( w http . Response Writer , req * http . Request , port Forwarder Port Forwarder , pod Name string , uid types . UID , port Forward Options * V4Options , idle Timeout time . Duration , stream Creation Timeout time . Duration , supported if wsstream . Is Web Socket Request ( req ) { err = handle Web Socket Streams ( req , w , port Forwarder , pod Name , uid , port Forward Options , supported Protocols , idle Timeout , stream Creation } else { err = handle HTTP Streams ( req , w , port Forwarder , pod Name , uid , supported Protocols , idle Timeout , stream Creation if err != nil { runtime . Handle } 
func Retrieve Validated Config Info ( filepath , clustername string ) ( * clientcmdapi . Config , error ) { config , err := clientcmd . Load From return Validate Config } 
func Validate Config Info ( config * clientcmdapi . Config , clustername string ) ( * clientcmdapi . Config , error ) { err := validate Kube // This is the cluster object we've got from the cluster-info kubeconfig file default Cluster := kubeconfigutil . Get Cluster From Kube // Create a new kubeconfig object from the given, just copy over the server and the CA cert // We do this in order to not pick up other possible misconfigurations in the clusterinfo file kubeconfig := kubeconfigutil . Create Basic ( default Cluster . Server , clustername , " " , // no user provided default Cluster . Certificate Authority // load pre-existing client certificates if config . Contexts [ config . Current Context ] != nil && len ( config . Auth Infos ) > 0 { user := config . Contexts [ config . Current Context ] . Auth auth Info , ok := config . Auth if ! ok || auth if len ( auth Info . Client Certificate Data ) == 0 && len ( auth Info . Client Certificate ) != 0 { client Cert , err := ioutil . Read File ( auth Info . Client auth Info . Client Certificate Data = client if len ( auth Info . Client Key Data ) == 0 && len ( auth Info . Client Key ) != 0 { client Key , err := ioutil . Read File ( auth Info . Client auth Info . Client Key Data = client if len ( auth Info . Client Certificate Data ) == 0 || len ( auth Info . Client Key kubeconfig = kubeconfigutil . Create With Certs ( default Cluster . Server , clustername , " " , // no user provided default Cluster . Certificate Authority Data , auth Info . Client Key Data , auth Info . Client Certificate client , err := kubeconfigutil . To Client klog . V ( 1 ) . Infof ( " \n " , default var clusterinfo CM * v1 . Config wait . Poll Infinite ( constants . Discovery Retry clusterinfo CM , err = client . Core V1 ( ) . Config Maps ( metav1 . Namespace Public ) . Get ( bootstrapapi . Config Map Cluster Info , metav1 . Get if err != nil { if apierrors . Is Forbidden ( err ) { // If the request is unauthorized, the cluster admin has not granted access to the cluster info configmap for unauthenticated users // In that case, trust the cluster admin and do not refresh the cluster-info credentials klog . Warningf ( " \n " , bootstrapapi . Config Map Cluster // If we couldn't fetch the cluster-info Config Map, just return the cluster-info object the user provided if clusterinfo // We somehow got hold of the Config Map, try to read some data from it. If we can't, fallback on the user-provided file refreshed Base Kube Config , err := try Parse Cluster Info From Config Map ( clusterinfo if err != nil { klog . V ( 1 ) . Infof ( " \n " , bootstrapapi . Config Map Cluster // In an HA world in the future, this will make more sense, because now we've got new information, possibly about new API Servers to talk to return refreshed Base Kube } 
func try Parse Cluster Info From Config Map ( cm * v1 . Config Map ) ( * clientcmdapi . Config , error ) { kube Config String , ok := cm . Data [ bootstrapapi . Kube Config if ! ok || len ( kube Config String ) == 0 { return nil , errors . Errorf ( " " , bootstrapapi . Kube Config parsed Kube Config , err := clientcmd . Load ( [ ] byte ( kube Config if err != nil { return nil , errors . Wrapf ( err , " " , bootstrapapi . Config Map Cluster return parsed Kube } 
func validate Kube default Cluster := kubeconfigutil . Get Cluster From Kube if default } 
func ( c * Critical Pod Admission Handler ) Handle Admission Failure ( admit Pod * v1 . Pod , failure Reasons [ ] predicates . Predicate Failure Reason ) ( bool , [ ] predicates . Predicate Failure Reason , error ) { if ! kubetypes . Is Critical Pod ( admit Pod ) { return false , failure // Insufficient Resource Error is not a reason to reject a critical pod. // Instead of rejecting, we free up resources to admit it, if no other reasons for rejection exist. non Resource Reasons := [ ] predicates . Predicate Failure resource Reasons := [ ] * admission for _ , reason := range failure Reasons { if r , ok := reason . ( * predicates . Insufficient Resource Error ) ; ok { resource Reasons = append ( resource Reasons , & admission Requirement { resource Name : r . Resource Name , quantity : r . Get Insufficient } else { non Resource Reasons = append ( non Resource if len ( non Resource Reasons ) > 0 { // Return only reasons that are not resource related, since critical pods cannot fail admission for resource reasons. return false , non Resource err := c . evict Pods To Free Requests ( admit Pod , admission Requirement List ( resource } 
func ( c * Critical Pod Admission Handler ) evict Pods To Free Requests ( admit Pod * v1 . Pod , insufficient Resources admission Requirement List ) error { pods To Preempt , err := get Pods To Preempt ( admit Pod , c . get Pods Func ( ) , insufficient klog . Infof ( " " , pods To Preempt , insufficient Resources . to for _ , pod := range pods To Preempt { status := v1 . Pod Status { Phase : v1 . Pod Failed , Message : message , Reason : events . Preempt // record that we are evicting the pod c . recorder . Eventf ( pod , v1 . Event Type Warning , events . Preempt // this is a blocking call and should only return when the pod and its containers are killed. err := c . kill Pod } 
func get Pods To Preempt ( pod * v1 . Pod , pods [ ] * v1 . Pod , requirements admission Requirement List ) ( [ ] * v1 . Pod , error ) { best Effort Pods , burstable Pods , guaranteed Pods := sort Pods By // make sure that pods exist to reclaim the requirements unable To Meet Requirements := requirements . subtract ( append ( append ( best Effort Pods , burstable Pods ... ) , guaranteed if len ( unable To Meet Requirements ) > 0 { return nil , fmt . Errorf ( " " , unable To Meet Requirements . to // find the guaranteed pods we would need to evict if we already evicted ALL burstable and besteffort pods. guarateed To Evict , err := get Pods To Preempt By Distance ( guaranteed Pods , requirements . subtract ( append ( best Effort Pods , burstable // Find the burstable pods we would need to evict if we already evicted ALL besteffort pods, and the required guaranteed pods. burstable To Evict , err := get Pods To Preempt By Distance ( burstable Pods , requirements . subtract ( append ( best Effort Pods , guarateed To // Find the besteffort pods we would need to evict if we already evicted the required guaranteed and burstable pods. best Effort To Evict , err := get Pods To Preempt By Distance ( best Effort Pods , requirements . subtract ( append ( burstable To Evict , guarateed To return append ( append ( best Effort To Evict , burstable To Evict ... ) , guarateed To } 
func get Pods To Preempt By Distance ( pods [ ] * v1 . Pod , requirements admission Requirement List ) ( [ ] * v1 . Pod , error ) { pods To // evict pods by shortest distance from remaining requirements, updating requirements every round. for len ( requirements ) > 0 { if len ( pods ) == 0 { return nil , fmt . Errorf ( " " , requirements . to // all distances must be less than len(requirements), because the max distance for a single requirement is 1 best best Pod if dist < best Distance || ( best Distance == dist && smaller Resource Request ( pod , pods [ best Pod Index ] ) ) { best best Pod // subtract the pod from requirements, and transfer the pod from input-pods to pods-to-evicted requirements = requirements . subtract ( pods [ best Pod pods To Evict = append ( pods To Evict , pods [ best Pod pods [ best Pod return pods To } 
func ( a admission Requirement for _ , req := range a { remaining Request := float64 ( req . quantity - resource . Get Resource Request ( pod , req . resource if remaining Request < 0 { remaining dist += math . Pow ( remaining } 
func ( a admission Requirement List ) subtract ( pods ... * v1 . Pod ) admission Requirement List { new List := [ ] * admission for _ , req := range a { new for _ , pod := range pods { new Quantity -= resource . Get Resource Request ( pod , req . resource if new Quantity > 0 { new List = append ( new List , & admission Requirement { resource Name : req . resource Name , quantity : new return new } 
func sort Pods By QOS ( preemptor * v1 . Pod , pods [ ] * v1 . Pod ) ( best Effort , burstable , guaranteed [ ] * v1 . Pod ) { for _ , pod := range pods { if kubetypes . Preemptable ( preemptor , pod ) { switch v1qos . Get Pod QOS ( pod ) { case v1 . Pod QOS Best Effort : best Effort = append ( best case v1 . Pod QOS case v1 . Pod QOS } 
func smaller Resource Request ( pod1 * v1 . Pod , pod2 * v1 . Pod ) bool { priority List := [ ] v1 . Resource Name { v1 . Resource Memory , v1 . Resource for _ , res := range priority List { req1 := resource . Get Resource req2 := resource . Get Resource } 
func ( s Service Common Generator if s . Cluster IP == v1 . Cluster IP None && s . Type != v1 . Service Type Cluster if s . Cluster IP != v1 . Cluster IP None && len ( s . TCP ) == 0 && s . Type != v1 . Service Type External if s . Type == v1 . Service Type External Name { if errs := validation . Is DNS1123Subdomain ( s . External Name ) ; len ( errs ) != 0 { return fmt . Errorf ( " " , s . External } 
func ( in * Runtime Class ) Deep Copy Into ( out * Runtime out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Runtime Class Spec ) Deep Copy Into ( out * Runtime Class if in . Runtime Handler != nil { in , out := & in . Runtime Handler , & out . Runtime } 
func ( in * Runtime Class Spec ) Deep Copy ( ) * Runtime Class out := new ( Runtime Class in . Deep Copy } 
func ( o * Env o . resources , o . env Args , ok = envutil . Split Environment From o . update Pod Spec For Object = polymorphichelpers . Update Pod Spec For Object o . output = cmdutil . Get Flag o . dry Run = cmdutil . Get Dry Run if o . dry Run { // TODO(juanvallejo): This can be cleaned up even further by creating // a Print Flags struct that binds the --dry-run flag, and whose // To Printer method returns a printer that understands how to print // this success message. o . Print printer , err := o . Print Flags . To o . Print Obj = printer . Print o . clientset , err = f . Kubernetes Client o . namespace , o . enforce Namespace , err = f . To Raw Kube Config o . builder = f . New } 
func ( o * Env } 
func ( o * Env Options ) Run Env ( ) error { env , remove , err := envutil . Parse Env ( append ( o . Env Params , o . env if len ( o . From ) != 0 { b := o . builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Local Param ( o . Local ) . Continue On Error ( ) . Namespace Param ( o . namespace ) . Default Namespace ( ) . Filename Param ( o . enforce Namespace , & o . Filename if ! o . Local { b = b . Label Selector Param ( o . Selector ) . Resource Type Or Name for _ , info := range infos { switch from := info . Object . ( type ) { case * v1 . Secret : for key := range from . Data { if contains ( key , o . Keys ) { env Var := v1 . Env Var { Name : key To Env Name ( key ) , Value From : & v1 . Env Var Source { Secret Key Ref : & v1 . Secret Key Selector { Local Object Reference : v1 . Local Object env = append ( env , env case * v1 . Config Map : for key := range from . Data { if contains ( key , o . Keys ) { env Var := v1 . Env Var { Name : key To Env Name ( key ) , Value From : & v1 . Env Var Source { Config Map Key Ref : & v1 . Config Map Key Selector { Local Object Reference : v1 . Local Object env = append ( env , env b := o . builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Local Param ( o . Local ) . Continue On Error ( ) . Namespace Param ( o . namespace ) . Default Namespace ( ) . Filename Param ( o . enforce Namespace , & o . Filename if ! o . Local { b . Label Selector Param ( o . Selector ) . Resource Type Or Name patches := Calculate Patches ( infos , scheme . Default JSON Encoder ( ) , func ( obj runtime . Object ) ( [ ] byte , error ) { _ , err := o . update Pod Spec For Object ( obj , func ( spec * v1 . Pod Spec ) error { resolution Errors containers , _ := select Containers ( spec . Containers , o . Container obj Name , err := meta . New gvks , _ , err := scheme . Scheme . Object obj Kind := obj . Get Object Kind ( ) . Group Version if len ( obj if len ( gvk . Version ) == 0 || gvk . Version == runtime . API Version obj if len ( containers ) == 0 { if gvks , _ , err := scheme . Scheme . Object Kinds ( obj ) ; err == nil { obj Kind := obj . Get Object Kind ( ) . Group Version if len ( obj if len ( gvk . Version ) == 0 || gvk . Version == runtime . API Version obj fmt . Fprintf ( o . Err Out , " \n " , obj Kind , obj Name , o . Container for _ , c := range containers { if ! o . Overwrite { if err := validate No c . Env = update if o . List { resolve store := envutil . New Resource fmt . Fprintf ( o . Out , " \n " , obj Kind , obj for _ , env := range c . Env { // Print the simple value if env . Value // Print the reference version if ! o . Resolve { fmt . Fprintf ( o . Out , " \n " , env . Name , envutil . Get Env Var Ref String ( env . Value value , err := envutil . Get Env Var Ref Value ( o . clientset , o . namespace , store , env . Value // Print the reference version and save the resolve error fmt . Fprintf ( o . Out , " \n " , env . Name , envutil . Get Env Var Ref String ( env . Value err resolve Errors [ err String ] = append ( resolve Errors [ err resolution Errors for err , vars := range resolve for _ , err := range errs { fmt . Fprintln ( o . Err if resolution Errors if err == nil { return runtime . Encode ( scheme . Default JSON all if patch . Err != nil { name := info . Object all Errs = append ( all if o . Local || o . dry Run { if err := o . Print Obj ( info . Object , o . Out ) ; err != nil { all Errs = append ( all actual , err := resource . New Helper ( info . Client , info . Mapping ) . Patch ( info . Namespace , info . Name , types . Strategic Merge Patch if err != nil { all Errs = append ( all // make sure arguments to set or replace environment variables are set // before returning a successful message if len ( env ) == 0 && len ( o . env if err := o . Print Obj ( actual , o . Out ) ; err != nil { all Errs = append ( all return utilerrors . New Aggregate ( all } 
func ( in * Bound Object Reference ) Deep Copy ( ) * Bound Object out := new ( Bound Object in . Deep Copy } 
func ( in * Token Request ) Deep Copy Into ( out * Token out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Token Request ) Deep Copy ( ) * Token out := new ( Token in . Deep Copy } 
func ( in * Token Request ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Token Request Spec ) Deep Copy Into ( out * Token Request if in . Bound Object Ref != nil { in , out := & in . Bound Object Ref , & out . Bound Object * out = new ( Bound Object } 
func ( in * Token Request Spec ) Deep Copy ( ) * Token Request out := new ( Token Request in . Deep Copy } 
func ( in * Token Request Status ) Deep Copy Into ( out * Token Request in . Expiration Timestamp . Deep Copy Into ( & out . Expiration } 
func ( in * Token Request Status ) Deep Copy ( ) * Token Request out := new ( Token Request in . Deep Copy } 
func ( in * Token Review ) Deep Copy Into ( out * Token out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Token Review ) Deep Copy ( ) * Token out := new ( Token in . Deep Copy } 
func ( in * Token Review ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Token Review Spec ) Deep Copy Into ( out * Token Review } 
func ( in * Token Review Spec ) Deep Copy ( ) * Token Review out := new ( Token Review in . Deep Copy } 
func ( in * Token Review Status ) Deep Copy Into ( out * Token Review in . User . Deep Copy } 
func ( in * Token Review Status ) Deep Copy ( ) * Token Review out := new ( Token Review in . Deep Copy } 
func Generate Bootstrap Token ( ) ( string , error ) { token ID , err := rand Bytes ( api . Bootstrap Token ID token Secret , err := rand Bytes ( api . Bootstrap Token Secret return Token From ID And Secret ( token ID , token } 
func rand Bytes ( length int ) ( string , error ) { // len("0123456789abcdefghijklmnopqrstuvwxyz") = 36 which doesn't evenly divide // the possible values of a byte: 256 mod 36 = 4. Discard any random bytes we // read that are >= 252 so the bytes we evenly divide the character set. const max Byte reader := bufio . New Reader for i := range token { for { if b , err = reader . Read if b < max Byte token [ i ] = valid Bootstrap Token Chars [ int ( b ) % len ( valid Bootstrap Token } 
func Validate Bootstrap Group Name ( name string ) error { if Bootstrap Group return fmt . Errorf ( " " , name , api . Bootstrap Group } 
func Validate Usages ( usages [ ] string ) error { valid Usages := sets . New String ( api . Known Token invalid Usages := sets . New for _ , usage := range usages { if ! valid Usages . Has ( usage ) { invalid if len ( invalid Usages ) > 0 { return fmt . Errorf ( " " , strings . Join ( invalid Usages . List ( ) , " " ) , strings . Join ( api . Known Token } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Event List ) ( nil ) , ( * core . Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Event List_To_core_Event List ( a . ( * v1beta1 . Event List ) , b . ( * core . Event if err := s . Add Generated Conversion Func ( ( * core . Event List ) ( nil ) , ( * v1beta1 . Event List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Event List_To_v1beta1_Event List ( a . ( * core . Event List ) , b . ( * v1beta1 . Event if err := s . Add Generated Conversion Func ( ( * v1beta1 . Event Series ) ( nil ) , ( * core . Event Series ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Event Series_To_core_Event Series ( a . ( * v1beta1 . Event Series ) , b . ( * core . Event if err := s . Add Generated Conversion Func ( ( * core . Event Series ) ( nil ) , ( * v1beta1 . Event Series ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_core_Event Series_To_v1beta1_Event Series ( a . ( * core . Event Series ) , b . ( * v1beta1 . Event if err := s . Add Conversion if err := s . Add Conversion } 
func Convert_v1beta1_Event List_To_core_Event List ( in * v1beta1 . Event List , out * core . Event List , s conversion . Scope ) error { return auto Convert_v1beta1_Event List_To_core_Event } 
func Convert_core_Event List_To_v1beta1_Event List ( in * core . Event List , out * v1beta1 . Event List , s conversion . Scope ) error { return auto Convert_core_Event List_To_v1beta1_Event } 
func Convert_v1beta1_Event Series_To_core_Event Series ( in * v1beta1 . Event Series , out * core . Event Series , s conversion . Scope ) error { return auto Convert_v1beta1_Event Series_To_core_Event } 
func Convert_core_Event Series_To_v1beta1_Event Series ( in * core . Event Series , out * v1beta1 . Event Series , s conversion . Scope ) error { return auto Convert_core_Event Series_To_v1beta1_Event } 
func Generate Containers Ready Condition ( spec * v1 . Pod Spec , container Statuses [ ] v1 . Container Status , pod Phase v1 . Pod Phase ) v1 . Pod Condition { // Find if all containers are ready or not. if container Statuses == nil { return v1 . Pod Condition { Type : v1 . Containers Ready , Status : v1 . Condition False , Reason : Unknown Container unknown unready for _ , container := range spec . Containers { if container Status , ok := podutil . Get Container Status ( container Statuses , container . Name ) ; ok { if ! container Status . Ready { unready Containers = append ( unready } else { unknown Containers = append ( unknown // If all containers are known and succeeded, just return Pod Completed. if pod Phase == v1 . Pod Succeeded && len ( unknown Containers ) == 0 { return v1 . Pod Condition { Type : v1 . Containers Ready , Status : v1 . Condition False , Reason : Pod // Generate message for containers in unknown condition. unready if len ( unknown Containers ) > 0 { unready Messages = append ( unready Messages , fmt . Sprintf ( " " , unknown if len ( unready Containers ) > 0 { unready Messages = append ( unready Messages , fmt . Sprintf ( " " , unready unready Message := strings . Join ( unready if unready Message != " " { return v1 . Pod Condition { Type : v1 . Containers Ready , Status : v1 . Condition False , Reason : Containers Not Ready , Message : unready return v1 . Pod Condition { Type : v1 . Containers Ready , Status : v1 . Condition } 
func Generate Pod Ready Condition ( spec * v1 . Pod Spec , conditions [ ] v1 . Pod Condition , container Statuses [ ] v1 . Container Status , pod Phase v1 . Pod Phase ) v1 . Pod Condition { containers Ready := Generate Containers Ready Condition ( spec , container Statuses , pod // If the status of Containers Ready is not True, return the same status, reason and message as Containers Ready. if containers Ready . Status != v1 . Condition True { return v1 . Pod Condition { Type : v1 . Pod Ready , Status : containers Ready . Status , Reason : containers Ready . Reason , Message : containers // Evaluate corresponding conditions specified in readiness gate // Generate message if any readiness gate is not satisfied. unready for _ , rg := range spec . Readiness Gates { _ , c := podutil . Get Pod Condition From List ( conditions , rg . Condition if c == nil { unready Messages = append ( unready Messages , fmt . Sprintf ( " " , string ( rg . Condition } else if c . Status != v1 . Condition True { unready Messages = append ( unready Messages , fmt . Sprintf ( " \" \" " , string ( rg . Condition // Set "Ready" condition to "False" if any readiness gate is not ready. if len ( unready Messages ) != 0 { unready Message := strings . Join ( unready return v1 . Pod Condition { Type : v1 . Pod Ready , Status : v1 . Condition False , Reason : Readiness Gates Not Ready , Message : unready return v1 . Pod Condition { Type : v1 . Pod Ready , Status : v1 . Condition } 
func ( i * image } 
func ( i * image sort . Sort ( sliceutils . By Image } 
func New Image GC Manager ( runtime container . Runtime , stats Provider Stats Provider , recorder record . Event Recorder , node Ref * v1 . Object Reference , policy Image GC Policy , sandbox Image string ) ( Image GC Manager , error ) { // Validate policy. if policy . High Threshold Percent < 0 || policy . High Threshold Percent > 100 { return nil , fmt . Errorf ( " " , policy . High Threshold if policy . Low Threshold Percent < 0 || policy . Low Threshold Percent > 100 { return nil , fmt . Errorf ( " " , policy . Low Threshold if policy . Low Threshold Percent > policy . High Threshold Percent { return nil , fmt . Errorf ( " " , policy . Low Threshold Percent , policy . High Threshold im := & real Image GC Manager { runtime : runtime , policy : policy , image Records : make ( map [ string ] * image Record ) , stats Provider : stats Provider , recorder : recorder , node Ref : node Ref , initialized : false , sandbox Image : sandbox } 
func ( im * real Image GC Manager ) Get Image List ( ) ( [ ] container . Image , error ) { return im . image } 
func ( im * real Image GC Manager ) free Space ( bytes To Free int64 , free Time time . Time ) ( int64 , error ) { images In Use , err := im . detect Images ( free im . image Records defer im . image Records // Get all images in eviction order. images := make ( [ ] eviction Info , 0 , len ( im . image for image , record := range im . image Records { if is Image Used ( image , images In images = append ( images , eviction Info { id : image , image sort . Sort ( by Last Used And // Delete unused images until we've freed up enough space. var deletion space // Images that are currently in used were given a newer last Used. if image . last Used . Equal ( free Time ) || image . last Used . After ( free Time ) { klog . V ( 5 ) . Infof ( " " , image . id , image . last Used , free // Avoid garbage collect the image if the image is not old enough. // In such a case, the image may have just been pulled down, and will be used by a container right away. if free Time . Sub ( image . first Detected ) < im . policy . Min Age { klog . V ( 5 ) . Infof ( " " , image . id , free Time . Sub ( image . first Detected ) , im . policy . Min err := im . runtime . Remove Image ( container . Image if err != nil { deletion Errors = append ( deletion delete ( im . image space if space Freed >= bytes To if len ( deletion Errors ) > 0 { return space Freed , fmt . Errorf ( " " , bytes To Free , space Freed , errors . New Aggregate ( deletion return space } 
func ( in * Aggregation Rule ) Deep Copy Into ( out * Aggregation if in . Cluster Role Selectors != nil { in , out := & in . Cluster Role Selectors , & out . Cluster Role * out = make ( [ ] v1 . Label for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in Sortable Rule Slice ) Deep Copy Into ( out * Sortable Rule * out = make ( Sortable Rule for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in Sortable Rule Slice ) Deep Copy ( ) Sortable Rule out := new ( Sortable Rule in . Deep Copy } 
func ( c * Fake Replica Sets ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( replicasets } 
func ( c * Fake Replica Sets ) Update ( replica Set * v1beta1 . Replica Set ) ( result * v1beta1 . Replica Set , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( replicasets Resource , c . ns , replica Set ) , & v1beta1 . Replica return obj . ( * v1beta1 . Replica } 
func ( c * Fake Replica Sets ) Update Status ( replica Set * v1beta1 . Replica Set ) ( * v1beta1 . Replica Set , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( replicasets Resource , " " , c . ns , replica Set ) , & v1beta1 . Replica return obj . ( * v1beta1 . Replica } 
func Resource Limits Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { node := node if node == nil { return schedulerapi . Host allocatable Resources := node Info . Allocatable // compute pod limits pod Limits := get Resource cpu Score := compute Score ( pod Limits . Milli CPU , allocatable Resources . Milli mem Score := compute Score ( pod Limits . Memory , allocatable if cpu Score == 1 || mem if klog . V ( 10 ) { // We explicitly don't do klog.V(10).Infof() to avoid computing all the parameters if this is // not logged. There is visible performance gain from it. klog . Infof ( " " , pod . Name , node . Name , allocatable Resources . Milli CPU , allocatable Resources . Memory , pod Limits . Milli CPU , pod return schedulerapi . Host } 
func get Resource // take max_resource(sum_pod, any_init_container) for _ , container := range pod . Spec . Init Containers { result . Set Max } 
func get Docker Client ( docker Endpoint string ) ( * dockerapi . Client , error ) { if len ( docker Endpoint ) > 0 { klog . Infof ( " " , docker return dockerapi . New Client ( docker return dockerapi . New Env } 
func Connect To Docker Or Die ( docker Endpoint string , request Timeout , image Pull Progress Deadline time . Duration , with Trace Disabled bool , enable Sleep bool ) Interface { if docker Endpoint == Fake Docker Endpoint { fake Client := New Fake Docker if with Trace Disabled { fake Client = fake Client . With Trace if enable Sleep { fake Client . Enable return fake client , err := get Docker Client ( docker klog . Infof ( " " , request return new Kube Docker Client ( client , request Timeout , image Pull Progress } 
func New Cluster Role Aggregation ( cluster Role Informer rbacinformers . Cluster Role Informer , cluster Role Client rbacclient . Cluster Roles Getter ) * Cluster Role Aggregation Controller { c := & Cluster Role Aggregation Controller { cluster Role Client : cluster Role Client , cluster Role Lister : cluster Role Informer . Lister ( ) , cluster Roles Synced : cluster Role Informer . Informer ( ) . Has Synced , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate c . sync Handler = c . sync Cluster cluster Role Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add } , Update } , Delete } 
func ( c * Scheduling V1beta1Client ) REST return c . rest } 
func ( q * rate Limiting Type ) Add Rate Limited ( item interface { } ) { q . Delaying Interface . Add After ( item , q . rate } 
func ( os * Open Stack ) New Network V2 ( ) ( * gophercloud . Service Client , error ) { network , err := openstack . New Network V2 ( os . provider , gophercloud . Endpoint } 
func ( os * Open Stack ) New Compute V2 ( ) ( * gophercloud . Service Client , error ) { compute , err := openstack . New Compute V2 ( os . provider , gophercloud . Endpoint } 
func ( os * Open Stack ) New Load Balancer V2 ( ) ( * gophercloud . Service Client , error ) { var lb * gophercloud . Service if os . lb Opts . Use Octavia { lb , err = openstack . New Load Balancer V2 ( os . provider , gophercloud . Endpoint } else { lb , err = openstack . New Network V2 ( os . provider , gophercloud . Endpoint } 
func ( stateful Set Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { stateful Set := obj . ( * apps . Stateful // create cannot set status stateful Set . Status = apps . Stateful Set stateful pod . Drop Disabled Template Fields ( & stateful } 
func ( stateful Set Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Stateful Set := obj . ( * apps . Stateful old Stateful Set := old . ( * apps . Stateful // Update is not allowed to set status new Stateful Set . Status = old Stateful pod . Drop Disabled Template Fields ( & new Stateful Set . Spec . Template , & old Stateful // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. // See metav1.Object Meta description for more information on Generation. if ! apiequality . Semantic . Deep Equal ( old Stateful Set . Spec , new Stateful Set . Spec ) { new Stateful Set . Generation = old Stateful } 
func ( stateful Set Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { stateful Set := obj . ( * apps . Stateful all Errs := validation . Validate Stateful Set ( stateful all Errs = append ( all Errs , corevalidation . Validate Conditional Pod Template ( & stateful Set . Spec . Template , nil , field . New return all } 
func ( stateful Set Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new Stateful Set := obj . ( * apps . Stateful old Stateful Set := old . ( * apps . Stateful validation Error List := validation . Validate Stateful Set ( new Stateful update Error List := validation . Validate Stateful Set Update ( new Stateful Set , old Stateful update Error List = append ( update Error List , corevalidation . Validate Conditional Pod Template ( & new Stateful Set . Spec . Template , & old Stateful Set . Spec . Template , field . New return append ( validation Error List , update Error } 
func ( stateful Set Status Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Stateful Set := obj . ( * apps . Stateful old Stateful Set := old . ( * apps . Stateful // status changes are not allowed to update spec new Stateful Set . Spec = old Stateful } 
func ( stateful Set Status Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { // TODO: Validate status updates. return validation . Validate Stateful Set Status Update ( obj . ( * apps . Stateful Set ) , old . ( * apps . Stateful } 
func compute Reconciled Role ( existing , expected Rule Owner , remove Extra Permissions bool ) ( * Reconcile Cluster Role Result , error ) { result := & Reconcile Cluster Role Result { Operation : Reconcile result . Protected = ( existing . Get Annotations ( ) [ rbacv1 . Auto Update Annotation // Start with a copy of the existing object result . Role = existing . Deep Copy Rule // Merge expected annotations and labels result . Role . Set Annotations ( merge ( expected . Get Annotations ( ) , result . Role . Get if ! reflect . Deep Equal ( result . Role . Get Annotations ( ) , existing . Get Annotations ( ) ) { result . Operation = Reconcile result . Role . Set Labels ( merge ( expected . Get Labels ( ) , result . Role . Get if ! reflect . Deep Equal ( result . Role . Get Labels ( ) , existing . Get Labels ( ) ) { result . Operation = Reconcile // Compute extra and missing rules // Don't compute extra permissions if expected and existing roles are both aggregated if expected . Get Aggregation Rule ( ) == nil || existing . Get Aggregation Rule ( ) == nil { _ , result . Extra Rules = validation . Covers ( expected . Get Rules ( ) , existing . Get _ , result . Missing Rules = validation . Covers ( existing . Get Rules ( ) , expected . Get switch { case ! remove Extra Permissions && len ( result . Missing Rules ) > 0 : // add missing rules in the union case result . Role . Set Rules ( append ( result . Role . Get Rules ( ) , result . Missing result . Operation = Reconcile case remove Extra Permissions && ( len ( result . Missing Rules ) > 0 || len ( result . Extra Rules ) > 0 ) : // stomp to expected rules in the non-union case result . Role . Set Rules ( expected . Get result . Operation = Reconcile // Compute extra and missing rules _ , result . Extra Aggregation Rule Selectors = aggregation Rule Covers ( expected . Get Aggregation Rule ( ) , existing . Get Aggregation _ , result . Missing Aggregation Rule Selectors = aggregation Rule Covers ( existing . Get Aggregation Rule ( ) , expected . Get Aggregation switch { case expected . Get Aggregation Rule ( ) == nil && existing . Get Aggregation Rule ( ) != nil : // we didn't expect this to be an aggregated role at all, remove the existing aggregation result . Role . Set Aggregation result . Operation = Reconcile case ! remove Extra Permissions && len ( result . Missing Aggregation Rule Selectors ) > 0 : // add missing rules in the union case aggregation Rule := result . Role . Get Aggregation if aggregation Rule == nil { aggregation Rule = & rbacv1 . Aggregation aggregation Rule . Cluster Role Selectors = append ( aggregation Rule . Cluster Role Selectors , result . Missing Aggregation Rule result . Role . Set Aggregation Rule ( aggregation result . Operation = Reconcile case remove Extra Permissions && ( len ( result . Missing Aggregation Rule Selectors ) > 0 || len ( result . Extra Aggregation Rule Selectors ) > 0 ) : result . Role . Set Aggregation Rule ( expected . Get Aggregation result . Operation = Reconcile } 
} 
func aggregation Rule Covers ( owner Rule , servant Rule * rbacv1 . Aggregation Rule ) ( bool , [ ] metav1 . Label Selector ) { switch { case owner Rule == nil && servant Rule == nil : return true , [ ] metav1 . Label case owner Rule == nil && servant Rule != nil : return false , servant Rule . Cluster Role case owner Rule != nil && servant Rule == nil : return true , [ ] metav1 . Label owner Selectors := owner Rule . Cluster Role servant Selectors := servant Rule . Cluster Role uncovered Selectors := [ ] metav1 . Label for _ , servant Selector := range servant for _ , owner Selector := range owner Selectors { if equality . Semantic . Deep Equal ( owner Selector , servant if ! covered { uncovered Selectors = append ( uncovered Selectors , servant return ( len ( uncovered Selectors ) == 0 ) , uncovered } 
func ( s * service Account Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Service Account , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Service } 
func ( s * service Account Lister ) Service Accounts ( namespace string ) Service Account Namespace Lister { return service Account Namespace } 
func ( s service Account Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Service Account , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Service } 
func Remove All One if err == nil || os . Is Not if serr != nil { if serr , ok := serr . ( * os . Path Error ) ; ok && ( os . Is Not if ! dir . Is // Directory. is Not Mount , err := mounter . Is Likely Not Mount if ! is Not if err != nil { if os . Is Not Exist ( err ) { // Race. It was deleted between the Lstat and Open. // Return nil per Remove for _ , name := range names { err1 := Remove All One Filesystem ( mounter , path + string ( os . Path if err1 == nil || os . Is Not } 
func new Resource Quotas ( c * Core V1Client , namespace string ) * resource Quotas { return & resource Quotas { client : c . REST } 
func New For Config Or cs . apiextensions = apiextensionsinternalversion . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . Discovery Client = discovery . New Discovery } 
if a == most Negative && b == most } 
func int64Multiply if a == most } 
func int64Multiply if a == most } 
func int64Multiply if a == most } 
func positive Scale case 1 : return int64Multiply case 2 : return int64Multiply case 3 : return int64Multiply case 6 : return int64Multiply case 9 : return int64Multiply for i := Scale ( 0 ) ; i < scale ; i ++ { if value , ok = int64Multiply } 
func negative Scale } 
func divide By Scale } 
func remove } 
func remove Big Int Factors ( d , factor * big . Int ) ( result * big . Int , times int32 ) { q := big . New m := big . New for d . Cmp ( big Zero ) != 0 { q . Div if m . Cmp ( big } 
func ( s * Selection labels , fields , err := s . Get } 
func ( s * Selection Predicate ) Matches Object } 
func ( s * Selection Predicate ) Matches // TODO: should be namespace.name if name , ok := s . Field . Requires Exact } 
func ( s * Selection Predicate ) Matcher Index ( ) [ ] Match Value { var result [ ] Match for _ , field := range s . Index Fields { if value , ok := s . Field . Requires Exact Match ( field ) ; ok { result = append ( result , Match Value { Index } 
func ( s * Selection } 
func ( always Deny ) Admit ( a admission . Attributes , o admission . Object Interfaces ) ( err error ) { return admission . New } 
func new Container GC ( client internalapi . Runtime Service , pod State Provider pod State Provider , manager * kube Generic Runtime Manager ) * container GC { return & container GC { client : client , manager : manager , pod State Provider : pod State } 
func ( cu containers By Evict Unit ) Num } 
func ( cgc * container GC ) enforce Max Containers Per Evict Unit ( evict Units containers By Evict Unit , Max Containers int ) { for key := range evict Units { to Remove := len ( evict Units [ key ] ) - Max if to Remove > 0 { evict Units [ key ] = cgc . remove Oldest N ( evict Units [ key ] , to } 
func ( cgc * container GC ) remove Oldest N ( containers [ ] container GC Info , to Remove int ) [ ] container GC Info { // Remove from oldest to newest (last to first). num To Keep := len ( containers ) - to for i := len ( containers ) - 1 ; i >= num To Keep ; i -- { if containers [ i ] . unknown { // Containers in known state could be running, we should try // to stop it before removal. id := kubecontainer . Container ID { Type : cgc . manager . runtime if err := cgc . manager . kill if err := cgc . manager . remove // Assume we removed the containers so that we're not too aggressive. return containers [ : num To } 
func ( cgc * container GC ) remove Oldest N Sandboxes ( sandboxes [ ] sandbox GC Info , to Remove int ) { // Remove from oldest to newest (last to first). num To Keep := len ( sandboxes ) - to for i := len ( sandboxes ) - 1 ; i >= num To Keep ; i -- { if ! sandboxes [ i ] . active { cgc . remove } 
func ( cgc * container GC ) remove Sandbox ( sandbox ID string ) { klog . V ( 4 ) . Infof ( " " , sandbox // In normal cases, kubelet should've already called Stop Pod Sandbox before // GC kicks in. To guard against the rare cases where this is not true, try // stopping the sandbox before removing it. if err := cgc . client . Stop Pod Sandbox ( sandbox ID ) ; err != nil { klog . Errorf ( " " , sandbox if err := cgc . client . Remove Pod Sandbox ( sandbox ID ) ; err != nil { klog . Errorf ( " " , sandbox } 
func ( cgc * container GC ) evictable Containers ( min Age time . Duration ) ( containers By Evict Unit , error ) { containers , err := cgc . manager . get Kubelet if err != nil { return containers By Evict evict Units := make ( containers By Evict newest GC Time := time . Now ( ) . Add ( - min for _ , container := range containers { // Prune out running containers. if container . State == runtimeapi . Container created At := time . Unix ( 0 , container . Created if newest GC Time . Before ( created labeled Info := get Container Info From container Info := container GC Info { id : container . Id , name : container . Metadata . Name , create Time : created At , unknown : container . State == runtimeapi . Container key := evict Unit { uid : labeled Info . Pod UID , name : container evict Units [ key ] = append ( evict Units [ key ] , container // Sort the containers by age. for uid := range evict Units { sort . Sort ( by Created ( evict return evict } 
func ( cgc * container GC ) evict Containers ( gc Policy kubecontainer . Container GC Policy , all Sources Ready bool , evict Terminated Pods bool ) error { // Separate containers by evict units. evict Units , err := cgc . evictable Containers ( gc Policy . Min // Remove deleted pod containers if all sources are ready. if all Sources Ready { for key , unit := range evict Units { if cgc . pod State Provider . Is Pod Deleted ( key . uid ) || ( cgc . pod State Provider . Is Pod Terminated ( key . uid ) && evict Terminated Pods ) { cgc . remove Oldest delete ( evict // Enforce max containers per evict unit. if gc Policy . Max Per Pod Container >= 0 { cgc . enforce Max Containers Per Evict Unit ( evict Units , gc Policy . Max Per Pod // Enforce max total number of containers. if gc Policy . Max Containers >= 0 && evict Units . Num Containers ( ) > gc Policy . Max Containers { // Leave an equal number of containers per evict unit (min: 1). num Containers Per Evict Unit := gc Policy . Max Containers / evict Units . Num Evict if num Containers Per Evict Unit < 1 { num Containers Per Evict cgc . enforce Max Containers Per Evict Unit ( evict Units , num Containers Per Evict // If we still need to evict, evict oldest first. num Containers := evict Units . Num if num Containers > gc Policy . Max Containers { flattened := make ( [ ] container GC Info , 0 , num for key := range evict Units { flattened = append ( flattened , evict sort . Sort ( by cgc . remove Oldest N ( flattened , num Containers - gc Policy . Max } 
func ( cgc * container GC ) evict Sandboxes ( evict Terminated Pods bool ) error { containers , err := cgc . manager . get Kubelet // collect all the Pod Sandbox Id of container sandbox I Ds := sets . New for _ , container := range containers { sandbox I Ds . Insert ( container . Pod Sandbox sandboxes , err := cgc . manager . get Kubelet sandboxes By Pod := make ( sandboxes By Pod for _ , sandbox := range sandboxes { pod sandbox Info := sandbox GC Info { id : sandbox . Id , create Time : time . Unix ( 0 , sandbox . Created // Set ready sandboxes to be active. if sandbox . State == runtimeapi . Pod Sandbox State_SANDBOX_READY { sandbox // Set sandboxes that still have containers to be active. if sandbox I Ds . Has ( sandbox . Id ) { sandbox sandboxes By Pod [ pod UID ] = append ( sandboxes By Pod [ pod UID ] , sandbox // Sort the sandboxes by age. for uid := range sandboxes By Pod { sort . Sort ( sandbox By Created ( sandboxes By for pod UID , sandboxes := range sandboxes By Pod { if cgc . pod State Provider . Is Pod Deleted ( pod UID ) || ( cgc . pod State Provider . Is Pod Terminated ( pod UID ) && evict Terminated Pods ) { // Remove all evictable sandboxes if the pod has been removed. // Note that the latest dead sandbox is also removed if there is // already an active one. cgc . remove Oldest N } else { // Keep latest one if the pod still exists. cgc . remove Oldest N } 
func ( cgc * container GC ) evict Pod Logs Directories ( all Sources Ready bool ) error { os Interface := cgc . manager . os if all Sources Ready { // Only remove pod logs directories when all sources are ready. dirs , err := os Interface . Read Dir ( pod Logs Root if err != nil { return fmt . Errorf ( " " , pod Logs Root pod UID := parse Pod UID From Logs if ! cgc . pod State Provider . Is Pod Deleted ( pod err := os Interface . Remove All ( filepath . Join ( pod Logs Root // Remove dead container log symlinks. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. log Symlinks , _ := os Interface . Glob ( filepath . Join ( legacy Container Logs Dir , fmt . Sprintf ( " " , legacy Log for _ , log Symlink := range log Symlinks { if _ , err := os Interface . Stat ( log Symlink ) ; os . Is Not Exist ( err ) { err := os Interface . Remove ( log if err != nil { klog . Errorf ( " " , log } 
func ( cgc * container GC ) Garbage Collect ( gc Policy kubecontainer . Container GC Policy , all Sources Ready bool , evict Terminated // Remove evictable containers if err := cgc . evict Containers ( gc Policy , all Sources Ready , evict Terminated // Remove sandboxes with zero containers if err := cgc . evict Sandboxes ( evict Terminated // Remove pod sandbox log directory if err := cgc . evict Pod Logs Directories ( all Sources return utilerrors . New } 
func ( v * Error ) Error ( ) string { return fmt . Sprintf ( " " , v . Field , v . Error } 
func ( v * Error ) Error switch v . Type { case Error Type Required , Error Type Forbidden , Error Type Too Long , Error Type default : value := v . Bad value Type := reflect . Type if value == nil || value } else if value Type . Kind ( ) == reflect . Ptr { if reflect Value := reflect . Value Of ( value ) ; reflect Value . Is } else { value = reflect } 
func ( t Error Type ) String ( ) string { switch t { case Error Type Not case Error Type case Error Type case Error Type case Error Type Not case Error Type case Error Type Too case Error Type } 
func Not Found ( field * Path , value interface { } ) * Error { return & Error { Error Type Not } 
func Required ( field * Path , detail string ) * Error { return & Error { Error Type } 
func Duplicate ( field * Path , value interface { } ) * Error { return & Error { Error Type } 
func Invalid ( field * Path , value interface { } , detail string ) * Error { return & Error { Error Type } 
func Not Supported ( field * Path , value interface { } , valid if valid Values != nil && len ( valid Values ) > 0 { quoted Values := make ( [ ] string , len ( valid for i , v := range valid Values { quoted detail = " " + strings . Join ( quoted return & Error { Error Type Not } 
func Forbidden ( field * Path , detail string ) * Error { return & Error { Error Type } 
func Too Long ( field * Path , value interface { } , max Length int ) * Error { return & Error { Error Type Too Long , field . String ( ) , value , fmt . Sprintf ( " " , max } 
func Internal Error ( field * Path , err error ) * Error { return & Error { Error Type } 
func New Error Type Matcher ( t Error } 
func ( list Error List ) To error Msgs := sets . New if error error return utilerrors . New } 
func ( list Error List ) Filter ( fns ... utilerrors . Matcher ) Error List { err := utilerrors . Filter Out ( list . To // Filter Out takes an Aggregate and returns an Aggregate return from } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( audit . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( v1alpha1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1 . Scheme Group Version , v1beta1 . Scheme Group Version , v1alpha1 . Scheme Group } 
func ( f * Match Version Flags ) To REST Config ( ) ( * rest . Config , error ) { if err := f . check Matching Server client Config , err := f . Delegate . To REST // TODO we should not have to do this. It smacks of something going wrong. set Kubernetes Defaults ( client return client } 
func ( f * Match Version Flags ) To REST Mapper ( ) ( meta . REST Mapper , error ) { if err := f . check Matching Server return f . Delegate . To REST } 
func set Kubernetes Defaults ( config * rest . Config ) error { // TODO remove this hack. This is allowing the Get Options to be serialized. config . Group Version = & schema . Group if config . API Path == " " { config . API if config . Negotiated Serializer == nil { // This codec factory ensures the resources are not converted. Therefore, resources // will not be round-tripped through internal versions. Defaulting does not happen // on the client. config . Negotiated Serializer = scheme . Codecs . Without return rest . Set Kubernetes } 
func New Manager ( ) Manager { return & manager { cache : make ( map [ kubecontainer . Container } 
func ( m * manager ) set Internal ( id kubecontainer . Container } 
func New Delta FIFO ( key Func Key Func , known Objects Key Lister Getter ) * Delta FIFO { f := & Delta FIFO { items : map [ string ] Deltas { } , queue : [ ] string { } , key Func : key Func , known Objects : known } 
func ( f * Delta FIFO ) Close ( ) { f . closed defer f . closed } 
func ( f * Delta FIFO ) Key Of ( obj interface { } ) ( string , error ) { if d , ok := obj . ( Deltas ) ; ok { if len ( d ) == 0 { return " " , Key Error { obj , Err Zero Length Deltas if d , ok := obj . ( Deleted Final State return f . key } 
func ( f * Delta FIFO ) Has return f . populated && f . initial Population } 
func ( f * Delta return f . queue Action } 
func ( f * Delta return f . queue Action } 
func ( f * Delta FIFO ) Delete ( obj interface { } ) error { id , err := f . Key if err != nil { return Key if f . known } else { // We only want to skip the "deletion" action if the object doesn't // exist in known Objects and it doesn't have corresponding item in items. // Note that even if there is a "deletion" action in items, we can ignore it, // because it will be deduped automatically in "queue Action Locked" _ , exists , err := f . known Objects . Get By _ , items if err == nil && ! exists && ! items return f . queue Action } 
func ( f * Delta FIFO ) Add If Not id , err := f . Key if err != nil { return Key f . add If Not } 
func ( f * Delta FIFO ) add If Not } 
func dedup if out := is } 
func is Dup ( a , b * Delta ) * Delta { if out := is Deletion } 
func is Deletion // Do more sophisticated checks, or is this sufficient? if _ , ok := b . Object . ( Deleted Final State } 
func ( f * Delta FIFO ) will Object Be Deleted } 
func ( f * Delta FIFO ) queue Action Locked ( action Type Delta Type , obj interface { } ) error { id , err := f . Key if err != nil { return Key // If object is supposed to be deleted (last event is Deleted), // then we should ignore Sync events, because it would result in // recreation of this object. if action Type == Sync && f . will Object Be Deleted new Deltas := append ( f . items [ id ] , Delta { action new Deltas = dedup Deltas ( new if len ( new f . items [ id ] = new } 
func ( f * Delta FIFO ) List ( ) [ ] interface { } { f . lock . R defer f . lock . R return f . list } 
func ( f * Delta FIFO ) List Keys ( ) [ ] string { f . lock . R defer f . lock . R } 
func ( f * Delta FIFO ) Get ( obj interface { } ) ( item interface { } , exists bool , err error ) { key , err := f . Key if err != nil { return nil , false , Key return f . Get By } 
func ( f * Delta FIFO ) Is Closed ( ) bool { f . closed defer f . closed } 
func ( f * Delta FIFO ) Replace ( list [ ] interface { } , resource for _ , item := range list { key , err := f . Key if err != nil { return Key if err := f . queue Action if f . known Objects == nil { // Do deletion detection against our own list. queued for k , old var deleted if n := old Item . Newest ( ) ; n != nil { deleted queued if err := f . queue Action Locked ( Deleted , Deleted Final State Unknown { k , deleted // While there shouldn't be any queued deletions in the initial // population of the queue, it's better to be on the safe side. f . initial Population Count = len ( list ) + queued // Detect deletions not already in the queue. known Keys := f . known Objects . List queued for _ , k := range known deleted Obj , exists , err := f . known Objects . Get By if err != nil { deleted } else if ! exists { deleted queued if err := f . queue Action Locked ( Deleted , Deleted Final State Unknown { k , deleted f . initial Population Count = len ( list ) + queued } 
func ( f * Delta if f . known keys := f . known Objects . List for _ , k := range keys { if err := f . sync Key } 
} 
func copy } 
func ( gv Group } 
func ( gv Group Version ) String ( ) string { // special case the internal api } 
func ( gv Group Version ) Marshal } 
func ( c * Samplecontroller V1alpha1Client ) REST return c . rest } 
func New Dialer ( dial Dial Func ) * Dialer { return & Dialer { dial : dial , conns : make ( map [ * closable } 
func ( d * Dialer ) Close d . conns = make ( map [ * closable } 
func ( d * Dialer ) Dial ( network , address string ) ( net . Conn , error ) { return d . Dial } 
func ( d * Dialer ) Dial closable := & closable // When the connection is closed, remove it from the map. This will // be no-op if the connection isn't in the map, e.g. if Close All() // is called. closable . on } 
func Drop Disabled Fields ( res Spec * api . Resource Quota Spec , old Res Spec * api . Resource Quota Spec ) { if ! utilfeature . Default Feature Gate . Enabled ( features . Resource Quota Scope Selectors ) && ! resource Quota Scope Selector In Use ( old Res Spec ) { res Spec . Scope } 
func ( g * Cloud ) Get Target HTTP Proxy ( name string ) ( * compute . Target Http Proxy , error ) { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric v , err := g . c . Target Http Proxies ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Create Target HTTP Proxy ( proxy * compute . Target Http Proxy ) error { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric return mc . Observe ( g . c . Target Http Proxies ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) Set URL Map For Target HTTP Proxy ( proxy * compute . Target Http Proxy , url Map Link string ) error { ctx , cancel := cloud . Context With Call ref := & compute . Url Map Reference { Url Map : url Map mc := new Target Proxy Metric return mc . Observe ( g . c . Target Http Proxies ( ) . Set Url Map ( ctx , meta . Global } 
func ( g * Cloud ) Delete Target HTTP Proxy ( name string ) error { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric return mc . Observe ( g . c . Target Http Proxies ( ) . Delete ( ctx , meta . Global } 
func ( g * Cloud ) List Target HTTP Proxies ( ) ( [ ] * compute . Target Http Proxy , error ) { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric v , err := g . c . Target Http } 
func ( g * Cloud ) Get Target HTTPS Proxy ( name string ) ( * compute . Target Https Proxy , error ) { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric v , err := g . c . Target Https Proxies ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Create Target HTTPS Proxy ( proxy * compute . Target Https Proxy ) error { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric return mc . Observe ( g . c . Target Https Proxies ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) Set URL Map For Target HTTPS Proxy ( proxy * compute . Target Https Proxy , url Map Link string ) error { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric ref := & compute . Url Map Reference { Url Map : url Map return mc . Observe ( g . c . Target Https Proxies ( ) . Set Url Map ( ctx , meta . Global } 
func ( g * Cloud ) Set Ssl Certificate For Target HTTPS Proxy ( proxy * compute . Target Https Proxy , ssl Cert UR Ls [ ] string ) error { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric req := & compute . Target Https Proxies Set Ssl Certificates Request { Ssl Certificates : ssl Cert UR return mc . Observe ( g . c . Target Https Proxies ( ) . Set Ssl Certificates ( ctx , meta . Global } 
func ( g * Cloud ) List Target HTTPS Proxies ( ) ( [ ] * compute . Target Https Proxy , error ) { ctx , cancel := cloud . Context With Call mc := new Target Proxy Metric v , err := g . c . Target Https } 
func ( writer Klog Writer ) Write ( data [ ] byte ) ( n int , err error ) { klog . Info } 
func Init Logs ( ) { log . Set Output ( Klog log . Set // The default glog flush interval is 5 seconds. go wait . Until ( klog . Flush , * log Flush Freq , wait . Never } 
func New Client ( conf * restclient . Config , version schema . Group Version ) ( * Client , error ) { delegate , err := dynamic . New For } 
func ( c * Client ) Resource ( resource * metav1 . API Resource , namespace string ) Resource Interface { resource Tokens := strings . Split if len ( resource Tokens ) > 1 { subresources = strings . Split ( resource if len ( namespace ) == 0 { return old Resource Shim ( c . delegate . Resource ( c . version . With Resource ( resource return old Resource Shim ( c . delegate . Resource ( c . version . With Resource ( resource } 
func old Resource Shim ( in dynamic . Resource Interface , subresources [ ] string ) Resource Interface { return old Resource Shim Type { Resource } 
func New Conflict Error ( conflicts merge . Conflicts ) * errors . Status Error { causes := [ ] metav1 . Status for _ , conflict := range conflicts { causes = append ( causes , metav1 . Status Cause { Type : metav1 . Cause Type Field Manager Conflict , Message : fmt . Sprintf ( " " , print return errors . New Apply Conflict ( causes , get Conflict } 
func ( m * kube Generic Runtime Manager ) Pull Image ( image kubecontainer . Image Spec , pull Secrets [ ] v1 . Secret , pod Sandbox Config * runtimeapi . Pod Sandbox repo To Pull , _ , _ , err := parsers . Parse Image keyring , err := credentialprovidersecrets . Make Docker Keyring ( pull img Spec := & runtimeapi . Image creds , with Credentials := keyring . Lookup ( repo To if ! with image Ref , err := m . image Service . Pull Image ( img Spec , nil , pod Sandbox return image var pull for _ , current Creds := range creds { auth Config := credentialprovider . Lazy Provide ( current Creds , repo To auth := & runtimeapi . Auth Config { Username : auth Config . Username , Password : auth Config . Password , Auth : auth Config . Auth , Server Address : auth Config . Server Address , Identity Token : auth Config . Identity Token , Registry Token : auth Config . Registry image Ref , err := m . image Service . Pull Image ( img Spec , auth , pod Sandbox // If there was no error, return success if err == nil { return image pull Errs = append ( pull return " " , utilerrors . New Aggregate ( pull } 
func ( m * kube Generic Runtime Manager ) Get Image Ref ( image kubecontainer . Image Spec ) ( string , error ) { status , err := m . image Service . Image Status ( & runtimeapi . Image } 
func ( m * kube Generic Runtime Manager ) List all Images , err := m . image Service . List for _ , img := range all Images { images = append ( images , kubecontainer . Image { ID : img . Id , Size : int64 ( img . Size_ ) , Repo Tags : img . Repo Tags , Repo Digests : img . Repo } 
func ( m * kube Generic Runtime Manager ) Remove Image ( image kubecontainer . Image Spec ) error { err := m . image Service . Remove Image ( & runtimeapi . Image } 
func ( m * kube Generic Runtime Manager ) Image Stats ( ) ( * kubecontainer . Image Stats , error ) { all Images , err := m . image Service . List stats := & kubecontainer . Image for _ , img := range all Images { stats . Total Storage } 
func New File Store ( path string , fs utilfs . Filesystem ) ( Store , error ) { if err := ensure return & File Store { directory } 
func ( f * File Store ) Write ( key string , data [ ] byte ) error { if err := Validate if err := ensure Directory ( f . filesystem , f . directory return write File ( f . filesystem , f . get Path By } 
func ( f * File Store ) Read ( key string ) ( [ ] byte , error ) { if err := Validate bytes , err := f . filesystem . Read File ( f . get Path By if os . Is Not Exist ( err ) { return bytes , Err Key Not } 
func ( f * File Store ) Delete ( key string ) error { if err := Validate return remove Path ( f . filesystem , f . get Path By } 
func ( f * File files , err := f . filesystem . Read Dir ( f . directory for _ , f := range files { if ! strings . Has Prefix ( f . Name ( ) , tmp } 
func ( f * File Store ) get Path By Key ( key string ) string { return filepath . Join ( f . directory } 
func ensure Directory ( fs utilfs . Filesystem , path string ) error { if _ , err := fs . Stat ( path ) ; err != nil { // Mkdir All returns nil if directory already exists. return fs . Mkdir } 
func write File ( fs utilfs . Filesystem , path string , data [ ] byte ) ( ret Err error ) { // Create a temporary file in the base directory of `path` with a prefix. tmp File , err := fs . Temp File ( filepath . Dir ( path ) , tmp tmp Path := tmp should defer func ( ) { // Close the file. if should Close { if err := tmp File . Close ( ) ; err != nil { if ret Err == nil { ret } else { ret Err = fmt . Errorf ( " " , ret // Clean up the temp file on error. if ret Err != nil && tmp Path != " " { if err := remove Path ( fs , tmp Path ) ; err != nil { ret Err = fmt . Errorf ( " " , tmp Path , ret // Write data. if _ , err := tmp // Sync file. if err := tmp // Closing the file before renaming. err = tmp should return fs . Rename ( tmp } 
func New Controller ( clock clock . Clock , client clientset . Interface , holder Identity string , lease Duration Seconds int32 , on Repeated Heartbeat Failure func ( ) ) Controller { var lease Client coordclientset . Lease if client != nil { lease Client = client . Coordination V1beta1 ( ) . Leases ( corev1 . Namespace Node return & controller { client : client , lease Client : lease Client , holder Identity : holder Identity , lease Duration Seconds : lease Duration Seconds , renew Interval : renew Interval , clock : clock , on Repeated Heartbeat Failure : on Repeated Heartbeat } 
func ( c * controller ) Run ( stop Ch <- chan struct { } ) { if c . lease wait . Until ( c . sync , c . renew Interval , stop } 
func ( c * controller ) backoff Ensure for { lease , created , err = c . ensure sleep = min Duration ( 2 * sleep , max } 
func ( c * controller ) ensure Lease ( ) ( * coordv1beta1 . Lease , bool , error ) { lease , err := c . lease Client . Get ( c . holder Identity , metav1 . Get if apierrors . Is Not Found ( err ) { // lease does not exist, create it lease , err := c . lease Client . Create ( c . new } 
func ( c * controller ) retry Update Lease ( base * coordv1beta1 . Lease ) { for i := 0 ; i < max Update Retries ; i ++ { _ , err := c . lease Client . Update ( c . new if i > 0 && c . on Repeated Heartbeat Failure != nil { c . on Repeated Heartbeat klog . Errorf ( " " , max Update Retries , c . renew } 
func ( c * controller ) new if base == nil { lease = & coordv1beta1 . Lease { Object Meta : metav1 . Object Meta { Name : c . holder Identity , Namespace : corev1 . Namespace Node Lease , } , Spec : coordv1beta1 . Lease Spec { Holder Identity : pointer . String Ptr ( c . holder Identity ) , Lease Duration Seconds : pointer . Int32Ptr ( c . lease Duration } else { lease = base . Deep lease . Spec . Renew Time = & metav1 . Micro // Setting owner reference needs node's UID. Note that it is different from // kubelet.node Ref.UID. When lease is initially created, it is possible that // the connection between master and node is not ready yet. So try to set // owner reference every time when renewing the lease, until successful. if lease . Owner References == nil || len ( lease . Owner References ) == 0 { if node , err := c . client . Core V1 ( ) . Nodes ( ) . Get ( c . holder Identity , metav1 . Get Options { } ) ; err == nil { lease . Owner References = [ ] metav1 . Owner Reference { { API Version : corev1 . Scheme Group Version . With Kind ( " " ) . Version , Kind : corev1 . Scheme Group Version . With Kind ( " " ) . Kind , Name : c . holder } else { klog . Errorf ( " " , c . holder } 
func ( svc * Virtual Server ) Equal ( other * Virtual } 
func ( rs * Real Server ) Equal ( other * Real } 
func Get Kernel Version And IPVS Mods ( Executor exec . Interface ) ( kernel Version string , ipvs Modules [ ] string , err error ) { kernel Version out , err := Executor . Command ( " " , " " , " " , " " , kernel Version File ) . Combined kernel Version = strings . Trim // parse kernel version ver1 , err := version . Parse Generic ( kernel if err != nil { return kernel Version , nil , fmt . Errorf ( " " , err , kernel // "nf_conntrack_ipv4" has been removed since v4.19 // see https://github.com/torvalds/linux/commit/a0ae2562c6c4b2721d9fddba63b7286c13517d9f ver2 , _ := version . Parse // get required ipvs modules if ver1 . Less Than ( ver2 ) { ipvs Modules = append ( ipvs Modules , Mod IPVS , Mod IPVSRR , Mod IPVSWRR , Mod IPVSSH , Mod Nf Conntrack } else { ipvs Modules = append ( ipvs Modules , Mod IPVS , Mod IPVSRR , Mod IPVSWRR , Mod IPVSSH , Mod Nf return kernel Version , ipvs } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & Cluster Configuration { } , func ( obj interface { } ) { Set Object Defaults_Cluster Configuration ( obj . ( * Cluster scheme . Add Type Defaulting Func ( & Cluster Status { } , func ( obj interface { } ) { Set Object Defaults_Cluster Status ( obj . ( * Cluster scheme . Add Type Defaulting Func ( & Init Configuration { } , func ( obj interface { } ) { Set Object Defaults_Init Configuration ( obj . ( * Init scheme . Add Type Defaulting Func ( & Join Configuration { } , func ( obj interface { } ) { Set Object Defaults_Join Configuration ( obj . ( * Join } 
func New Auto Register Controller ( api Service Informer informers . API Service Informer , api Service Client apiregistrationclient . API Services Getter ) * auto Register Controller { c := & auto Register Controller { api Service Lister : api Service Informer . Lister ( ) , api Service Synced : api Service Informer . Informer ( ) . Has Synced , api Service Client : api Service Client , api Services To Sync : map [ string ] * apiregistration . API Service { } , api Services At Start : map [ string ] bool { } , synced Successfully Lock : & sync . RW Mutex { } , synced Successfully : map [ string ] bool { } , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate c . sync Handler = c . check API api Service Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { cast := obj . ( * apiregistration . API } , Update Func : func ( _ , obj interface { } ) { cast := obj . ( * apiregistration . API } , Delete Func : func ( obj interface { } ) { cast , ok := obj . ( * apiregistration . API if ! ok { tombstone , ok := obj . ( cache . Deleted Final State cast , ok = tombstone . Obj . ( * apiregistration . API } 
func ( c * auto Register Controller ) Run ( threadiness int , stop Ch <- chan struct { } ) { // don't let panics crash the process defer utilruntime . Handle // make sure the work queue is shutdown which will trigger workers to end defer c . queue . Shut // wait for your secondary caches to fill before starting your work if ! controllers . Wait For Cache Sync ( " " , stop Ch , c . api Service // record API Service objects that existed when we started if services , err := c . api Service Lister . List ( labels . Everything ( ) ) ; err == nil { for _ , service := range services { c . api Services At // start up your worker threads based on threadiness. Some controllers have multiple kinds of workers for i := 0 ; i < threadiness ; i ++ { // run Worker will loop until "something bad" happens. The .Until will then rekick the worker // after one second go wait . Until ( c . run Worker , time . Second , stop // wait until we're told to stop <- stop } 
func ( c * auto Register Controller ) check API Service ( name string ) ( err error ) { desired := c . Get API Service To curr , err := c . api Service // if we've never synced this service successfully, record a successful sync. has Synced := c . has Synced if ! has Synced { defer func ( ) { if err == nil { c . set Synced switch { // we had a real error, just return it (1A,1B,1C) case err != nil && ! apierrors . Is Not // we don't have an entry and we don't want one (2A) case apierrors . Is Not // the local object only wants to sync on start and has already synced (2B,5B,6B "once" enforcement) case is Automanaged On Start ( desired ) && has // we don't have an entry and we do want one (2B,2C) case apierrors . Is Not Found ( err ) && desired != nil : _ , err := c . api Service Client . API if apierrors . Is Already // we aren't trying to manage this API Service (3A,3B,3C) case ! is // the remote object only wants to sync on start, but was added after we started (4A,4B,4C) case is Automanaged On Start ( curr ) && ! c . api Services At // the remote object only wants to sync on start and has already synced (5A,5B,5C "once" enforcement) case is Automanaged On Start ( curr ) && has // we have a spurious API Service that we're managing, delete it (5A,6A) case desired == nil : opts := & metav1 . Delete Options { Preconditions : metav1 . New UID err := c . api Service Client . API if apierrors . Is Not Found ( err ) || apierrors . Is // if the specs already match, nothing for us to do case reflect . Deep // we have an entry and we have a desired, now we deconflict. Only a few fields matter. (5B,5C,6B,6C) api Service := curr . Deep api _ , err = c . api Service Client . API Services ( ) . Update ( api if apierrors . Is Not Found ( err ) || apierrors . Is } 
func ( c * auto Register Controller ) Get API Service To Sync ( name string ) * apiregistration . API Service { c . api Services To Sync Lock . R defer c . api Services To Sync Lock . R return c . api Services To } 
func ( c * auto Register Controller ) Add API Service To Sync On Start ( in * apiregistration . API Service ) { c . add API Service To Sync ( in , manage On } 
func ( c * auto Register Controller ) Add API Service To Sync ( in * apiregistration . API Service ) { c . add API Service To Sync ( in , manage } 
func ( c * auto Register Controller ) Remove API Service To Sync ( name string ) { c . api Services To Sync defer c . api Services To Sync delete ( c . api Services To } 
func ( dc * Deployment Controller ) rollout Recreate ( d * apps . Deployment , rs List [ ] * apps . Replica Set , pod Map map [ types . UID ] * v1 . Pod List ) error { // Don't create a new RS if not already existed, so that we avoid scaling up before scaling down. new RS , old R Ss , err := dc . get All Replica Sets And Sync Revision ( d , rs all R Ss := append ( old R Ss , new active Old R Ss := controller . Filter Active Replica Sets ( old R // scale down old replica sets. scaled Down , err := dc . scale Down Old Replica Sets For Recreate ( active Old R if scaled Down { // Update Deployment Status. return dc . sync Rollout Status ( all R Ss , new // Do not process a deployment when it has old pods running. if old Pods Running ( new RS , old R Ss , pod Map ) { return dc . sync Rollout Status ( all R Ss , new // If we need to create a new RS, create it now. if new RS == nil { new RS , old R Ss , err = dc . get All Replica Sets And Sync Revision ( d , rs all R Ss = append ( old R Ss , new // scale up new replica set. if _ , err := dc . scale Up New Replica Set For Recreate ( new if util . Deployment Complete ( d , & d . Status ) { if err := dc . cleanup Deployment ( old R // Sync deployment status. return dc . sync Rollout Status ( all R Ss , new } 
func ( dc * Deployment Controller ) scale Down Old Replica Sets For Recreate ( old R Ss [ ] * apps . Replica for i := range old R Ss { rs := old R scaled RS , updated RS , err := dc . scale Replica Set And Record if scaled RS { old R Ss [ i ] = updated } 
func old Pods Running ( new RS * apps . Replica Set , old R Ss [ ] * apps . Replica Set , pod Map map [ types . UID ] * v1 . Pod List ) bool { if old Pods := util . Get Actual Replica Count For Replica Sets ( old R Ss ) ; old for rs UID , pod List := range pod Map { // If the pods belong to the new Replica Set, ignore. if new RS != nil && new RS . UID == rs for _ , pod := range pod List . Items { switch pod . Status . Phase { case v1 . Pod Failed , v1 . Pod case v1 . Pod } 
func ( dc * Deployment Controller ) scale Up New Replica Set For Recreate ( new RS * apps . Replica Set , deployment * apps . Deployment ) ( bool , error ) { scaled , _ , err := dc . scale Replica Set And Record Event ( new } 
func ( a * claim Defaulter Plugin ) Validate } 
func ( a * claim Defaulter Plugin ) Admit ( attr admission . Attributes , o admission . Object Interfaces ) error { if attr . Get Resource ( ) . Group if len ( attr . Get pvc , ok := attr . Get Object ( ) . ( * api . Persistent Volume if helper . Persistent Volume Claim Has klog . V ( 4 ) . Infof ( " " , pvc . Name , pvc . Generate def , err := get Default if err != nil { return admission . New klog . V ( 4 ) . Infof ( " " , pvc . Name , pvc . Generate pvc . Spec . Storage Class } 
func get Default Class ( lister storagev1listers . Storage Class Lister ) ( * storagev1 . Storage default Classes := [ ] * storagev1 . Storage for _ , class := range list { if storageutil . Is Default Annotation ( class . Object Meta ) { default Classes = append ( default if len ( default if len ( default Classes ) > 1 { klog . V ( 4 ) . Infof ( " " , len ( default return nil , errors . New Internal Error ( fmt . Errorf ( " " , len ( default return default } 
func New CRI Stats Provider ( cadvisor cadvisor . Interface , resource Analyzer stats . Resource Analyzer , pod Manager kubepod . Manager , runtime Cache kubecontainer . Runtime Cache , runtime Service internalapi . Runtime Service , image Service internalapi . Image Manager Service , log Metrics Service Log Metrics Service , os Interface kubecontainer . OS Interface , ) * Stats Provider { return new Stats Provider ( cadvisor , pod Manager , runtime Cache , new CRI Stats Provider ( cadvisor , resource Analyzer , runtime Service , image Service , log Metrics Service , os } 
func New Cadvisor Stats Provider ( cadvisor cadvisor . Interface , resource Analyzer stats . Resource Analyzer , pod Manager kubepod . Manager , runtime Cache kubecontainer . Runtime Cache , image Service kubecontainer . Image Service , status Provider status . Pod Status Provider , ) * Stats Provider { return new Stats Provider ( cadvisor , pod Manager , runtime Cache , new Cadvisor Stats Provider ( cadvisor , resource Analyzer , image Service , status } 
func new Stats Provider ( cadvisor cadvisor . Interface , pod Manager kubepod . Manager , runtime Cache kubecontainer . Runtime Cache , container Stats Provider container Stats Provider , ) * Stats Provider { return & Stats Provider { cadvisor : cadvisor , pod Manager : pod Manager , runtime Cache : runtime Cache , container Stats Provider : container Stats } 
func ( p * Stats Provider ) Get Cgroup Stats ( cgroup Name string , update Stats bool ) ( * statsapi . Container Stats , * statsapi . Network Stats , error ) { info , err := get Cgroup Info ( p . cadvisor , cgroup Name , update if err != nil { return nil , nil , fmt . Errorf ( " " , cgroup // Rootfs and imagefs doesn't make sense for raw cgroup. s := cadvisor Info To Container Stats ( cgroup n := cadvisor Info To Network Stats ( cgroup } 
func ( p * Stats Provider ) Get Cgroup CPU And Memory Stats ( cgroup Name string , update Stats bool ) ( * statsapi . Container Stats , error ) { info , err := get Cgroup Info ( p . cadvisor , cgroup Name , update if err != nil { return nil , fmt . Errorf ( " " , cgroup // Rootfs and imagefs doesn't make sense for raw cgroup. s := cadvisor Info To Container CPU And Memory Stats ( cgroup } 
func ( p * Stats Provider ) Root Fs Stats ( ) ( * statsapi . Fs Stats , error ) { root Fs Info , err := p . cadvisor . Root Fs var node Fs Inodes if root Fs Info . Inodes != nil && root Fs Info . Inodes Free != nil { node Fs IU := * root Fs Info . Inodes - * root Fs Info . Inodes node Fs Inodes Used = & node Fs // Get the root container stats's timestamp, which will be used as the // image Fs stats timestamp. Dont force a stats update, as we only want the timestamp. root Stats , err := get Cgroup return & statsapi . Fs Stats { Time : metav1 . New Time ( root Stats . Timestamp ) , Available Bytes : & root Fs Info . Available , Capacity Bytes : & root Fs Info . Capacity , Used Bytes : & root Fs Info . Usage , Inodes Free : root Fs Info . Inodes Free , Inodes : root Fs Info . Inodes , Inodes Used : node Fs Inodes } 
func ( p * Stats Provider ) Get Container Info ( pod Full Name string , pod UID types . UID , container Name string , req * cadvisorapiv1 . Container Info Request ) ( * cadvisorapiv1 . Container Info , error ) { // Resolve and type convert back again. // We need the static pod UID but the kubecontainer API works with types.UID. pod UID = types . UID ( p . pod Manager . Translate Pod UID ( pod pods , err := p . runtime Cache . Get pod := kubecontainer . Pods ( pods ) . Find Pod ( pod Full Name , pod container := pod . Find Container By Name ( container if container == nil { return nil , kubecontainer . Err Container Not ci , err := p . cadvisor . Docker } 
func ( p * Stats Provider ) Get Raw Container Info ( container Name string , req * cadvisorapiv1 . Container Info Request , subcontainers bool ) ( map [ string ] * cadvisorapiv1 . Container Info , error ) { if subcontainers { return p . cadvisor . Subcontainer Info ( container container Info , err := p . cadvisor . Container Info ( container return map [ string ] * cadvisorapiv1 . Container Info { container Info . Name : container } 
func ( p * Stats Provider ) Has Dedicated Image Fs ( ) ( bool , error ) { device , err := p . container Stats Provider . Image Fs root Fs Info , err := p . cadvisor . Root Fs return device != root Fs } 
func New Cmd Cp ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Copy Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : " " , Example : cp Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmd . Flags ( ) . String Var cmd . Flags ( ) . Bool Var P ( & o . No } 
func ( o * Copy o . Namespace , _ , err = f . To Raw Kube Config o . Clientset , err = f . Kubernetes Client o . Client Config , err = f . To REST } 
func ( o * Copy Options ) Validate ( cmd * cobra . Command , args [ ] string ) error { if len ( args ) != 2 { return cmdutil . Usage Errorf ( cmd , cp Usage } 
func ( o * Copy src Spec , err := extract File dest Spec , err := extract File if len ( src Spec . Pod Name ) != 0 && len ( dest Spec . Pod Name ) != 0 { if _ , err := os . Stat ( args [ 0 ] ) ; err == nil { return o . copy To Pod ( file Spec { File : args [ 0 ] } , dest Spec , & exec . Exec if len ( src Spec . Pod Name ) != 0 { return o . copy From Pod ( src Spec , dest if len ( dest Spec . Pod Name ) != 0 { return o . copy To Pod ( src Spec , dest Spec , & exec . Exec } 
func ( o * Copy Options ) check Destination Is Dir ( dest file Spec ) error { options := & exec . Exec Options { Stream Options : exec . Stream Options { IO Streams : genericclioptions . IO Streams { Out : bytes . New Buffer ( [ ] byte { } ) , Err Out : bytes . New Buffer ( [ ] byte { } ) , } , Namespace : dest . Pod Namespace , Pod Name : dest . Pod Name , } , Command : [ ] string { " " , " " , dest . File } , Executor : & exec . Default Remote } 
func strip Path Shortcuts ( p string ) string { new trimmed := strings . Trim Prefix ( new for trimmed != new Path { new trimmed = strings . Trim Prefix ( new // trim leftover {".", ".."} if new Path == " " || new Path == " " { new if len ( new Path ) > 0 && string ( new Path [ 0 ] ) == " " { return new return new } 
func clean ( file Name string ) string { return path . Clean ( string ( os . Path Separator ) + file } 
func link Join ( base , link string ) string { if filepath . Is } 
func is Dest Relative ( base , dest string ) bool { full if ! filepath . Is Abs ( dest ) { full relative , err := filepath . Rel ( base , full return relative == " " || relative == strip Path } 
func ( v Element Building Visitor ) type Element ( meta apply . Field Meta Impl , item * type Item ) ( * apply . Type Element , error ) { // Function to get the schema of a field from its key var fn schema // Collect same fields from multiple maps into a map of elements values , err := v . create Map Values ( fn , meta , item . Map Element // Return the result return & apply . Type Element { Field Meta Impl : meta , Map Element Data : item . Map Element } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Deployment { } , & Deployment List { } , & Scale { } , & Stateful Set { } , & Stateful Set List { } , & Daemon Set { } , & Daemon Set List { } , & Replica Set { } , & Replica Set List { } , & Controller Revision { } , & Controller Revision metav1 . Add To Group Version ( scheme , Scheme Group } 
func New Map String String No Split ( m * map [ string ] string ) * Map String String { return & Map String String { Map : m , No } 
func ( m * Map String // account for comma-separated key-value pairs in a single invocation if ! m . No arr := strings . Split k := strings . Trim v := strings . Trim // account for only one key-value pair in a single invocation arr := strings . Split k := strings . Trim v := strings . Trim } 
func disk Set Up ( manager disk Manager , b fc Disk Mounter , vol Path string , mounter mount . Interface , fs Group * int64 ) error { global PD Path := manager . Make Global PD Name ( * b . fc no Mnt , err := mounter . Is Likely Not Mount Point ( vol if err != nil && ! os . Is Not Exist ( err ) { klog . Errorf ( " " , vol if ! no if err := os . Mkdir All ( vol Path , 0750 ) ; err != nil { klog . Errorf ( " " , vol if b . read mount Options := util . Join Mount Options ( options , b . mount err = mounter . Mount ( global PD Path , vol Path , " " , mount if err != nil { klog . Errorf ( " " , global PD Path , vol no Mnt , mnt Err := b . mounter . Is Likely Not Mount Point ( vol if mnt Err != nil { klog . Errorf ( " " , mnt if ! no Mnt { if mnt Err = b . mounter . Unmount ( vol Path ) ; mnt Err != nil { klog . Errorf ( " " , mnt no Mnt , mnt Err = b . mounter . Is Likely Not Mount Point ( vol if mnt Err != nil { klog . Errorf ( " " , mnt if ! no Mnt { // will most likely retry on next sync loop. klog . Errorf ( " " , vol os . Remove ( vol if ! b . read Only { volume . Set Volume Ownership ( & b , fs } 
func ( az * Cloud ) Node Addresses ( ctx context . Context , name types . Node Name ) ( [ ] v1 . Node Address , error ) { // Returns nil for unmanaged nodes because azure cloud provider couldn't fetch information for them. unmanaged , err := az . Is Node address Getter := func ( node Name types . Node Name ) ( [ ] v1 . Node Address , error ) { ip , public IP , err := az . get IP For Machine ( node if err != nil { klog . V ( 2 ) . Infof ( " " , node addresses := [ ] v1 . Node Address { { Type : v1 . Node Internal IP , Address : ip } , { Type : v1 . Node Host if len ( public IP ) > 0 { addresses = append ( addresses , v1 . Node Address { Type : v1 . Node External IP , Address : public if az . Use Instance Metadata { metadata , err := az . metadata . Get is Local Instance , err := az . is Current // Not local instance, get addresses from Azure ARM API. if ! is Local Instance { return address // Use ip address got from instance metadata. ip addresses := [ ] v1 . Node Address { { Type : v1 . Node Host if len ( ip Address . IPV4 . IP Address ) > 0 && len ( ip Address . IPV4 . IP Address [ 0 ] . Private IP ) > 0 { address := ip Address . IPV4 . IP addresses = append ( addresses , v1 . Node Address { Type : v1 . Node Internal IP , Address : address . Private if len ( address . Public IP ) > 0 { addresses = append ( addresses , v1 . Node Address { Type : v1 . Node External IP , Address : address . Public if len ( ip Address . IPV6 . IP Address ) > 0 && len ( ip Address . IPV6 . IP Address [ 0 ] . Private IP ) > 0 { address := ip Address . IPV6 . IP addresses = append ( addresses , v1 . Node Address { Type : v1 . Node Internal IP , Address : address . Private if len ( address . Public IP ) > 0 { addresses = append ( addresses , v1 . Node Address { Type : v1 . Node External IP , Address : address . Public if len ( addresses ) == 1 { // No IP addresses is got from instance metadata service, clean up cache and report errors. az . metadata . ims Cache . Delete ( metadata Cache return address } 
func ( az * Cloud ) Node Addresses By Provider ID ( ctx context . Context , provider ID string ) ( [ ] v1 . Node Address , error ) { // Returns nil for unmanaged nodes because azure cloud provider couldn't fetch information for them. if az . Is Node Unmanaged By Provider ID ( provider ID ) { klog . V ( 4 ) . Infof ( " " , provider name , err := az . vm Set . Get Node Name By Provider ID ( provider return az . Node } 
func ( az * Cloud ) Instance Exists By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { // Returns true for unmanaged nodes because azure cloud provider always assumes them exists. if az . Is Node Unmanaged By Provider ID ( provider ID ) { klog . V ( 4 ) . Infof ( " " , provider name , err := az . vm Set . Get Node Name By Provider ID ( provider if err != nil { if err == cloudprovider . Instance Not _ , err = az . Instance if err != nil { if err == cloudprovider . Instance Not } 
func ( az * Cloud ) Instance Shutdown By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { node Name , err := az . vm Set . Get Node Name By Provider ID ( provider power Status , err := az . vm Set . Get Power Status By Node Name ( string ( node klog . V ( 5 ) . Infof ( " " , power Status , node return strings . To Lower ( power Status ) == vm Power State Stopped || strings . To Lower ( power Status ) == vm Power State } 
func ( az * Cloud ) Instance ID ( ctx context . Context , name types . Node Name ) ( string , error ) { node Name := map Node Name To VM unmanaged , err := az . Is Node Unmanaged ( node if unmanaged { // Instance ID is same with node return node if az . Use Instance Metadata { metadata , err := az . metadata . Get is Local Instance , err := az . is Current // Not local instance, get instance ID from Azure ARM API. if ! is Local Instance { return az . vm Set . Get Instance ID By Node Name ( node // Get resource group name. resource Group := strings . To Lower ( metadata . Compute . Resource // Compose instance ID based on node Name for standard instance. if az . VM Type == vm Type Standard { return az . get Standard Machine ID ( resource Group , node // Get scale set name and instance ID from vm Name for vmss. ss Name , instance ID , err := extract Vmss VM if err != nil { if err == Error Not Vmss Instance { // Compose machine ID for standard Node. return az . get Standard Machine ID ( resource Group , node // Compose instance ID based on ss Name and instance ID for vmss instance. return az . get Vmss Machine ID ( resource Group , ss Name , instance return az . vm Set . Get Instance ID By Node Name ( node } 
func ( az * Cloud ) Instance Type By Provider ID ( ctx context . Context , provider ID string ) ( string , error ) { // Returns "" for unmanaged nodes because azure cloud provider couldn't fetch information for them. if az . Is Node Unmanaged By Provider ID ( provider ID ) { klog . V ( 4 ) . Infof ( " " , provider name , err := az . vm Set . Get Node Name By Provider ID ( provider return az . Instance } 
func ( az * Cloud ) Instance Type ( ctx context . Context , name types . Node Name ) ( string , error ) { // Returns "" for unmanaged nodes because azure cloud provider couldn't fetch information for them. unmanaged , err := az . Is Node if az . Use Instance Metadata { metadata , err := az . metadata . Get is Local Instance , err := az . is Current if is Local Instance { if metadata . Compute . VM Size != " " { return metadata . Compute . VM return az . vm Set . Get Instance Type By Node } 
func ( az * Cloud ) Add SSH Key To All Instances ( ctx context . Context , user string , key Data [ ] byte ) error { return cloudprovider . Not } 
func Add To Scheme ( scheme * runtime . Scheme ) { utilruntime . Must ( kubeschedulerconfig . Add To utilruntime . Must ( kubeschedulerconfigv1alpha1 . Add To utilruntime . Must ( scheme . Set Version Priority ( kubeschedulerconfigv1alpha1 . Scheme Group } 
func ( persistentvolumeclaim Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { pvc := obj . ( * api . Persistent Volume pvc . Status = api . Persistent Volume Claim pvcutil . Drop Disabled } 
func ( persistentvolumeclaim Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Pvc := obj . ( * api . Persistent Volume old Pvc := old . ( * api . Persistent Volume new Pvc . Status = old pvcutil . Drop Disabled Fields ( & new Pvc . Spec , & old } 
func ( persistentvolumeclaim Status Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Pv := obj . ( * api . Persistent Volume old Pv := old . ( * api . Persistent Volume new Pv . Spec = old if ! utilfeature . Default Feature Gate . Enabled ( features . Expand Persistent Volumes ) && old Pv . Status . Conditions == nil { new } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { persistentvolumeclaim Obj , ok := obj . ( * api . Persistent Volume return labels . Set ( persistentvolumeclaim Obj . Labels ) , Persistent Volume Claim To Selectable Fields ( persistentvolumeclaim } 
func Persistent Volume Claim To Selectable Fields ( persistentvolumeclaim * api . Persistent Volume Claim ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & persistentvolumeclaim . Object specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , specific Fields } 
func ( gc * Garbage Collector ) get Metadata ( api Version , kind , namespace , name string ) ( metav1 . Object , error ) { api Resource , _ , err := gc . api Resource ( api gc . dependency Graph Builder . monitor Lock . R defer gc . dependency Graph Builder . monitor Lock . R m , ok := gc . dependency Graph Builder . monitors [ api if ! ok || m == nil { // If local cache doesn't exist for mapping.Resource, send a GET request to API server return gc . dynamic Client . Resource ( api Resource ) . Namespace ( namespace ) . Get ( name , metav1 . Get raw , exist , err := m . store . Get By if ! exist { // If local cache doesn't contain the object, send a GET request to API server return gc . dynamic Client . Resource ( api Resource ) . Namespace ( namespace ) . Get ( name , metav1 . Get } 
func ( gc * Garbage Collector ) patch ( item * node , smp [ ] byte , jmp json Merge Patch Func ) ( * unstructured . Unstructured , error ) { smp Result , err := gc . patch Object ( item . identity , smp , types . Strategic Merge Patch if err == nil { return smp if ! errors . Is Unsupported Media // Strategic Merge return gc . patch Object ( item . identity , patch , types . Merge Patch } 
func ( gc * Garbage Collector ) delete Owner Ref JSON Merge Patch ( item * node , owner UI Ds ... types . UID ) ( [ ] byte , error ) { accessor , err := gc . get Metadata ( item . identity . API expected Object Meta := Object Meta For expected Object Meta . Resource Version = accessor . Get Resource refs := accessor . Get Owner for _ , owner UID := range owner UI Ds { if ref . UID == owner if ! skip { expected Object Meta . Owner References = append ( expected Object Meta . Owner return json . Marshal ( object For Patch { expected Object } 
func ( n * node ) unblock Owner References Strategic Merge Patch ( ) ( [ ] byte , error ) { var dummy metaonly . Metadata Only var blocking Refs [ ] metav1 . Owner false for _ , owner := range n . owners { if owner . Block Owner Deletion != nil && * owner . Block Owner ref . Block Owner Deletion = & false blocking Refs = append ( blocking dummy . Object Meta . Set Owner References ( blocking dummy . Object } 
func ( gc * Garbage Collector ) unblock Owner References JSON Merge Patch ( n * node ) ( [ ] byte , error ) { accessor , err := gc . get Metadata ( n . identity . API expected Object Meta := Object Meta For expected Object Meta . Resource Version = accessor . Get Resource var expected Owners [ ] metav1 . Owner false for _ , owner := range n . owners { owner . Block Owner Deletion = & false expected Owners = append ( expected expected Object Meta . Owner References = expected return json . Marshal ( object For Patch { expected Object } 
func ( v * version ) Leases ( ) Lease Informer { return & lease Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func New CSI Driver Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered CSI Driver Informer ( client , resync } 
func validate Kubelet OS Configuration ( kc * kubeletconfig . Kubelet all if kc . Cgroups Per QOS { all Errors = append ( all Errors , fmt . Errorf ( message , " " , " " , kc . Cgroups Per if len ( kc . Enforce Node Allocatable ) > 0 { all Errors = append ( all Errors , fmt . Errorf ( message , " " , " " , kc . Enforce Node return utilerrors . New Aggregate ( all } 
func ( c * Fake Priority Classes ) List ( opts v1 . List Options ) ( result * schedulingv1 . Priority Class List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( priorityclasses Resource , priorityclasses Kind , opts ) , & schedulingv1 . Priority Class label , _ , _ := testing . Extract From List list := & schedulingv1 . Priority Class List { List Meta : obj . ( * schedulingv1 . Priority Class List ) . List for _ , item := range obj . ( * schedulingv1 . Priority Class } 
func Deployed DNS Addon ( client clientset . Interface ) ( kubeadmapi . DNS Add On Type , string , error ) { deployments Client := client . Apps V1 ( ) . Deployments ( metav1 . Namespace deployments , err := deployments Client . List ( metav1 . List Options { Label case 1 : addon addon Type := kubeadmapi . Core if addon Name == kubeadmconstants . Kube DNS Deployment Name { addon Type = kubeadmapi . Kube addon addon Image Parts := strings . Split ( addon addon Version := addon Image Parts [ len ( addon Image return addon Type , addon } 
func Ensure DNS Addon ( cfg * kubeadmapi . Cluster Configuration , client clientset . Interface ) error { if cfg . DNS . Type == kubeadmapi . Core DNS { return core DNS return kube DNS } 
func Create Service Account ( client clientset . Interface ) error { return apiclient . Create Or Update Service Account ( client , & v1 . Service Account { Object Meta : metav1 . Object Meta { Name : Kube DNS Service Account Name , Namespace : metav1 . Namespace } 
func translate Stub Domain Of Kube DNS To Forward Core DNS ( data Field string , kube DNS Config Map * v1 . Config Map ) ( string , error ) { if kube DNS Config if proxy , ok := kube DNS Config Map . Data [ data Field ] ; ok { stub Domain err := json . Unmarshal ( [ ] byte ( proxy ) , & stub Domain var proxy for domain , proxy Hosts := range stub Domain Data { proxy IP , err := omit Hostname In Translation ( proxy if len ( proxy p p p Stanza [ " " ] = [ ] [ ] string { { " " } , { " " , " " } , { " " } , append ( [ ] string { " " , " " } , proxy proxy Stanza = append ( proxy Stanza , p stanzas Bytes , err := json . Marshal ( proxy corefile Stanza , err := caddyfile . From JSON ( stanzas return prep Corefile Format ( string ( corefile } 
func translate Upstream Name Server Of Kube DNS To Upstream Forward Core DNS ( data Field string , kube DNS Config Map * v1 . Config Map ) ( string , error ) { if kube DNS Config if upstream Values , ok := kube DNS Config Map . Data [ data Field ] ; ok { var upstream Proxy err := json . Unmarshal ( [ ] byte ( upstream Values ) , & upstream Proxy upstream Proxy Values , err = omit Hostname In Translation ( upstream Proxy core DNS Proxy Stanza List := strings . Join ( upstream Proxy return core DNS Proxy Stanza } 
func translate Federationsof Kube DNS To Core DNS ( data Field , core DNS Domain string , kube DNS Config Map * v1 . Config Map ) ( string , error ) { if kube DNS Config if federation , ok := kube DNS Config Map . Data [ data Field ] ; ok { var ( federation federation err := json . Unmarshal ( [ ] byte ( federation ) , & federation f for name , domain := range federation federation Stanza = append ( federation Stanza , f f Stanza [ " " ] = [ ] string { " " + core DNS f stanzas Bytes , err := json . Marshal ( federation corefile Stanza , err := caddyfile . From JSON ( stanzas return prep Corefile Format ( string ( corefile } 
func prep Corefile } 
func omit Hostname In Translation ( forward I for _ , value := range forward I Ps { proxy Host , _ , err := kubeadmutil . Parse Host parse IP := net . Parse IP ( proxy if parse IP == nil { klog . Warningf ( " " , proxy } else { forward I forward I Ps = forward I return forward I } 
func ( cephfs Volume * cephfs Mounter ) Set Up ( fs Group * int64 ) error { return cephfs Volume . Set Up At ( cephfs Volume . Get Path ( ) , fs } 
func ( cephfs Volume * cephfs Mounter ) Set Up At ( dir string , fs Group * int64 ) error { not Mnt , err := cephfs Volume . mounter . Is Likely Not Mount klog . V ( 4 ) . Infof ( " " , dir , ! not if err != nil && ! os . Is Not if ! not if err := os . Mkdir // check whether it belongs to fuse, if not, default to use kernel mount. if cephfs Volume . check Fuse err = cephfs Volume . exec Fuse // cleanup no matter if fuse mount fail. keyring Path := cephfs Volume . Get Keyring _ , Stat Err := os . Stat ( keyring if ! os . Is Not Exist ( Stat Err ) { os . Remove All ( keyring err = cephfs Volume . exec if err != nil { // cleanup upon failure. mount . Cleanup Mount Point ( dir , cephfs } 
func ( cephfs Volume * cephfs Unmounter ) Tear Down At ( dir string ) error { return mount . Cleanup Mount Point ( dir , cephfs } 
func ( cephfs Volume * cephfs ) Get Path ( ) string { name := cephfs Plugin return cephfs Volume . plugin . host . Get Pod Volume Dir ( cephfs Volume . pod UID , utilstrings . Escape Qualified Name ( name ) , cephfs Volume . vol } 
func ( cephfs Volume * cephfs ) Get Keyring Path ( ) string { name := cephfs Plugin volume Dir := cephfs Volume . plugin . host . Get Pod Volume Dir ( cephfs Volume . pod UID , utilstrings . Escape Qualified Name ( name ) , cephfs Volume . vol volume Keyring Dir := volume return volume Keyring } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( events . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1beta1 . Scheme Group } 
func New Field Manager ( models openapiproto . Models , object Converter runtime . Object Convertor , object Defaulter runtime . Object Defaulter , gv schema . Group Version , hub schema . Group Version ) ( * Field Manager , error ) { type Converter , err := internal . New Type return & Field Manager { type Converter : type Converter , object Converter : object Converter , object Defaulter : object Defaulter , group Version : gv , hub Version : hub , updater : merge . Updater { Converter : internal . New Version Converter ( type Converter , object } 
func New CRD Field Manager ( object Converter runtime . Object Convertor , object Defaulter runtime . Object Defaulter , gv schema . Group Version , hub schema . Group Version ) * Field Manager { return & Field Manager { type Converter : internal . Deduced Type Converter { } , object Converter : object Converter , object Defaulter : object Defaulter , group Version : gv , hub Version : hub , updater : merge . Updater { Converter : internal . New CRD Version Converter ( internal . Deduced Type Converter { } , object } 
func ( f * Field Manager ) Update ( live Obj , new Obj runtime . Object , manager string ) ( runtime . Object , error ) { // If the object doesn't have metadata, we should just return without trying to // set the managed Fields at all, so creates/updates/patches will work normally. if _ , err := meta . Accessor ( new Obj ) ; err != nil { return new // First try to decode the managed fields provided in the update, // This is necessary to allow directly updating managed fields. managed , err := internal . Decode Object Managed Fields ( new // If the managed field is empty or we failed to decode it, // let's try the live object. This is to prevent clients who // don't understand managed Fields from deleting it accidentally. if err != nil || len ( managed ) == 0 { managed , err = internal . Decode Object Managed Fields ( live new Obj Versioned , err := f . to Versioned ( new live Obj Versioned , err := f . to Versioned ( live internal . Remove Object Managed Fields ( live Obj internal . Remove Object Managed Fields ( new Obj new Obj Typed , err := f . type Converter . Object To Typed ( new Obj live Obj Typed , err := f . type Converter . Object To Typed ( live Obj api Version := fieldpath . API Version ( f . group manager , err = f . build Manager Info ( manager , metav1 . Managed Fields Operation managed , err = f . updater . Update ( live Obj Typed , new Obj Typed , api managed = f . strip if err := internal . Encode Object Managed Fields ( new return new } 
func ( f * Field Manager ) Apply ( live Obj runtime . Object , patch [ ] byte , field Manager string , force bool ) ( runtime . Object , error ) { // If the object doesn't have metadata, apply isn't allowed. if _ , err := meta . Accessor ( live managed , err := internal . Decode Object Managed Fields ( live // Check that the patch object has the same version as the live object patch if err := yaml . Unmarshal ( patch , & patch if patch Obj . Get API Version ( ) != f . group Version . String ( ) { return nil , errors . New Bad Request ( fmt . Sprintf ( " " + " " , patch Obj . Get API Version ( ) , f . group live Obj Versioned , err := f . to Versioned ( live internal . Remove Object Managed Fields ( live Obj patch Obj Typed , err := f . type Converter . YAML To live Obj Typed , err := f . type Converter . Object To Typed ( live Obj manager , err := f . build Manager Info ( field Manager , metav1 . Managed Fields Operation api Version := fieldpath . API Version ( f . group new Obj Typed , managed , err := f . updater . Apply ( live Obj Typed , patch Obj Typed , api if err != nil { if conflicts , ok := err . ( merge . Conflicts ) ; ok { return nil , internal . New Conflict managed = f . strip new Obj , err := f . type Converter . Typed To Object ( new Obj if err := internal . Encode Object Managed Fields ( new new Obj Versioned , err := f . to Versioned ( new f . object Defaulter . Default ( new Obj new Obj Unversioned , err := f . to Unversioned ( new Obj return new Obj } 
func ( f * Field Manager ) strip Fields ( managed fieldpath . Managed Fields , manager string ) fieldpath . Managed vs . Set = vs . Set . Difference ( strip } 
func New Certificate Authority ( config * certutil . Config ) ( * x509 . Certificate , crypto . Signer , error ) { key , err := New Private cert , err := certutil . New Self Signed CA } 
func New Cert And Key ( ca Cert * x509 . Certificate , ca Key crypto . Signer , config * certutil . Config ) ( * x509 . Certificate , crypto . Signer , error ) { key , err := New Private cert , err := New Signed Cert ( config , key , ca Cert , ca } 
func New CSR And Key ( config * certutil . Config ) ( * x509 . Certificate Request , crypto . Signer , error ) { key , err := New Private csr , err := New } 
func Has Server Auth ( cert * x509 . Certificate ) bool { for i := range cert . Ext Key Usage { if cert . Ext Key Usage [ i ] == x509 . Ext Key Usage Server } 
func Write Cert And Key ( pki Path string , name string , cert * x509 . Certificate , key crypto . Signer ) error { if err := Write Key ( pki return Write Cert ( pki } 
func Write Cert ( pki certificate Path := path For Cert ( pki if err := certutil . Write Cert ( certificate Path , Encode Cert PEM ( cert ) ) ; err != nil { return errors . Wrapf ( err , " " , certificate } 
func Write Key ( pki private Key Path := path For Key ( pki encoded , err := keyutil . Marshal Private Key To if err := keyutil . Write Key ( private Key Path , encoded ) ; err != nil { return errors . Wrapf ( err , " " , private Key } 
func Write CSR ( csr Dir , name string , csr * x509 . Certificate csr Path := path For CSR ( csr if err := os . Mkdir All ( filepath . Dir ( csr Path ) , os . File Mode ( 0755 ) ) ; err != nil { return errors . Wrapf ( err , " " , filepath . Dir ( csr if err := ioutil . Write File ( csr Path , Encode CSRPEM ( csr ) , os . File Mode ( 0644 ) ) ; err != nil { return errors . Wrapf ( err , " " , csr } 
func Write Public Key ( pki Path , name string , key crypto . Public public Key Bytes , err := Encode Public Key public Key Path := path For Public Key ( pki if err := keyutil . Write Key ( public Key Path , public Key Bytes ) ; err != nil { return errors . Wrapf ( err , " " , public Key } 
func Cert Or Key Exist ( pki Path , name string ) bool { certificate Path , private Key Path := Paths For Cert And Key ( pki _ , cert Err := os . Stat ( certificate _ , key Err := os . Stat ( private Key if os . Is Not Exist ( cert Err ) && os . Is Not Exist ( key } 
func CSR Or Key Exist ( csr Dir , name string ) bool { csr Path := path For CSR ( csr key Path := path For Key ( csr _ , csr Err := os . Stat ( csr _ , key Err := os . Stat ( key return ! ( os . Is Not Exist ( csr Err ) && os . Is Not Exist ( key } 
func Try Load Cert And Key From Disk ( pki Path , name string ) ( * x509 . Certificate , crypto . Signer , error ) { cert , err := Try Load Cert From Disk ( pki key , err := Try Load Key From Disk ( pki } 
func Try Load Cert From Disk ( pki Path , name string ) ( * x509 . Certificate , error ) { certificate Path := path For Cert ( pki certs , err := certutil . Certs From File ( certificate if err != nil { return nil , errors . Wrapf ( err , " " , certificate if now . Before ( cert . Not if now . After ( cert . Not } 
func Try Load Key From Disk ( pki Path , name string ) ( crypto . Signer , error ) { private Key Path := path For Key ( pki // Parse the private key from a file priv Key , err := keyutil . Private Key From File ( private Key if err != nil { return nil , errors . Wrapf ( err , " " , private Key switch k := priv Key . ( type ) { case * rsa . Private case * ecdsa . Private default : return nil , errors . Errorf ( " " , private Key } 
func Try Load CSR And Key From Disk ( pki Path , name string ) ( * x509 . Certificate Request , crypto . Signer , error ) { csr Path := path For CSR ( pki csr , err := Certificate Request From File ( csr if err != nil { return nil , nil , errors . Wrapf ( err , " " , csr key , err := Try Load Key From Disk ( pki } 
func Try Load Private Public Key From Disk ( pki Path , name string ) ( * rsa . Private Key , * rsa . Public Key , error ) { private Key Path := path For Key ( pki // Parse the private key from a file priv Key , err := keyutil . Private Key From File ( private Key if err != nil { return nil , nil , errors . Wrapf ( err , " " , private Key public Key Path := path For Public Key ( pki // Parse the public key from a file pub Keys , err := keyutil . Public Keys From File ( public Key if err != nil { return nil , nil , errors . Wrapf ( err , " " , public Key // Allow RSA format only k , ok := priv Key . ( * rsa . Private if ! ok { return nil , nil , errors . Errorf ( " " , private Key p := pub Keys [ 0 ] . ( * rsa . Public } 
func Paths For Cert And Key ( pki Path , name string ) ( string , string ) { return path For Cert ( pki Path , name ) , path For Key ( pki } 
func Get API Server Alt Names ( cfg * kubeadmapi . Init Configuration ) ( * certutil . Alt Names , error ) { // advertise address advertise Address := net . Parse IP ( cfg . Local API Endpoint . Advertise if advertise Address == nil { return nil , errors . Errorf ( " " , cfg . Local API Endpoint . Advertise // internal IP address for the API server _ , svc Subnet , err := net . Parse CIDR ( cfg . Networking . Service if err != nil { return nil , errors . Wrapf ( err , " " , cfg . Networking . Service internal API Server Virtual IP , err := ipallocator . Get Indexed IP ( svc if err != nil { return nil , errors . Wrapf ( err , " " , svc // create Alt Names with defaults DNS Names/I Ps alt Names := & certutil . Alt Names { DNS Names : [ ] string { cfg . Node Registration . Name , " " , " " , " " , fmt . Sprintf ( " " , cfg . Networking . DNS Domain ) , } , I Ps : [ ] net . IP { internal API Server Virtual IP , advertise // add cluster control Plane Endpoint if present (dns or ip) if len ( cfg . Control Plane Endpoint ) > 0 { if host , _ , err := kubeadmutil . Parse Host Port ( cfg . Control Plane Endpoint ) ; err == nil { if ip := net . Parse IP ( host ) ; ip != nil { alt Names . I Ps = append ( alt Names . I } else { alt Names . DNS Names = append ( alt Names . DNS } else { return nil , errors . Wrapf ( err , " " , cfg . Control Plane append SA Ns To Alt Names ( alt Names , cfg . API Server . Cert SA Ns , kubeadmconstants . API Server Cert return alt } 
func Get Etcd Alt Names ( cfg * kubeadmapi . Init Configuration ) ( * certutil . Alt Names , error ) { return get Alt Names ( cfg , kubeadmconstants . Etcd Server Cert } 
func Get Etcd Peer Alt Names ( cfg * kubeadmapi . Init Configuration ) ( * certutil . Alt Names , error ) { return get Alt Names ( cfg , kubeadmconstants . Etcd Peer Cert } 
func get Alt Names ( cfg * kubeadmapi . Init Configuration , cert Name string ) ( * certutil . Alt Names , error ) { // advertise address advertise Address := net . Parse IP ( cfg . Local API Endpoint . Advertise if advertise Address == nil { return nil , errors . Errorf ( " " , cfg . Local API Endpoint . Advertise // create Alt Names with defaults DNS Names/I Ps alt Names := & certutil . Alt Names { DNS Names : [ ] string { cfg . Node Registration . Name , " " } , I Ps : [ ] net . IP { advertise Address , net . I Pv4 ( 127 , 0 , 0 , 1 ) , net . I if cfg . Etcd . Local != nil { if cert Name == kubeadmconstants . Etcd Server Cert Name { append SA Ns To Alt Names ( alt Names , cfg . Etcd . Local . Server Cert SA Ns , kubeadmconstants . Etcd Server Cert } else if cert Name == kubeadmconstants . Etcd Peer Cert Name { append SA Ns To Alt Names ( alt Names , cfg . Etcd . Local . Peer Cert SA Ns , kubeadmconstants . Etcd Peer Cert return alt } 
func append SA Ns To Alt Names ( alt Names * certutil . Alt Names , SA Ns [ ] string , cert Name string ) { for _ , altname := range SA Ns { if ip := net . Parse IP ( altname ) ; ip != nil { alt Names . I Ps = append ( alt Names . I } else if len ( validation . Is DNS1123Subdomain ( altname ) ) == 0 { alt Names . DNS Names = append ( alt Names . DNS } else if len ( validation . Is Wildcard DNS1123Subdomain ( altname ) ) == 0 { alt Names . DNS Names = append ( alt Names . DNS } else { fmt . Printf ( " \n " , altname , cert } 
func Encode CSRPEM ( csr * x509 . Certificate Request ) [ ] byte { block := pem . Block { Type : certutil . Certificate Request Block return pem . Encode To } 
func Certificate Request From File ( file string ) ( * x509 . Certificate Request , error ) { pem Block , err := ioutil . Read csr , err := parse CSRPEM ( pem } 
func New CSR ( cfg certutil . Config , key crypto . Signer ) ( * x509 . Certificate Request , error ) { template := & x509 . Certificate Request { Subject : pkix . Name { Common Name : cfg . Common Name , Organization : cfg . Organization , } , DNS Names : cfg . Alt Names . DNS Names , IP Addresses : cfg . Alt Names . I csr Bytes , err := x509 . Create Certificate return x509 . Parse Certificate Request ( csr } 
func Encode Cert PEM ( cert * x509 . Certificate ) [ ] byte { block := pem . Block { Type : Certificate Block return pem . Encode To } 
func New Signed Cert ( cfg * certutil . Config , key crypto . Signer , ca Cert * x509 . Certificate , ca Key crypto . Signer ) ( * x509 . Certificate , error ) { serial , err := rand . Int ( rand . Reader , new ( big . Int ) . Set Int64 ( math . Max if len ( cfg . Common cert Tmpl := x509 . Certificate { Subject : pkix . Name { Common Name : cfg . Common Name , Organization : cfg . Organization , } , DNS Names : cfg . Alt Names . DNS Names , IP Addresses : cfg . Alt Names . I Ps , Serial Number : serial , Not Before : ca Cert . Not Before , Not After : time . Now ( ) . Add ( duration365d ) . UTC ( ) , Key Usage : x509 . Key Usage Key Encipherment | x509 . Key Usage Digital Signature , Ext Key cert DER Bytes , err := x509 . Create Certificate ( cryptorand . Reader , & cert Tmpl , ca Cert , key . Public ( ) , ca return x509 . Parse Certificate ( cert DER } 
func ( in * Pod Spec ) Deep Copy Into ( out * Pod if in . Termination Grace Period Seconds != nil { in , out := & in . Termination Grace Period Seconds , & out . Termination Grace Period if in . Active Deadline Seconds != nil { in , out := & in . Active Deadline Seconds , & out . Active Deadline if in . Node Selector != nil { in , out := & in . Node Selector , & out . Node } 
func ( in * Pod Status ) Deep Copy Into ( out * Pod * out = make ( [ ] Pod for i := range * in { ( * in ) [ i ] . Deep Copy if in . Start Time != nil { in , out := & in . Start Time , & out . Start * out = ( * in ) . Deep } 
func ( in * Replica Set ) Deep Copy Into ( out * Replica out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object } 
func New Reconciler ( loop Period time . Duration , max Wait For Unmount Duration time . Duration , sync Duration time . Duration , disable Reconciliation Sync bool , desired State Of World cache . Desired State Of World , actual State Of World cache . Actual State Of World , attacher Detacher operationexecutor . Operation Executor , node Status Updater statusupdater . Node Status Updater , recorder record . Event Recorder ) Reconciler { return & reconciler { loop Period : loop Period , max Wait For Unmount Duration : max Wait For Unmount Duration , sync Duration : sync Duration , disable Reconciliation Sync : disable Reconciliation Sync , desired State Of World : desired State Of World , actual State Of World : actual State Of World , attacher Detacher : attacher Detacher , node Status Updater : node Status Updater , time Of Last } 
func ( rc * reconciler ) reconciliation Loop if rc . disable Reconciliation } else if rc . sync } else if time . Since ( rc . time Of Last Sync ) > rc . sync } 
func ( rc * reconciler ) is Multi Attach Forbidden ( volume Spec * volume . Spec ) bool { if volume Spec . Volume != nil { // Check for volume types which are known to fail slow or cause trouble when trying to multi-attach if volume Spec . Volume . Azure Disk != nil || volume // Only if this volume is a persistent volume, we have reliable information on whether it's allowed or not to // multi-attach. We trust in the individual volume implementations to not allow unsupported access modes if volume Spec . Persistent Volume != nil { // Check for persistent volume types which do not fail when trying to multi-attach if len ( volume Spec . Persistent Volume . Spec . Access // check if this volume is allowed to be attached to multiple PO Ds/nodes, if yes, return false for _ , access Mode := range volume Spec . Persistent Volume . Spec . Access Modes { if access Mode == v1 . Read Write Many || access Mode == v1 . Read Only } 
func ( rc * reconciler ) report Multi Attach Error ( volume To Attach cache . Volume To Attach , nodes [ ] types . Node Name ) { // Filter out the current node from list of nodes where the volume is // attached. // Some methods need []string, some other needs []Node Name, collect both. // In theory, these arrays should have always only one element - the // controller does not allow more than one attachment. But use array just // in case... other Nodes := [ ] types . Node other Nodes for _ , node := range nodes { if node != volume To Attach . Node Name { other Nodes = append ( other other Nodes Str = append ( other Nodes // Get list of pods that use the volume on the other nodes. pods := rc . desired State Of World . Get Volume Pods On Nodes ( other Nodes , volume To Attach . Volume if len ( pods ) == 0 { // We did not find any pods that requests the volume. The pod must have been deleted already. simple Msg , _ := volume To Attach . Generate for _ , pod := range volume To Attach . Scheduled Pods { rc . recorder . Eventf ( pod , v1 . Event Type Warning , kevents . Failed Attach Volume , simple // Log detailed message to system admin node List := strings . Join ( other Nodes detailed Msg := volume To Attach . Generate Msg Detailed ( " " , fmt . Sprintf ( " " , node klog . Warningf ( detailed // There are pods that require the volume and run on another node. Typically // it's user error, e.g. a Replica Set uses a PVC and has >1 replicas. Let // the user know what pods are blocking the volume. for _ , scheduled Pod := range volume To Attach . Scheduled Pods { // Each scheduled Pod must get a custom message. They can run in // different namespaces and user of a namespace should not see names of // pods in other namespaces. local Pod Names := [ ] string { } // Names of pods in scheduled other for _ , pod := range pods { if pod . Namespace == scheduled Pod . Namespace { local Pod Names = append ( local Pod } else { other if len ( local Pod Names ) > 0 { msg = fmt . Sprintf ( " " , strings . Join ( local Pod if other Pods > 0 { msg = fmt . Sprintf ( " " , msg , other } else { // No local pods, there are pods only in different namespaces. msg = fmt . Sprintf ( " " , other simple Msg , _ := volume To Attach . Generate rc . recorder . Eventf ( scheduled Pod , v1 . Event Type Warning , kevents . Failed Attach Volume , simple // Log all pods for system admin pod for _ , pod := range pods { pod Names = append ( pod detailed Msg := volume To Attach . Generate Msg Detailed ( " " , fmt . Sprintf ( " " , strings . Join ( pod Names , " " ) , strings . Join ( other Nodes klog . Warningf ( detailed } 
func New Replication Controller Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Replication Controller Informer ( client , namespace , resync } 
func parse KV ( kv * mvccpb . Key Value ) * event { return & event { key : string ( kv . Key ) , value : kv . Value , prev Value : nil , rev : kv . Mod Revision , is Deleted : false , is } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . HPA Controller Configuration ) ( nil ) , ( * config . HPA Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_HPA Controller Configuration_To_config_HPA Controller Configuration ( a . ( * v1alpha1 . HPA Controller Configuration ) , b . ( * config . HPA Controller if err := s . Add Generated Conversion Func ( ( * config . HPA Controller Configuration ) ( nil ) , ( * v1alpha1 . HPA Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_HPA Controller Configuration_To_v1alpha1_HPA Controller Configuration ( a . ( * config . HPA Controller Configuration ) , b . ( * v1alpha1 . HPA Controller if err := s . Add Conversion Func ( ( * config . HPA Controller Configuration ) ( nil ) , ( * v1alpha1 . HPA Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_HPA Controller Configuration_To_v1alpha1_HPA Controller Configuration ( a . ( * config . HPA Controller Configuration ) , b . ( * v1alpha1 . HPA Controller if err := s . Add Conversion Func ( ( * v1alpha1 . HPA Controller Configuration ) ( nil ) , ( * config . HPA Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_HPA Controller Configuration_To_config_HPA Controller Configuration ( a . ( * v1alpha1 . HPA Controller Configuration ) , b . ( * config . HPA Controller } 
func ( g * expansion } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Bound Object Reference ) ( nil ) , ( * authentication . Bound Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Bound Object Reference_To_authentication_Bound Object Reference ( a . ( * v1 . Bound Object Reference ) , b . ( * authentication . Bound Object if err := s . Add Generated Conversion Func ( ( * authentication . Bound Object Reference ) ( nil ) , ( * v1 . Bound Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Bound Object Reference_To_v1_Bound Object Reference ( a . ( * authentication . Bound Object Reference ) , b . ( * v1 . Bound Object if err := s . Add Generated Conversion Func ( ( * v1 . Token Request ) ( nil ) , ( * authentication . Token Request ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Token Request_To_authentication_Token Request ( a . ( * v1 . Token Request ) , b . ( * authentication . Token if err := s . Add Generated Conversion Func ( ( * authentication . Token Request ) ( nil ) , ( * v1 . Token Request ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Request_To_v1_Token Request ( a . ( * authentication . Token Request ) , b . ( * v1 . Token if err := s . Add Generated Conversion Func ( ( * v1 . Token Request Spec ) ( nil ) , ( * authentication . Token Request Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Token Request Spec_To_authentication_Token Request Spec ( a . ( * v1 . Token Request Spec ) , b . ( * authentication . Token Request if err := s . Add Generated Conversion Func ( ( * authentication . Token Request Spec ) ( nil ) , ( * v1 . Token Request Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Request Spec_To_v1_Token Request Spec ( a . ( * authentication . Token Request Spec ) , b . ( * v1 . Token Request if err := s . Add Generated Conversion Func ( ( * v1 . Token Request Status ) ( nil ) , ( * authentication . Token Request Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Token Request Status_To_authentication_Token Request Status ( a . ( * v1 . Token Request Status ) , b . ( * authentication . Token Request if err := s . Add Generated Conversion Func ( ( * authentication . Token Request Status ) ( nil ) , ( * v1 . Token Request Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Request Status_To_v1_Token Request Status ( a . ( * authentication . Token Request Status ) , b . ( * v1 . Token Request if err := s . Add Generated Conversion Func ( ( * v1 . Token Review ) ( nil ) , ( * authentication . Token Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Token Review_To_authentication_Token Review ( a . ( * v1 . Token Review ) , b . ( * authentication . Token if err := s . Add Generated Conversion Func ( ( * authentication . Token Review ) ( nil ) , ( * v1 . Token Review ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Review_To_v1_Token Review ( a . ( * authentication . Token Review ) , b . ( * v1 . Token if err := s . Add Generated Conversion Func ( ( * v1 . Token Review Spec ) ( nil ) , ( * authentication . Token Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Token Review Spec_To_authentication_Token Review Spec ( a . ( * v1 . Token Review Spec ) , b . ( * authentication . Token Review if err := s . Add Generated Conversion Func ( ( * authentication . Token Review Spec ) ( nil ) , ( * v1 . Token Review Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Review Spec_To_v1_Token Review Spec ( a . ( * authentication . Token Review Spec ) , b . ( * v1 . Token Review if err := s . Add Generated Conversion Func ( ( * v1 . Token Review Status ) ( nil ) , ( * authentication . Token Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Token Review Status_To_authentication_Token Review Status ( a . ( * v1 . Token Review Status ) , b . ( * authentication . Token Review if err := s . Add Generated Conversion Func ( ( * authentication . Token Review Status ) ( nil ) , ( * v1 . Token Review Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_Token Review Status_To_v1_Token Review Status ( a . ( * authentication . Token Review Status ) , b . ( * v1 . Token Review if err := s . Add Generated Conversion Func ( ( * v1 . User Info ) ( nil ) , ( * authentication . User Info ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_User Info_To_authentication_User Info ( a . ( * v1 . User Info ) , b . ( * authentication . User if err := s . Add Generated Conversion Func ( ( * authentication . User Info ) ( nil ) , ( * v1 . User Info ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_authentication_User Info_To_v1_User Info ( a . ( * authentication . User Info ) , b . ( * v1 . User } 
func Convert_v1_Bound Object Reference_To_authentication_Bound Object Reference ( in * v1 . Bound Object Reference , out * authentication . Bound Object Reference , s conversion . Scope ) error { return auto Convert_v1_Bound Object Reference_To_authentication_Bound Object } 
func Convert_authentication_Bound Object Reference_To_v1_Bound Object Reference ( in * authentication . Bound Object Reference , out * v1 . Bound Object Reference , s conversion . Scope ) error { return auto Convert_authentication_Bound Object Reference_To_v1_Bound Object } 
func Convert_v1_Token Request_To_authentication_Token Request ( in * v1 . Token Request , out * authentication . Token Request , s conversion . Scope ) error { return auto Convert_v1_Token Request_To_authentication_Token } 
func Convert_authentication_Token Request_To_v1_Token Request ( in * authentication . Token Request , out * v1 . Token Request , s conversion . Scope ) error { return auto Convert_authentication_Token Request_To_v1_Token } 
func Convert_v1_Token Request Spec_To_authentication_Token Request Spec ( in * v1 . Token Request Spec , out * authentication . Token Request Spec , s conversion . Scope ) error { return auto Convert_v1_Token Request Spec_To_authentication_Token Request } 
func Convert_authentication_Token Request Spec_To_v1_Token Request Spec ( in * authentication . Token Request Spec , out * v1 . Token Request Spec , s conversion . Scope ) error { return auto Convert_authentication_Token Request Spec_To_v1_Token Request } 
func Convert_v1_Token Request Status_To_authentication_Token Request Status ( in * v1 . Token Request Status , out * authentication . Token Request Status , s conversion . Scope ) error { return auto Convert_v1_Token Request Status_To_authentication_Token Request } 
func Convert_authentication_Token Request Status_To_v1_Token Request Status ( in * authentication . Token Request Status , out * v1 . Token Request Status , s conversion . Scope ) error { return auto Convert_authentication_Token Request Status_To_v1_Token Request } 
func Convert_v1_Token Review_To_authentication_Token Review ( in * v1 . Token Review , out * authentication . Token Review , s conversion . Scope ) error { return auto Convert_v1_Token Review_To_authentication_Token } 
func Convert_authentication_Token Review_To_v1_Token Review ( in * authentication . Token Review , out * v1 . Token Review , s conversion . Scope ) error { return auto Convert_authentication_Token Review_To_v1_Token } 
func Convert_v1_Token Review Spec_To_authentication_Token Review Spec ( in * v1 . Token Review Spec , out * authentication . Token Review Spec , s conversion . Scope ) error { return auto Convert_v1_Token Review Spec_To_authentication_Token Review } 
func Convert_authentication_Token Review Spec_To_v1_Token Review Spec ( in * authentication . Token Review Spec , out * v1 . Token Review Spec , s conversion . Scope ) error { return auto Convert_authentication_Token Review Spec_To_v1_Token Review } 
func Convert_v1_Token Review Status_To_authentication_Token Review Status ( in * v1 . Token Review Status , out * authentication . Token Review Status , s conversion . Scope ) error { return auto Convert_v1_Token Review Status_To_authentication_Token Review } 
func Convert_authentication_Token Review Status_To_v1_Token Review Status ( in * authentication . Token Review Status , out * v1 . Token Review Status , s conversion . Scope ) error { return auto Convert_authentication_Token Review Status_To_v1_Token Review } 
func Convert_v1_User Info_To_authentication_User Info ( in * v1 . User Info , out * authentication . User Info , s conversion . Scope ) error { return auto Convert_v1_User Info_To_authentication_User } 
func Convert_authentication_User Info_To_v1_User Info ( in * authentication . User Info , out * v1 . User Info , s conversion . Scope ) error { return auto Convert_authentication_User Info_To_v1_User } 
func ( b * git Repo Volume Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func ( b * git Repo Volume Mounter ) Set Up At ( dir string , fs Group * int64 ) error { if volumeutil . Is Ready ( b . get Meta // Wrap Empty Dir, let it do the setup. wrapped , err := b . plugin . host . New Wrapper Mounter ( b . vol Name , wrapped Volume if err := wrapped . Set Up At ( dir , fs if output , err := b . exec files , err := ioutil . Read if len ( b . revision ) == 0 { // Done! volumeutil . Set Ready ( b . get Meta if output , err := b . exec if output , err := b . exec volume . Set Volume Ownership ( b , fs volumeutil . Set Ready ( b . get Meta } 
func ( m Priority REST Mapper ) Resource For ( partially Specified Resource schema . Group Version Resource ) ( schema . Group Version Resource , error ) { original GV Rs , original Err := m . Delegate . Resources For ( partially Specified if original Err != nil && len ( original GV Rs ) == 0 { return schema . Group Version Resource { } , original if len ( original GV Rs ) == 1 { return original GV Rs [ 0 ] , original remaining GV Rs := append ( [ ] schema . Group Version Resource { } , original GV for _ , pattern := range m . Resource Priority { matched GV Rs := [ ] schema . Group Version for _ , gvr := range remaining GV Rs { if resource Matches ( pattern , gvr ) { matched GV Rs = append ( matched GV switch len ( matched GV case 1 : // one match, return return matched GV Rs [ 0 ] , original default : // more than one match, use the matched hits as the list moving to the next pattern. // this way you can have a series of selection criteria remaining GV Rs = matched GV return schema . Group Version Resource { } , & Ambiguous Resource Error { Partial Resource : partially Specified Resource , Matching Resources : original GV } 
func ( m Priority REST Mapper ) Kind For ( partially Specified Resource schema . Group Version Resource ) ( schema . Group Version Kind , error ) { original GV Ks , original Err := m . Delegate . Kinds For ( partially Specified if original Err != nil && len ( original GV Ks ) == 0 { return schema . Group Version Kind { } , original if len ( original GV Ks ) == 1 { return original GV Ks [ 0 ] , original remaining GV Ks := append ( [ ] schema . Group Version Kind { } , original GV for _ , pattern := range m . Kind Priority { matched GV Ks := [ ] schema . Group Version for _ , gvr := range remaining GV Ks { if kind Matches ( pattern , gvr ) { matched GV Ks = append ( matched GV switch len ( matched GV case 1 : // one match, return return matched GV Ks [ 0 ] , original default : // more than one match, use the matched hits as the list moving to the next pattern. // this way you can have a series of selection criteria remaining GV Ks = matched GV return schema . Group Version Kind { } , & Ambiguous Resource Error { Partial Resource : partially Specified Resource , Matching Kinds : original GV } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Network Policy { } , & Network Policy metav1 . Add To Group Version ( scheme , Scheme Group } 
func Asset } 
func Asset if len ( name ) != 0 { cannonical path List := strings . Split ( cannonical for _ , p := range path for child Name := range node . Children { rv = append ( rv , child } 
func Restore info , err := Asset err = os . Mkdir All ( _file Path ( dir , filepath . Dir ( name ) ) , os . File err = ioutil . Write File ( _file err = os . Chtimes ( _file Path ( dir , name ) , info . Mod Time ( ) , info . Mod } 
func Restore Assets ( dir , name string ) error { children , err := Asset // File if err != nil { return Restore // Dir for _ , child := range children { err = Restore } 
func JWT Token Generator ( iss string , private Key interface { } ) ( Token Generator , error ) { var alg jose . Signature switch pk := private Key . ( type ) { case * rsa . Private case * ecdsa . Private case jose . Opaque Signer : alg = jose . Signature default : return nil , fmt . Errorf ( " " , private signer , err := jose . New Signer ( jose . Signing Key { Algorithm : alg , Key : private return & jwt Token } 
func JWT Token Authenticator ( iss string , keys [ ] interface { } , implicit Auds authenticator . Audiences , validator Validator ) authenticator . Token { return & jwt Token Authenticator { iss : iss , keys : keys , implicit Auds : implicit } 
func ( j * jwt Token Authenticator ) has Correct Issuer ( token Data string ) bool { parts := strings . Split ( token payload , err := base64 . Raw URL Encoding . Decode } 
func Convert_config_HPA Controller Configuration_To_v1alpha1_HPA Controller Configuration ( in * config . HPA Controller Configuration , out * v1alpha1 . HPA Controller Configuration , s conversion . Scope ) error { return auto Convert_config_HPA Controller Configuration_To_v1alpha1_HPA Controller } 
func Convert_v1alpha1_HPA Controller Configuration_To_config_HPA Controller Configuration ( in * v1alpha1 . HPA Controller Configuration , out * config . HPA Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_HPA Controller Configuration_To_config_HPA Controller } 
func New Operation Generator ( kube Client clientset . Interface , volume Plugin Mgr * volume . Volume Plugin Mgr , recorder record . Event Recorder , check Node Capabilities Before Mount bool , blk Util volumepathhandler . Block Volume Path Handler ) Operation Generator { return & operation Generator { kube Client : kube Client , volume Plugin Mgr : volume Plugin Mgr , recorder : recorder , check Node Capabilities Before Mount : check Node Capabilities Before Mount , blk Util : blk } 
func ( og * operation Generator ) Generate Map Volume Func ( wait For Attach Timeout time . Duration , volume To Mount Volume To Mount , actual State Of World Actual State Of World Mounter Updater ) ( volumetypes . Generated Operations , error ) { original Spec := volume To Mount . Volume // Translate to CSI spec if migration enabled if use CSI Plugin ( og . volume Plugin Mgr , original Spec ) { csi Spec , err := translate Spec ( original if err != nil { return volumetypes . Generated Operations { } , volume To Mount . Generate Error volume To Mount . Volume Spec = csi // Get block volume mapper plugin block Volume Plugin , err := og . volume Plugin Mgr . Find Mapper Plugin By Spec ( volume To Mount . Volume if err != nil { return volumetypes . Generated Operations { } , volume To Mount . Generate Error if block Volume Plugin == nil { return volumetypes . Generated Operations { } , volume To Mount . Generate Error affinity Err := check Node Affinity ( og , volume To if affinity Err != nil { event Err , detailed Err := volume To Mount . Generate Error ( " " , affinity og . recorder . Eventf ( volume To Mount . Pod , v1 . Event Type Warning , kevents . Failed Mount Volume , event return volumetypes . Generated Operations { } , detailed block Volume Mapper , new Mapper Err := block Volume Plugin . New Block Volume Mapper ( volume To Mount . Volume Spec , volume To Mount . Pod , volume . Volume if new Mapper Err != nil { event Err , detailed Err := volume To Mount . Generate Error ( " " , new Mapper og . recorder . Eventf ( volume To Mount . Pod , v1 . Event Type Warning , kevents . Failed Map Volume , event return volumetypes . Generated Operations { } , detailed // Get attacher, if possible attachable Volume Plugin , _ := og . volume Plugin Mgr . Find Attachable Plugin By Spec ( volume To Mount . Volume var volume if attachable Volume Plugin != nil { volume Attacher , _ = attachable Volume Plugin . New map Volume Func := func ( ) ( error , error ) { var device // Set up global map path under the given plugin directory using symbolic link global Map Path , err := block Volume Mapper . Get Global Map Path ( volume To Mount . Volume if err != nil { // On failure, return error. Caller will log and retry. return volume To Mount . Generate if volume Attacher != nil { // Wait for attachable volumes to finish attaching klog . Infof ( volume To Mount . Generate Msg Detailed ( " " , fmt . Sprintf ( " " , volume To Mount . Device device Path , err = volume Attacher . Wait For Attach ( volume To Mount . Volume Spec , volume To Mount . Device Path , volume To Mount . Pod , wait For Attach if err != nil { // On failure, return error. Caller will log and retry. return volume To Mount . Generate klog . Infof ( volume To Mount . Generate Msg Detailed ( " " , fmt . Sprintf ( " " , device // A plugin doesn't have attacher also needs to map device to global map path with Set Up Device() plugin Device Path , map Err := block Volume Mapper . Set Up if map Err != nil { // On failure, return error. Caller will log and retry. return volume To Mount . Generate Error ( " " , map // if plugin Device Path is provided, assume attacher may not provide device // or attachment flow uses Setup Device to get device path if len ( plugin Device Path ) != 0 { device Path = plugin Device if len ( device Path ) == 0 { return volume To Mount . Generate // When kubelet is containerized, device Path may be a symlink at a place unavailable to // kubelet, so evaluate it on the host and expect that it links to a device in /dev, // which will be available to containerized kubelet. If still it does not exist, // Attach File Device will fail. If kubelet is not containerized, eval it anyway. mounter := og . Get Volume Plugin Mgr ( ) . Host . Get Mounter ( block Volume Plugin . Get Plugin device Path , err = mounter . Eval Host Symlinks ( device if err != nil { return volume To Mount . Generate // Map device to global and pod device map path volume Map Path , vol Name := block Volume Mapper . Get Pod Device Map map Err = block Volume Mapper . Map Device ( device Path , global Map Path , volume Map Path , vol Name , volume To if map Err != nil { // On failure, return error. Caller will log and retry. return volume To Mount . Generate Error ( " " , map // Take filedescriptor lock to keep a block device opened. Otherwise, there is a case // that the block device is silently removed and attached another device with same name. // Container runtime can't handler this problem. To avoid unexpected condition fd lock // for the block device is required. _ , err = og . blk Util . Attach File Device ( device if err != nil { return volume To Mount . Generate // Update actual state of world to reflect volume is globally mounted mark Device Mapped Err := actual State Of World . Mark Device As Mounted ( volume To Mount . Volume Name , device Path , global Map if mark Device Mapped Err != nil { // On failure, return error. Caller will log and retry. return volume To Mount . Generate Error ( " " , mark Device Mapped // Device mapping for global map path succeeded simple Msg , detailed Msg := volume To Mount . Generate Msg ( " " , fmt . Sprintf ( " " , global Map og . recorder . Eventf ( volume To Mount . Pod , v1 . Event Type Normal , kevents . Successful Mount Volume , simple klog . V ( verbosity ) . Infof ( detailed // Device mapping for pod device map path succeeded simple Msg , detailed Msg = volume To Mount . Generate Msg ( " " , fmt . Sprintf ( " " , volume Map og . recorder . Eventf ( volume To Mount . Pod , v1 . Event Type Normal , kevents . Successful Mount Volume , simple klog . V ( verbosity ) . Infof ( detailed // Update actual state of world mark Vol Mounted Err := actual State Of World . Mark Volume As Mounted ( volume To Mount . Pod Name , volume To Mount . Pod . UID , volume To Mount . Volume Name , nil , block Volume Mapper , volume To Mount . Outer Volume Spec Name , volume To Mount . Volume Gid Value , original if mark Vol Mounted Err != nil { // On failure, return error. Caller will log and retry. return volume To Mount . Generate Error ( " " , mark Vol Mounted event Recorder Func := func ( err * error ) { if * err != nil { og . recorder . Eventf ( volume To Mount . Pod , v1 . Event Type Warning , kevents . Failed Map return volumetypes . Generated Operations { Operation Name : " " , Operation Func : map Volume Func , Event Recorder Func : event Recorder Func , Complete Func : util . Operation Complete Hook ( util . Get Full Qualified Plugin Name For Volume ( block Volume Plugin . Get Plugin Name ( ) , volume To Mount . Volume } 
func ( og * operation Generator ) Generate Unmap Volume Func ( volume To Unmount Mounted Volume , actual State Of World Actual State Of World Mounter Updater ) ( volumetypes . Generated Operations , error ) { var block Volume Plugin volume . Block Volume // Translate to CSI spec if migration enabled // And get block volume unmapper plugin if volume To Unmount . Volume Spec != nil && use CSI Plugin ( og . volume Plugin Mgr , volume To Unmount . Volume Spec ) { csi Spec , err := translate Spec ( volume To Unmount . Volume if err != nil { return volumetypes . Generated Operations { } , volume To Unmount . Generate Error volume To Unmount . Volume Spec = csi block Volume Plugin , err = og . volume Plugin Mgr . Find Mapper Plugin By Name ( csi . CSI Plugin if err != nil { return volumetypes . Generated Operations { } , volume To Unmount . Generate Error } else { block Volume Plugin , err = og . volume Plugin Mgr . Find Mapper Plugin By Name ( volume To Unmount . Plugin if err != nil { return volumetypes . Generated Operations { } , volume To Unmount . Generate Error var block Volume Unmapper volume . Block Volume if block Volume Plugin == nil { return volumetypes . Generated Operations { } , volume To Unmount . Generate Error block Volume Unmapper , new Unmapper Err := block Volume Plugin . New Block Volume Unmapper ( volume To Unmount . Inner Volume Spec Name , volume To Unmount . Pod if new Unmapper Err != nil { return volumetypes . Generated Operations { } , volume To Unmount . Generate Error Detailed ( " " , new Unmapper unmap Volume Func := func ( ) ( error , error ) { // Try to unmap volume Name symlink under pod device map path dir // pods/{pod Uid}/volume Devices/{escape Qualified Plugin Name}/{volume Name} pod Device Unmap Path , vol Name := block Volume Unmapper . Get Pod Device Map unmap Device Err := og . blk Util . Unmap Device ( pod Device Unmap Path , vol if unmap Device Err != nil { // On failure, return error. Caller will log and retry. return volume To Unmount . Generate Error ( " " , unmap Device // Try to unmap pod UID symlink under global map path dir // plugins/kubernetes.io/{Plugin Name}/volume Devices/{volume Plugin Dependent Path}/{pod UID} global Unmap Path := volume To Unmount . Device Mount unmap Device Err = og . blk Util . Unmap Device ( global Unmap Path , string ( volume To Unmount . Pod if unmap Device Err != nil { // On failure, return error. Caller will log and retry. return volume To Unmount . Generate Error ( " " , unmap Device klog . Infof ( " " , volume To Unmount . Volume Name , volume To Unmount . Outer Volume Spec Name , volume To Unmount . Pod Name , volume To Unmount . Pod UID , volume To Unmount . Inner Volume Spec Name , volume To Unmount . Plugin Name , volume To Unmount . Volume Gid // Update actual state of world mark Vol Unmounted Err := actual State Of World . Mark Volume As Unmounted ( volume To Unmount . Pod Name , volume To Unmount . Volume if mark Vol Unmounted Err != nil { // On failure, just log and exit klog . Errorf ( volume To Unmount . Generate Error Detailed ( " " , mark Vol Unmounted return volumetypes . Generated Operations { Operation Name : " " , Operation Func : unmap Volume Func , Complete Func : util . Operation Complete Hook ( util . Get Full Qualified Plugin Name For Volume ( block Volume Plugin . Get Plugin Name ( ) , volume To Unmount . Volume Spec ) , " " ) , Event Recorder } 
func ( og * operation Generator ) Generate Unmap Device Func ( device To Detach Attached Volume , actual State Of World Actual State Of World Mounter Updater , mounter mount . Interface ) ( volumetypes . Generated Operations , error ) { var block Volume Plugin volume . Block Volume // Translate to CSI spec if migration enabled if use CSI Plugin ( og . volume Plugin Mgr , device To Detach . Volume Spec ) { csi Spec , err := translate Spec ( device To Detach . Volume if err != nil { return volumetypes . Generated Operations { } , device To Detach . Generate Error device To Detach . Volume Spec = csi block Volume Plugin , err = og . volume Plugin Mgr . Find Mapper Plugin By Name ( csi . CSI Plugin if err != nil { return volumetypes . Generated Operations { } , device To Detach . Generate Error } else { block Volume Plugin , err = og . volume Plugin Mgr . Find Mapper Plugin By Name ( device To Detach . Plugin if err != nil { return volumetypes . Generated Operations { } , device To Detach . Generate Error if block Volume Plugin == nil { return volumetypes . Generated Operations { } , device To Detach . Generate Error block Volume Unmapper , new Unmapper Err := block Volume Plugin . New Block Volume Unmapper ( device To Detach . Volume Spec . Name ( ) , " " /* pod if new Unmapper Err != nil { return volumetypes . Generated Operations { } , device To Detach . Generate Error Detailed ( " " , new Unmapper unmap Device Func := func ( ) ( error , error ) { // Search under global Map Path dir if all symbolic links from pods have been removed already. // If symbolic links are there, pods may still refer the volume. global Map Path := device To Detach . Device Mount refs , err := og . blk Util . Get Device Symlink Refs ( device To Detach . Device Path , global Map if err != nil { return device To Detach . Generate if len ( refs ) > 0 { err = fmt . Errorf ( " " , global Map return device To Detach . Generate // The block volume is not referenced from Pods. Release file descriptor lock. // This should be done before calling Tear Down Device, because some plugins that do local detach // in Tear Down Device will fail in detaching device due to the refcnt on the loopback device. klog . V ( 4 ) . Infof ( " " , device To Detach . Device loop Path , err := og . blk Util . Get Loop Device ( device To Detach . Device if err != nil { if err . Error ( ) == volumepathhandler . Err Device Not Found { klog . Warningf ( device To Detach . Generate Msg Detailed ( " " , fmt . Sprintf ( " " , device To Detach . Device } else { err Info := " " + fmt . Sprintf ( " " , device To Detach . Device return device To Detach . Generate Error ( err } else { if len ( loop Path ) != 0 { err = og . blk Util . Remove Loop Device ( loop if err != nil { return device To Detach . Generate // Execute tear down device unmap Err := block Volume Unmapper . Tear Down Device ( global Map Path , device To Detach . Device if unmap Err != nil { // On failure, return error. Caller will log and retry. return device To Detach . Generate Error ( " " , unmap // Plugin finished Tear Down Device(). Now global Map Path dir and plugin's stored data // on the dir are unnecessary, clean up it. remove Map Path Err := og . blk Util . Remove Map Path ( global Map if remove Map Path Err != nil { // On failure, return error. Caller will log and retry. return device To Detach . Generate Error ( " " , remove Map Path // Before logging that Unmap Device succeeded and moving on, // use mounter.Path Is Device to check if the path is a device, // if so use mounter.Device Opened to check if the device is in use anywhere // else on the system. Retry if it returns true. device Opened , device Opened Err := is Device Opened ( device To if device Opened Err != nil { return nil , device Opened // The device is still in use elsewhere. Caller will log and retry. if device Opened { return device To Detach . Generate klog . Infof ( device To Detach . Generate Msg // Update actual state of world mark Device Unmounted Err := actual State Of World . Mark Device As Unmounted ( device To Detach . Volume if mark Device Unmounted Err != nil { // On failure, return error. Caller will log and retry. return device To Detach . Generate Error ( " " , mark Device Unmounted return volumetypes . Generated Operations { Operation Name : " " , Operation Func : unmap Device Func , Complete Func : util . Operation Complete Hook ( util . Get Full Qualified Plugin Name For Volume ( block Volume Plugin . Get Plugin Name ( ) , device To Detach . Volume Spec ) , " " ) , Event Recorder } 
func check Node Affinity ( og * operation Generator , volume To Mount Volume To Mount ) error { if ! utilfeature . Default Feature Gate . Enabled ( features . Persistent Local pv := volume To Mount . Volume Spec . Persistent if pv != nil { node Labels , err := og . volume Plugin Mgr . Host . Get Node err = util . Check Node Affinity ( pv , node } 
func is Device Opened ( device To Detach Attached Volume , mounter mount . Interface ) ( bool , error ) { is Device Path , device Path Err := mounter . Path Is Device ( device To Detach . Device var device var device Opened if ! is Device Path && device Path Err == nil || ( device Path Err != nil && strings . Contains ( device Path Err . Error ( ) , " " ) ) { // not a device path or path doesn't exist //TODO: refer to #36092 klog . V ( 3 ) . Infof ( " " , device To Detach . Device device } else if device Path Err != nil { return false , device To Detach . Generate Error Detailed ( " " , device Path } else { device Opened , device Opened Err = mounter . Device Opened ( device To Detach . Device if device Opened Err != nil { return false , device To Detach . Generate Error Detailed ( " " , device Opened return device } 
func new Lease Manager ( client * clientv3 . Client , lease Reuse Duration Seconds int64 , lease Reuse Duration Percent float64 ) * lease Manager { return & lease Manager { client : client , lease Reuse Duration Seconds : lease Reuse Duration Seconds , lease Reuse Duration Percent : lease Reuse Duration } 
func ( l * lease Manager ) set Lease Reuse Duration Seconds ( duration int64 ) { l . lease defer l . lease l . lease Reuse Duration } 
func ( l * lease Manager ) Get Lease ( ctx context . Context , ttl int64 ) ( clientv3 . Lease l . lease defer l . lease // check if previous lease can be reused reuse Duration Seconds := l . get Reuse Duration Seconds valid := now . Add ( time . Duration ( ttl ) * time . Second ) . Before ( l . prev Lease Expiration sufficient := now . Add ( time . Duration ( ttl + reuse Duration Seconds ) * time . Second ) . After ( l . prev Lease Expiration if valid && sufficient { return l . prev Lease // request a lease with a little extra ttl from etcd ttl += reuse Duration if err != nil { return clientv3 . Lease // cache the new lease id l . prev Lease l . prev Lease Expiration } 
func ( l * lease Manager ) get Reuse Duration Seconds Locked ( ttl int64 ) int64 { reuse Duration Seconds := int64 ( l . lease Reuse Duration if reuse Duration Seconds > l . lease Reuse Duration Seconds { reuse Duration Seconds = l . lease Reuse Duration return reuse Duration } 
func ( obj * Type Meta ) Group Version Kind ( ) schema . Group Version Kind { return schema . From API Version And Kind ( obj . API } 
func ( obj * Versioned Objects ) Get Object Kind ( ) schema . Object if last == nil { return schema . Empty Object return last . Get Object } 
func ( obj * Versioned } 
func ( obj * Versioned } 
func ( h * Kust kind := u . Get switch kind { case " " : cm , err := unstructured To return Config Map case " " : sec , err := unstructured To return Secret } 
func String } 
func Object return String } 
func Object Go Print Diff ( a , b interface { } ) string { s := spew . Config State { Disable return String } 
func Object Reflect v A , v B := reflect . Value Of ( a ) , reflect . Value if v A . Type ( ) != v diffs := object Reflect Diff ( field . New Path ( " " ) , v A , v for _ , d := range diffs { elided A , elided out = append ( out , fmt . Sprintf ( " " , d . path ) , fmt . Sprintf ( " " , elided A ) , fmt . Sprintf ( " " , elided } 
func limit ( a Obj , b Obj interface { } , max int ) ( string , string ) { elided elided A elided B a , b := fmt . Sprintf ( " " , a Obj ) , fmt . Sprintf ( " " , b if a Obj != nil && b Obj != nil { if a Type , b Type := fmt . Sprintf ( " " , a Obj ) , fmt . Sprintf ( " " , b Obj ) ; a Type != b Type { a = fmt . Sprintf ( " " , a , a b = fmt . Sprintf ( " " , b , b for { switch { case len ( a ) > max && len ( a ) > 4 && len ( b ) > 4 && a [ : 4 ] == b [ : 4 ] : // a is too long, b has data, and the first several characters are the same elided case len ( b ) > max && len ( b ) > 4 && len ( a ) > 4 && a [ : 4 ] == b [ : 4 ] : // b is too long, a has data, and the first several characters are the same elided elided A elided B default : // both are short enough return elided Prefix + a + elided A Suffix , elided Prefix + b + elided B } 
func Object Go Print Side By Side ( a , b interface { } ) string { s := spew . Config State { Indent : " " , // Extra deep spew. Disable s s lines A := strings . Split ( s lines B := strings . Split ( s for _ , s := range lines for _ , s := range lines w := tabwriter . New max := len ( lines if len ( lines B ) > max { max = len ( lines if i < len ( lines A ) { a = lines if i < len ( lines B ) { b = lines } 
func ( in * CSI Driver ) Deep Copy Into ( out * CSI out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * CSI Driver ) Deep Copy ( ) * CSI out := new ( CSI in . Deep Copy } 
func ( in * CSI Driver ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * CSI Driver List ) Deep Copy Into ( out * CSI Driver out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] CSI for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * CSI Driver List ) Deep Copy ( ) * CSI Driver out := new ( CSI Driver in . Deep Copy } 
func ( in * CSI Driver List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * CSI Driver Spec ) Deep Copy Into ( out * CSI Driver if in . Attach Required != nil { in , out := & in . Attach Required , & out . Attach if in . Pod Info On Mount != nil { in , out := & in . Pod Info On Mount , & out . Pod Info On } 
func ( in * CSI Driver Spec ) Deep Copy ( ) * CSI Driver out := new ( CSI Driver in . Deep Copy } 
func ( in * CSI Node ) Deep Copy Into ( out * CSI out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * CSI Node ) Deep Copy ( ) * CSI out := new ( CSI in . Deep Copy } 
func ( in * CSI Node ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * CSI Node Driver ) Deep Copy Into ( out * CSI Node if in . Topology Keys != nil { in , out := & in . Topology Keys , & out . Topology } 
func ( in * CSI Node Driver ) Deep Copy ( ) * CSI Node out := new ( CSI Node in . Deep Copy } 
func ( in * CSI Node List ) Deep Copy Into ( out * CSI Node out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] CSI for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * CSI Node List ) Deep Copy ( ) * CSI Node out := new ( CSI Node in . Deep Copy } 
func ( in * CSI Node List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * CSI Node Spec ) Deep Copy Into ( out * CSI Node * out = make ( [ ] CSI Node for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * CSI Node Spec ) Deep Copy ( ) * CSI Node out := new ( CSI Node in . Deep Copy } 
func ( in * Storage Class ) Deep Copy Into ( out * Storage out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object if in . Reclaim Policy != nil { in , out := & in . Reclaim Policy , & out . Reclaim * out = new ( core . Persistent Volume Reclaim if in . Mount Options != nil { in , out := & in . Mount Options , & out . Mount if in . Allow Volume Expansion != nil { in , out := & in . Allow Volume Expansion , & out . Allow Volume if in . Volume Binding Mode != nil { in , out := & in . Volume Binding Mode , & out . Volume Binding * out = new ( Volume Binding if in . Allowed Topologies != nil { in , out := & in . Allowed Topologies , & out . Allowed * out = make ( [ ] core . Topology Selector for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Storage Class ) Deep Copy ( ) * Storage out := new ( Storage in . Deep Copy } 
func ( in * Storage Class ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Storage Class List ) Deep Copy Into ( out * Storage Class out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Storage for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Storage Class List ) Deep Copy ( ) * Storage Class out := new ( Storage Class in . Deep Copy } 
func ( in * Storage Class List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Volume Attachment ) Deep Copy Into ( out * Volume out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Volume Attachment ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func ( in * Volume Attachment ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Volume Attachment List ) Deep Copy Into ( out * Volume Attachment out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Volume for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Volume Attachment List ) Deep Copy ( ) * Volume Attachment out := new ( Volume Attachment in . Deep Copy } 
func ( in * Volume Attachment List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Volume Attachment Source ) Deep Copy Into ( out * Volume Attachment if in . Persistent Volume Name != nil { in , out := & in . Persistent Volume Name , & out . Persistent Volume } 
func ( in * Volume Attachment Source ) Deep Copy ( ) * Volume Attachment out := new ( Volume Attachment in . Deep Copy } 
func ( in * Volume Attachment Spec ) Deep Copy Into ( out * Volume Attachment in . Source . Deep Copy } 
func ( in * Volume Attachment Spec ) Deep Copy ( ) * Volume Attachment out := new ( Volume Attachment in . Deep Copy } 
func ( in * Volume Attachment Status ) Deep Copy Into ( out * Volume Attachment if in . Attachment Metadata != nil { in , out := & in . Attachment Metadata , & out . Attachment if in . Attach Error != nil { in , out := & in . Attach Error , & out . Attach * out = new ( Volume ( * in ) . Deep Copy if in . Detach Error != nil { in , out := & in . Detach Error , & out . Detach * out = new ( Volume ( * in ) . Deep Copy } 
func ( in * Volume Attachment Status ) Deep Copy ( ) * Volume Attachment out := new ( Volume Attachment in . Deep Copy } 
func ( in * Volume Error ) Deep Copy Into ( out * Volume in . Time . Deep Copy } 
func ( in * Volume Error ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func extract Bool Tag Or Die ( key string , lines [ ] string ) bool { val , err := types . Extract Single Bool Comment } 
func New Persistent Volume Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Persistent Volume Informer ( client , resync } 
func Find Labels In Set ( labels To Keep [ ] string , selector labels . Set ) map [ string ] string { a for _ , l := range labels To Keep { if selector . Has ( l ) { a return a } 
func Add Unset Labels To Map ( a L map [ string ] string , labels To Add [ ] string , label Set labels . Set ) { for _ , l := range labels To Add { // if the label is already there, dont overwrite it. if _ , exists := a // otherwise, backfill this label. if label Set . Has ( l ) { a L [ l ] = label } 
func Filter Pods By for _ , ns Pod := range pods { if ns Pod . Namespace == ns { filtered = append ( filtered , ns } 
func Create Selector From Labels ( a L map [ string ] string ) labels . Selector { if a L == nil || len ( a return labels . Set ( a L ) . As } 
func ports Conflict ( existing Ports schedulernodeinfo . Host Port Info , want Ports [ ] * v1 . Container Port ) bool { for _ , cp := range want Ports { if existing Ports . Check Conflict ( cp . Host IP , string ( cp . Protocol ) , cp . Host } 
func New Cmd Can I ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := & Can I Options { IO cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : " " , Long : can I Long , Example : can I Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check if o . List { err = o . Run Access allowed , err = o . Run Access cmdutil . Check cmd . Flags ( ) . Bool Var P ( & o . All Namespaces , " " , " " , o . All cmd . Flags ( ) . Bool Var cmd . Flags ( ) . String cmd . Flags ( ) . Bool cmd . Flags ( ) . Bool Var ( & o . No Headers , " " , o . No } 
func ( o * Can I if strings . Has Prefix ( args [ 1 ] , " " ) { o . Non Resource resource Tokens := strings . Split rest Mapper , err := f . To REST o . Resource = o . resource For ( rest Mapper , resource if len ( resource Tokens ) > 1 { o . Resource Name = resource client , err := f . Kubernetes Client o . Auth Client = client . Authorization o . Discovery if ! o . All Namespaces { o . Namespace , _ , err = f . To Raw Kube Config } 
func ( o * Can I Options ) Validate ( ) error { if o . List { if o . Quiet || o . All if o . Non Resource if o . Resource != ( schema . Group Version Resource { } ) || o . Resource } else if ! o . Resource . Empty ( ) && ! o . All Namespaces && o . Discovery Client != nil { if namespaced , err := is Namespaced ( o . Resource , o . Discovery Client ) ; err == nil && ! namespaced { if len ( o . Resource . Group ) == 0 { fmt . Fprintf ( o . Err } else { fmt . Fprintf ( o . Err if o . No } 
func ( o * Can I Options ) Run Access List ( ) error { sar := & authorizationv1 . Self Subject Rules Review { Spec : authorizationv1 . Self Subject Rules Review response , err := o . Auth Client . Self Subject Rules return o . print } 
func ( o * Can I Options ) Run Access Check ( ) ( bool , error ) { var sar * authorizationv1 . Self Subject Access if o . Non Resource URL == " " { sar = & authorizationv1 . Self Subject Access Review { Spec : authorizationv1 . Self Subject Access Review Spec { Resource Attributes : & authorizationv1 . Resource Attributes { Namespace : o . Namespace , Verb : o . Verb , Group : o . Resource . Group , Resource : o . Resource . Resource , Subresource : o . Subresource , Name : o . Resource } else { sar = & authorizationv1 . Self Subject Access Review { Spec : authorizationv1 . Self Subject Access Review Spec { Non Resource Attributes : & authorizationv1 . Non Resource Attributes { Verb : o . Verb , Path : o . Non Resource response , err := o . Auth Client . Self Subject Access if len ( response . Status . Evaluation Error ) > 0 { fmt . Fprintf ( o . Out , " " , response . Status . Evaluation } 
func Matches Server Version ( client Version apimachineryversion . Info , client Discovery Interface ) error { s Ver , err := client . Server // Git Version includes Git Commit and Git Tree State, but best to be safe? if client Version . Git Version != s Ver . Git Version || client Version . Git Commit != s Ver . Git Commit || client Version . Git Tree State != s Ver . Git Tree State { return fmt . Errorf ( " " , s Ver , client } 
func Server Supports Version ( client Discovery Interface , required GV schema . Group Version ) error { groups , err := client . Server versions := metav1 . Extract Group server for _ , v := range versions { server if server Versions . Has ( required // If the server supports no versions, then we should pretend it has the version because of old servers. // This can happen because discovery fails due to 403 Forbidden errors if len ( server return fmt . Errorf ( " " , required } 
func Group Version Resources ( rls [ ] * metav1 . API Resource List ) ( map [ schema . Group Version Resource ] struct { } , error ) { gvrs := map [ schema . Group Version for _ , rl := range rls { gv , err := schema . Parse Group Version ( rl . Group for i := range rl . API Resources { gvrs [ schema . Group Version Resource { Group : gv . Group , Version : gv . Version , Resource : rl . API } 
func Filtered By ( pred Resource Predicate , rls [ ] * metav1 . API Resource List ) [ ] * metav1 . API Resource List { result := [ ] * metav1 . API Resource filtered . API for i := range rl . API Resources { if pred . Match ( rl . Group Version , & rl . API Resources [ i ] ) { filtered . API Resources = append ( filtered . API Resources , rl . API if filtered . API } 
func ( fn Resource Predicate Func ) Match ( group Version string , r * metav1 . API Resource ) bool { return fn ( group } 
func ( p Supports All Verbs ) Match ( group Version string , r * metav1 . API Resource ) bool { return sets . New String ( [ ] string ( r . Verbs ) ... ) . Has } 
func ( Real } 
func ( Real OS ) Stat ( path string ) ( os . File } 
func ( Real } 
func ( Real OS ) Chmod ( path string , perm os . File } 
func ( Real } 
func ( Real } 
func ( Real OS ) Read Dir ( dirname string ) ( [ ] os . File Info , error ) { return ioutil . Read } 
func ( o * Recommended Options ) Apply To ( config * server . Recommended Config ) error { if err := o . Etcd . Apply if err := o . Secure Serving . Apply To ( & config . Config . Secure Serving , & config . Config . Loopback Client if err := o . Authentication . Apply To ( & config . Config . Authentication , config . Secure Serving , config . Open API if err := o . Authorization . Apply if err := o . Audit . Apply To ( & config . Config , config . Client Config , config . Shared Informer Factory , o . Process if err := o . Features . Apply if err := o . Core API . Apply if initializers , err := o . Extra Admission } else if err := o . Admission . Apply To ( & config . Config , config . Shared Informer Factory , config . Client } 
func Add To Group Version ( scheme * runtime . Scheme , group Version schema . Group Version ) { scheme . Add Known Type With Name ( group Version . With Kind ( Watch Event Kind ) , & Watch scheme . Add Known Type With Name ( schema . Group Version { Group : group Version . Group , Version : runtime . API Version Internal } . With Kind ( Watch Event Kind ) , & Internal // Supports legacy code paths, most callers should use metav1.Parameter Codec for now scheme . Add Known Types ( group Version , & List Options { } , & Export Options { } , & Get Options { } , & Delete Options { } , & Create Options { } , & Update Options { } , & Patch utilruntime . Must ( scheme . Add Conversion Funcs ( Convert_v1_Watch Event_To_watch_Event , Convert_v1_Internal Event_To_v1_Watch Event , Convert_watch_Event_To_v1_Watch Event , Convert_v1_Watch Event_To_v1_Internal // Register Unversioned types under their own special group scheme . Add Unversioned Types ( Unversioned , & Status { } , & API Versions { } , & API Group List { } , & API Group { } , & API Resource // register manually. This usually goes through the Scheme Builder, which we cannot use here. utilruntime . Must ( Add Conversion utilruntime . Must ( Register } 
func New Attach Detach Controller ( kube Client clientset . Interface , pod Informer coreinformers . Pod Informer , node Informer coreinformers . Node Informer , pvc Informer coreinformers . Persistent Volume Claim Informer , pv Informer coreinformers . Persistent Volume Informer , csi Node Informer storageinformers . CSI Node Informer , csi Driver Informer storageinformers . CSI Driver Informer , cloud cloudprovider . Interface , plugins [ ] volume . Volume Plugin , prober volume . Dynamic Plugin Prober , disable Reconciliation Sync bool , reconciler Sync Duration time . Duration , timer Config Timer Config ) ( Attach Detach Controller , error ) { // TODO: The default resync Period for shared informers is 12 hours, this is // unacceptable for the attach/detach controller. For example, if a pod is // skipped because the node it is scheduled to didn't set its annotation in // time, we don't want to have to wait 12hrs before processing the pod // again. // Luckily https://github.com/kubernetes/kubernetes/issues/23394 is being // worked on and will split resync in to resync and relist. Once that // happens the resync period can be set to something much faster (30 // seconds). // If that issue is not resolved in time, then this controller will have to // consider some unappealing alternate options: use a non-shared informer // and set a faster resync period even if it causes relist, or requeue // dropped pods so they are continuously processed until it is accepted or // deleted (probably can't do this with shared Informer), etc. adc := & attach Detach Controller { kube Client : kube Client , pvc Lister : pvc Informer . Lister ( ) , pvcs Synced : pvc Informer . Informer ( ) . Has Synced , pv Lister : pv Informer . Lister ( ) , pvs Synced : pv Informer . Informer ( ) . Has Synced , pod Lister : pod Informer . Lister ( ) , pods Synced : pod Informer . Informer ( ) . Has Synced , pod Indexer : pod Informer . Informer ( ) . Get Indexer ( ) , node Lister : node Informer . Lister ( ) , nodes Synced : node Informer . Informer ( ) . Has Synced , cloud : cloud , pvc Queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate if utilfeature . Default Feature Gate . Enabled ( features . CSI Migration ) && utilfeature . Default Feature Gate . Enabled ( features . CSI Node Info ) { adc . csi Node Lister = csi Node adc . csi Node Synced = csi Node Informer . Informer ( ) . Has if utilfeature . Default Feature Gate . Enabled ( features . CSI Driver Registry ) { adc . csi Driver Lister = csi Driver adc . csi Drivers Synced = csi Driver Informer . Informer ( ) . Has if err := adc . volume Plugin Mgr . Init event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Client . Core recorder := event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event blkutil := volumepathhandler . New Block Volume Path adc . desired State Of World = cache . New Desired State Of World ( & adc . volume Plugin adc . actual State Of World = cache . New Actual State Of World ( & adc . volume Plugin adc . attacher Detacher = operationexecutor . New Operation Executor ( operationexecutor . New Operation Generator ( kube Client , & adc . volume Plugin adc . node Status Updater = statusupdater . New Node Status Updater ( kube Client , node Informer . Lister ( ) , adc . actual State Of // Default these to values in options adc . reconciler = reconciler . New Reconciler ( timer Config . Reconciler Loop Period , timer Config . Reconciler Max Wait For Unmount Duration , reconciler Sync Duration , disable Reconciliation Sync , adc . desired State Of World , adc . actual State Of World , adc . attacher Detacher , adc . node Status adc . desired State Of World Populator = populator . New Desired State Of World Populator ( timer Config . Desired State Of World Populator Loop Sleep Period , timer Config . Desired State Of World Populator List Pods Retry Duration , pod Informer . Lister ( ) , adc . desired State Of World , & adc . volume Plugin Mgr , pvc Informer . Lister ( ) , pv pod Informer . Informer ( ) . Add Event Handler ( kcache . Resource Event Handler Funcs { Add Func : adc . pod Add , Update Func : adc . pod Update , Delete Func : adc . pod // This custom indexer will index pods by its PVC keys. Then we don't need // to iterate all pods every time to find pods which reference given PVC. adc . pod Indexer . Add Indexers ( kcache . Indexers { pvc Key Index : index By PVC node Informer . Informer ( ) . Add Event Handler ( kcache . Resource Event Handler Funcs { Add Func : adc . node Add , Update Func : adc . node Update , Delete Func : adc . node pvc Informer . Informer ( ) . Add Event Handler ( kcache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { adc . enqueue } , Update Func : func ( old , new interface { } ) { adc . enqueue } 
func index By PVC if len ( pod . Spec . Node Name ) == 0 || volumeutil . Is Pod for _ , pod Volume := range pod . Spec . Volumes { if pvc Source := pod Volume . Volume Source . Persistent Volume Claim ; pvc Source != nil { keys = append ( keys , fmt . Sprintf ( " " , pod . Namespace , pvc Source . Claim } 
func ( adc * attach Detach Controller ) process Volumes In Use ( node Name types . Node Name , volumes In Use [ ] v1 . Unique Volume Name ) { klog . V ( 4 ) . Infof ( " " , node for _ , attached Volume := range adc . actual State Of World . Get Attached Volumes For Node ( node for _ , volume In Use := range volumes In Use { if attached Volume . Volume Name == volume In err := adc . actual State Of World . Set Volume Mounted By Node ( attached Volume . Volume Name , node if err != nil { klog . Warningf ( " " , attached Volume . Volume Name , node } 
func Is Ready ( dir string ) bool { ready File := filepath . Join ( dir , ready File s , err := os . Stat ( ready if ! s . Mode ( ) . Is Regular ( ) { klog . Errorf ( " " , ready } 
func Set Ready ( dir string ) { if err := os . Mkdir All ( dir , 0750 ) ; err != nil && ! os . Is ready File := filepath . Join ( dir , ready File file , err := os . Create ( ready if err != nil { klog . Errorf ( " " , ready } 
func Get Secret For Pod ( pod * v1 . Pod , secret Name string , kube if kube secrets , err := kube Client . Core V1 ( ) . Secrets ( pod . Namespace ) . Get ( secret Name , metav1 . Get } 
func Get Secret For PV ( secret Namespace , secret Name , volume Plugin Name string , kube if kube secrets , err := kube Client . Core V1 ( ) . Secrets ( secret Namespace ) . Get ( secret Name , metav1 . Get if secrets . Type != v1 . Secret Type ( volume Plugin Name ) { return secret , fmt . Errorf ( " " , volume Plugin } 
func Get Class For Volume ( kube Client clientset . Interface , pv * v1 . Persistent Volume ) ( * storage . Storage Class , error ) { if kube class Name := v1helper . Get Persistent Volume if class class , err := kube Client . Storage V1 ( ) . Storage Classes ( ) . Get ( class Name , metav1 . Get } 
func Check Node Affinity ( pv * v1 . Persistent Volume , node Labels map [ string ] string ) error { return check Volume Node Affinity ( pv , node } 
func Load Pod From File ( file Path string ) ( * v1 . Pod , error ) { if file pod Def , err := ioutil . Read File ( file if err != nil { return nil , fmt . Errorf ( " " , file if len ( pod Def ) == 0 { return nil , fmt . Errorf ( " " , file codec := legacyscheme . Codecs . Universal if err := runtime . Decode Into ( codec , pod } 
func Calculate Timeout For Volume ( minimum Timeout , timeout Increment int , pv * v1 . Persistent Volume ) int64 { gi Qty := resource . Must pv Qty := pv . Spec . Capacity [ v1 . Resource gi Size := gi pv Size := pv timeout := ( pv Size / gi Size ) * int64 ( timeout if timeout < int64 ( minimum Timeout ) { return int64 ( minimum } 
func Generate Volume Name ( cluster Name , pv Name string , max Length int ) string { prefix := cluster pv Len := len ( pv // cut the "<cluster Name>-dynamic" to fit full pv Name into max Length // +1 for the '-' dash if pv Len + 1 + len ( prefix ) > max Length { prefix = prefix [ : max Length - pv return prefix + " " + pv } 
func Get Path ( mounter volume . Mounter ) ( string , error ) { path := mounter . Get if path == " " { return " " , fmt . Errorf ( " " , reflect . Type } 
func Unmount Via Empty Dir ( dir string , host volume . Volume Host , vol Name string , vol Spec volume . Spec , pod UID utypes . UID ) error { klog . V ( 3 ) . Infof ( " " , vol Name , pod // Wrap Empty Dir, let it do the teardown. wrapped , err := host . New Wrapper Unmounter ( vol Name , vol Spec , pod return wrapped . Tear Down } 
func Mount Option From Spec ( spec * volume . Spec , options ... string ) [ ] string { pv := spec . Persistent if pv != nil { // Use beta annotation first if mo , ok := pv . Annotations [ v1 . Mount Option Annotation ] ; ok { mo return Join Mount Options ( mo if len ( pv . Spec . Mount Options ) > 0 { return Join Mount Options ( pv . Spec . Mount } 
func Join Mount Options ( user Options [ ] string , system Options [ ] string ) [ ] string { all Mount Options := sets . New for _ , mount Option := range user Options { if len ( mount Option ) > 0 { all Mount Options . Insert ( mount for _ , mount Option := range system Options { all Mount Options . Insert ( mount return all Mount } 
func Access Modes Contains ( modes [ ] v1 . Persistent Volume Access Mode , mode v1 . Persistent Volume Access } 
func Access Modes Contained In All ( indexed Modes [ ] v1 . Persistent Volume Access Mode , requested Modes [ ] v1 . Persistent Volume Access Mode ) bool { for _ , mode := range requested Modes { if ! Access Modes Contains ( indexed } 
func Get Windows Path ( path string ) string { windows if strings . Has Prefix ( windows Path , " \\ " ) { windows Path = " " + windows return windows } 
func Get Unique Pod Name ( pod * v1 . Pod ) types . Unique Pod Name { return types . Unique Pod } 
func Get Unique Volume Name ( plugin Name , volume Name string ) v1 . Unique Volume Name { return v1 . Unique Volume Name ( fmt . Sprintf ( " " , plugin Name , volume } 
func Get Unique Volume Name From Spec With Pod ( pod Name types . Unique Pod Name , volume Plugin volume . Volume Plugin , volume Spec * volume . Spec ) v1 . Unique Volume Name { return v1 . Unique Volume Name ( fmt . Sprintf ( " " , volume Plugin . Get Plugin Name ( ) , pod Name , volume } 
func Get Unique Volume Name From Spec ( volume Plugin volume . Volume Plugin , volume Spec * volume . Spec ) ( v1 . Unique Volume Name , error ) { if volume Plugin == nil { return " " , fmt . Errorf ( " " , volume volume Name , err := volume Plugin . Get Volume Name ( volume if err != nil || volume Name == " " { return " " , fmt . Errorf ( " " , volume return Get Unique Volume Name ( volume Plugin . Get Plugin Name ( ) , volume } 
func Is Pod Terminated ( pod * v1 . Pod , pod Status v1 . Pod Status ) bool { return pod Status . Phase == v1 . Pod Failed || pod Status . Phase == v1 . Pod Succeeded || ( pod . Deletion Timestamp != nil && not Running ( pod Status . Container } 
func not Running ( statuses [ ] v1 . Container } 
func Split Unique Name ( unique Name v1 . Unique Volume Name ) ( string , string , error ) { components := strings . Split N ( string ( unique if len ( components ) != 3 { return " " , " " , fmt . Errorf ( " " , unique plugin return plugin } 
func New Safe Format And Mount From Host ( plugin Name string , host volume . Volume Host ) * mount . Safe Format And Mount { mounter := host . Get Mounter ( plugin exec := host . Get Exec ( plugin return & mount . Safe Format And } 
func Get Volume Mode ( volume Spec * volume . Spec ) ( v1 . Persistent Volume Mode , error ) { if volume Spec == nil || volume Spec . Persistent Volume == nil { return v1 . Persistent Volume if volume Spec . Persistent Volume . Spec . Volume Mode != nil { return * volume Spec . Persistent Volume . Spec . Volume return " " , fmt . Errorf ( " " , volume } 
func Get Persistent Volume Claim Volume Mode ( claim * v1 . Persistent Volume Claim ) ( v1 . Persistent Volume Mode , error ) { if claim . Spec . Volume Mode != nil { return * claim . Spec . Volume } 
func Get Persistent Volume Claim Qualified Name ( claim * v1 . Persistent Volume Claim ) string { return utilstrings . Join Qualified Name ( claim . Get Namespace ( ) , claim . Get } 
func Check Volume Mode Filesystem ( volume Spec * volume . Spec ) ( bool , error ) { if utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) { volume Mode , err := Get Volume Mode ( volume if volume Mode == v1 . Persistent Volume } 
func Check Persistent Volume Claim Mode Block ( pvc * v1 . Persistent Volume Claim ) bool { return utilfeature . Default Feature Gate . Enabled ( features . Block Volume ) && pvc . Spec . Volume Mode != nil && * pvc . Spec . Volume Mode == v1 . Persistent Volume } 
func Is Windows UNC // Check for UNC prefix \\ if strings . Has } 
func Is Windows Local if Is Windows UNC if ! ( strings . Has Prefix ( path , `/` ) || strings . Has } 
func Make Absolute // If there is a slash, but no drive, add 'c:' if strings . Has Prefix ( path , " " ) || strings . Has } 
func Map Block Volume ( device Path , global Map Path , pod Volume Map Path , volume Map Name string , pod UID utypes . UID , ) error { blk Util := volumepathhandler . New Block Volume Path // map device Path to global node path map Err := blk Util . Map Device ( device Path , global Map Path , string ( pod if map Err != nil { return map // map device Path to pod volume path map Err = blk Util . Map Device ( device Path , pod Volume Map Path , volume Map if map Err != nil { return map } 
func Get Plugin Mount Dir ( host volume . Volume Host , name string ) string { mnt Dir := filepath . Join ( host . Get Plugin Dir ( name ) , Mounts In Global PD return mnt } 
func is Etcd Error Num ( err error , error Code int ) bool { if err != nil { if etcd Error , ok := err . ( etcd . Error ) ; ok { return etcd Error . Code == error } 
func Get Etcd if response . Status Code != http . Status version Bytes , err := ioutil . Read return string ( version } 
func new Pod labels [ types . Kubernetes Pod Name labels [ types . Kubernetes Pod Namespace labels [ types . Kubernetes Pod UID } 
func new Container labels [ types . Kubernetes Pod Name labels [ types . Kubernetes Pod Namespace labels [ types . Kubernetes Pod UID labels [ types . Kubernetes Container Name } 
func new Container Annotations ( container * v1 . Container , pod * v1 . Pod , restart Count int , opts * kubecontainer . Run Container annotations [ container Hash Label ] = strconv . Format Uint ( kubecontainer . Hash annotations [ container Restart Count Label ] = strconv . Itoa ( restart annotations [ container Termination Message Path Label ] = container . Termination Message annotations [ container Termination Message Policy Label ] = string ( container . Termination Message if pod . Deletion Grace Period Seconds != nil { annotations [ pod Deletion Grace Period Label ] = strconv . Format Int ( * pod . Deletion Grace Period if pod . Spec . Termination Grace Period Seconds != nil { annotations [ pod Termination Grace Period Label ] = strconv . Format Int ( * pod . Spec . Termination Grace Period if container . Lifecycle != nil && container . Lifecycle . Pre Stop != nil { // Using json encoding so that the Pre Stop handler object is readable after writing as a label raw Pre Stop , err := json . Marshal ( container . Lifecycle . Pre } else { annotations [ container Pre Stop Handler Label ] = string ( raw Pre if len ( container . Ports ) > 0 { raw Container } else { annotations [ container Ports Label ] = string ( raw Container } 
func get Pod Sandbox Info From Labels ( labels map [ string ] string ) * labeled Pod Sandbox Info { pod Sandbox Info := & labeled Pod Sandbox Info { Labels : make ( map [ string ] string ) , Pod Name : get String Value From Label ( labels , types . Kubernetes Pod Name Label ) , Pod Namespace : get String Value From Label ( labels , types . Kubernetes Pod Namespace Label ) , Pod UID : kubetypes . UID ( get String Value From Label ( labels , types . Kubernetes Pod UID // Remain only labels from v1.Pod for k , v := range labels { if k != types . Kubernetes Pod Name Label && k != types . Kubernetes Pod Namespace Label && k != types . Kubernetes Pod UID Label { pod Sandbox return pod Sandbox } 
func get Container Info From Labels ( labels map [ string ] string ) * labeled Container Info { return & labeled Container Info { Pod Name : get String Value From Label ( labels , types . Kubernetes Pod Name Label ) , Pod Namespace : get String Value From Label ( labels , types . Kubernetes Pod Namespace Label ) , Pod UID : kubetypes . UID ( get String Value From Label ( labels , types . Kubernetes Pod UID Label ) ) , Container Name : get String Value From Label ( labels , types . Kubernetes Container Name } 
func get Container Info From Annotations ( annotations map [ string ] string ) * annotated Container container Info := & annotated Container Info { Termination Message Path : get String Value From Label ( annotations , container Termination Message Path Label ) , Termination Message Policy : v1 . Termination Message Policy ( get String Value From Label ( annotations , container Termination Message Policy if container Info . Hash , err = get Uint64Value From Label ( annotations , container Hash Label ) ; err != nil { klog . Errorf ( " " , container Hash if container Info . Restart Count , err = get Int Value From Label ( annotations , container Restart Count Label ) ; err != nil { klog . Errorf ( " " , container Restart Count if container Info . Pod Deletion Grace Period , err = get Int64Pointer From Label ( annotations , pod Deletion Grace Period Label ) ; err != nil { klog . Errorf ( " " , pod Deletion Grace Period if container Info . Pod Termination Grace Period , err = get Int64Pointer From Label ( annotations , pod Termination Grace Period Label ) ; err != nil { klog . Errorf ( " " , pod Termination Grace Period pre Stop if found , err := get JSON Object From Label ( annotations , container Pre Stop Handler Label , pre Stop Handler ) ; err != nil { klog . Errorf ( " " , container Pre Stop Handler } else if found { container Info . Pre Stop Handler = pre Stop container Ports := [ ] v1 . Container if found , err := get JSON Object From Label ( annotations , container Ports Label , & container Ports ) ; err != nil { klog . Errorf ( " " , container Ports } else if found { container Info . Container Ports = container return container } 
func get JSON Object From Label ( labels map [ string ] string , label string , value interface { } ) ( bool , error ) { if str Value , found := labels [ label ] ; found { err := json . Unmarshal ( [ ] byte ( str } 
func ( f * shared Informer Factory ) For Resource ( resource schema . Group Version Resource ) ( Generic Informer , error ) { switch resource { // Group=samplecontroller.k8s.io, Version=v1alpha1 case v1alpha1 . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group } 
func New Informer Factory ( typed Informer Factory informers . Shared Informer Factory , dynamic Informer Factory dynamicinformer . Dynamic Shared Informer Factory ) Informer Factory { return & informer Factory { typed Informer Factory : typed Informer Factory , dynamic Informer Factory : dynamic Informer } 
func With CORS ( handler http . Handler , allowed Origin Patterns [ ] string , allowed Methods [ ] string , allowed Headers [ ] string , exposed Headers [ ] string , allow Credentials string ) http . Handler { if len ( allowed Origin allowed Origin Patterns R Es := allowed Origin Regexps ( allowed Origin return http . Handler Func ( func ( w http . Response for _ , re := range allowed Origin Patterns R Es { if allowed = re . Match // Set defaults for methods and headers if nothing was passed if allowed Methods == nil { allowed if allowed Headers == nil { allowed if exposed Headers == nil { exposed w . Header ( ) . Set ( " " , strings . Join ( allowed w . Header ( ) . Set ( " " , strings . Join ( allowed w . Header ( ) . Set ( " " , strings . Join ( exposed w . Header ( ) . Set ( " " , allow // Stop here if its a preflight OPTIONS request if req . Method == " " { w . Write Header ( http . Status No // Dispatch to the next handler handler . Serve } 
func compile Regexps ( regexp for _ , regexp Str := range regexp Strings { r , err := regexp . Compile ( regexp } 
func ( p * cri Stats Provider ) list Container Network Stats ( ) ( map [ string ] * statsapi . Network Stats , error ) { containers , err := hcsshim . Get Containers ( hcsshim . Compute System stats := make ( map [ string ] * statsapi . Network for _ , c := range containers { container , err := hcsshim . Open if len ( cstats . Network ) > 0 { stats [ c . ID ] = hcs Stats To Network } 
func hcs Stats To Network Stats ( timestamp time . Time , hcs Stats [ ] hcsshim . Network Stats ) * statsapi . Network Stats { result := & statsapi . Network Stats { Time : metav1 . New Time ( timestamp ) , Interfaces : make ( [ ] statsapi . Interface adapters := sets . New for _ , stat := range hcs Stats { i Stat , err := hcs Stats To Interface if err != nil { klog . Warningf ( " " , stat . Endpoint // Only count each adapter once. if adapters . Has ( i result . Interfaces = append ( result . Interfaces , * i adapters . Insert ( i // TODO(feiskyer): add support of multiple interfaces for getting default interface. if len ( result . Interfaces ) > 0 { result . Interface } 
func hcs Stats To Interface Stats ( stat hcsshim . Network Stats ) ( * statsapi . Interface Stats , error ) { endpoint , err := hcsshim . Get HNS Endpoint By ID ( stat . Endpoint return & statsapi . Interface Stats { Name : endpoint . Name , Rx Bytes : & stat . Bytes Received , Tx Bytes : & stat . Bytes } 
func ( c * client Cache ) set Client ( issuer , client ID string , client * oidc Auth Provider ) * oidc Auth key := cache Key { issuer , client // If another client has already initialized a client for the given provider we want // to use that client instead of the one we're trying to set. This is so all transports // share a client and can coordinate around the same mutex when refreshing and writing // to the kubeconfig. if old Client , ok := c . cache [ key ] ; ok { return old } 
func token Endpoint ( client * http . Client , issuer string ) ( string , error ) { // Well known URL for getting Open ID Connect metadata. // // https://openid.net/specs/openid-connect-discovery-1_0.html#Provider Config well Known := strings . Trim resp , err := client . Get ( well body , err := ioutil . Read if resp . Status Code != http . Status // Metadata object. We only care about the token_endpoint, the thing endpoint // we'll be refreshing against. // // https://openid.net/specs/openid-connect-discovery-1_0.html#Provider Metadata var metadata struct { Token if metadata . Token return metadata . Token } 
func Split Scheme Name if len ( name ) > 0 && valid } 
func Join Scheme Name } 
func Register ( asw cache . Actual State Of World , dsw cache . Desired State Of World , plugin Mgr * volume . Volume Plugin Mgr ) { register Metrics . Do ( func ( ) { prometheus . Must Register ( & total Volumes Collector { asw , dsw , plugin } 
func ( c * total Volumes Collector ) Collect ( ch chan <- prometheus . Metric ) { for state Name , plugin Count := range c . get Volume Count ( ) { for plugin Name , count := range plugin Count { metric , err := prometheus . New Const Metric ( total Volumes Desc , prometheus . Gauge Value , float64 ( count ) , plugin Name , state } 
func is Default Registry Match ( image string ) bool { parts := strings . Split // From: http://blog.docker.com/2013/07/how-to-use-your-own-registry/ // Docker looks for either a . (domain separator) or : (port separator) // to learn that the first part of the repository name is a location and not // a user name. return ! strings . Contains } 
func parse Schemeless Url ( schemeless Url string ) ( * url . URL , error ) { parsed , err := url . Parse ( " " + schemeless } 
func split Url ( url * url . URL ) ( parts [ ] string , port string ) { host , port , err := net . Split Host } 
func urls Match Str ( glob string , target string ) ( bool , error ) { glob Url , err := parse Schemeless target Url , err := parse Schemeless return urls Match ( glob Url , target } 
func urls Match ( glob Url * url . URL , target Url * url . URL ) ( bool , error ) { glob Url Parts , glob Port := split Url ( glob target Url Parts , target Port := split Url ( target if glob Port != target if len ( glob Url Parts ) != len ( target Url if ! strings . Has Prefix ( target Url . Path , glob for k , glob Url Part := range glob Url Parts { target Url Part := target Url matched , err := filepath . Match ( glob Url Part , target Url } 
func ( attacher * aws Elastic Block Store Attacher ) Mount Device ( spec * volume . Spec , device Path string , device Mount Path string ) error { mounter := attacher . host . Get Mounter ( aws Elastic Block Store Plugin not Mnt , err := mounter . Is Likely Not Mount Point ( device Mount if err != nil { if os . Is Not Exist ( err ) { if err := os . Mkdir All ( device Mount not volume Source , read Only , err := get Volume if read if not Mnt { disk Mounter := volumeutil . New Safe Format And Mount From Host ( aws Elastic Block Store Plugin mount Options := volumeutil . Mount Option From err = disk Mounter . Format And Mount ( device Path , device Mount Path , volume Source . FS Type , mount if err != nil { os . Remove ( device Mount } 
func ( c * Fake Pod Metricses ) List ( opts v1 . List Options ) ( result * v1beta1 . Pod Metrics List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( podmetricses Resource , podmetricses Kind , c . ns , opts ) , & v1beta1 . Pod Metrics label , _ , _ := testing . Extract From List list := & v1beta1 . Pod Metrics List { List Meta : obj . ( * v1beta1 . Pod Metrics List ) . List for _ , item := range obj . ( * v1beta1 . Pod Metrics } 
func exists ( item interface { } , indices ... interface { } ) bool { v := reflect . Value for _ , i := range indices { index := reflect . Value var is if v , is Nil = indirect ( v ) ; is case reflect . Map : if ! index . Is if ! index . Type ( ) . Assignable if x := v . Map Index ( index ) ; x . Is if _ , is Nil := indirect ( v ) ; is } 
func ( j * JSON Path Printer ) Print Obj ( obj runtime . Object , w io . Writer ) error { // we use reflect.Indirect here in order to obtain the actual value from a pointer. // we need an actual value in order to retrieve the package path for an object. // using reflect.Indirect indiscriminately is valid here, as all runtime.Objects are supposed to be pointers. if Internal Object Preventer . Is Forbidden ( reflect . Indirect ( reflect . Value Of ( obj ) ) . Type ( ) . Pkg Path ( ) ) { return fmt . Errorf ( Internal Object Printer var query if unstructured , ok := obj . ( runtime . Unstructured ) ; ok { query Obj = unstructured . Unstructured query if err := json . Unmarshal ( data , & query if err := j . JSON Path . Execute ( w , query Obj ) ; err != nil { buf := bytes . New fmt . Fprintf ( buf , " \t \n \t \t \n " , j . raw fmt . Fprintf ( buf , " \t \n \t \t \n \n " , query return fmt . Errorf ( " \n " , j . raw } 
func Current Package ( ) string { for _ , root := range gobuild . Default . Src Dirs ( ) { if pkg , ok := has } 
func ( ds * docker Service ) Image Fs Info ( _ context . Context , r * runtimeapi . Image Fs Info Request ) ( * runtimeapi . Image Fs Info } 
func Get Template Generation ( ds * apps . Daemon Set ) ( * int64 , error ) { annotation , found := ds . Annotations [ apps . Deprecated Template generation , err := strconv . Parse } 
func Add Or Update Daemon Pod Tolerations ( spec * v1 . Pod Spec ) { // Daemon Set pods shouldn't be deleted by Node Controller in case of node problems. // Add infinite toleration for taint not Ready:No Execute here // to survive taint-based eviction enforced by Node Controller // when node turns not ready. v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node Not Ready , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No // Daemon Set pods shouldn't be deleted by Node Controller in case of node problems. // Add infinite toleration for taint unreachable:No Execute here // to survive taint-based eviction enforced by Node Controller // when node turns unreachable. v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node Unreachable , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No // According to Taint Nodes By Condition feature, all Daemon Set pods should tolerate // Memory Pressure, Disk Pressure, PID Pressure, Unschedulable and Network Unavailable taints. v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node Disk Pressure , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node Memory Pressure , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node PID Pressure , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node Unschedulable , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No if spec . Host Network { v1helper . Add Or Update Toleration In Pod Spec ( spec , & v1 . Toleration { Key : schedulerapi . Taint Node Network Unavailable , Operator : v1 . Toleration Op Exists , Effect : v1 . Taint Effect No } 
func Create Pod Template ( template v1 . Pod Template Spec , generation * int64 , hash string ) v1 . Pod Template Spec { new Template := * template . Deep Add Or Update Daemon Pod Tolerations ( & new if new Template . Object Meta . Labels == nil { new Template . Object if generation != nil { new Template . Object Meta . Labels [ extensions . Daemon Set Template Generation // TODO: do we need to validate if the Daemon Set is Rolling Update or not? if len ( hash ) > 0 { new Template . Object Meta . Labels [ extensions . Default Daemon Set Unique Label return new } 
func Is Pod Updated ( pod * v1 . Pod , hash string , ds Template Generation * int64 ) bool { // Compare with hash to see if the pod is updated, need to maintain backward compatibility of template Generation template Matches := ds Template Generation != nil && pod . Labels [ extensions . Daemon Set Template Generation Key ] == fmt . Sprint ( ds Template hash Matches := len ( hash ) > 0 && pod . Labels [ extensions . Default Daemon Set Unique Label return hash Matches || template } 
func Split By Available Pods ( min Ready Seconds int32 , pods [ ] * v1 . Pod ) ( [ ] * v1 . Pod , [ ] * v1 . Pod ) { unavailable available for _ , pod := range pods { if podutil . Is Pod Available ( pod , min Ready Seconds , metav1 . Now ( ) ) { available Pods = append ( available } else { unavailable Pods = append ( unavailable return available Pods , unavailable } 
func Replace Daemon Set Pod Node Name Node Affinity ( affinity * v1 . Affinity , nodename string ) * v1 . Affinity { node Sel Req := v1 . Node Selector Requirement { Key : schedulerapi . Node Field Selector Key Node Name , Operator : v1 . Node Selector Op node Selector := & v1 . Node Selector { Node Selector Terms : [ ] v1 . Node Selector Term { { Match Fields : [ ] v1 . Node Selector Requirement { node Sel if affinity == nil { return & v1 . Affinity { Node Affinity : & v1 . Node Affinity { Required During Scheduling Ignored During Execution : node if affinity . Node Affinity == nil { affinity . Node Affinity = & v1 . Node Affinity { Required During Scheduling Ignored During Execution : node node Affinity := affinity . Node if node Affinity . Required During Scheduling Ignored During Execution == nil { node Affinity . Required During Scheduling Ignored During Execution = node // Replace node selector with the new one. node Affinity . Required During Scheduling Ignored During Execution . Node Selector Terms = [ ] v1 . Node Selector Term { { Match Fields : [ ] v1 . Node Selector Requirement { node Sel } 
func Get Target Node Name ( pod * v1 . Pod ) ( string , error ) { if len ( pod . Spec . Node Name ) != 0 { return pod . Spec . Node // If Schedule Daemon Set Pods was enabled before, retrieve node name of unscheduled pods from Node Affinity if pod . Spec . Affinity == nil || pod . Spec . Affinity . Node Affinity == nil || pod . Spec . Affinity . Node Affinity . Required During Scheduling Ignored During terms := pod . Spec . Affinity . Node Affinity . Required During Scheduling Ignored During Execution . Node Selector for _ , term := range terms { for _ , exp := range term . Match Fields { if exp . Key == schedulerapi . Node Field Selector Key Node Name && exp . Operator == v1 . Node Selector Op In { if len ( exp . Values ) != 1 { return " " , fmt . Errorf ( " " , schedulerapi . Node Field Selector Key Node } 
func New Runtime Admit Handler ( runtime container . Runtime ) ( * runtime Admit Handler , error ) { switch runtime . Type ( ) { case docker Type Name : v , err := runtime . API // only Docker API version >= 1.24 supports sysctls c , err := v . Compare ( docker Minimum API if c >= 0 { return & runtime Admit Handler { result : lifecycle . Pod Admit return & runtime Admit Handler { result : lifecycle . Pod Admit Result { Admit : false , Reason : Unsupported default : // Return admit for other runtimes. return & runtime Admit Handler { result : lifecycle . Pod Admit } 
func ( w * runtime Admit Handler ) Admit ( attrs * lifecycle . Pod Admit Attributes ) lifecycle . Pod Admit Result { if attrs . Pod . Spec . Security Context != nil { if len ( attrs . Pod . Spec . Security return lifecycle . Pod Admit } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Lease List ) ( nil ) , ( * coordination . Lease List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Lease List_To_coordination_Lease List ( a . ( * v1beta1 . Lease List ) , b . ( * coordination . Lease if err := s . Add Generated Conversion Func ( ( * coordination . Lease List ) ( nil ) , ( * v1beta1 . Lease List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_coordination_Lease List_To_v1beta1_Lease List ( a . ( * coordination . Lease List ) , b . ( * v1beta1 . Lease if err := s . Add Generated Conversion Func ( ( * v1beta1 . Lease Spec ) ( nil ) , ( * coordination . Lease Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Lease Spec_To_coordination_Lease Spec ( a . ( * v1beta1 . Lease Spec ) , b . ( * coordination . Lease if err := s . Add Generated Conversion Func ( ( * coordination . Lease Spec ) ( nil ) , ( * v1beta1 . Lease Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_coordination_Lease Spec_To_v1beta1_Lease Spec ( a . ( * coordination . Lease Spec ) , b . ( * v1beta1 . Lease } 
func Convert_v1beta1_Lease_To_coordination_Lease ( in * v1beta1 . Lease , out * coordination . Lease , s conversion . Scope ) error { return auto } 
func Convert_coordination_Lease_To_v1beta1_Lease ( in * coordination . Lease , out * v1beta1 . Lease , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Lease List_To_coordination_Lease List ( in * v1beta1 . Lease List , out * coordination . Lease List , s conversion . Scope ) error { return auto Convert_v1beta1_Lease List_To_coordination_Lease } 
func Convert_coordination_Lease List_To_v1beta1_Lease List ( in * coordination . Lease List , out * v1beta1 . Lease List , s conversion . Scope ) error { return auto Convert_coordination_Lease List_To_v1beta1_Lease } 
func Convert_v1beta1_Lease Spec_To_coordination_Lease Spec ( in * v1beta1 . Lease Spec , out * coordination . Lease Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Lease Spec_To_coordination_Lease } 
func Convert_coordination_Lease Spec_To_v1beta1_Lease Spec ( in * coordination . Lease Spec , out * v1beta1 . Lease Spec , s conversion . Scope ) error { return auto Convert_coordination_Lease Spec_To_v1beta1_Lease } 
func ( c * Apiextensions V1beta1Client ) REST return c . rest } 
func From Services ( services [ ] * v1 . Service ) [ ] v1 . Env Var { var result [ ] v1 . Env // ignore services where Cluster IP is "None" or empty // the services passed to this method should be pre-filtered // only services that have the cluster IP set should be included here if ! v1helper . Is Service IP // Host name := make Env Variable result = append ( result , v1 . Env Var { Name : name , Value : service . Spec . Cluster // First port - give it the backwards-compatible name name = make Env Variable result = append ( result , v1 . Env if sp . Name != " " { pn := name + " " + make Env Variable result = append ( result , v1 . Env // Docker-compatible vars. result = append ( result , make Link } 
func parse Scopes ( gcp Config map [ string ] string ) [ ] string { scopes , ok := gcp if ! ok { return default return strings . Split ( gcp } 
func ( t * cached Token Source ) base } 
func ( c * Cache nodes , err := c . Node pods , err := c . Pod pending Pods := c . Pod Queue . Pending if missed , redundant := c . Compare if missed , redundant := c . Compare Pods ( pods , pending } 
func ( c * Cache Comparer ) Compare Nodes ( nodes [ ] * v1 . Node , nodeinfos map [ string ] * schedulernodeinfo . Node for node Name := range nodeinfos { cached = append ( cached , node return compare } 
func ( c * Cache Comparer ) Compare Pods ( pods , waiting Pods [ ] * v1 . Pod , nodeinfos map [ string ] * schedulernodeinfo . Node for _ , pod := range waiting return compare } 
func New REST ( scheme * runtime . Scheme , opts Getter generic . REST Options Getter ) * REST { strategy := New store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & apiextensions . Custom Resource Definition { } } , New List Func : func ( ) runtime . Object { return & apiextensions . Custom Resource Definition List { } } , Predicate Func : Match Custom Resource Definition , Default Qualified Resource : apiextensions . Resource ( " " ) , Create Strategy : strategy , Update Strategy : strategy , Delete options := & generic . Store Options { REST Options : opts Getter , Attr Func : Get if err := store . Complete With } 
func ( r * REST ) Delete ( ctx context . Context , name string , options * metav1 . Delete Options ) ( runtime . Object , bool , error ) { obj , err := r . Get ( ctx , name , & metav1 . Get crd := obj . ( * apiextensions . Custom Resource // Ensure we have a UID precondition if options == nil { options = metav1 . New Delete } else if * options . Preconditions . UID != crd . UID { err = apierrors . New if options . Preconditions . Resource Version != nil && * options . Preconditions . Resource Version != crd . Resource Version { err = apierrors . New Conflict ( apiextensions . Resource ( " " ) , name , fmt . Errorf ( " " , * options . Preconditions . Resource Version , crd . Resource // upon first request to delete, add our finalizer and then delegate if crd . Deletion Timestamp . Is Zero ( ) { key , err := r . Store . Key preconditions := storage . Preconditions { UID : options . Preconditions . UID , Resource Version : options . Preconditions . Resource out := r . Store . New err = r . Store . Storage . Guaranteed Update ( ctx , key , out , false , & preconditions , storage . Simple Update ( func ( existing runtime . Object ) ( runtime . Object , error ) { existing CRD , ok := existing . ( * apiextensions . Custom Resource // Set the deletion timestamp if needed if existing CRD . Deletion Timestamp . Is existing CRD . Deletion if ! apiextensions . CRD Has Finalizer ( existing CRD , apiextensions . Custom Resource Cleanup Finalizer ) { existing CRD . Finalizers = append ( existing CRD . Finalizers , apiextensions . Custom Resource Cleanup // update the status condition too apiextensions . Set CRD Condition ( existing CRD , apiextensions . Custom Resource Definition Condition { Type : apiextensions . Terminating , Status : apiextensions . Condition return existing } ) , dryrun . Is Dry Run ( options . Dry if err != nil { err = storageerr . Interpret Get err = storageerr . Interpret Update if _ , ok := err . ( * apierrors . Status Error ) ; ! ok { err = apierrors . New Internal } 
func New Status REST ( scheme * runtime . Scheme , rest * REST ) * Status REST { status status Store . Create status Store . Delete status Store . Update Strategy = New Status return & Status REST { store : & status } 
func Validate Kubelet Configuration ( kc * kubeletconfig . Kubelet Configuration ) error { all // Make a local copy of the global feature gates and combine it with the gates set by this configuration. // This allows us to validate the config against the set of gates it will actually run against. local Feature Gate := utilfeature . Default Feature Gate . Deep if err := local Feature Gate . Set From Map ( kc . Feature if kc . Node Lease Duration Seconds <= 0 { all Errors = append ( all if ! kc . Cgroups Per QOS && len ( kc . Enforce Node Allocatable ) > 0 { all Errors = append ( all if kc . System Cgroups != " " && kc . Cgroup Root == " " { all Errors = append ( all if kc . Event Burst < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Event if kc . Event Record QPS < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Event Record if kc . Healthz Port != 0 && utilvalidation . Is Valid Port Num ( int ( kc . Healthz Port ) ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Healthz if local Feature Gate . Enabled ( features . CPUCFS Quota Period ) && utilvalidation . Is In Range ( int ( kc . CPUCFS Quota Period . Duration ) , int ( 1 * time . Microsecond ) , int ( time . Second ) ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . CPUCFS Quota if utilvalidation . Is In Range ( int ( kc . Image GC High Threshold Percent ) , 0 , 100 ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Image GC High Threshold if utilvalidation . Is In Range ( int ( kc . Image GC Low Threshold Percent ) , 0 , 100 ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Image GC Low Threshold if kc . Image GC Low Threshold Percent >= kc . Image GC High Threshold Percent { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Image GC Low Threshold Percent , kc . Image GC High Threshold if utilvalidation . Is In Range ( int ( kc . IP Tables Drop Bit ) , 0 , 31 ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . IP Tables Drop if utilvalidation . Is In Range ( int ( kc . IP Tables Masquerade Bit ) , 0 , 31 ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . IP Tables Masquerade if kc . Kube API Burst < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Kube API if kc . Kube APIQPS < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Kube if kc . Max Open Files < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Max Open if kc . Max Pods < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Max if utilvalidation . Is In Range ( int ( kc . OOM Score Adj ) , - 1000 , 1000 ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . OOM Score if kc . Pods Per Core < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Pods Per if utilvalidation . Is Valid Port Num ( int ( kc . Port ) ) != nil { all Errors = append ( all if kc . Read Only Port != 0 && utilvalidation . Is Valid Port Num ( int ( kc . Read Only Port ) ) != nil { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Read Only if kc . Registry Burst < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Registry if kc . Registry Pull QPS < 0 { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Registry Pull if kc . Rotate Certificates && ! local Feature Gate . Enabled ( features . Rotate Kubelet Client Certificate ) { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Rotate if kc . Server TLS Bootstrap && ! local Feature Gate . Enabled ( features . Rotate Kubelet Server Certificate ) { all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Server TLS for _ , val := range kc . Enforce Node Allocatable { switch val { case kubetypes . Node Allocatable Enforcement Key : case kubetypes . System Reserved Enforcement Key : if kc . System Reserved Cgroup == " " { all Errors = append ( all case kubetypes . Kube Reserved Enforcement Key : if kc . Kube Reserved Cgroup == " " { all Errors = append ( all case kubetypes . Node Allocatable None Key : if len ( kc . Enforce Node Allocatable ) > 1 { all Errors = append ( all Errors , fmt . Errorf ( " " , kubetypes . Node Allocatable None default : all Errors = append ( all Errors , fmt . Errorf ( " " , val , kubetypes . Node Allocatable Enforcement Key , kubetypes . System Reserved Enforcement Key , kubetypes . Kube Reserved Enforcement Key , kubetypes . Node Allocatable None switch kc . Hairpin Mode { case kubeletconfig . Hairpin None : case kubeletconfig . Hairpin Veth : case kubeletconfig . Promiscuous Bridge : default : all Errors = append ( all Errors , fmt . Errorf ( " " , kc . Hairpin Mode , kubeletconfig . Hairpin None , kubeletconfig . Hairpin Veth , kubeletconfig . Promiscuous if err := validate Kubelet OS Configuration ( kc ) ; err != nil { all Errors = append ( all return utilerrors . New Aggregate ( all } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1beta1 . Priority Class ) ( nil ) , ( * scheduling . Priority Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Priority Class_To_scheduling_Priority Class ( a . ( * v1beta1 . Priority Class ) , b . ( * scheduling . Priority if err := s . Add Generated Conversion Func ( ( * scheduling . Priority Class ) ( nil ) , ( * v1beta1 . Priority Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheduling_Priority Class_To_v1beta1_Priority Class ( a . ( * scheduling . Priority Class ) , b . ( * v1beta1 . Priority if err := s . Add Generated Conversion Func ( ( * v1beta1 . Priority Class List ) ( nil ) , ( * scheduling . Priority Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Priority Class List_To_scheduling_Priority Class List ( a . ( * v1beta1 . Priority Class List ) , b . ( * scheduling . Priority Class if err := s . Add Generated Conversion Func ( ( * scheduling . Priority Class List ) ( nil ) , ( * v1beta1 . Priority Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheduling_Priority Class List_To_v1beta1_Priority Class List ( a . ( * scheduling . Priority Class List ) , b . ( * v1beta1 . Priority Class } 
func Convert_v1beta1_Priority Class_To_scheduling_Priority Class ( in * v1beta1 . Priority Class , out * scheduling . Priority Class , s conversion . Scope ) error { return auto Convert_v1beta1_Priority Class_To_scheduling_Priority } 
func Convert_scheduling_Priority Class_To_v1beta1_Priority Class ( in * scheduling . Priority Class , out * v1beta1 . Priority Class , s conversion . Scope ) error { return auto Convert_scheduling_Priority Class_To_v1beta1_Priority } 
func Convert_v1beta1_Priority Class List_To_scheduling_Priority Class List ( in * v1beta1 . Priority Class List , out * scheduling . Priority Class List , s conversion . Scope ) error { return auto Convert_v1beta1_Priority Class List_To_scheduling_Priority Class } 
func Convert_scheduling_Priority Class List_To_v1beta1_Priority Class List ( in * scheduling . Priority Class List , out * v1beta1 . Priority Class List , s conversion . Scope ) error { return auto Convert_scheduling_Priority Class List_To_v1beta1_Priority Class } 
func new CS Cloud ( cfg * CS Config ) ( * CS Cloud , error ) { cs := & CS Cloud { project ID : cfg . Global . Project // When running the kubelet service it's fine to not specify a config file (or only a // partial config file) as all needed info can be retrieved anonymously using metadata. if filepath . Base ( exe ) == " " || filepath . Base ( exe ) == " " { // In Cloud Stack your metadata is always served by the DHCP server. dhcp Server , err := find DHCP if err == nil { klog . V ( 4 ) . Infof ( " " , dhcp cs . metadata = & metadata { dhcp Server : dhcp if cfg . Global . APIURL != " " && cfg . Global . API Key != " " && cfg . Global . Secret Key != " " { cs . client = cloudstack . New Async Client ( cfg . Global . APIURL , cfg . Global . API Key , cfg . Global . Secret Key , ! cfg . Global . SSL No } 
func ( cs * CS Cloud ) Load Balancer ( ) ( cloudprovider . Load } 
func ( cs * CS } 
func ( cs * CS } 
func ( cs * CS } 
func ( cs * CS Cloud ) Get instance , count , err := cs . client . Virtual Machine . Get Virtual Machine By zone . Failure } 
func ( cs * CS Cloud ) Get Zone By Provider ID ( ctx context . Context , provider instance , count , err := cs . client . Virtual Machine . Get Virtual Machine By ID ( provider ID , cloudstack . With Project ( cs . project if err != nil { if count == 0 { return zone , fmt . Errorf ( " " , provider zone . Failure } 
func ( cs * CS Cloud ) Get Zone By Node Name ( ctx context . Context , node Name types . Node instance , count , err := cs . client . Virtual Machine . Get Virtual Machine By Name ( string ( node Name ) , cloudstack . With Project ( cs . project if err != nil { if count == 0 { return zone , fmt . Errorf ( " " , node zone . Failure } 
func ( s * config Map Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Config Map , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Config } 
func ( s * config Map Lister ) Config Maps ( namespace string ) Config Map Namespace Lister { return config Map Namespace } 
func ( s config Map Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Config Map , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Config } 
func ( c * Cgroups Validator ) Validate ( spec Sys Spec ) ( error , error ) { subsystems , err := c . get Cgroup return nil , c . validate Cgroup } 
func ( c * Clientset ) Apiregistration V1beta1 ( ) apiregistrationv1beta1 . Apiregistration V1beta1Interface { return & fakeapiregistrationv1beta1 . Fake Apiregistration } 
func ( c * Clientset ) Apiregistration V1 ( ) apiregistrationv1 . Apiregistration V1Interface { return & fakeapiregistrationv1 . Fake Apiregistration } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . Certificate Signing Request { } , func ( obj interface { } ) { Set Object Defaults_Certificate Signing Request ( obj . ( * v1beta1 . Certificate Signing scheme . Add Type Defaulting Func ( & v1beta1 . Certificate Signing Request List { } , func ( obj interface { } ) { Set Object Defaults_Certificate Signing Request List ( obj . ( * v1beta1 . Certificate Signing Request } 
func Long } 
} 
func Normalize ( cmd * cobra . Command ) * cobra . Command { if len ( cmd . Long ) > 0 { cmd . Long = Long } 
func Normalize All ( cmd * cobra . Command ) * cobra . Command { if cmd . Has Sub Commands ( ) { for _ , sub Cmd := range cmd . Commands ( ) { Normalize All ( sub } 
func Simple Update ( fn Simple Update Func ) Update Func { return func ( input runtime . Object , _ Response } 
func ( hwm * High Water Mark ) Update ( current int64 ) bool { for { old := atomic . Load if atomic . Compare And Swap } 
func New Etcd ( alloc allocator . Snapshottable , base Key string , resource schema . Group Resource , config * storagebackend . Config ) * Etcd { storage , d := generic . New Raw // TODO : Remove Register Storage Cleanup below when PR // https://github.com/kubernetes/kubernetes/pull/50690 // merges as that shuts down storage properly registry . Register Storage return & Etcd { alloc : alloc , storage : storage , base Key : base } 
err = e . try if ! ok { return error Unable To if err != nil { if err == error Unable To } 
return e . try } 
func ( e * Etcd ) try Update ( fn func ( ) error ) error { err := e . storage . Guaranteed Update ( context . TODO ( ) , e . base Key , & api . Range Allocation { } , true , nil , storage . Simple Update ( func ( input runtime . Object ) ( output runtime . Object , err error ) { existing := input . ( * api . Range if len ( existing . Resource if existing . Resource e . last = existing . Resource range existing . Range = range return storeerr . Interpret Update } 
func ( e * Etcd ) Get ( ) ( * api . Range Allocation , error ) { existing := & api . Range if err := e . storage . Get ( context . TODO ( ) , e . base Key , " " , existing , true ) ; err != nil { return nil , storeerr . Interpret Get } 
func ( e * Etcd ) Create Or Update ( snapshot * api . Range err := e . storage . Guaranteed Update ( context . TODO ( ) , e . base Key , & api . Range Allocation { } , true , nil , storage . Simple Update ( func ( input runtime . Object ) ( output runtime . Object , err error ) { existing := input . ( * api . Range switch { case len ( snapshot . Resource Version ) != 0 && len ( existing . Resource Version ) != 0 : if snapshot . Resource Version != existing . Resource Version { return nil , k8serr . New case len ( existing . Resource Version ) != 0 : return nil , k8serr . New last = snapshot . Resource if err != nil { return storeerr . Interpret Update } 
} 
} 
func new Event Rate Limit ( config * eventratelimitapi . Configuration , clock flowcontrol . Clock ) ( * Plugin , error ) { limit Enforcers := make ( [ ] * limit for _ , limit Config := range config . Limits { enforcer , err := new Limit Enforcer ( limit limit Enforcers = append ( limit event Rate Limit Admission := & Plugin { Handler : admission . New Handler ( admission . Create , admission . Update ) , limit Enforcers : limit return event Rate Limit } 
func ( a * Plugin ) Validate ( attr admission . Attributes , o admission . Object Interfaces ) ( err error ) { // ignore all operations that do not correspond to an Event kind if attr . Get Kind ( ) . Group // ignore all requests that specify dry-run // because they don't correspond to any calls to etcd, // they should not be affected by the ratelimit if attr . Is Dry // give each limit enforcer a chance to reject the event for _ , enforcer := range a . limit if aggregated Err := utilerrors . New Aggregate ( errors ) ; aggregated Err != nil { return apierrors . New Too Many Requests Error ( aggregated } 
func New Rollout Status Options ( streams genericclioptions . IO Streams ) * Rollout Status Options { return & Rollout Status Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , Filename Options : & resource . Filename Options { } , IO } 
func New Cmd Rollout Status ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := New Rollout Status valid cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : status Long , Example : status Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check } , Valid Args : valid cmdutil . Add Filename Option Flags ( cmd , o . Filename cmd . Flags ( ) . Bool Var cmd . Flags ( ) . Duration } 
func ( o * Rollout Status Options ) Complete ( f cmdutil . Factory , args [ ] string ) error { o . Builder = f . New o . Namespace , o . Enforce Namespace , err = f . To Raw Kube Config o . Builder o . Status Viewer Fn = polymorphichelpers . Status Viewer client Config , err := f . To REST o . Dynamic Client , err = dynamic . New For Config ( client } 
func ( o * Rollout Status Options ) Validate ( ) error { if len ( o . Builder Args ) == 0 && cmdutil . Is Filename Slice Empty ( o . Filename Options . Filenames , o . Filename } 
func ( o * Rollout Status Options ) Run ( ) error { r := o . Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . Filename Param ( o . Enforce Namespace , o . Filename Options ) . Resource Type Or Name Args ( true , o . Builder Args ... ) . Single Resource mapping := info . Resource status Viewer , err := o . Status Viewer field Selector := fields . One Term Equal lw := & cache . List Watch { List Func : func ( options metav1 . List Options ) ( runtime . Object , error ) { options . Field Selector = field return o . Dynamic } , Watch Func : func ( options metav1 . List Options ) ( watch . Interface , error ) { options . Field Selector = field return o . Dynamic precondition Func := func ( store cache . Store ) ( bool , error ) { _ , exists , err := store . Get ( & metav1 . Object if ! exists { // We need to make sure we see the object in the cache before we start waiting for events // or we would be waiting for the timeout if such object didn't exist. return true , apierrors . New Not Found ( mapping . Resource . Group // if the rollout isn't done yet, keep watching deployment status ctx , cancel := watchtools . Context With Optional return intr . Run ( func ( ) error { _ , err = watchtools . Until With Sync ( ctx , lw , & unstructured . Unstructured { } , precondition Func , func ( e watch . Event ) ( bool , error ) { switch t := e . Type ; t { case watch . Added , watch . Modified : status , done , err := status should if ! should } 
func New Mounter ( root Dir string , ne * nsenter . Nsenter ) * Mounter { return & Mounter { root Dir : root } 
func ( n * Mounter ) Mount ( source string , target string , fstype string , options [ ] string ) error { bind , bind Opts , bind Remount Opts := mount . Is if bind { err := n . do Nsenter Mount ( source , target , fstype , bind return n . do Nsenter Mount ( source , target , fstype , bind Remount return n . do Nsenter } 
func ( n * Mounter ) do Nsenter cmd , args := n . make Nsenter output Bytes , err := n . ne . Exec ( cmd , args ) . Combined if len ( output Bytes ) != 0 { klog . V ( 5 ) . Infof ( " " , source , target , string ( output } 
func ( n * Mounter ) make Nsenter Args ( source , target , fstype string , options [ ] string ) ( string , [ ] string ) { mount Cmd := n . ne . Abs Host mount Args := mount . Make Mount if systemd Run Path , has Systemd := n . ne . Supports Systemd ( ) ; has Systemd { // Complete command line: // nsenter --mount=/rootfs/proc/1/ns/mnt -- /bin/systemd-run --description=... --scope -- /bin/mount -t <type> <what> <where> // Expected flow is: // * nsenter breaks out of container's mount namespace and executes // host's systemd-run. // * systemd-run creates a transient scope (=~ cgroup) and executes its // argument (/bin/mount) there. // * mount does its job, forks a fuse daemon if necessary and finishes. // (systemd-run --scope finishes at this point, returning mount's exit // code and stdout/stderr - thats one of --scope benefits). // * systemd keeps the fuse daemon running in the scope (i.e. in its own // cgroup) until the fuse daemon dies (another --scope benefit). // Kubelet container can be restarted and the fuse daemon survives. // * When the daemon dies (e.g. during unmount) systemd removes the // scope automatically. mount Cmd , mount Args = mount . Add Systemd Scope ( systemd Run Path , target , mount Cmd , mount } else { // Fall back to simple mount when the host has no systemd. // Complete command line: // nsenter --mount=/rootfs/proc/1/ns/mnt -- /bin/mount -t <type> <what> <where> // Expected flow is: // * nsenter breaks out of container's mount namespace and executes host's /bin/mount. // * mount does its job, forks a fuse daemon if necessary and finishes. // * Any fuse daemon runs in cgroup of kubelet docker container, // restart of kubelet container will kill it! // No code here, mount Cmd and mount return mount Cmd , mount } 
output Bytes , err := n . ne . Exec ( " " , args ) . Combined if len ( output Bytes ) != 0 { klog . V ( 5 ) . Infof ( " " , target , string ( output } 
func ( * Mounter ) Is Mount Point Match ( mp mount . Mount Point , dir string ) bool { deleted return ( mp . Path == dir ) || ( mp . Path == deleted } 
func ( n * Mounter ) Is Likely Not Mount // Check the directory exists if _ , err = os . Stat ( file ) ; os . Is Not // Resolve any symlinks in file, kernel would do the same and use the resolved path in /proc/mounts resolved File , err := n . Eval Host // Add --first-only option: since we are testing for the absence of a mountpoint, it is sufficient to get only // the first of multiple possible mountpoints using --first-only. // Also add fstype output to make sure that the output of target file will give the full path // TODO: Need more refactoring for this function. Track the solution with issue #26996 args := [ ] string { " " , " " , " " , " " , " " , resolved out , err := n . ne . Exec ( " " , args ) . Combined if err != nil { klog . V ( 2 ) . Infof ( " " , resolved mount Target , err := parse Find klog . V ( 5 ) . Infof ( " " , resolved File , mount if mount Target == resolved File { klog . V ( 5 ) . Infof ( " " , resolved klog . V ( 5 ) . Infof ( " " , resolved } 
func parse Find Mnt ( out string ) ( string , error ) { // cut trailing newline out = strings . Trim // cut everything after the last space - it's the filesystem type i := strings . Last } 
func ( n * Mounter ) Device Opened ( pathname string ) ( bool , error ) { return mount . Exclusive Open Fails On } 
func ( n * Mounter ) Path Is Device ( pathname string ) ( bool , error ) { path Type , err := n . Get File is Device := path Type == mount . File Type Char Dev || path Type == mount . File Type Block return is } 
func ( n * Mounter ) Get Device Name From Mount ( mount Path , plugin Mount Dir string ) ( string , error ) { return mount . Get Device Name From Mount Linux ( n , mount Path , plugin Mount } 
func ( n * Mounter ) Make R Shared ( path string ) error { return mount . Do Make R Shared ( path , host Proc Mountinfo } 
func ( n * Mounter ) Get File Type ( pathname string ) ( mount . File Type , error ) { var path Type mount . File output Bytes , err := n . ne . Exec ( " " , [ ] string { " " , " " , pathname } ) . Combined if err != nil { if strings . Contains ( string ( output } else { err = fmt . Errorf ( " " , pathname , string ( output return path switch string ( output Bytes ) { case " " : return mount . File Type case " " : return mount . File Type Char case " " : return mount . File Type Block case " " : return mount . File Type case " " : return mount . File Type return path } 
func ( n * Mounter ) Make if _ , err := n . ne . Exec ( " " , args ) . Combined } 
func ( n * Mounter ) Exists Path ( pathname string ) ( bool , error ) { // Resolve the symlinks but allow the target not to exist. Eval Symlinks // would return an generic error when the target does not exist. host Path , err := n . ne . Eval Symlinks ( pathname , false /* must kubeletpath := n . ne . Kubelet Path ( host return utilpath . Exists ( utilpath . Check Follow } 
func ( n * Mounter ) Eval Host Symlinks ( pathname string ) ( string , error ) { return n . ne . Eval } 
func ( n * Mounter ) Get Mount Refs ( pathname string ) ( [ ] string , error ) { path Exists , path Err := mount . Path if ! path Exists || mount . Is Corrupted Mnt ( path } else if path Err != nil { return nil , fmt . Errorf ( " " , pathname , path hostpath , err := n . ne . Eval Symlinks ( pathname , true /* must return mount . Search Mount Points ( hostpath , host Proc Mountinfo } 
func ( n * Mounter ) Get FS Group ( pathname string ) ( int64 , error ) { host Path , err := n . ne . Eval Symlinks ( pathname , true /* must kubeletpath := n . ne . Kubelet Path ( host return mount . Get FS Group } 
func ( n * Mounter ) Get SE Linux Support ( pathname string ) ( bool , error ) { return mount . Get SE Linux ( pathname , host Proc Mounts } 
func ( n * Mounter ) Get Mode ( pathname string ) ( os . File Mode , error ) { host Path , err := n . ne . Eval Symlinks ( pathname , true /* must kubeletpath := n . ne . Kubelet Path ( host return mount . Get Mode } 
func fatal ( msg string , code int ) { if len ( msg ) > 0 { // add newline if needed if ! strings . Has } 
func check Err ( err error , handle case preflight Error : handle Err ( err . Error ( ) , Pre Flight Exit case errorsutil . Aggregate : handle Err ( err . Error ( ) , Validation Exit default : handle Err ( err . Error ( ) , Default Error Exit } 
func Format Err Msg ( errs [ ] error ) string { var err for _ , err := range errs { err Msg = fmt . Sprintf ( " \t \n " , err return err } 
func Start Compactor ( ctx context . Context , client * clientv3 . Client , compact Interval time . Duration ) { endpoints Map defer endpoints Map // In one process, we can have only one compactor for one cluster. // Currently we rely on endpoints to differentiate clusters. for _ , ep := range client . Endpoints ( ) { if _ , ok := endpoints for _ , ep := range client . Endpoints ( ) { endpoints if compact Interval != 0 { go compactor ( ctx , client , compact } 
func compactor ( ctx context . Context , client * clientv3 . Client , interval time . Duration ) { // Technical definitions: // We have a special key in etcd defined as *compact Rev Key*. // compact Rev Key's value will be set to the string of last compacted revision. // compact Rev Key's version will be used as logical time for comparison. T He version is referred as compact time. // Initially, because the key doesn't exist, the compact time (version) is 0. // // Algorithm: // - Compare to see if (local compact_time) = (remote compact_time). // - If yes, increment both local and remote compact_time, and do a compaction. // - If not, set local to remote compact_time. // // Technical details/insights: // // The protocol here is lease based. If one compactor CAS successfully, the others would know it when they fail in // CAS later and would try again in 10 minutes. If an API Server crashed, another one would "take over" the lease. // // For example, in the following diagram, we have a compactor C1 doing compaction in t1, t2. Another compactor C2 // at t1' (t1 < t1' < t2) would CAS fail, set its known old Rev to rev at t1', and try again in t2' (t2' > t2). // If C1 crashed and wouldn't compact at t2, C2 would CAS successfully at t2'. // // old Rev(t2) cur Rev(t2) // + // old Rev cur Rev | // + + | // | | | // | | t1' | t2' // +---v-------------v----^---------v------^----> // t0 t1 t2 // // We have the guarantees: // - in normal cases, the interval is 10 minutes. // - in failover, the interval is >10m and <20m // // FAQ: // - What if time is not accurate? We don't care as long as someone did the compaction. Atomicity is ensured using // etcd API. // - What happened under heavy load scenarios? Initially, each apiserver will do only one compaction // every 10 minutes. This is very unlikely affecting or affected w.r.t. server load. var compact compact Time , rev , err = compact ( ctx , client , compact } 
func compact ( ctx context . Context , client * clientv3 . Client , t , rev int64 ) ( int64 , int64 , error ) { resp , err := client . KV . Txn ( ctx ) . If ( clientv3 . Compare ( clientv3 . Version ( compact Rev Key ) , " " , t ) , ) . Then ( clientv3 . Op Put ( compact Rev Key , strconv . Format Int ( rev , 10 ) ) , // Expect side effect: increment Version ) . Else ( clientv3 . Op Get ( compact Rev cur if ! resp . Succeeded { cur Time := resp . Responses [ 0 ] . Get Response return cur Time , cur cur if rev == 0 { // We don't compact on bootstrap. return cur Time , cur if _ , err = client . Compact ( ctx , rev ) ; err != nil { return cur Time , cur return cur Time , cur } 
func New Admission Options ( ) * Admission Options { options := & Admission Options { Plugins : admission . New Plugins ( ) , Decorators : admission . Decorators { admission . Decorator Func ( admissionmetrics . With Controller Metrics ) } , // This list is mix of mutating admission plugins and validating // admission plugins. The apiserver always runs the validating ones // after all the mutating ones, so their relative order in this list // doesn't matter. Recommended Plugin Order : [ ] string { lifecycle . Plugin Name , mutatingwebhook . Plugin Name , validatingwebhook . Plugin Name } , Default Off Plugins : sets . New server . Register All Admission } 
func ( a * Admission Options ) Add Flags ( fs * pflag . Flag fs . String Slice Var ( & a . Enable Plugins , " " , a . Enable Plugins , " " + " " + strings . Join ( a . default Enabled Plugin fs . String Slice Var ( & a . Disable Plugins , " " , a . Disable Plugins , " " + " " + strings . Join ( a . default Enabled Plugin fs . String Var ( & a . Config File , " " , a . Config } 
func ( a * Admission Options ) Apply To ( c * server . Config , informers informers . Shared Informer Factory , kube API Server Client Config * rest . Config , plugin Initializers ... admission . Plugin // Admission depends on Core API to set Shared Informer Factory and Client plugin Names := a . enabled Plugin plugins Config Provider , err := admission . Read Admission Configuration ( plugin Names , a . Config File , config clientset , err := kubernetes . New For Config ( kube API Server Client generic initializers Chain := admission . Plugin plugin Initializers = append ( plugin Initializers , generic initializers Chain = append ( initializers Chain , plugin admission Chain , err := a . Plugins . New From Plugins ( plugin Names , plugins Config Provider , initializers c . Admission Control = admissionmetrics . With Step Metrics ( admission } 
func ( a * Admission registered Plugins := sets . New for _ , name := range a . Enable Plugins { if ! registered for _ , name := range a . Disable Plugins { if ! registered enable Plugins := sets . New String ( a . Enable disable Plugins := sets . New String ( a . Disable if len ( enable Plugins . Intersection ( disable Plugins ) . List ( ) ) > 0 { errs = append ( errs , fmt . Errorf ( " " + " " , enable Plugins . Intersection ( disable // Verify Recommended Plugin Order. recommend Plugins := sets . New String ( a . Recommended Plugin intersections := registered Plugins . Intersection ( recommend if ! intersections . Equal ( recommend Plugins ) { // Developer error, this should never run in. errs = append ( errs , fmt . Errorf ( " " , recommend if ! intersections . Equal ( registered Plugins ) { // Developer error, this should never run in. errs = append ( errs , fmt . Errorf ( " " , registered } 
func ( a * Admission Options ) enabled Plugin Names ( ) [ ] string { all Off Plugins := append ( a . Default Off Plugins . List ( ) , a . Disable disabled Plugins := sets . New String ( all Off enabled Plugins := sets . New String ( a . Enable disabled Plugins = disabled Plugins . Difference ( enabled ordered for _ , plugin := range a . Recommended Plugin Order { if ! disabled Plugins . Has ( plugin ) { ordered Plugins = append ( ordered return ordered } 
func ( a * Admission Options ) default Enabled Plugin Names ( ) [ ] string { default On Plugin for _ , plugin Name := range a . Recommended Plugin Order { if ! a . Default Off Plugins . Has ( plugin Name ) { default On Plugin Names = append ( default On Plugin Names , plugin return default On Plugin } 
func New API Server Command ( stop Ch <- chan struct { } ) * cobra . Command { s := options . New Server Run cluster's shared state through which all other components interact.` , Run E : func ( cmd * cobra . Command , args [ ] string ) error { verflag . Print And Exit If utilflag . Print // set default options completed // validate options if errs := completed Options . Validate ( ) ; len ( errs ) != 0 { return utilerrors . New return Run ( completed Options , stop named Flag verflag . Add Flags ( named Flag Sets . Flag globalflag . Add Global Flags ( named Flag Sets . Flag options . Add Custom Global Flags ( named Flag Sets . Flag for _ , f := range named Flag Sets . Flag Sets { fs . Add Flag usage cols , _ , _ := term . Terminal Size ( cmd . Out Or cmd . Set Usage Func ( func ( cmd * cobra . Command ) error { fmt . Fprintf ( cmd . Out Or Stderr ( ) , usage Fmt , cmd . Use cliflag . Print Sections ( cmd . Out Or Stderr ( ) , named Flag cmd . Set Help Func ( func ( cmd * cobra . Command , args [ ] string ) { fmt . Fprintf ( cmd . Out Or Stdout ( ) , " \n \n " + usage Fmt , cmd . Long , cmd . Use cliflag . Print Sections ( cmd . Out Or Stdout ( ) , named Flag } 
func Run ( complete Options completed Server Run Options , stop server , err := Create Server Chain ( complete Options , stop return server . Prepare Run ( ) . Run ( stop } 
func Create Server Chain ( completed Options completed Server Run Options , stop Ch <- chan struct { } ) ( * genericapiserver . Generic API Server , error ) { node Tunneler , proxy Transport , err := Create Node Dialer ( completed kube API Server Config , insecure Serving Info , service Resolver , plugin Initializer , admission Post Start Hook , err := Create Kube API Server Config ( completed Options , node Tunneler , proxy // If additional API servers are added, they should be gated. api Extensions Config , err := create API Extensions Config ( * kube API Server Config . Generic Config , kube API Server Config . Extra Config . Versioned Informers , plugin Initializer , completed Options . Server Run Options , completed Options . Master Count , service Resolver , webhook . New Default Authentication Info Resolver Wrapper ( proxy Transport , kube API Server Config . Generic Config . Loopback Client api Extensions Server , err := create API Extensions Server ( api Extensions Config , genericapiserver . New Empty kube API Server , err := Create Kube API Server ( kube API Server Config , api Extensions Server . Generic API Server , admission Post Start // otherwise go down the normal path of standing the aggregator up in front of the API server // this wires up openapi kube API Server . Generic API Server . Prepare // This will wire up openapi for extension api server api Extensions Server . Generic API Server . Prepare // aggregator comes last in the chain aggregator Config , err := create Aggregator Config ( * kube API Server Config . Generic Config , completed Options . Server Run Options , kube API Server Config . Extra Config . Versioned Informers , service Resolver , proxy Transport , plugin aggregator Server , err := create Aggregator Server ( aggregator Config , kube API Server . Generic API Server , api Extensions if err != nil { // we don't need special handling for inner Stop if insecure Serving Info != nil { insecure Handler Chain := kubeserver . Build Insecure Handler Chain ( aggregator Server . Generic API Server . Unprotected Handler ( ) , kube API Server Config . Generic if err := insecure Serving Info . Serve ( insecure Handler Chain , kube API Server Config . Generic Config . Request Timeout , stop return aggregator Server . Generic API } 
func Create Kube API Server ( kube API Server Config * master . Config , delegate API Server genericapiserver . Delegation Target , admission Post Start Hook genericapiserver . Post Start Hook Func ) ( * master . Master , error ) { kube API Server , err := kube API Server Config . Complete ( ) . New ( delegate API kube API Server . Generic API Server . Add Post Start Hook Or Die ( " " , admission Post Start return kube API } 
func Create Node Dialer ( s completed Server Run Options ) ( tunneler . Tunneler , * http . Transport , error ) { // Setup node Tunneler if needed var node var proxy Dialer Fn utilnet . Dial if len ( s . SSH User ) > 0 { // Get ssh key distribution func, if supported var install SSH Key tunneler . Install SSH cloud , err := cloudprovider . Init Cloud Provider ( s . Cloud Provider . Cloud Provider , s . Cloud Provider . Cloud Config if cloud != nil { if instances , supported := cloud . Instances ( ) ; supported { install SSH Key = instances . Add SSH Key To All if s . Kubelet if s . Kubelet Config . Read Only // Set up the node Tunneler // TODO(cjcullen): If we want this to handle per-kubelet ports or other // kubelet listen-addresses, we need to plumb through options. health Check Path := & url . URL { Scheme : " " , Host : net . Join Host Port ( " " , strconv . Format Uint ( uint64 ( s . Kubelet Config . Read Only node Tunneler = tunneler . New ( s . SSH User , s . SSH Keyfile , health Check Path , install SSH // Use the node Tunneler's dialer when proxying to pods, services, and nodes proxy Dialer Fn = node // Proxying to pods and services is IP-based... don't expect to be able to verify the hostname proxy TLS Client Config := & tls . Config { Insecure Skip proxy Transport := utilnet . Set Transport Defaults ( & http . Transport { Dial Context : proxy Dialer Fn , TLS Client Config : proxy TLS Client return node Tunneler , proxy } 
func Create Kube API Server Config ( s completed Server Run Options , node Tunneler tunneler . Tunneler , proxy Transport * http . Transport , ) ( config * master . Config , insecure Serving Info * genericapiserver . Deprecated Insecure Serving Info , service Resolver aggregatorapiserver . Service Resolver , plugin Initializers [ ] admission . Plugin Initializer , admission Post Start Hook genericapiserver . Post Start Hook Func , last Err error , ) { var generic var storage Factory * serverstorage . Default Storage var versioned Informers clientgoinformers . Shared Informer generic Config , versioned Informers , insecure Serving Info , service Resolver , plugin Initializers , admission Post Start Hook , storage Factory , last Err = build Generic Config ( s . Server Run Options , proxy if last if _ , port , err := net . Split Host Port ( s . Etcd . Storage Config . Transport . Server List [ 0 ] ) ; err == nil && port != " " && len ( port ) != 0 { if err := utilwait . Poll Immediate ( etcd Retry Interval , etcd Retry Limit * etcd Retry Interval , preflight . Etcd Connection { Server List : s . Etcd . Storage Config . Transport . Server List } . Check Etcd Servers ) ; err != nil { last capabilities . Initialize ( capabilities . Capabilities { Allow Privileged : s . Allow Privileged , // TODO(vmarmol): Implement support for Host Network Sources. Privileged Sources : capabilities . Privileged Sources { Host Network Sources : [ ] string { } , Host PID Sources : [ ] string { } , Host IPC Sources : [ ] string { } , } , Per Connection Bandwidth Limit Bytes Per Sec : s . Max Connection Bytes Per service IP Range , api Server Service IP , last Err := master . Default Service IP Range ( s . Service Cluster IP if last client CA , last Err := read C Aor Nil ( s . Authentication . Client Cert . Client if last request Header Proxy CA , last Err := read C Aor Nil ( s . Authentication . Request Header . Client CA if last config = & master . Config { Generic Config : generic Config , Extra Config : master . Extra Config { Client CA Registration Hook : master . Client CA Registration Hook { Client CA : client CA , Request Header Username Headers : s . Authentication . Request Header . Username Headers , Request Header Group Headers : s . Authentication . Request Header . Group Headers , Request Header Extra Header Prefixes : s . Authentication . Request Header . Extra Header Prefixes , Request Header CA : request Header Proxy CA , Request Header Allowed Names : s . Authentication . Request Header . Allowed Names , } , API Resource Config Source : storage Factory . API Resource Config Source , Storage Factory : storage Factory , Event TTL : s . Event TTL , Kubelet Client Config : s . Kubelet Config , Enable Logs Support : s . Enable Logs Handler , Proxy Transport : proxy Transport , Tunneler : node Tunneler , Service IP Range : service IP Range , API Server Service IP : api Server Service IP , API Server Service Port : 443 , Service Node Port Range : s . Service Node Port Range , Kubernetes Service Node Port : s . Kubernetes Service Node Port , Endpoint Reconciler Type : reconcilers . Type ( s . Endpoint Reconciler Type ) , Master Count : s . Master Count , Service Account Issuer : s . Service Account Issuer , Service Account Max Expiration : s . Service Account Token Max Expiration , Versioned Informers : versioned if node Tunneler != nil { // Use the node Tunneler's dialer to connect to the kubelet config . Extra Config . Kubelet Client Config . Dial = node } 
func build Generic Config ( s * options . Server Run Options , proxy Transport * http . Transport , ) ( generic Config * genericapiserver . Config , versioned Informers clientgoinformers . Shared Informer Factory , insecure Serving Info * genericapiserver . Deprecated Insecure Serving Info , service Resolver aggregatorapiserver . Service Resolver , plugin Initializers [ ] admission . Plugin Initializer , admission Post Start Hook genericapiserver . Post Start Hook Func , storage Factory * serverstorage . Default Storage Factory , last Err error , ) { generic Config = genericapiserver . New generic Config . Merged Resource Config = master . Default API Resource Config if last Err = s . Generic Server Run Options . Apply To ( generic Config ) ; last if last Err = s . Insecure Serving . Apply To ( & insecure Serving Info , & generic Config . Loopback Client Config ) ; last if last Err = s . Secure Serving . Apply To ( & generic Config . Secure Serving , & generic Config . Loopback Client Config ) ; last if last Err = s . Authentication . Apply To ( generic Config ) ; last if last Err = s . Features . Apply To ( generic Config ) ; last if last Err = s . API Enablement . Apply To ( generic Config , master . Default API Resource Config Source ( ) , legacyscheme . Scheme ) ; last generic Config . Open API Config = genericapiserver . Default Open API Config ( generatedopenapi . Get Open API Definitions , openapinamer . New Definition generic Config . Open API generic Config . Long Running Func = filters . Basic Long Running Request Check ( sets . New String ( " " , " " ) , sets . New kube generic Config . Version = & kube storage Factory Config := kubeapiserver . New Storage Factory storage Factory Config . Api Resource Config = generic Config . Merged Resource completed Storage Factory Config , err := storage Factory if err != nil { last storage Factory , last Err = completed Storage Factory if last if last Err = s . Etcd . Apply With Storage Factory To ( storage Factory , generic Config ) ; last // Use protobufs for self-communication. // Since not every generic apiserver has to support protobufs, we // cannot default to it in generic apiserver and need to explicitly // set it in kube-apiserver. generic Config . Loopback Client Config . Content Config . Content kube Client Config := generic Config . Loopback Client clientgo External Client , err := clientgoclientset . New For Config ( kube Client if err != nil { last versioned Informers = clientgoinformers . New Shared Informer Factory ( clientgo External generic Config . Authentication . Authenticator , generic Config . Open API Config . Security Definitions , err = Build Authenticator ( s , clientgo External Client , versioned if err != nil { last generic Config . Authorization . Authorizer , generic Config . Rule Resolver , err = Build Authorizer ( s , versioned if err != nil { last if ! sets . New String ( s . Authorization . Modes ... ) . Has ( modes . Mode RBAC ) { generic Config . Disabled Post Start Hooks . Insert ( rbacrest . Post Start Hook admission Config := & kubeapiserveradmission . Config { External Informers : versioned Informers , Loopback Client Config : generic Config . Loopback Client Config , Cloud Config File : s . Cloud Provider . Cloud Config service Resolver = build Service Resolver ( s . Enable Aggregator Routing , generic Config . Loopback Client Config . Host , versioned auth Info Resolver Wrapper := webhook . New Default Authentication Info Resolver Wrapper ( proxy Transport , generic Config . Loopback Client last Err = s . Audit . Apply To ( generic Config , generic Config . Loopback Client Config , versioned Informers , serveroptions . New Process Info ( " " , " " ) , & serveroptions . Webhook Options { Auth Info Resolver Wrapper : auth Info Resolver Wrapper , Service Resolver : service if last plugin Initializers , admission Post Start Hook , err = admission Config . New ( proxy Transport , service if err != nil { last err = s . Admission . Apply To ( generic Config , versioned Informers , kube Client Config , plugin if err != nil { last } 
func Build Authenticator ( s * options . Server Run Options , extclient clientgoclientset . Interface , versioned Informer clientgoinformers . Shared Informer Factory ) ( authenticator . Request , * spec . Security Definitions , error ) { authenticator Config := s . Authentication . To Authentication if s . Authentication . Service Accounts . Lookup { authenticator Config . Service Account Token Getter = serviceaccountcontroller . New Getter From Client ( extclient , versioned Informer . Core ( ) . V1 ( ) . Secrets ( ) . Lister ( ) , versioned Informer . Core ( ) . V1 ( ) . Service Accounts ( ) . Lister ( ) , versioned authenticator Config . Bootstrap Token Authenticator = bootstrap . New Token Authenticator ( versioned Informer . Core ( ) . V1 ( ) . Secrets ( ) . Lister ( ) . Secrets ( v1 . Namespace return authenticator } 
func Build Authorizer ( s * options . Server Run Options , versioned Informers clientgoinformers . Shared Informer Factory ) ( authorizer . Authorizer , authorizer . Rule Resolver , error ) { authorization Config := s . Authorization . To Authorization Config ( versioned return authorization } 
func Complete ( s * options . Server Run Options ) ( completed Server Run Options , error ) { var options completed Server Run // set defaults if err := s . Generic Server Run Options . Default Advertise Address ( s . Secure Serving . Secure Serving if err := kubeoptions . Default Advertise Address ( s . Generic Server Run Options , s . Insecure Serving . Deprecated Insecure Serving service IP Range , api Server Service IP , err := master . Default Service IP Range ( s . Service Cluster IP s . Service Cluster IP Range = service IP if err := s . Secure Serving . Maybe Default With Self Signed Certs ( s . Generic Server Run Options . Advertise Address . String ( ) , [ ] string { " " , " " , " " } , [ ] net . IP { api Server Service if len ( s . Generic Server Run Options . External Host ) == 0 { if len ( s . Generic Server Run Options . Advertise Address ) > 0 { s . Generic Server Run Options . External Host = s . Generic Server Run Options . Advertise } else { if hostname , err := os . Hostname ( ) ; err == nil { s . Generic Server Run Options . External klog . Infof ( " " , s . Generic Server Run Options . External s . Authentication . Apply // Use (Service Account Signing Key File != "") as a proxy to the user enabling // Token Request functionality. This defaulting was convenient, but messed up // a lot of people when they rotated their serving cert with no idea it was // connected to their service account keys. We are taking this oppurtunity to // remove this problematic defaulting. if s . Service Account Signing Key File == " " { // Default to the private server key for service account token signing if len ( s . Authentication . Service Accounts . Key Files ) == 0 && s . Secure Serving . Server Cert . Cert Key . Key File != " " { if kubeauthenticator . Is Valid Service Account Key File ( s . Secure Serving . Server Cert . Cert Key . Key File ) { s . Authentication . Service Accounts . Key Files = [ ] string { s . Secure Serving . Server Cert . Cert Key . Key if s . Service Account Signing Key File != " " && s . Authentication . Service Accounts . Issuer != " " { sk , err := keyutil . Private Key From File ( s . Service Account Signing Key if s . Authentication . Service Accounts . Max Expiration != 0 { low up if s . Authentication . Service Accounts . Max Expiration < low Bound || s . Authentication . Service Accounts . Max Expiration > up s . Service Account Issuer , err = serviceaccount . JWT Token Generator ( s . Authentication . Service s . Service Account Token Max Expiration = s . Authentication . Service Accounts . Max if s . Etcd . Enable Watch Cache { klog . V ( 2 ) . Infof ( " " , s . Generic Server Run Options . Target sizes := cachesize . New Heuristic Watch Cache Sizes ( s . Generic Server Run Options . Target if user Specified , err := serveroptions . Parse Watch Cache Sizes ( s . Etcd . Watch Cache Sizes ) ; err == nil { for resource , size := range user s . Etcd . Watch Cache Sizes , err = serveroptions . Write Watch Cache // TODO: remove when we stop supporting the legacy group version. if s . API Enablement . Runtime Config != nil { for key , value := range s . API Enablement . Runtime Config { if key == " " || strings . Has Prefix ( key , " " ) || key == " " || strings . Has Prefix ( key , " " ) { delete ( s . API Enablement . Runtime s . API Enablement . Runtime if key == " " { delete ( s . API Enablement . Runtime options . Server Run } 
func ( p Listed Path Providers ) Listed for _ , provider := range p { for _ , path := range provider . Listed } 
func ( i Index ) Install ( path Provider Listed Path Provider , mux * mux . Path Recorder Mux ) { handler := Index Lister { Status Code : http . Status OK , Path Provider : path mux . Unlisted mux . Unlisted } 
func ( i Index Lister ) Serve HTTP ( w http . Response Writer , r * http . Request ) { responsewriters . Write Raw JSON ( i . Status Code , metav1 . Root Paths { Paths : i . Path Provider . Listed } 
func ( f Formatter ) Indent ( indent int ) * Formatter { f . Indent Level = f . Indent } 
func ( f * Formatter ) Write ( str string , a ... interface { } ) error { // Don't indent empty lines if str == " " { _ , err := io . Write for i := 0 ; i < f . Indent _ , err := io . Write } 
func ( f * Formatter ) Write strs := wrap String ( text , f . Wrap - f . Indent } 
func ( l * line ) Add ( word string ) bool { new if new Line . Len ( ) <= l . wrap || len ( l . words ) == 0 { l . words = new } 
} 
func ( a * Authenticator ) Update Transport Config ( c * transport . Config ) error { c . Wrap ( func ( rt http . Round Tripper ) http . Round Tripper { return & round if c . TLS . Get c . TLS . Get } else { dial = ( & net . Dialer { Timeout : 30 * time . Second , Keep Alive : 30 * time . Second } ) . Dial d := connrotation . New a . on Rotate = d . Close c . Dial = d . Dial } 
func ( a * Authenticator ) maybe Refresh // Since we're not making a new pointer to a.cached Creds in get Creds, no // need to do deep comparison. if creds != a . cached return a . refresh Creds } 
func ( a * Authenticator ) refresh Creds Locked ( r * clientauthentication . Response ) error { cred := & clientauthentication . Exec Credential { Spec : clientauthentication . Exec Credential if a . group == v1alpha1 . Scheme Group Version { // Input spec disabled for beta due to lack of use. Possibly re-enable this later if // someone wants it back. // // See: https://github.com/kubernetes/kubernetes/issues/61796 data , err := runtime . Encode ( codecs . Legacy env = append ( env , fmt . Sprintf ( " " , exec Info _ , gvk , err := codecs . Universal if gvk . Group != a . group . Group || gvk . Version != a . group . Version { return fmt . Errorf ( " " , a . group , schema . Group if cred . Status . Token == " " && cred . Status . Client Certificate Data == " " && cred . Status . Client Key if ( cred . Status . Client Certificate Data == " " ) != ( cred . Status . Client Key if cred . Status . Expiration Timestamp != nil { a . exp = cred . Status . Expiration new if cred . Status . Client Key Data != " " && cred . Status . Client Certificate Data != " " { cert , err := tls . X509Key Pair ( [ ] byte ( cred . Status . Client Certificate Data ) , [ ] byte ( cred . Status . Client Key new old Creds := a . cached a . cached Creds = new // Only close all connections when TLS cert rotates. Token rotation doesn't // need the extra noise. if a . on Rotate != nil && old Creds != nil && ! reflect . Deep Equal ( old Creds . cert , a . cached Creds . cert ) { a . on } 
func ( c * runtime Classes ) Create ( runtime Class * v1alpha1 . Runtime Class ) ( result * v1alpha1 . Runtime Class , err error ) { result = & v1alpha1 . Runtime err = c . client . Post ( ) . Resource ( " " ) . Body ( runtime } 
func ( s * Generic API Server ) Add Post Start Hook ( name string , hook Post Start Hook if s . disabled Post Start s . post Start Hook defer s . post Start Hook if s . post Start Hooks if post Start Hook , exists := s . post Start Hooks [ name ] ; exists { // this is programmer error, but it can be hard to debug return fmt . Errorf ( " " , name , post Start Hook . originating if err := s . Add Healthz Checks ( post Start Hook s . post Start Hooks [ name ] = post Start Hook Entry { hook : hook , originating } 
func ( s * Generic API Server ) Add Post Start Hook Or Die ( name string , hook Post Start Hook Func ) { if err := s . Add Post Start } 
func ( s * Generic API Server ) Add Pre Shutdown Hook ( name string , hook Pre Shutdown Hook s . pre Shutdown Hook defer s . pre Shutdown Hook if s . pre Shutdown Hooks if _ , exists := s . pre Shutdown s . pre Shutdown Hooks [ name ] = pre Shutdown Hook } 
func ( s * Generic API Server ) Add Pre Shutdown Hook Or Die ( name string , hook Pre Shutdown Hook Func ) { if err := s . Add Pre Shutdown } 
func ( s * Generic API Server ) Run Post Start Hooks ( stop Ch <- chan struct { } ) { s . post Start Hook defer s . post Start Hook s . post Start Hooks context := Post Start Hook Context { Loopback Client Config : s . Loopback Client Config , Stop Ch : stop for hook Name , hook Entry := range s . post Start Hooks { go run Post Start Hook ( hook Name , hook } 
func ( s * Generic API Server ) Run Pre Shutdown Hooks ( ) error { var error s . pre Shutdown Hook defer s . pre Shutdown Hook s . pre Shutdown Hooks for hook Name , hook Entry := range s . pre Shutdown Hooks { if err := run Pre Shutdown Hook ( hook Name , hook Entry ) ; err != nil { error List = append ( error return utilerrors . New Aggregate ( error } 
func ( s * Generic API Server ) is Post Start Hook Registered ( name string ) bool { s . post Start Hook defer s . post Start Hook _ , exists := s . post Start } 
func ( w * watch Cache ) Add ( obj interface { } ) error { object , resource Version , err := w . object To Versioned Runtime f := func ( elem * store return w . process Event ( event , resource } 
func ( w * watch Cache ) process Event ( event watch . Event , resource Version uint64 , update Func func ( * store Element ) error ) error { key , err := w . key elem := & store elem . Labels , elem . Fields , err = w . get Attrs watch Cache Event := & watch Cache Event { Type : event . Type , Object : elem . Object , Obj Labels : elem . Labels , Obj Fields : elem . Fields , Key : key , Resource Version : resource if err := func ( ) error { // TODO: We should consider moving this lock below after the watch Cache if exists { previous Elem := previous . ( * store watch Cache Event . Prev Object = previous watch Cache Event . Prev Obj Labels = previous watch Cache Event . Prev Obj Fields = previous w . update Cache ( watch Cache w . resource Version = resource return update // Avoid calling event handler under lock. // This is safe as long as there is at most one call to process Event in flight // at any point in time. if w . event Handler != nil { w . event Handler ( watch Cache } 
func ( w * watch Cache ) update Cache ( event * watch Cache Event ) { if w . end Index == w . start Index + w . capacity { // Cache is full - remove the oldest element. w . start w . cache [ w . end w . end } 
func ( w * watch Cache ) wait Until Fresh And Block ( resource Version uint64 , trace * utiltrace . Trace ) error { start go func ( ) { // Wake us up when the time limit has expired. The docs // promise that time.After (well, New Timer, which it calls) // will wait *at least* the duration given. Since this go // routine starts sometime after we record the start time, and // it will wake up the loop below sometime after the broadcast, // we don't need to worry about waking it up before the time // has expired accidentally. <- w . clock . After ( block w . R for w . resource Version < resource Version { if w . clock . Since ( start Time ) >= block Timeout { // Timeout with retry after 1 second. return errors . New Timeout Error ( fmt . Sprintf ( " " , resource Version , w . resource } 
func ( w * watch Cache ) Wait Until Fresh And List ( resource Version uint64 , trace * utiltrace . Trace ) ( [ ] interface { } , uint64 , error ) { err := w . wait Until Fresh And Block ( resource defer w . R return w . store . List ( ) , w . resource } 
func ( w * watch Cache ) Wait Until Fresh And Get ( resource Version uint64 , key string , trace * utiltrace . Trace ) ( interface { } , bool , uint64 , error ) { err := w . wait Until Fresh And Block ( resource defer w . R value , exists , err := w . store . Get By return value , exists , w . resource } 
func ( w * watch key , err := w . key return w . store . Get ( & store } 
func ( w * watch Cache ) Get By Key ( key string ) ( interface { } , bool , error ) { return w . store . Get By } 
func ( w * watch Cache ) Replace ( objs [ ] interface { } , resource Version string ) error { version , err := w . versioner . Parse Resource Version ( resource to key , err := w . key obj Labels , obj Fields , err := w . get Attrs to Replace = append ( to Replace , & store Element { Key : key , Object : object , Labels : obj Labels , Fields : obj w . start w . end if err := w . store . Replace ( to Replace , resource w . list Resource w . resource if w . on Replace != nil { w . on klog . V ( 3 ) . Infof ( " " , resource } 
func ( c * Autoscaling V1Client ) REST return c . rest } 
func ( in * Persistent Volume Binder Controller Configuration ) Deep Copy Into ( out * Persistent Volume Binder Controller out . PV Claim Binder Sync Period = in . PV Claim Binder Sync out . Volume Configuration = in . Volume } 
func ( in * Persistent Volume Binder Controller Configuration ) Deep Copy ( ) * Persistent Volume Binder Controller out := new ( Persistent Volume Binder Controller in . Deep Copy } 
func ( in * Persistent Volume Recycler Configuration ) Deep Copy ( ) * Persistent Volume Recycler out := new ( Persistent Volume Recycler in . Deep Copy } 
func ( in * Volume Configuration ) Deep Copy Into ( out * Volume out . Persistent Volume Recycler Configuration = in . Persistent Volume Recycler } 
func ( in * Volume Configuration ) Deep Copy ( ) * Volume out := new ( Volume in . Deep Copy } 
func ( c * Fake Leases ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( leases } 
func ( c * Fake Leases ) Create ( lease * v1beta1 . Lease ) ( result * v1beta1 . Lease , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( leases } 
func ( c * Fake Leases ) Update ( lease * v1beta1 . Lease ) ( result * v1beta1 . Lease , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( leases } 
func ( c * Fake Leases ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( leases } 
func ( c * Fake Leases ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( leases Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Lease } 
func ( c * Fake Leases ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Lease , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( leases } 
func ( t * Toleration ) Match Toleration ( toleration To Match * Toleration ) bool { return t . Key == toleration To Match . Key && t . Effect == toleration To Match . Effect && t . Operator == toleration To Match . Operator && t . Value == toleration To } 
func ( t * Transport ) Round Trip ( req * http . Request ) ( * http . Response , error ) { // Add reverse proxy headers. forwarded URI := path . Join ( t . Path if strings . Has Suffix ( req . URL . Path , " " ) { forwarded URI = forwarded req . Header . Set ( " " , forwarded rt := t . Round if rt == nil { rt = http . Default resp , err := rt . Round resp = & http . Response { Status Code : http . Status Service Unavailable , Body : ioutil . Nop Closer ( strings . New if redirect := resp . Header . Get ( " " ) ; redirect != " " { resp . Header . Set ( " " , t . rewrite c c Type = strings . Trim Space ( strings . Split N ( c if c return t . rewrite } 
func rewrite HTML ( reader io . Reader , writer io . Writer , url Rewriter func ( string ) string ) error { // Note: This assumes the content is UTF-8. tokenizer := html . New for err == nil { token switch token Type { case html . Error case html . Start Tag Token , html . Self Closing Tag if url Attrs , ok := atoms To Attrs [ token . Data Atom ] ; ok { for i , attr := range token . Attr { if url Attrs . Has ( attr . Key ) { token . Attr [ i ] . Val = url } 
func ( cp * CPU Manager Checkpoint ) Marshal } 
func ( cp * CPU Manager Checkpoint ) Unmarshal } 
func ( cp * CPU Manager Checkpoint ) Verify } 
func New For Config ( c * rest . Config ) ( * Clientset , error ) { config Shallow if config Shallow Copy . Rate Limiter == nil && config Shallow Copy . QPS > 0 { config Shallow Copy . Rate Limiter = flowcontrol . New Token Bucket Rate Limiter ( config Shallow Copy . QPS , config Shallow cs . admissionregistration V1beta1 , err = admissionregistrationv1beta1 . New For Config ( & config Shallow cs . apps V1 , err = appsv1 . New For Config ( & config Shallow cs . apps V1beta1 , err = appsv1beta1 . New For Config ( & config Shallow cs . apps V1beta2 , err = appsv1beta2 . New For Config ( & config Shallow cs . auditregistration V1alpha1 , err = auditregistrationv1alpha1 . New For Config ( & config Shallow cs . authentication V1 , err = authenticationv1 . New For Config ( & config Shallow cs . authentication V1beta1 , err = authenticationv1beta1 . New For Config ( & config Shallow cs . authorization V1 , err = authorizationv1 . New For Config ( & config Shallow cs . authorization V1beta1 , err = authorizationv1beta1 . New For Config ( & config Shallow cs . autoscaling V1 , err = autoscalingv1 . New For Config ( & config Shallow cs . autoscaling V2beta1 , err = autoscalingv2beta1 . New For Config ( & config Shallow cs . autoscaling V2beta2 , err = autoscalingv2beta2 . New For Config ( & config Shallow cs . batch V1 , err = batchv1 . New For Config ( & config Shallow cs . batch V1beta1 , err = batchv1beta1 . New For Config ( & config Shallow cs . batch V2alpha1 , err = batchv2alpha1 . New For Config ( & config Shallow cs . certificates V1beta1 , err = certificatesv1beta1 . New For Config ( & config Shallow cs . coordination V1beta1 , err = coordinationv1beta1 . New For Config ( & config Shallow cs . coordination V1 , err = coordinationv1 . New For Config ( & config Shallow cs . core V1 , err = corev1 . New For Config ( & config Shallow cs . events V1beta1 , err = eventsv1beta1 . New For Config ( & config Shallow cs . extensions V1beta1 , err = extensionsv1beta1 . New For Config ( & config Shallow cs . networking V1 , err = networkingv1 . New For Config ( & config Shallow cs . networking V1beta1 , err = networkingv1beta1 . New For Config ( & config Shallow cs . node V1alpha1 , err = nodev1alpha1 . New For Config ( & config Shallow cs . node V1beta1 , err = nodev1beta1 . New For Config ( & config Shallow cs . policy V1beta1 , err = policyv1beta1 . New For Config ( & config Shallow cs . rbac V1 , err = rbacv1 . New For Config ( & config Shallow cs . rbac V1beta1 , err = rbacv1beta1 . New For Config ( & config Shallow cs . rbac V1alpha1 , err = rbacv1alpha1 . New For Config ( & config Shallow cs . scheduling V1alpha1 , err = schedulingv1alpha1 . New For Config ( & config Shallow cs . scheduling V1beta1 , err = schedulingv1beta1 . New For Config ( & config Shallow cs . scheduling V1 , err = schedulingv1 . New For Config ( & config Shallow cs . settings V1alpha1 , err = settingsv1alpha1 . New For Config ( & config Shallow cs . storage V1beta1 , err = storagev1beta1 . New For Config ( & config Shallow cs . storage V1 , err = storagev1 . New For Config ( & config Shallow cs . storage V1alpha1 , err = storagev1alpha1 . New For Config ( & config Shallow cs . Discovery Client , err = discovery . New Discovery Client For Config ( & config Shallow } 
func New For Config Or cs . admissionregistration V1beta1 = admissionregistrationv1beta1 . New For Config Or cs . apps V1 = appsv1 . New For Config Or cs . apps V1beta1 = appsv1beta1 . New For Config Or cs . apps V1beta2 = appsv1beta2 . New For Config Or cs . auditregistration V1alpha1 = auditregistrationv1alpha1 . New For Config Or cs . authentication V1 = authenticationv1 . New For Config Or cs . authentication V1beta1 = authenticationv1beta1 . New For Config Or cs . authorization V1 = authorizationv1 . New For Config Or cs . authorization V1beta1 = authorizationv1beta1 . New For Config Or cs . autoscaling V1 = autoscalingv1 . New For Config Or cs . autoscaling V2beta1 = autoscalingv2beta1 . New For Config Or cs . autoscaling V2beta2 = autoscalingv2beta2 . New For Config Or cs . batch V1 = batchv1 . New For Config Or cs . batch V1beta1 = batchv1beta1 . New For Config Or cs . batch V2alpha1 = batchv2alpha1 . New For Config Or cs . certificates V1beta1 = certificatesv1beta1 . New For Config Or cs . coordination V1beta1 = coordinationv1beta1 . New For Config Or cs . coordination V1 = coordinationv1 . New For Config Or cs . core V1 = corev1 . New For Config Or cs . events V1beta1 = eventsv1beta1 . New For Config Or cs . extensions V1beta1 = extensionsv1beta1 . New For Config Or cs . networking V1 = networkingv1 . New For Config Or cs . networking V1beta1 = networkingv1beta1 . New For Config Or cs . node V1alpha1 = nodev1alpha1 . New For Config Or cs . node V1beta1 = nodev1beta1 . New For Config Or cs . policy V1beta1 = policyv1beta1 . New For Config Or cs . rbac V1 = rbacv1 . New For Config Or cs . rbac V1beta1 = rbacv1beta1 . New For Config Or cs . rbac V1alpha1 = rbacv1alpha1 . New For Config Or cs . scheduling V1alpha1 = schedulingv1alpha1 . New For Config Or cs . scheduling V1beta1 = schedulingv1beta1 . New For Config Or cs . scheduling V1 = schedulingv1 . New For Config Or cs . settings V1alpha1 = settingsv1alpha1 . New For Config Or cs . storage V1beta1 = storagev1beta1 . New For Config Or cs . storage V1 = storagev1 . New For Config Or cs . storage V1alpha1 = storagev1alpha1 . New For Config Or cs . Discovery Client = discovery . New Discovery Client For Config Or } 
cs . admissionregistration cs . apps cs . apps cs . apps cs . auditregistration cs . authentication cs . authentication cs . authorization cs . authorization cs . autoscaling cs . autoscaling cs . autoscaling cs . batch cs . batch cs . batch cs . certificates cs . coordination cs . coordination cs . core cs . events cs . extensions cs . networking cs . networking cs . node cs . node cs . policy cs . rbac cs . rbac cs . rbac cs . scheduling cs . scheduling cs . scheduling cs . settings cs . storage cs . storage cs . storage cs . Discovery Client = discovery . New Discovery } 
func New Node Ipam Controller ( node Informer coreinformers . Node Informer , cloud cloudprovider . Interface , kube Client clientset . Interface , cluster CIDR * net . IP Net , service CIDR * net . IP Net , node CIDR Mask Size int , allocator Type ipam . CIDR Allocator Type ) ( * Controller , error ) { if kube event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : kube Client . Core if kube Client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { metrics . Register Metric And Track Rate Limiter Usage ( " " , kube Client . Core V1 ( ) . REST Client ( ) . Get Rate if allocator Type != ipam . Cloud Allocator Type { // Cloud CIDR allocator does not rely on cluster CIDR or node CIDR Mask Size for allocation. if cluster if mask Size , _ := cluster CIDR . Mask . Size ( ) ; mask Size > node CIDR Mask ic := & Controller { cloud : cloud , kube Client : kube Client , lookup IP : net . Lookup IP , cluster CIDR : cluster CIDR , service CIDR : service CIDR , allocator Type : allocator // TODO: Abstract this check into a generic controller manager should run method. if ic . allocator Type == ipam . IPAM From Cluster Allocator Type || ic . allocator Type == ipam . IPAM From Cloud Allocator Type { cfg := & ipam . Config { Resync : ipam Resync Interval , Max Backoff : ipam Max Backoff , Initial Retry : ipam Initial switch ic . allocator Type { case ipam . IPAM From Cluster Allocator Type : cfg . Mode = nodesync . Sync From case ipam . IPAM From Cloud Allocator Type : cfg . Mode = nodesync . Sync From ipamc , err := ipam . New Controller ( cfg , kube Client , cloud , cluster CIDR , service CIDR , node CIDR Mask if err := ipamc . Start ( node ic . cidr Allocator , err = ipam . New ( kube Client , cloud , node Informer , ic . allocator Type , ic . cluster CIDR , ic . service CIDR , node CIDR Mask ic . node Lister = node ic . node Informer Synced = node Informer . Informer ( ) . Has } 
func ( nc * Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle if ! controller . Wait For Cache Sync ( " " , stop Ch , nc . node Informer if nc . allocator Type != ipam . IPAM From Cluster Allocator Type && nc . allocator Type != ipam . IPAM From Cloud Allocator Type { go nc . cidr Allocator . Run ( stop <- stop } 
func New Manager ( c clientset . Interface ) * Manager { m := & Manager { get Token : func ( name , namespace string , tr * authenticationv1 . Token Request ) ( * authenticationv1 . Token return c . Core V1 ( ) . Service Accounts ( namespace ) . Create } , cache : make ( map [ string ] * authenticationv1 . Token Request ) , clock : clock . Real go wait . Forever ( m . cleanup , gc } 
func ( m * Manager ) Get Service Account Token ( namespace , name string , tr * authenticationv1 . Token Request ) ( * authenticationv1 . Token Request , error ) { key := key if ok && ! m . requires tr , err := m . get } 
func ( m * Manager ) Delete Service Account Token ( pod UID types . UID ) { m . cache defer m . cache for k , tr := range m . cache { if tr . Spec . Bound Object Ref . UID == pod } 
func ( m * Manager ) requires Refresh ( tr * authenticationv1 . Token Request ) bool { if tr . Spec . Expiration exp := tr . Status . Expiration iat := exp . Add ( - 1 * time . Duration ( * tr . Spec . Expiration if now . After ( iat . Add ( max // Require a refresh if within 20% of the TTL from the expiration time. if now . After ( exp . Add ( - 1 * time . Duration ( ( * tr . Spec . Expiration } 
func key Func ( name , namespace string , tr * authenticationv1 . Token if tr . Spec . Expiration Seconds != nil { exp = * tr . Spec . Expiration var ref authenticationv1 . Bound Object if tr . Spec . Bound Object Ref != nil { ref = * tr . Spec . Bound Object } 
func Is Config Empty ( config * Config ) bool { return len ( config . Auth Infos ) == 0 && len ( config . Clusters ) == 0 && len ( config . Contexts ) == 0 && len ( config . Current } 
func Minify Config ( config * Config ) error { if len ( config . Current curr Context , exists := config . Contexts [ config . Current if ! exists { return fmt . Errorf ( " " , config . Current new new Contexts [ config . Current Context ] = curr new if len ( curr Context . Cluster ) > 0 { if _ , exists := config . Clusters [ curr Context . Cluster ] ; ! exists { return fmt . Errorf ( " " , curr new Clusters [ curr Context . Cluster ] = config . Clusters [ curr new Auth Infos := map [ string ] * Auth if len ( curr Context . Auth Info ) > 0 { if _ , exists := config . Auth Infos [ curr Context . Auth Info ] ; ! exists { return fmt . Errorf ( " " , curr Context . Auth new Auth Infos [ curr Context . Auth Info ] = config . Auth Infos [ curr Context . Auth config . Auth Infos = new Auth config . Clusters = new config . Contexts = new } 
func Shorten Config ( config * Config ) { // trick json encoder into printing a human readable string in the raw data // by base64 decoding what we want to print. Relies on implementation of // http://golang.org/pkg/encoding/json/#Marshal using base64 to encode []byte for key , auth Info := range config . Auth Infos { if len ( auth Info . Client Key Data ) > 0 { auth Info . Client Key Data = redacted if len ( auth Info . Client Certificate Data ) > 0 { auth Info . Client Certificate Data = redacted config . Auth Infos [ key ] = auth for key , cluster := range config . Clusters { if len ( cluster . Certificate Authority Data ) > 0 { cluster . Certificate Authority Data = data Omitted } 
func Flatten Config ( config * Config ) error { for key , auth Info := range config . Auth Infos { base Dir , err := Make Abs ( path . Dir ( auth Info . Location Of if err := Flatten Content ( & auth Info . Client Certificate , & auth Info . Client Certificate Data , base if err := Flatten Content ( & auth Info . Client Key , & auth Info . Client Key Data , base config . Auth Infos [ key ] = auth for key , cluster := range config . Clusters { base Dir , err := Make Abs ( path . Dir ( cluster . Location Of if err := Flatten Content ( & cluster . Certificate Authority , & cluster . Certificate Authority Data , base } 
func Resolve Path ( path string , base string ) string { // Don't resolve empty paths if len ( path ) > 0 { // Don't resolve absolute paths if ! filepath . Is } 
func ( s stateful Set Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Stateful Set , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Stateful } 
func Convert_v1alpha1_Replica Set Controller Configuration_To_config_Replica Set Controller Configuration ( in * v1alpha1 . Replica Set Controller Configuration , out * config . Replica Set Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Replica Set Controller Configuration_To_config_Replica Set Controller } 
func Convert_config_Replica Set Controller Configuration_To_v1alpha1_Replica Set Controller Configuration ( in * config . Replica Set Controller Configuration , out * v1alpha1 . Replica Set Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Replica Set Controller Configuration_To_v1alpha1_Replica Set Controller } 
func ( s * Delegating Authorization Options ) With Always Allow Groups ( groups ... string ) * Delegating Authorization Options { s . Always Allow Groups = append ( s . Always Allow } 
func ( s * Delegating Authorization Options ) With Always Allow Paths ( paths ... string ) * Delegating Authorization Options { s . Always Allow Paths = append ( s . Always Allow } 
func Default JSON Encoder ( ) runtime . Encoder { return unstructured . JSON Fallback Encoder { Encoder : Codecs . Legacy Codec ( Scheme . Prioritized Versions All } 
func Validate Pod Preset Name ( name string , prefix bool ) [ ] string { // TODO: Validate that there's name for the suffix inserted by the pods. // Currently this is just "-index". In the future we may allow a user // specified list of suffixes and we need to validate the longest one. return apimachineryvalidation . Name Is DNS } 
func Validate Pod Preset Spec ( spec * settings . Pod Preset Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( & spec . Selector , fld if spec . Env == nil && spec . Env From == nil && spec . Volume Mounts == nil && spec . Volumes == nil { all Errs = append ( all Errs , field . Required ( fld vols , v Errs := apivalidation . Validate Volumes ( spec . Volumes , fld all Errs = append ( all Errs , v all Errs = append ( all Errs , apivalidation . Validate Env ( spec . Env , fld all Errs = append ( all Errs , apivalidation . Validate Env From ( spec . Env From , fld all Errs = append ( all Errs , apivalidation . Validate Volume Mounts ( spec . Volume Mounts , nil , vols , nil , fld return all } 
func Validate Pod Preset ( pip * settings . Pod Preset ) field . Error List { all Errs := apivalidation . Validate Object Meta ( & pip . Object Meta , true , Validate Pod Preset Name , field . New all Errs = append ( all Errs , Validate Pod Preset Spec ( & pip . Spec , field . New return all } 
func Validate Pod Preset Update ( pip , old Pip * settings . Pod Preset ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & pip . Object Meta , & old Pip . Object Meta , field . New all Errs = append ( all Errs , Validate Pod Preset Spec ( & pip . Spec , field . New return all } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1beta1 . Scale Spec ) ( nil ) , ( * scheme . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Scale Spec_To_scheme_Scale Spec ( a . ( * v1beta1 . Scale Spec ) , b . ( * scheme . Scale if err := s . Add Generated Conversion Func ( ( * scheme . Scale Spec ) ( nil ) , ( * v1beta1 . Scale Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheme_Scale Spec_To_v1beta1_Scale Spec ( a . ( * scheme . Scale Spec ) , b . ( * v1beta1 . Scale if err := s . Add Generated Conversion Func ( ( * v1beta1 . Scale Status ) ( nil ) , ( * scheme . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Scale Status_To_scheme_Scale Status ( a . ( * v1beta1 . Scale Status ) , b . ( * scheme . Scale if err := s . Add Generated Conversion Func ( ( * scheme . Scale Status ) ( nil ) , ( * v1beta1 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheme_Scale Status_To_v1beta1_Scale Status ( a . ( * scheme . Scale Status ) , b . ( * v1beta1 . Scale if err := s . Add Conversion Func ( ( * scheme . Scale Status ) ( nil ) , ( * v1beta1 . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheme_Scale Status_To_v1beta1_Scale Status ( a . ( * scheme . Scale Status ) , b . ( * v1beta1 . Scale if err := s . Add Conversion Func ( ( * v1beta1 . Scale Status ) ( nil ) , ( * scheme . Scale Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Scale Status_To_scheme_Scale Status ( a . ( * v1beta1 . Scale Status ) , b . ( * scheme . Scale } 
func Convert_v1beta1_Scale_To_scheme_Scale ( in * v1beta1 . Scale , out * scheme . Scale , s conversion . Scope ) error { return auto } 
func Convert_scheme_Scale_To_v1beta1_Scale ( in * scheme . Scale , out * v1beta1 . Scale , s conversion . Scope ) error { return auto } 
func Convert_v1beta1_Scale Spec_To_scheme_Scale Spec ( in * v1beta1 . Scale Spec , out * scheme . Scale Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Scale Spec_To_scheme_Scale } 
func Convert_scheme_Scale Spec_To_v1beta1_Scale Spec ( in * scheme . Scale Spec , out * v1beta1 . Scale Spec , s conversion . Scope ) error { return auto Convert_scheme_Scale Spec_To_v1beta1_Scale } 
func New Converter ( name Fn Name Func ) * Converter { c := & Converter { conversion Funcs : New Conversion Funcs ( ) , generated Conversion Funcs : New Conversion Funcs ( ) , ignored Conversions : make ( map [ type Pair ] struct { } ) , name Func : name Fn , struct Field Dests : make ( map [ type Name Pair ] [ ] type Name Pair ) , struct Field Sources : make ( map [ type Name Pair ] [ ] type Name Pair ) , input Field Mapping Funcs : make ( map [ reflect . Type ] Field Mapping Func ) , input Default Flags : make ( map [ reflect . Type ] Field Matching c . Register Conversion } 
func ( c * Converter ) With Conversions ( fns Conversion copied . conversion Funcs = c . conversion } 
func ( c * Converter ) Default Meta ( t reflect . Type ) ( Field Matching Flags , * Meta ) { return c . input Default Flags [ t ] , & Meta { Key Name Mapping : c . input Field Mapping } 
} 
func ( c Conversion Funcs ) Add ( fns ... interface { } ) error { for _ , fn := range fns { fv := reflect . Value if err := verify Conversion Function c . fns [ type } 
func ( c Conversion Funcs ) Add Untyped ( a , b interface { } , fn Conversion Func ) error { t A , t B := reflect . Type Of ( a ) , reflect . Type if t if t c . untyped [ type Pair { t A , t } 
func ( c Conversion Funcs ) Merge ( other Conversion Funcs ) Conversion Funcs { merged := New Conversion } 
func ( s * scope ) set Indices ( src , dest int ) { s . src s . dest } 
func ( s * scope ) set Keys ( src , dest interface { } ) { s . src s . dest } 
func ( s * scope ) Convert ( src , dest interface { } , flags Field Matching } 
func ( s * scope ) describe ( ) ( src , dest string ) { return s . src Stack . describe ( ) , s . dest } 
func ( s * scope ) errorf ( message string , args ... interface { } ) error { src Path , dest where := fmt . Sprintf ( " " , src Path , dest } 
func verify Conversion Function if ft . Num if ft . Num scope if e , a := reflect . Type Of ( & scope var for Error // This convolution is necessary, otherwise Type Of picks up on the fact // that for Error Type is nil. error Type := reflect . Type Of ( & for Error if ft . Out ( 0 ) != error } 
func ( c * Converter ) Register Untyped Conversion Func ( a , b interface { } , fn Conversion Func ) error { return c . conversion Funcs . Add } 
func ( c * Converter ) Register Generated Untyped Conversion Func ( a , b interface { } , fn Conversion Func ) error { return c . generated Conversion Funcs . Add } 
func ( c * Converter ) Register Ignored Conversion ( from , to interface { } ) error { type From := reflect . Type type To := reflect . Type if reflect . Type Of ( from ) . Kind ( ) != reflect . Ptr { return fmt . Errorf ( " " , type if type To . Kind ( ) != reflect . Ptr { return fmt . Errorf ( " " , type c . ignored Conversions [ type Pair { type From . Elem ( ) , type } 
func ( c * Converter ) Register Input Defaults ( in interface { } , fn Field Mapping Func , default Flags Field Matching Flags ) error { fv := reflect . Value c . input Field Mapping c . input Default Flags [ ft ] = default } 
func ( f Field Matching Flags ) Is Set ( flag Field Matching Flags ) bool { if flag == Dest From Source { // The bit logic doesn't work on the default value. return f & Source To Dest != Source To } 
func ( c * Converter ) Convert ( src , dest interface { } , flags Field Matching Flags , meta * Meta ) error { return c . do } 
func ( c * Converter ) Default Convert ( src , dest interface { } , flags Field Matching Flags , meta * Meta ) error { return c . do Conversion ( src , dest , flags , meta , c . default } 
func ( c * Converter ) call Custom ( sv , dv , custom reflect . Value , scope * scope ) error { if ! sv . Can if ! dv . Can Addr ( ) { if ! dv . Can dv dv := reflect . New ( dv defer func ( ) { dv args := [ ] reflect . Value { sv , dv , reflect . Value } 
func ( c * Converter ) default if ! dv . Can if ! scope . flags . Is Set ( Allow Different Field Type Names ) && c . name Func ( dt ) != c . name Func ( st ) { return scope . errorf ( " " , c . name Func ( st ) , c . name switch st . Kind ( ) { case reflect . Map , reflect . Ptr , reflect . Slice , reflect . Interface , reflect . Struct : // Don't copy these via assignment/conversion! default : // This should handle all simple types. if st . Assignable if st . Convertible scope . src Stack . push ( scope Stack scope . dest Stack . push ( scope Stack defer scope . src defer scope . dest switch dv . Kind ( ) { case reflect . Struct : return c . convert KV ( to KV Value ( sv ) , to KV case reflect . Slice : if sv . Is dv . Set ( reflect . Make for i := 0 ; i < sv . Len ( ) ; i ++ { scope . set case reflect . Ptr : if sv . Is case reflect . Map : if sv . Is dv . Set ( reflect . Make for _ , sk := range sv . Map scope . set // TODO: sv.Map Index(sk) may return a value with Can Addr() == false, // because a map[string]struct{} does not allow a pointer reference. // Calling a custom conversion function defined for the map value // will panic. Example is Pod Info map[string]Container Status. if err := c . convert ( sv . Map dv . Set Map case reflect . Interface : if sv . Is dv . Set ( reflect . Value } 
func ( c * Converter ) convert KV ( skv , dkv kv if scope . flags . Is Set ( Source To var mapping Field Mapping if scope . meta != nil && scope . meta . Key Name Mapping != nil { mapping = scope . meta . Key Name for _ , key := range lister . keys ( ) { if found , err := c . check stag := skv . tag dtag := dkv . tag if mapping != nil { skey , dkey = scope . meta . Key Name if ! df . Is Valid ( ) || ! sf . Is Valid ( ) { switch { case scope . flags . Is Set ( Ignore Missing Fields ) : // No error. case scope . flags . Is Set ( Source To scope . src scope . src scope . dest scope . dest } 
func ( c * Converter ) check Field ( field Name string , skv , dkv kv Value , scope * scope ) ( bool , error ) { replacement if scope . flags . Is Set ( Dest From Source ) { df := dkv . value ( field if ! df . Is dest Key := type Name Pair { df . Type ( ) , field // Check each of the potential source (type, name) pairs to see if they're // present in sv. for _ , potential Source Key := range c . struct Field Sources [ dest Key ] { sf := skv . value ( potential Source Key . field if ! sf . Is if sf . Type ( ) == potential Source Key . field Type { // Both the source's name and type matched, so copy. scope . src Stack . top ( ) . key = potential Source Key . field scope . dest Stack . top ( ) . key = field dkv . confirm Set ( field replacement return replacement sf := skv . value ( field if ! sf . Is src Key := type Name Pair { sf . Type ( ) , field // Check each of the potential dest (type, name) pairs to see if they're // present in dv. for _ , potential Dest Key := range c . struct Field Dests [ src Key ] { df := dkv . value ( potential Dest Key . field if ! df . Is if df . Type ( ) == potential Dest Key . field Type { // Both the dest's name and type matched, so copy. scope . src Stack . top ( ) . key = field scope . dest Stack . top ( ) . key = potential Dest Key . field dkv . confirm Set ( potential Dest Key . field replacement return replacement } 
func New Integer Resource Version Mutation Cache ( backing Cache Store , indexer Indexer , ttl time . Duration , include Adds bool ) Mutation Cache { return & mutation Cache { backing Cache : backing Cache , indexer : indexer , mutation Cache : utilcache . New LRU Expire Cache ( 100 ) , comparator : etcd Object Versioner { } , ttl : ttl , include Adds : include } 
func ( c * mutation Cache ) Get By obj , exists , err := c . backing Cache . Get By if ! exists { if ! c . include obj , exists = c . mutation obj return c . newer Object ( key , obj } 
func ( c * mutation Cache ) By Index ( name string , index keys , err := c . indexer . Index Keys ( name , index key Set := sets . New for _ , key := range keys { key obj , exists , err := c . indexer . Get By if obj Runtime , ok := obj . ( runtime . Object ) ; ok { items = append ( items , c . newer Object ( key , obj if c . include Adds { fn := c . indexer . Get // Keys() is returned oldest to newest, so full traversal does not alter the LRU behavior for _ , key := range c . mutation Cache . Keys ( ) { updated , ok := c . mutation if key for _ , in Index := range elements { if in Index != index } 
func ( c * mutation Cache ) newer Object ( key string , backing runtime . Object ) runtime . Object { mutated Obj , exists := c . mutation mutated Obj Runtime , ok := mutated if c . comparator . Compare Resource Version ( backing , mutated Obj Runtime ) >= 0 { c . mutation return mutated Obj } 
func ( c * mutation key , err := Deletion Handling Meta Namespace Key if err != nil { // this is a "nice to have", so failures shouldn't do anything weird utilruntime . Handle if obj Runtime , ok := obj . ( runtime . Object ) ; ok { if mutated Obj , exists := c . mutation Cache . Get ( key ) ; exists { if mutated Obj Runtime , ok := mutated Obj . ( runtime . Object ) ; ok { if c . comparator . Compare Resource Version ( obj Runtime , mutated Obj c . mutation } 
func New Default Authentication Info Resolver Wrapper ( proxy Transport * http . Transport , kubeapiserver Client Config * rest . Config ) Authentication Info Resolver Wrapper { webhook Auth Resolver Wrapper := func ( delegate Authentication Info Resolver ) Authentication Info Resolver { return & Authentication Info Resolver Delegator { Client Config For Func : func ( server string ) ( * rest . Config , error ) { if server == " " { return kubeapiserver Client return delegate . Client Config } , Client Config For Service Func : func ( service Name , service Namespace string ) ( * rest . Config , error ) { if service Name == " " && service Namespace == corev1 . Namespace Default { return kubeapiserver Client ret , err := delegate . Client Config For Service ( service Name , service if proxy Transport != nil && proxy Transport . Dial Context != nil { ret . Dial = proxy Transport . Dial return webhook Auth Resolver } 
func ( a * Authentication Info Resolver Delegator ) Client Config For ( server string ) ( * rest . Config , error ) { return a . Client Config For } 
func ( a * Authentication Info Resolver Delegator ) Client Config For Service ( service Name , service Namespace string ) ( * rest . Config , error ) { return a . Client Config For Service Func ( service Name , service } 
func New Default Authentication Info Resolver ( kubeconfig File string ) ( Authentication Info Resolver , error ) { if len ( kubeconfig File ) == 0 { return & default Authentication Info loading Rules := clientcmd . New Default Client Config Loading loading Rules . Explicit Path = kubeconfig loader := clientcmd . New Non Interactive Deferred Loading Client Config ( loading Rules , & clientcmd . Config client Config , err := loader . Raw return & default Authentication Info Resolver { kubeconfig : client } 
func ( c * UID } 
func ( c * UID } 
func New Lazy REST Mapper Loader ( fn func ( ) ( REST Mapper , error ) ) REST Mapper { obj := & lazy } 
func ( o * lazy } 
func ( cfg * Cluster Configuration ) Get Control Plane Image Repository ( ) string { if cfg . CI Image Repository != " " { return cfg . CI Image return cfg . Image } 
func New ( auth Request Handlers ... authenticator . Request ) authenticator . Request { if len ( auth Request Handlers ) == 1 { return auth Request return & union Auth Request Handler { Handlers : auth Request Handlers , Fail On } 
func New Fail On Error ( auth Request Handlers ... authenticator . Request ) authenticator . Request { if len ( auth Request Handlers ) == 1 { return auth Request return & union Auth Request Handler { Handlers : auth Request Handlers , Fail On } 
func ( auth Handler * union Auth Request Handler ) Authenticate for _ , curr Auth Request Handler := range auth Handler . Handlers { resp , ok , err := curr Auth Request Handler . Authenticate if err != nil { if auth Handler . Fail On return nil , false , utilerrors . New } 
func ( d * flex Volume Detacher ) Detach ( volume Name string , host Name types . Node Name ) error { call := d . plugin . New Driver Call ( detach call . Append ( volume call . Append ( string ( host if is Cmd Not Supported Err ( err ) { return ( * detacher Defaults ) ( d ) . Detach ( volume Name , host } 
func ( d * flex Volume Detacher ) Unmount Device ( device Mount Path string ) error { path Exists , path Err := mount . Path Exists ( device Mount if ! path Exists { klog . Warningf ( " " , device Mount if path Err != nil && ! mount . Is Corrupted Mnt ( path Err ) { return fmt . Errorf ( " " , path notmnt , err := is Not Mounted ( d . plugin . host . Get Mounter ( d . plugin . Get Plugin Name ( ) ) , device Mount if err != nil { if mount . Is Corrupted if notmnt { klog . Warningf ( " " , device Mount } else { call := d . plugin . New Driver Call ( unmount Device call . Append ( device Mount if is Cmd Not Supported Err ( err ) { err = ( * detacher Defaults ) ( d ) . Unmount Device ( device Mount // Flexvolume driver may remove the directory. Ignore if it does. if path Exists , path Err := mount . Path Exists ( device Mount Path ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path return os . Remove ( device Mount } 
func ( c * Fake Storage Classes ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Storage Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( storageclasses Resource , name ) , & v1beta1 . Storage return obj . ( * v1beta1 . Storage } 
func ( c * Fake Storage Classes ) List ( opts v1 . List Options ) ( result * v1beta1 . Storage Class List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( storageclasses Resource , storageclasses Kind , opts ) , & v1beta1 . Storage Class label , _ , _ := testing . Extract From List list := & v1beta1 . Storage Class List { List Meta : obj . ( * v1beta1 . Storage Class List ) . List for _ , item := range obj . ( * v1beta1 . Storage Class } 
func ( c * Fake Storage Classes ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( storageclasses } 
func ( c * Fake Storage Classes ) Create ( storage Class * v1beta1 . Storage Class ) ( result * v1beta1 . Storage Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( storageclasses Resource , storage Class ) , & v1beta1 . Storage return obj . ( * v1beta1 . Storage } 
func ( c * Fake Storage Classes ) Update ( storage Class * v1beta1 . Storage Class ) ( result * v1beta1 . Storage Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( storageclasses Resource , storage Class ) , & v1beta1 . Storage return obj . ( * v1beta1 . Storage } 
func ( c * Fake Storage Classes ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( storageclasses Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Storage Class } 
func ( c * Fake Storage Classes ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Storage Class , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( storageclasses Resource , name , pt , data , subresources ... ) , & v1beta1 . Storage return obj . ( * v1beta1 . Storage } 
func ( az * Cloud ) Create File Share ( share Name , account Name , account Type , account Kind , resource Group , location string , request Gi B int ) ( string , string , error ) { if resource Group == " " { resource Group = az . resource account , key , err := az . Ensure Storage Account ( account Name , account Type , account Kind , resource Group , location , file Share Account Name if err != nil { return " " , " " , fmt . Errorf ( " " , account if err := az . create File Share ( account , key , share Name , request Gi B ) ; err != nil { return " " , " " , fmt . Errorf ( " " , share klog . V ( 4 ) . Infof ( " " , share } 
func ( az * Cloud ) Delete File Share ( account Name , account Key , share Name string ) error { if err := az . delete File Share ( account Name , account Key , share klog . V ( 4 ) . Infof ( " " , share } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v2beta1 . Cross Version Object Reference ) ( nil ) , ( * autoscaling . Cross Version Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Cross Version Object Reference_To_autoscaling_Cross Version Object Reference ( a . ( * v2beta1 . Cross Version Object Reference ) , b . ( * autoscaling . Cross Version Object if err := s . Add Generated Conversion Func ( ( * autoscaling . Cross Version Object Reference ) ( nil ) , ( * v2beta1 . Cross Version Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Cross Version Object Reference_To_v2beta1_Cross Version Object Reference ( a . ( * autoscaling . Cross Version Object Reference ) , b . ( * v2beta1 . Cross Version Object if err := s . Add Generated Conversion Func ( ( * v2beta1 . External Metric Source ) ( nil ) , ( * autoscaling . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_External Metric Source_To_autoscaling_External Metric Source ( a . ( * v2beta1 . External Metric Source ) , b . ( * autoscaling . External Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . External Metric Source ) ( nil ) , ( * v2beta1 . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Source_To_v2beta1_External Metric Source ( a . ( * autoscaling . External Metric Source ) , b . ( * v2beta1 . External Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . External Metric Status ) ( nil ) , ( * autoscaling . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_External Metric Status_To_autoscaling_External Metric Status ( a . ( * v2beta1 . External Metric Status ) , b . ( * autoscaling . External Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . External Metric Status ) ( nil ) , ( * v2beta1 . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Status_To_v2beta1_External Metric Status ( a . ( * autoscaling . External Metric Status ) , b . ( * v2beta1 . External Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Horizontal Pod Autoscaler ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Horizontal Pod Autoscaler_To_autoscaling_Horizontal Pod Autoscaler ( a . ( * v2beta1 . Horizontal Pod Autoscaler ) , b . ( * autoscaling . Horizontal Pod if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler ) ( nil ) , ( * v2beta1 . Horizontal Pod Autoscaler ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler_To_v2beta1_Horizontal Pod Autoscaler ( a . ( * autoscaling . Horizontal Pod Autoscaler ) , b . ( * v2beta1 . Horizontal Pod if err := s . Add Generated Conversion Func ( ( * v2beta1 . Horizontal Pod Autoscaler Condition ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Horizontal Pod Autoscaler Condition_To_autoscaling_Horizontal Pod Autoscaler Condition ( a . ( * v2beta1 . Horizontal Pod Autoscaler Condition ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Condition ) ( nil ) , ( * v2beta1 . Horizontal Pod Autoscaler Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Condition_To_v2beta1_Horizontal Pod Autoscaler Condition ( a . ( * autoscaling . Horizontal Pod Autoscaler Condition ) , b . ( * v2beta1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v2beta1 . Horizontal Pod Autoscaler List ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Horizontal Pod Autoscaler List_To_autoscaling_Horizontal Pod Autoscaler List ( a . ( * v2beta1 . Horizontal Pod Autoscaler List ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler List ) ( nil ) , ( * v2beta1 . Horizontal Pod Autoscaler List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler List_To_v2beta1_Horizontal Pod Autoscaler List ( a . ( * autoscaling . Horizontal Pod Autoscaler List ) , b . ( * v2beta1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v2beta1 . Horizontal Pod Autoscaler Spec ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Horizontal Pod Autoscaler Spec_To_autoscaling_Horizontal Pod Autoscaler Spec ( a . ( * v2beta1 . Horizontal Pod Autoscaler Spec ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Spec ) ( nil ) , ( * v2beta1 . Horizontal Pod Autoscaler Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Spec_To_v2beta1_Horizontal Pod Autoscaler Spec ( a . ( * autoscaling . Horizontal Pod Autoscaler Spec ) , b . ( * v2beta1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v2beta1 . Horizontal Pod Autoscaler Status ) ( nil ) , ( * autoscaling . Horizontal Pod Autoscaler Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Horizontal Pod Autoscaler Status_To_autoscaling_Horizontal Pod Autoscaler Status ( a . ( * v2beta1 . Horizontal Pod Autoscaler Status ) , b . ( * autoscaling . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * autoscaling . Horizontal Pod Autoscaler Status ) ( nil ) , ( * v2beta1 . Horizontal Pod Autoscaler Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Horizontal Pod Autoscaler Status_To_v2beta1_Horizontal Pod Autoscaler Status ( a . ( * autoscaling . Horizontal Pod Autoscaler Status ) , b . ( * v2beta1 . Horizontal Pod Autoscaler if err := s . Add Generated Conversion Func ( ( * v2beta1 . Metric Spec ) ( nil ) , ( * autoscaling . Metric Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Metric Spec_To_autoscaling_Metric Spec ( a . ( * v2beta1 . Metric Spec ) , b . ( * autoscaling . Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Metric Spec ) ( nil ) , ( * v2beta1 . Metric Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Metric Spec_To_v2beta1_Metric Spec ( a . ( * autoscaling . Metric Spec ) , b . ( * v2beta1 . Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Metric Status ) ( nil ) , ( * autoscaling . Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Metric Status_To_autoscaling_Metric Status ( a . ( * v2beta1 . Metric Status ) , b . ( * autoscaling . Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Metric Status ) ( nil ) , ( * v2beta1 . Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Metric Status_To_v2beta1_Metric Status ( a . ( * autoscaling . Metric Status ) , b . ( * v2beta1 . Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Object Metric Source ) ( nil ) , ( * autoscaling . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Object Metric Source_To_autoscaling_Object Metric Source ( a . ( * v2beta1 . Object Metric Source ) , b . ( * autoscaling . Object Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Object Metric Source ) ( nil ) , ( * v2beta1 . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Source_To_v2beta1_Object Metric Source ( a . ( * autoscaling . Object Metric Source ) , b . ( * v2beta1 . Object Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Object Metric Status ) ( nil ) , ( * autoscaling . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Object Metric Status_To_autoscaling_Object Metric Status ( a . ( * v2beta1 . Object Metric Status ) , b . ( * autoscaling . Object Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Object Metric Status ) ( nil ) , ( * v2beta1 . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Status_To_v2beta1_Object Metric Status ( a . ( * autoscaling . Object Metric Status ) , b . ( * v2beta1 . Object Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Pods Metric Source ) ( nil ) , ( * autoscaling . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Pods Metric Source_To_autoscaling_Pods Metric Source ( a . ( * v2beta1 . Pods Metric Source ) , b . ( * autoscaling . Pods Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Pods Metric Source ) ( nil ) , ( * v2beta1 . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Source_To_v2beta1_Pods Metric Source ( a . ( * autoscaling . Pods Metric Source ) , b . ( * v2beta1 . Pods Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Pods Metric Status ) ( nil ) , ( * autoscaling . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Pods Metric Status_To_autoscaling_Pods Metric Status ( a . ( * v2beta1 . Pods Metric Status ) , b . ( * autoscaling . Pods Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Pods Metric Status ) ( nil ) , ( * v2beta1 . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Status_To_v2beta1_Pods Metric Status ( a . ( * autoscaling . Pods Metric Status ) , b . ( * v2beta1 . Pods Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Resource Metric Source ) ( nil ) , ( * autoscaling . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Resource Metric Source_To_autoscaling_Resource Metric Source ( a . ( * v2beta1 . Resource Metric Source ) , b . ( * autoscaling . Resource Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Resource Metric Source ) ( nil ) , ( * v2beta1 . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Source_To_v2beta1_Resource Metric Source ( a . ( * autoscaling . Resource Metric Source ) , b . ( * v2beta1 . Resource Metric if err := s . Add Generated Conversion Func ( ( * v2beta1 . Resource Metric Status ) ( nil ) , ( * autoscaling . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Resource Metric Status_To_autoscaling_Resource Metric Status ( a . ( * v2beta1 . Resource Metric Status ) , b . ( * autoscaling . Resource Metric if err := s . Add Generated Conversion Func ( ( * autoscaling . Resource Metric Status ) ( nil ) , ( * v2beta1 . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Status_To_v2beta1_Resource Metric Status ( a . ( * autoscaling . Resource Metric Status ) , b . ( * v2beta1 . Resource Metric if err := s . Add Conversion Func ( ( * autoscaling . External Metric Source ) ( nil ) , ( * v2beta1 . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Source_To_v2beta1_External Metric Source ( a . ( * autoscaling . External Metric Source ) , b . ( * v2beta1 . External Metric if err := s . Add Conversion Func ( ( * autoscaling . External Metric Status ) ( nil ) , ( * v2beta1 . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_External Metric Status_To_v2beta1_External Metric Status ( a . ( * autoscaling . External Metric Status ) , b . ( * v2beta1 . External Metric if err := s . Add Conversion Func ( ( * autoscaling . Metric Target ) ( nil ) , ( * v2beta1 . Cross Version Object Reference ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Metric Target_To_v2beta1_Cross Version Object Reference ( a . ( * autoscaling . Metric Target ) , b . ( * v2beta1 . Cross Version Object if err := s . Add Conversion Func ( ( * autoscaling . Object Metric Source ) ( nil ) , ( * v2beta1 . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Source_To_v2beta1_Object Metric Source ( a . ( * autoscaling . Object Metric Source ) , b . ( * v2beta1 . Object Metric if err := s . Add Conversion Func ( ( * autoscaling . Object Metric Status ) ( nil ) , ( * v2beta1 . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Object Metric Status_To_v2beta1_Object Metric Status ( a . ( * autoscaling . Object Metric Status ) , b . ( * v2beta1 . Object Metric if err := s . Add Conversion Func ( ( * autoscaling . Pods Metric Source ) ( nil ) , ( * v2beta1 . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Source_To_v2beta1_Pods Metric Source ( a . ( * autoscaling . Pods Metric Source ) , b . ( * v2beta1 . Pods Metric if err := s . Add Conversion Func ( ( * autoscaling . Pods Metric Status ) ( nil ) , ( * v2beta1 . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Pods Metric Status_To_v2beta1_Pods Metric Status ( a . ( * autoscaling . Pods Metric Status ) , b . ( * v2beta1 . Pods Metric if err := s . Add Conversion Func ( ( * autoscaling . Resource Metric Source ) ( nil ) , ( * v2beta1 . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Source_To_v2beta1_Resource Metric Source ( a . ( * autoscaling . Resource Metric Source ) , b . ( * v2beta1 . Resource Metric if err := s . Add Conversion Func ( ( * autoscaling . Resource Metric Status ) ( nil ) , ( * v2beta1 . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_autoscaling_Resource Metric Status_To_v2beta1_Resource Metric Status ( a . ( * autoscaling . Resource Metric Status ) , b . ( * v2beta1 . Resource Metric if err := s . Add Conversion Func ( ( * v2beta1 . Cross Version Object Reference ) ( nil ) , ( * autoscaling . Metric Target ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Cross Version Object Reference_To_autoscaling_Metric Target ( a . ( * v2beta1 . Cross Version Object Reference ) , b . ( * autoscaling . Metric if err := s . Add Conversion Func ( ( * v2beta1 . External Metric Source ) ( nil ) , ( * autoscaling . External Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_External Metric Source_To_autoscaling_External Metric Source ( a . ( * v2beta1 . External Metric Source ) , b . ( * autoscaling . External Metric if err := s . Add Conversion Func ( ( * v2beta1 . External Metric Status ) ( nil ) , ( * autoscaling . External Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_External Metric Status_To_autoscaling_External Metric Status ( a . ( * v2beta1 . External Metric Status ) , b . ( * autoscaling . External Metric if err := s . Add Conversion Func ( ( * v2beta1 . Object Metric Source ) ( nil ) , ( * autoscaling . Object Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Object Metric Source_To_autoscaling_Object Metric Source ( a . ( * v2beta1 . Object Metric Source ) , b . ( * autoscaling . Object Metric if err := s . Add Conversion Func ( ( * v2beta1 . Object Metric Status ) ( nil ) , ( * autoscaling . Object Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Object Metric Status_To_autoscaling_Object Metric Status ( a . ( * v2beta1 . Object Metric Status ) , b . ( * autoscaling . Object Metric if err := s . Add Conversion Func ( ( * v2beta1 . Pods Metric Source ) ( nil ) , ( * autoscaling . Pods Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Pods Metric Source_To_autoscaling_Pods Metric Source ( a . ( * v2beta1 . Pods Metric Source ) , b . ( * autoscaling . Pods Metric if err := s . Add Conversion Func ( ( * v2beta1 . Pods Metric Status ) ( nil ) , ( * autoscaling . Pods Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Pods Metric Status_To_autoscaling_Pods Metric Status ( a . ( * v2beta1 . Pods Metric Status ) , b . ( * autoscaling . Pods Metric if err := s . Add Conversion Func ( ( * v2beta1 . Resource Metric Source ) ( nil ) , ( * autoscaling . Resource Metric Source ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Resource Metric Source_To_autoscaling_Resource Metric Source ( a . ( * v2beta1 . Resource Metric Source ) , b . ( * autoscaling . Resource Metric if err := s . Add Conversion Func ( ( * v2beta1 . Resource Metric Status ) ( nil ) , ( * autoscaling . Resource Metric Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v2beta1_Resource Metric Status_To_autoscaling_Resource Metric Status ( a . ( * v2beta1 . Resource Metric Status ) , b . ( * autoscaling . Resource Metric } 
func Convert_v2beta1_Cross Version Object Reference_To_autoscaling_Cross Version Object Reference ( in * v2beta1 . Cross Version Object Reference , out * autoscaling . Cross Version Object Reference , s conversion . Scope ) error { return auto Convert_v2beta1_Cross Version Object Reference_To_autoscaling_Cross Version Object } 
func Convert_autoscaling_Cross Version Object Reference_To_v2beta1_Cross Version Object Reference ( in * autoscaling . Cross Version Object Reference , out * v2beta1 . Cross Version Object Reference , s conversion . Scope ) error { return auto Convert_autoscaling_Cross Version Object Reference_To_v2beta1_Cross Version Object } 
func Convert_v2beta1_Horizontal Pod Autoscaler_To_autoscaling_Horizontal Pod Autoscaler ( in * v2beta1 . Horizontal Pod Autoscaler , out * autoscaling . Horizontal Pod Autoscaler , s conversion . Scope ) error { return auto Convert_v2beta1_Horizontal Pod Autoscaler_To_autoscaling_Horizontal Pod } 
func Convert_autoscaling_Horizontal Pod Autoscaler_To_v2beta1_Horizontal Pod Autoscaler ( in * autoscaling . Horizontal Pod Autoscaler , out * v2beta1 . Horizontal Pod Autoscaler , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler_To_v2beta1_Horizontal Pod } 
func Convert_v2beta1_Horizontal Pod Autoscaler Condition_To_autoscaling_Horizontal Pod Autoscaler Condition ( in * v2beta1 . Horizontal Pod Autoscaler Condition , out * autoscaling . Horizontal Pod Autoscaler Condition , s conversion . Scope ) error { return auto Convert_v2beta1_Horizontal Pod Autoscaler Condition_To_autoscaling_Horizontal Pod Autoscaler } 
func Convert_autoscaling_Horizontal Pod Autoscaler Condition_To_v2beta1_Horizontal Pod Autoscaler Condition ( in * autoscaling . Horizontal Pod Autoscaler Condition , out * v2beta1 . Horizontal Pod Autoscaler Condition , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler Condition_To_v2beta1_Horizontal Pod Autoscaler } 
func Convert_v2beta1_Horizontal Pod Autoscaler List_To_autoscaling_Horizontal Pod Autoscaler List ( in * v2beta1 . Horizontal Pod Autoscaler List , out * autoscaling . Horizontal Pod Autoscaler List , s conversion . Scope ) error { return auto Convert_v2beta1_Horizontal Pod Autoscaler List_To_autoscaling_Horizontal Pod Autoscaler } 
func Convert_autoscaling_Horizontal Pod Autoscaler List_To_v2beta1_Horizontal Pod Autoscaler List ( in * autoscaling . Horizontal Pod Autoscaler List , out * v2beta1 . Horizontal Pod Autoscaler List , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler List_To_v2beta1_Horizontal Pod Autoscaler } 
func Convert_v2beta1_Horizontal Pod Autoscaler Spec_To_autoscaling_Horizontal Pod Autoscaler Spec ( in * v2beta1 . Horizontal Pod Autoscaler Spec , out * autoscaling . Horizontal Pod Autoscaler Spec , s conversion . Scope ) error { return auto Convert_v2beta1_Horizontal Pod Autoscaler Spec_To_autoscaling_Horizontal Pod Autoscaler } 
func Convert_autoscaling_Horizontal Pod Autoscaler Spec_To_v2beta1_Horizontal Pod Autoscaler Spec ( in * autoscaling . Horizontal Pod Autoscaler Spec , out * v2beta1 . Horizontal Pod Autoscaler Spec , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler Spec_To_v2beta1_Horizontal Pod Autoscaler } 
func Convert_v2beta1_Horizontal Pod Autoscaler Status_To_autoscaling_Horizontal Pod Autoscaler Status ( in * v2beta1 . Horizontal Pod Autoscaler Status , out * autoscaling . Horizontal Pod Autoscaler Status , s conversion . Scope ) error { return auto Convert_v2beta1_Horizontal Pod Autoscaler Status_To_autoscaling_Horizontal Pod Autoscaler } 
func Convert_autoscaling_Horizontal Pod Autoscaler Status_To_v2beta1_Horizontal Pod Autoscaler Status ( in * autoscaling . Horizontal Pod Autoscaler Status , out * v2beta1 . Horizontal Pod Autoscaler Status , s conversion . Scope ) error { return auto Convert_autoscaling_Horizontal Pod Autoscaler Status_To_v2beta1_Horizontal Pod Autoscaler } 
func Convert_v2beta1_Metric Spec_To_autoscaling_Metric Spec ( in * v2beta1 . Metric Spec , out * autoscaling . Metric Spec , s conversion . Scope ) error { return auto Convert_v2beta1_Metric Spec_To_autoscaling_Metric } 
func Convert_autoscaling_Metric Spec_To_v2beta1_Metric Spec ( in * autoscaling . Metric Spec , out * v2beta1 . Metric Spec , s conversion . Scope ) error { return auto Convert_autoscaling_Metric Spec_To_v2beta1_Metric } 
func Convert_v2beta1_Metric Status_To_autoscaling_Metric Status ( in * v2beta1 . Metric Status , out * autoscaling . Metric Status , s conversion . Scope ) error { return auto Convert_v2beta1_Metric Status_To_autoscaling_Metric } 
func Convert_autoscaling_Metric Status_To_v2beta1_Metric Status ( in * autoscaling . Metric Status , out * v2beta1 . Metric Status , s conversion . Scope ) error { return auto Convert_autoscaling_Metric Status_To_v2beta1_Metric } 
func ( pd * gce Persistent Disk ) Get Global Map Path ( spec * volume . Spec ) ( string , error ) { volume Source , _ , err := get Volume return filepath . Join ( pd . plugin . host . Get Volume Device Plugin Dir ( gce Persistent Disk Plugin Name ) , string ( volume Source . PD } 
func ( pd * gce Persistent Disk ) Get Pod Device Map Path ( ) ( string , string ) { name := gce Persistent Disk Plugin return pd . plugin . host . Get Pod Volume Device Dir ( pd . pod UID , utilstrings . Escape Qualified Name ( name ) ) , pd . vol } 
func ( v * Cloud ) Node Addresses ( ctx context . Context , node Name types . Node Name ) ( [ ] v1 . Node Address , error ) { name := map Node Name To Instance Name ( node instance , err := v . fetch if instance . IP Address != " " { address = net . Parse IP ( instance . IP if address == nil { return nil , fmt . Errorf ( " " , instance . IP } else { resolved , err := net . Lookup return [ ] v1 . Node Address { { Type : v1 . Node Internal IP , Address : address . String ( ) } , { Type : v1 . Node External } 
func ( v * Cloud ) Node Addresses By Provider ID ( ctx context . Context , provider ID string ) ( [ ] v1 . Node Address , error ) { return [ ] v1 . Node Address { } , cloudprovider . Not } 
func ( v * Cloud ) Instance Exists By Provider ID ( ctx context . Context , provider ID string ) ( bool , error ) { return false , cloudprovider . Not } 
func ( v * Cloud ) Instance ID ( ctx context . Context , node Name types . Node Name ) ( string , error ) { name := map Node Name To Instance Name ( node instance , err := v . fetch } 
func ( v * Cloud ) Instance Type By Provider ID ( ctx context . Context , provider ID string ) ( string , error ) { return " " , cloudprovider . Not } 
func ( v * Cloud ) Instance Type ( ctx context . Context , name types . Node } 
func ( m * Instance Map ) List Sorted } 
func ( v * Cloud ) Current Node Name ( ctx context . Context , hostname string ) ( types . Node Name , error ) { return types . Node } 
func token } 
func ( t * Token Authenticator ) Authenticate Token ( ctx context . Context , token string ) ( * authenticator . Response , bool , error ) { token ID , token Secret , err := parse secret Name := bootstrapapi . Bootstrap Token Secret Prefix + token secret , err := t . lister . Get ( secret if err != nil { if errors . Is Not Found ( err ) { klog . V ( 3 ) . Infof ( " " , secret if secret . Deletion Timestamp != nil { token if string ( secret . Type ) != string ( bootstrapapi . Secret Type Bootstrap Token ) || secret . Data == nil { token Errorf ( secret , " " , bootstrapapi . Secret Type Bootstrap ts := get Secret String ( secret , bootstrapapi . Bootstrap Token Secret if subtle . Constant Time Compare ( [ ] byte ( ts ) , [ ] byte ( token Secret ) ) != 1 { token Errorf ( secret , " " , bootstrapapi . Bootstrap Token Secret Key , token id := get Secret String ( secret , bootstrapapi . Bootstrap Token ID if id != token ID { token Errorf ( secret , " " , bootstrapapi . Bootstrap Token ID Key , token if is Secret Expired ( secret ) { // logging done in is Secret if get Secret String ( secret , bootstrapapi . Bootstrap Token Usage Authentication ) != " " { token Errorf ( secret , " " , bootstrapapi . Bootstrap Token Usage groups , err := get if err != nil { token Errorf ( secret , " " , bootstrapapi . Bootstrap Token Extra Groups return & authenticator . Response { User : & user . Default Info { Name : bootstrapapi . Bootstrap User } 
func parse Token ( s string ) ( string , string , error ) { split := token Regexp . Find String if len ( split ) != 3 { return " " , " " , fmt . Errorf ( " " , s , token Regexp } 
func get Groups ( secret * corev1 . Secret ) ( [ ] string , error ) { // always include the default group groups := sets . New String ( bootstrapapi . Bootstrap Default // grab any extra groups and if there are none, return just the default extra Groups String := get Secret String ( secret , bootstrapapi . Bootstrap Token Extra Groups if extra Groups // validate the names of the extra groups for _ , group := range strings . Split ( extra Groups String , " " ) { if err := bootstraputil . Validate Bootstrap Group } 
func New Custom Resource Definitions Server Options ( out , err Out io . Writer ) * Custom Resource Definitions Server Options { o := & Custom Resource Definitions Server Options { Recommended Options : genericoptions . New Recommended Options ( default Etcd Path Prefix , apiserver . Codecs . Legacy Codec ( v1beta1 . Scheme Group Version ) , genericoptions . New Process Info ( " " , " " ) , ) , API Enablement : genericoptions . New API Enablement Options ( ) , Std Out : out , Std Err : err } 
func ( o Custom Resource Definitions Server Options ) Add Flags ( fs * pflag . Flag Set ) { o . Recommended Options . Add o . API Enablement . Add } 
func ( o Custom Resource Definitions Server errors = append ( errors , o . Recommended errors = append ( errors , o . API return utilerrors . New } 
func ( o Custom Resource Definitions Server Options ) Config ( ) ( * apiserver . Config , error ) { // TODO have a "real" external address if err := o . Recommended Options . Secure Serving . Maybe Default With Self Signed Certs ( " " , nil , [ ] net . IP { net . Parse server Config := genericapiserver . New Recommended if err := o . Recommended Options . Apply To ( server if err := o . API Enablement . Apply To ( & server Config . Config , apiserver . Default API Resource Config config := & apiserver . Config { Generic Config : server Config , Extra Config : apiserver . Extra Config { CRDREST Options Getter : New CRDREST Options Getter ( * o . Recommended Options . Etcd ) , Service Resolver : & service Resolver { server Config . Shared Informer Factory . Core ( ) . V1 ( ) . Services ( ) . Lister ( ) } , Auth Resolver Wrapper : webhook . New Default Authentication Info Resolver Wrapper ( nil , server Config . Loopback Client } 
func New CRDREST Options Getter ( etcd Options genericoptions . Etcd Options ) genericregistry . REST Options Getter { ret := apiserver . CRDREST Options Getter { Storage Config : etcd Options . Storage Config , Storage Prefix : etcd Options . Storage Config . Prefix , Enable Watch Cache : etcd Options . Enable Watch Cache , Default Watch Cache Size : etcd Options . Default Watch Cache Size , Enable Garbage Collection : etcd Options . Enable Garbage Collection , Delete Collection Workers : etcd Options . Delete Collection Workers , Count Metric Poll Period : etcd Options . Storage Config . Count Metric Poll ret . Storage Config . Codec = unstructured . Unstructured JSON } 
func ( s * deployment Lister ) Get Deployments For Replica Set ( rs * apps . Replica // TODO: MODIFY THIS METHOD so that it checks for the pod Template Spec Hash label d for _ , d := range d List { selector , err := metav1 . Label Selector As } 
func New Kubelet Auth ( authenticator authenticator . Request , authorizer Attribute Getter authorizer . Request Attributes Getter , authorizer authorizer . Authorizer ) Auth Interface { return & Kubelet Auth { authenticator , authorizer Attribute } 
func ( n node Authorizer Attributes Getter ) Get Request Attributes ( u user . Info , r * http . Request ) authorizer . Attributes { api switch r . Method { case " " : api case " " : api case " " : api case " " : api case " " : api request // Default attributes mirror the API attributes that would allow this access to the kubelet API attrs := authorizer . Attributes Record { User : u , Verb : api Verb , Namespace : " " , API Group : " " , API Version : " " , Resource : " " , Subresource : " " , Name : string ( n . node Name ) , Resource Request : true , Path : request // Override subresource for specific paths // This allows subdividing access to the kubelet API switch { case is Subpath ( request Path , stats case is Subpath ( request Path , metrics case is Subpath ( request Path , logs case is Subpath ( request Path , spec klog . V ( 5 ) . Infof ( " " , attrs . Get } 
func Validate Has Label ( meta metav1 . Object Meta , fld Path * field . Path , key , expected Value string ) field . Error List { all Errs := field . Error actual if ! found { all Errs = append ( all Errs , field . Required ( fld Path . Child ( " " ) . Key ( key ) , fmt . Sprintf ( " " , expected return all if actual Value != expected Value { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) . Key ( key ) , meta . Labels , fmt . Sprintf ( " " , expected return all } 
func Validate Annotations ( annotations map [ string ] string , fld Path * field . Path ) field . Error List { return apimachineryvalidation . Validate Annotations ( annotations , fld } 
func Validate DNS1123Subdomain ( value string , fld Path * field . Path ) field . Error List { all Errs := field . Error for _ , msg := range validation . Is DNS1123Subdomain ( value ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Tolerations In Pod Annotations ( annotations map [ string ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error tolerations , err := helper . Get Tolerations From Pod if err != nil { all Errs = append ( all Errs , field . Invalid ( fld Path , core . Tolerations Annotation return all if len ( tolerations ) > 0 { all Errs = append ( all Errs , Validate Tolerations ( tolerations , fld Path . Child ( core . Tolerations Annotation return all } 
func Validate Runtime Class Name ( name string , fld Path * field . Path ) field . Error List { var all Errs field . Error for _ , msg := range apimachineryvalidation . Name Is DNS Subdomain ( name , false ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Nonnegative Field ( value int64 , fld Path * field . Path ) field . Error List { return apimachineryvalidation . Validate Nonnegative Field ( value , fld } 
func Validate Positive Quantity Value ( value resource . Quantity , fld Path * field . Path ) field . Error List { all Errs := field . Error if value . Cmp ( resource . Quantity { } ) <= 0 { all Errs = append ( all Errs , field . Invalid ( fld Path , value . String ( ) , is Not Positive Error return all } 
func Validate Object Meta ( meta * metav1 . Object Meta , requires Namespace bool , name Fn Validate Name Func , fld Path * field . Path ) field . Error List { all Errs := apimachineryvalidation . Validate Object Meta ( meta , requires Namespace , apimachineryvalidation . Validate Name Func ( name Fn ) , fld // run additional checks for the finalizer name for i := range meta . Finalizers { all Errs = append ( all Errs , validate Kube Finalizer Name ( string ( meta . Finalizers [ i ] ) , fld return all } 
func Validate Object Meta Update ( new Meta , old Meta * metav1 . Object Meta , fld Path * field . Path ) field . Error List { all Errs := apimachineryvalidation . Validate Object Meta Update ( new Meta , old Meta , fld // run additional checks for the finalizer name for i := range new Meta . Finalizers { all Errs = append ( all Errs , validate Kube Finalizer Name ( string ( new Meta . Finalizers [ i ] ) , fld return all } 
func validate Local Descending Path ( target Path string , fld Path * field . Path ) field . Error List { all Errs := field . Error if path . Is Abs ( target Path ) { all Errs = append ( all Errs , field . Invalid ( fld Path , target all Errs = append ( all Errs , validate Path No Backsteps ( target Path , fld return all } 
func validate Path No Backsteps ( target Path string , fld Path * field . Path ) field . Error List { all Errs := field . Error parts := strings . Split ( filepath . To Slash ( target for _ , item := range parts { if item == " " { all Errs = append ( all Errs , field . Invalid ( fld Path , target return all } 
func validate Mount Propagation ( mount Propagation * core . Mount Propagation Mode , container * core . Container , fld Path * field . Path ) field . Error List { all Errs := field . Error if mount Propagation == nil { return all supported Mount Propagations := sets . New String ( string ( core . Mount Propagation Bidirectional ) , string ( core . Mount Propagation Host To Container ) , string ( core . Mount Propagation if ! supported Mount Propagations . Has ( string ( * mount Propagation ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path , * mount Propagation , supported Mount if container == nil { // The container is not available yet, e.g. during validation of // Pod Preset. Stop validation now, Pod validation will refuse final // Pods with Bidirectional propagation in non-privileged containers. return all privileged := container . Security Context != nil && container . Security Context . Privileged != nil && * container . Security if * mount Propagation == core . Mount Propagation Bidirectional && ! privileged { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func validate Local Non Reserved Path ( target Path string , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , validate Local Descending Path ( target Path , fld // Don't report this error if the check for .. elements already caught it. if strings . Has Prefix ( target Path , " " ) && ! strings . Has Prefix ( target Path , " " ) { all Errs = append ( all Errs , field . Invalid ( fld Path , target return all } 
func Validate Persistent Volume Update ( new Pv , old Pv * core . Persistent Volume ) field . Error List { all Errs := field . Error all Errs = Validate Persistent Volume ( new // Persistent Volume Source should be immutable after creation. if ! apiequality . Semantic . Deep Equal ( new Pv . Spec . Persistent Volume Source , old Pv . Spec . Persistent Volume Source ) { all Errs = append ( all Errs , field . Forbidden ( field . New new Pv . Status = old all Errs = append ( all Errs , Validate Immutable Field ( new Pv . Spec . Volume Mode , old Pv . Spec . Volume Mode , field . New // Allow setting Node Affinity if old Pv Node Affinity was not set if old Pv . Spec . Node Affinity != nil { all Errs = append ( all Errs , Validate Immutable Field ( new Pv . Spec . Node Affinity , old Pv . Spec . Node Affinity , field . New return all } 
func Validate Persistent Volume Claim ( pvc * core . Persistent Volume Claim ) field . Error List { all Errs := Validate Object Meta ( & pvc . Object Meta , true , Validate Persistent Volume Name , field . New all Errs = append ( all Errs , Validate Persistent Volume Claim Spec ( & pvc . Spec , field . New return all } 
func Validate Persistent Volume Claim Spec ( spec * core . Persistent Volume Claim Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( spec . Access Modes ) == 0 { all Errs = append ( all Errs , field . Required ( fld if spec . Selector != nil { all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( spec . Selector , fld for _ , mode := range spec . Access Modes { if mode != core . Read Write Once && mode != core . Read Only Many && mode != core . Read Write Many { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , mode , supported Access storage Value , ok := spec . Resources . Requests [ core . Resource if ! ok { all Errs = append ( all Errs , field . Required ( fld Path . Child ( " " ) . Key ( string ( core . Resource } else { all Errs = append ( all Errs , Validate Resource Quantity Value ( string ( core . Resource Storage ) , storage Value , fld Path . Child ( " " ) . Key ( string ( core . Resource all Errs = append ( all Errs , Validate Positive Quantity Value ( storage Value , fld Path . Child ( " " ) . Key ( string ( core . Resource if spec . Storage Class Name != nil && len ( * spec . Storage Class Name ) > 0 { for _ , msg := range Validate Class Name ( * spec . Storage Class Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * spec . Storage Class if spec . Volume Mode != nil && ! supported Volume Modes . Has ( string ( * spec . Volume Mode ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , * spec . Volume Mode , supported Volume if spec . Data Source != nil { if len ( spec . Data Source . Name ) == 0 { all Errs = append ( all Errs , field . Required ( fld group Kind := schema . Group Kind { Group : " " , Kind : spec . Data if spec . Data Source . API Group != nil { group Kind . Group = string ( * spec . Data Source . API group Kind List := make ( [ ] string , 0 , len ( supported Data Source API Group for grp := range supported Data Source API Group Kinds { group Kind List = append ( group Kind if ! supported Data Source API Group Kinds [ group Kind ] { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , group Kind . String ( ) , group Kind return all } 
func Validate Persistent Volume Claim Update ( new Pvc , old Pvc * core . Persistent Volume Claim ) field . Error List { all Errs := Validate Object Meta Update ( & new Pvc . Object Meta , & old Pvc . Object Meta , field . New all Errs = append ( all Errs , Validate Persistent Volume Claim ( new new Pvc Clone := new Pvc . Deep old Pvc Clone := old Pvc . Deep // PV Controller needs to update PVC.Spec w/ Volume Name. // Claims are immutable in order to enforce quota, range limits, etc. without gaming the system. if len ( old Pvc . Spec . Volume Name ) == 0 { // volume Name changes are allowed once. old Pvc Clone . Spec . Volume Name = new Pvc Clone . Spec . Volume if validate Storage Class Upgrade ( old Pvc Clone . Annotations , new Pvc Clone . Annotations , old Pvc Clone . Spec . Storage Class Name , new Pvc Clone . Spec . Storage Class Name ) { new Pvc Clone . Spec . Storage Class metav1 . Set Meta Data Annotation ( & new Pvc Clone . Object Meta , core . Beta Storage Class Annotation , old Pvc Clone . Annotations [ core . Beta Storage Class } else { // storageclass annotation should be immutable after creation // TODO: remove Beta when no longer needed all Errs = append ( all Errs , Validate Immutable Annotation ( new Pvc . Object Meta . Annotations [ v1 . Beta Storage Class Annotation ] , old Pvc . Object Meta . Annotations [ v1 . Beta Storage Class Annotation ] , v1 . Beta Storage Class Annotation , field . New if utilfeature . Default Feature Gate . Enabled ( features . Expand Persistent Volumes ) { // lets make sure storage values are same. if new Pvc . Status . Phase == core . Claim Bound && new Pvc Clone . Spec . Resources . Requests != nil { new Pvc Clone . Spec . Resources . Requests [ " " ] = old old Size := old new Size := new if ! apiequality . Semantic . Deep Equal ( new Pvc Clone . Spec , old Pvc Clone . Spec ) { all Errs = append ( all Errs , field . Forbidden ( field . New if new Size . Cmp ( old Size ) < 0 { all Errs = append ( all Errs , field . Forbidden ( field . New } else { // changes to Spec are not allowed, but updates to label/and some annotations are OK. // no-op updates pass validation. if ! apiequality . Semantic . Deep Equal ( new Pvc Clone . Spec , old Pvc Clone . Spec ) { all Errs = append ( all Errs , field . Forbidden ( field . New all Errs = append ( all Errs , Validate Immutable Field ( new Pvc . Spec . Volume Mode , old Pvc . Spec . Volume Mode , field . New return all } 
func validate Storage Class Upgrade ( old Annotations , new Annotations map [ string ] string , old Sc Name , new Sc Name * string ) bool { old Sc , old Annotation Exist := old Annotations [ core . Beta Storage Class new Sc In Annotation , new Annotation Exist := new Annotations [ core . Beta Storage Class return old Annotation Exist /* condition 1 */ && old Sc Name == nil /* condition 2*/ && ( new Sc Name != nil && * new Sc Name == old Sc ) /* condition 3 */ && ( ! new Annotation Exist || new Sc In Annotation == old } 
func Validate Persistent Volume Claim Status Update ( new Pvc , old Pvc * core . Persistent Volume Claim ) field . Error List { all Errs := Validate Object Meta Update ( & new Pvc . Object Meta , & old Pvc . Object Meta , field . New if len ( new Pvc . Resource Version ) == 0 { all Errs = append ( all Errs , field . Required ( field . New if len ( new Pvc . Spec . Access Modes ) == 0 { all Errs = append ( all Errs , field . Required ( field . New cap Path := field . New for r , qty := range new Pvc . Status . Capacity { all Errs = append ( all Errs , validate Basic Resource ( qty , cap new Pvc . Spec = old return all } 
func Validate Env ( vars [ ] core . Env Var , fld Path * field . Path ) field . Error List { all Errs := field . Error for i , ev := range vars { idx Path := fld if len ( ev . Name ) == 0 { all Errs = append ( all Errs , field . Required ( idx } else { for _ , msg := range validation . Is Env Var Name ( ev . Name ) { all Errs = append ( all Errs , field . Invalid ( idx all Errs = append ( all Errs , validate Env Var Value From ( ev , idx return all } 
func check Host Port Conflicts ( containers [ ] core . Container , fld Path * field . Path ) field . Error List { all return Accumulate Unique Host Ports ( containers , & all Ports , fld } 
func validate Affinity ( affinity * core . Affinity , fld Path * field . Path ) field . Error List { all Errs := field . Error if affinity != nil { if affinity . Node Affinity != nil { all Errs = append ( all Errs , validate Node Affinity ( affinity . Node Affinity , fld if affinity . Pod Affinity != nil { all Errs = append ( all Errs , validate Pod Affinity ( affinity . Pod Affinity , fld if affinity . Pod Anti Affinity != nil { all Errs = append ( all Errs , validate Pod Anti Affinity ( affinity . Pod Anti Affinity , fld return all } 
func validate Only Added Tolerations ( new Tolerations [ ] core . Toleration , old Tolerations [ ] core . Toleration , fld Path * field . Path ) field . Error List { all Errs := field . Error for _ , old := range old old . Toleration for _ , new := range new Tolerations { new . Toleration if reflect . Deep if ! found { all Errs = append ( all Errs , field . Forbidden ( fld return all all Errs = append ( all Errs , Validate Tolerations ( new Tolerations , fld return all } 
func Validate Tolerations ( tolerations [ ] core . Toleration , fld Path * field . Path ) field . Error List { all Errors := field . Error for i , toleration := range tolerations { idx Path := fld // validate the toleration key if len ( toleration . Key ) > 0 { all Errors = append ( all Errors , unversionedvalidation . Validate Label Name ( toleration . Key , idx // empty toleration key with Exists operator and empty value means match all taints if len ( toleration . Key ) == 0 && toleration . Operator != core . Toleration Op Exists { all Errors = append ( all Errors , field . Invalid ( idx if toleration . Toleration Seconds != nil && toleration . Effect != core . Taint Effect No Execute { all Errors = append ( all Errors , field . Invalid ( idx // validate toleration operator and value switch toleration . Operator { // empty operator means Equal case core . Toleration Op Equal , " " : if errs := validation . Is Valid Label Value ( toleration . Value ) ; len ( errs ) != 0 { all Errors = append ( all Errors , field . Invalid ( idx case core . Toleration Op Exists : if len ( toleration . Value ) > 0 { all Errors = append ( all Errors , field . Invalid ( idx default : valid Values := [ ] string { string ( core . Toleration Op Equal ) , string ( core . Toleration Op all Errors = append ( all Errors , field . Not Supported ( idx Path . Child ( " " ) , toleration . Operator , valid // validate toleration effect, empty toleration effect means match all taint effects if len ( toleration . Effect ) > 0 { all Errors = append ( all Errors , validate Taint Effect ( & toleration . Effect , true , idx return all } 
func validate Containers Only For Pod ( containers [ ] core . Container , fld Path * field . Path ) field . Error List { all Errs := field . Error for i , ctr := range containers { idx Path := fld if len ( ctr . Image ) != len ( strings . Trim Space ( ctr . Image ) ) { all Errs = append ( all Errs , field . Invalid ( idx return all } 
func Validate Pod ( pod * core . Pod ) field . Error List { fld Path := field . New all Errs := Validate Object Meta ( & pod . Object Meta , true , Validate Pod Name , fld all Errs = append ( all Errs , Validate Pod Specific Annotations ( pod . Object Meta . Annotations , & pod . Spec , fld all Errs = append ( all Errs , Validate Pod Spec ( & pod . Spec , field . New // we do additional validation only pertinent for pods and not pod templates // this was done to preserve backwards compatibility spec Path := field . New if pod . Spec . Service Account Name == " " { for vi , volume := range pod . Spec . Volumes { path := spec if volume . Projected != nil { for si , source := range volume . Projected . Sources { sa if source . Service Account Token != nil { all Errs = append ( all Errs , field . Forbidden ( sa all Errs = append ( all Errs , validate Containers Only For Pod ( pod . Spec . Containers , spec all Errs = append ( all Errs , validate Containers Only For Pod ( pod . Spec . Init Containers , spec huge Page Resources := sets . New for i := range pod . Spec . Containers { resource Set := to Container Resources for resource Str := range resource Set { if v1helper . Is Huge Page Resource Name ( v1 . Resource Name ( resource Str ) ) { huge Page Resources . Insert ( resource if len ( huge Page Resources ) > 1 { all Errs = append ( all Errs , field . Invalid ( spec Path , huge Page return all } 
func Validate Pod Spec ( spec * core . Pod Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error vols , v Errs := Validate Volumes ( spec . Volumes , fld all Errs = append ( all Errs , v all Errs = append ( all Errs , validate Containers ( spec . Containers , false , vols , fld all Errs = append ( all Errs , validate Init Containers ( spec . Init Containers , spec . Containers , vols , fld all Errs = append ( all Errs , validate Restart Policy ( & spec . Restart Policy , fld all Errs = append ( all Errs , validate DNS Policy ( & spec . DNS Policy , fld all Errs = append ( all Errs , unversionedvalidation . Validate Labels ( spec . Node Selector , fld all Errs = append ( all Errs , Validate Pod Security Context ( spec . Security Context , spec , fld Path , fld all Errs = append ( all Errs , validate Image Pull Secrets ( spec . Image Pull Secrets , fld all Errs = append ( all Errs , validate Affinity ( spec . Affinity , fld all Errs = append ( all Errs , validate Pod DNS Config ( spec . DNS Config , & spec . DNS Policy , fld all Errs = append ( all Errs , validate Readiness Gates ( spec . Readiness Gates , fld if len ( spec . Service Account Name ) > 0 { for _ , msg := range Validate Service Account Name ( spec . Service Account Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , spec . Service Account if len ( spec . Node Name ) > 0 { for _ , msg := range Validate Node Name ( spec . Node Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , spec . Node if spec . Active Deadline Seconds != nil { value := * spec . Active Deadline if value < 1 || value > math . Max Int32 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , value , validation . Inclusive Range Error ( 1 , math . Max if len ( spec . Hostname ) > 0 { all Errs = append ( all Errs , Validate DNS1123Label ( spec . Hostname , fld if len ( spec . Subdomain ) > 0 { all Errs = append ( all Errs , Validate DNS1123Label ( spec . Subdomain , fld if len ( spec . Tolerations ) > 0 { all Errs = append ( all Errs , Validate Tolerations ( spec . Tolerations , fld if len ( spec . Host Aliases ) > 0 { all Errs = append ( all Errs , Validate Host Aliases ( spec . Host Aliases , fld if len ( spec . Priority Class Name ) > 0 { for _ , msg := range Validate Priority Class Name ( spec . Priority Class Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , spec . Priority Class if spec . Runtime Class Name != nil { all Errs = append ( all Errs , Validate Runtime Class Name ( * spec . Runtime Class Name , fld return all } 
func Validate Node Field Selector Requirement ( req core . Node Selector Requirement , fld Path * field . Path ) field . Error List { all Errs := field . Error switch req . Operator { case core . Node Selector Op In , core . Node Selector Op Not In : if len ( req . Values ) != 1 { all Errs = append ( all Errs , field . Required ( fld default : all Errs = append ( all Errs , field . Invalid ( fld if vf , found := node Field Selector Validators [ req . Key ] ; ! found { all Errs = append ( all Errs , field . Invalid ( fld } else { for i , v := range req . Values { for _ , msg := range vf ( v , false ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Node Selector Term ( term core . Node Selector Term , fld Path * field . Path ) field . Error List { all Errs := field . Error for j , req := range term . Match Expressions { all Errs = append ( all Errs , Validate Node Selector Requirement ( req , fld for j , req := range term . Match Fields { all Errs = append ( all Errs , Validate Node Field Selector Requirement ( req , fld return all } 
func Validate Node Selector ( node Selector * core . Node Selector , fld Path * field . Path ) field . Error List { all Errs := field . Error term Fld Path := fld if len ( node Selector . Node Selector Terms ) == 0 { return append ( all Errs , field . Required ( term Fld for i , term := range node Selector . Node Selector Terms { all Errs = append ( all Errs , Validate Node Selector Term ( term , term Fld return all } 
func validate Topology Selector Label Requirement ( rq core . Topology Selector Label Requirement , fld Path * field . Path ) ( sets . String , field . Error List ) { all Errs := field . Error value values Path := fld if len ( rq . Values ) == 0 { all Errs = append ( all Errs , field . Required ( values // Validate set property of Values field for i , value := range rq . Values { if value Set . Has ( value ) { all Errs = append ( all Errs , field . Duplicate ( values value all Errs = append ( all Errs , unversionedvalidation . Validate Label Name ( rq . Key , fld return value Set , all } 
func Validate Topology Selector Term ( term core . Topology Selector Term , fld Path * field . Path ) ( map [ string ] sets . String , field . Error List ) { all Errs := field . Error expr expr Path := fld // Allow empty Match Label Expressions, in case this field becomes optional in the future. for i , req := range term . Match Label Expressions { idx Path := expr value Set , expr Errs := validate Topology Selector Label Requirement ( req , idx all Errs = append ( all Errs , expr // Validate no duplicate keys exist. if _ , exists := expr Map [ req . Key ] ; exists { all Errs = append ( all Errs , field . Duplicate ( idx expr Map [ req . Key ] = value return expr Map , all } 
func Validate Avoid Pods In Node Annotations ( annotations map [ string ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error v1Avoids , err := v1helper . Get Avoid Pods From Node if err != nil { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , core . Prefer Avoid Pods Annotation return all var avoids core . Avoid if err := corev1 . Convert_v1_Avoid Pods_To_core_Avoid Pods ( & v1Avoids , & avoids , nil ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , core . Prefer Avoid Pods Annotation return all if len ( avoids . Prefer Avoid Pods ) != 0 { for i , pa := range avoids . Prefer Avoid Pods { idx Path := fld Path . Child ( core . Prefer Avoid Pods Annotation all Errs = append ( all Errs , validate Prefer Avoid Pods Entry ( pa , idx return all } 
func validate Prefer Avoid Pods Entry ( avoid Pod Entry core . Prefer Avoid Pods Entry , fld Path * field . Path ) field . Error List { all Errors := field . Error if avoid Pod Entry . Pod Signature . Pod Controller == nil { all Errors = append ( all Errors , field . Required ( fld } else { if * ( avoid Pod Entry . Pod Signature . Pod Controller . Controller ) != true { all Errors = append ( all Errors , field . Invalid ( fld Path . Child ( " " ) . Child ( " " ) . Child ( " " ) , * ( avoid Pod Entry . Pod Signature . Pod return all } 
func validate Pod Affinity Term ( pod Affinity Term core . Pod Affinity Term , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , unversionedvalidation . Validate Label Selector ( pod Affinity Term . Label Selector , fld for _ , name := range pod Affinity Term . Namespaces { for _ , msg := range Validate Namespace Name ( name , false ) { all Errs = append ( all Errs , field . Invalid ( fld if len ( pod Affinity Term . Topology Key ) == 0 { all Errs = append ( all Errs , field . Required ( fld return append ( all Errs , unversionedvalidation . Validate Label Name ( pod Affinity Term . Topology Key , fld } 
func validate Pod Affinity Terms ( pod Affinity Terms [ ] core . Pod Affinity Term , fld Path * field . Path ) field . Error List { all Errs := field . Error for i , pod Affinity Term := range pod Affinity Terms { all Errs = append ( all Errs , validate Pod Affinity Term ( pod Affinity Term , fld return all } 
func validate Weighted Pod Affinity Terms ( weighted Pod Affinity Terms [ ] core . Weighted Pod Affinity Term , fld Path * field . Path ) field . Error List { all Errs := field . Error for j , weighted Term := range weighted Pod Affinity Terms { if weighted Term . Weight <= 0 || weighted Term . Weight > 100 { all Errs = append ( all Errs , field . Invalid ( fld Path . Index ( j ) . Child ( " " ) , weighted all Errs = append ( all Errs , validate Pod Affinity Term ( weighted Term . Pod Affinity Term , fld return all } 
func validate Pod Anti Affinity ( pod Anti Affinity * core . Pod Anti Affinity , fld Path * field . Path ) field . Error List { all Errs := field . Error // TODO:Uncomment below code once Required During Scheduling Required During Execution is implemented. // if pod Anti Affinity.Required During Scheduling Required During Execution != nil { // all Errs = append(all Errs, validate Pod Affinity Terms(pod Anti Affinity.Required During Scheduling Required During Execution, false, // fld Path.Child("required During Scheduling Required During Execution"))...) //} if pod Anti Affinity . Required During Scheduling Ignored During Execution != nil { all Errs = append ( all Errs , validate Pod Affinity Terms ( pod Anti Affinity . Required During Scheduling Ignored During Execution , fld if pod Anti Affinity . Preferred During Scheduling Ignored During Execution != nil { all Errs = append ( all Errs , validate Weighted Pod Affinity Terms ( pod Anti Affinity . Preferred During Scheduling Ignored During Execution , fld return all } 
func validate Node Affinity ( na * core . Node Affinity , fld Path * field . Path ) field . Error List { all Errs := field . Error // TODO: Uncomment the next three lines once Required During Scheduling Required During Execution is implemented. // if na.Required During Scheduling Required During Execution != nil { // all Errs = append(all Errs, Validate Node Selector(na.Required During Scheduling Required During Execution, fld Path.Child("required During Scheduling Required During Execution"))...) // } if na . Required During Scheduling Ignored During Execution != nil { all Errs = append ( all Errs , Validate Node Selector ( na . Required During Scheduling Ignored During Execution , fld if len ( na . Preferred During Scheduling Ignored During Execution ) > 0 { all Errs = append ( all Errs , Validate Preferred Scheduling Terms ( na . Preferred During Scheduling Ignored During Execution , fld return all } 
func validate Pod Affinity ( pod Affinity * core . Pod Affinity , fld Path * field . Path ) field . Error List { all Errs := field . Error // TODO:Uncomment below code once Required During Scheduling Required During Execution is implemented. // if pod Affinity.Required During Scheduling Required During Execution != nil { // all Errs = append(all Errs, validate Pod Affinity Terms(pod Affinity.Required During Scheduling Required During Execution, false, // fld Path.Child("required During Scheduling Required During Execution"))...) //} if pod Affinity . Required During Scheduling Ignored During Execution != nil { all Errs = append ( all Errs , validate Pod Affinity Terms ( pod Affinity . Required During Scheduling Ignored During Execution , fld if pod Affinity . Preferred During Scheduling Ignored During Execution != nil { all Errs = append ( all Errs , validate Weighted Pod Affinity Terms ( pod Affinity . Preferred During Scheduling Ignored During Execution , fld return all } 
func Is Valid Sysctl Name ( name string ) bool { if len ( name ) > Sysctl Max return sysctl Regexp . Match } 
func Validate Pod Security Context ( security Context * core . Pod Security Context , spec * core . Pod Spec , spec Path , fld Path * field . Path ) field . Error List { all Errs := field . Error if security Context != nil { all Errs = append ( all Errs , validate Host Network ( security Context . Host Network , spec . Containers , spec if security Context . FS Group != nil { for _ , msg := range validation . Is Valid Group ID ( * security Context . FS Group ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * ( security Context . FS if security Context . Run As User != nil { for _ , msg := range validation . Is Valid User ID ( * security Context . Run As User ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * ( security Context . Run As if security Context . Run As Group != nil { for _ , msg := range validation . Is Valid Group ID ( * security Context . Run As Group ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * ( security Context . Run As for g , gid := range security Context . Supplemental Groups { for _ , msg := range validation . Is Valid Group ID ( gid ) { all Errs = append ( all Errs , field . Invalid ( fld if security Context . Share Process Namespace != nil && security Context . Host PID && * security Context . Share Process Namespace { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * security Context . Share Process if len ( security Context . Sysctls ) != 0 { all Errs = append ( all Errs , validate Sysctls ( security Context . Sysctls , fld return all } 
func Validate Pod Update ( new Pod , old Pod * core . Pod ) field . Error List { fld Path := field . New all Errs := Validate Object Meta Update ( & new Pod . Object Meta , & old Pod . Object Meta , fld all Errs = append ( all Errs , Validate Pod Specific Annotation Updates ( new Pod , old Pod , fld spec Path := field . New // validate updateable fields: // 1. spec.containers[*].image // 2. spec.init Containers[*].image // 3. spec.active Deadline Seconds container Errs , stop := Validate Container Updates ( new Pod . Spec . Containers , old Pod . Spec . Containers , spec all Errs = append ( all Errs , container if stop { return all container Errs , stop = Validate Container Updates ( new Pod . Spec . Init Containers , old Pod . Spec . Init Containers , spec all Errs = append ( all Errs , container if stop { return all // validate updated spec.active Deadline Seconds. two types of updates are allowed: // 1. from nil to a positive value // 2. from a positive value to a lesser, non-negative value if new Pod . Spec . Active Deadline Seconds != nil { new Active Deadline Seconds := * new Pod . Spec . Active Deadline if new Active Deadline Seconds < 0 || new Active Deadline Seconds > math . Max Int32 { all Errs = append ( all Errs , field . Invalid ( spec Path . Child ( " " ) , new Active Deadline Seconds , validation . Inclusive Range Error ( 0 , math . Max return all if old Pod . Spec . Active Deadline Seconds != nil { old Active Deadline Seconds := * old Pod . Spec . Active Deadline if old Active Deadline Seconds < new Active Deadline Seconds { all Errs = append ( all Errs , field . Invalid ( spec Path . Child ( " " ) , new Active Deadline return all } else if old Pod . Spec . Active Deadline Seconds != nil { all Errs = append ( all Errs , field . Invalid ( spec Path . Child ( " " ) , new Pod . Spec . Active Deadline // handle updateable fields by munging those fields prior to deep equal comparison. munged Pod := * new // munge spec.containers[*].image var new for ix , container := range munged Pod . Spec . Containers { container . Image = old new Containers = append ( new munged Pod . Spec . Containers = new // munge spec.init Containers[*].image var new Init for ix , container := range munged Pod . Spec . Init Containers { container . Image = old Pod . Spec . Init new Init Containers = append ( new Init munged Pod . Spec . Init Containers = new Init // munge spec.active Deadline Seconds munged Pod . Spec . Active Deadline if old Pod . Spec . Active Deadline Seconds != nil { active Deadline Seconds := * old Pod . Spec . Active Deadline munged Pod . Spec . Active Deadline Seconds = & active Deadline // Allow only additions to tolerations updates. munged Pod . Spec . Tolerations = old all Errs = append ( all Errs , validate Only Added Tolerations ( new Pod . Spec . Tolerations , old Pod . Spec . Tolerations , spec if ! apiequality . Semantic . Deep Equal ( munged Pod . Spec , old Pod . Spec ) { // This diff isn't perfect, but it's a helluva lot better an "I'm not going to tell you what the difference is". //TODO: Pinpoint the specific field that causes the invalid error after we have strategic merge diff spec Diff := diff . Object Diff ( munged Pod . Spec , old all Errs = append ( all Errs , field . Forbidden ( spec Path , fmt . Sprintf ( " \n " , spec return all } 
func Validate Container State Transition ( new Statuses , old Statuses [ ] core . Container Status , fldpath * field . Path , restart Policy core . Restart Policy ) field . Error List { all Errs := field . Error // If we should always restart, containers are allowed to leave the terminated state if restart Policy == core . Restart Policy Always { return all for i , old Status := range old Statuses { // Skip any container that is not terminated if old // Skip any container that failed but is allowed to restart if old Status . State . Terminated . Exit Code != 0 && restart Policy == core . Restart Policy On for _ , new Status := range new Statuses { if old Status . Name == new Status . Name && new Status . State . Terminated == nil { all Errs = append ( all return all } 
func Validate Pod Status Update ( new Pod , old Pod * core . Pod ) field . Error List { fld Path := field . New all Errs := Validate Object Meta Update ( & new Pod . Object Meta , & old Pod . Object Meta , fld all Errs = append ( all Errs , Validate Pod Specific Annotation Updates ( new Pod , old Pod , fld all Errs = append ( all Errs , validate Pod Conditions ( new Pod . Status . Conditions , fld fld Path = field . New if new Pod . Spec . Node Name != old Pod . Spec . Node Name { all Errs = append ( all Errs , field . Forbidden ( fld if new Pod . Status . Nominated Node Name != old Pod . Status . Nominated Node Name && len ( new Pod . Status . Nominated Node Name ) > 0 { for _ , msg := range Validate Node Name ( new Pod . Status . Nominated Node Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , new Pod . Status . Nominated Node // If pod should not restart, make sure the status update does not transition // any terminated containers to a non-terminated state. all Errs = append ( all Errs , Validate Container State Transition ( new Pod . Status . Container Statuses , old Pod . Status . Container Statuses , fld Path . Child ( " " ) , old Pod . Spec . Restart all Errs = append ( all Errs , Validate Container State Transition ( new Pod . Status . Init Container Statuses , old Pod . Status . Init Container Statuses , fld Path . Child ( " " ) , old Pod . Spec . Restart // For status update we ignore changes to pod spec. new Pod . Spec = old return all } 
func validate Pod Conditions ( conditions [ ] core . Pod Condition , fld Path * field . Path ) field . Error List { all Errs := field . Error system Conditions := sets . New String ( string ( core . Pod Scheduled ) , string ( core . Pod Ready ) , string ( core . Pod for i , condition := range conditions { if system for _ , msg := range validation . Is Qualified Name ( string ( condition . Type ) ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Pod Binding ( binding * core . Binding ) field . Error List { all Errs := field . Error if len ( binding . Target . Kind ) != 0 && binding . Target . Kind != " " { // TODO: When validation becomes versioned, this gets more complicated. all Errs = append ( all Errs , field . Not Supported ( field . New if len ( binding . Target . Name ) == 0 { // TODO: When validation becomes versioned, this gets more complicated. all Errs = append ( all Errs , field . Required ( field . New return all } 
func Validate Pod Template ( pod * core . Pod Template ) field . Error List { all Errs := Validate Object Meta ( & pod . Object Meta , true , Validate Pod Name , field . New all Errs = append ( all Errs , Validate Pod Template Spec ( & pod . Template , field . New return all } 
func Validate Pod Template Update ( new Pod , old Pod * core . Pod Template ) field . Error List { all Errs := Validate Object Meta Update ( & new Pod . Object Meta , & old Pod . Object Meta , field . New all Errs = append ( all Errs , Validate Pod Template Spec ( & new Pod . Template , field . New return all } 
func Validate Service ( service * core . Service ) field . Error List { all Errs := Validate Object Meta ( & service . Object Meta , true , Validate Service Name , field . New spec Path := field . New is Headless Service := service . Spec . Cluster IP == core . Cluster IP if len ( service . Spec . Ports ) == 0 && ! is Headless Service && service . Spec . Type != core . Service Type External Name { all Errs = append ( all Errs , field . Required ( spec switch service . Spec . Type { case core . Service Type Load // This is a workaround for broken cloud environments that // over-open firewalls. Hopefully it can go away when more clouds // understand containers better. if port . Port == ports . Kubelet Port { port Path := spec all Errs = append ( all Errs , field . Invalid ( port Path , port . Port , fmt . Sprintf ( " " , ports . Kubelet if service . Spec . Cluster IP == " " { all Errs = append ( all Errs , field . Invalid ( spec Path . Child ( " " ) , service . Spec . Cluster case core . Service Type Node Port : if service . Spec . Cluster IP == " " { all Errs = append ( all Errs , field . Invalid ( spec Path . Child ( " " ) , service . Spec . Cluster case core . Service Type External Name : if service . Spec . Cluster IP != " " { all Errs = append ( all Errs , field . Forbidden ( spec if len ( service . Spec . External Name ) > 0 { all Errs = append ( all Errs , Validate DNS1123Subdomain ( service . Spec . External Name , spec } else { all Errs = append ( all Errs , field . Required ( spec all Port ports Path := spec for i := range service . Spec . Ports { port Path := ports all Errs = append ( all Errs , validate Service Port ( & service . Spec . Ports [ i ] , len ( service . Spec . Ports ) > 1 , is Headless Service , & all Port Names , port if service . Spec . Selector != nil { all Errs = append ( all Errs , unversionedvalidation . Validate Labels ( service . Spec . Selector , spec if len ( service . Spec . Session Affinity ) == 0 { all Errs = append ( all Errs , field . Required ( spec } else if ! supported Session Affinity Type . Has ( string ( service . Spec . Session Affinity ) ) { all Errs = append ( all Errs , field . Not Supported ( spec Path . Child ( " " ) , service . Spec . Session Affinity , supported Session Affinity if service . Spec . Session Affinity == core . Service Affinity Client IP { all Errs = append ( all Errs , validate Client IP Affinity Config ( service . Spec . Session Affinity Config , spec } else if service . Spec . Session Affinity == core . Service Affinity None { if service . Spec . Session Affinity Config != nil { all Errs = append ( all Errs , field . Forbidden ( spec Path . Child ( " " ) , fmt . Sprintf ( " " , string ( core . Service Affinity if helper . Is Service IP Set ( service ) { if ip := net . Parse IP ( service . Spec . Cluster IP ) ; ip == nil { all Errs = append ( all Errs , field . Invalid ( spec Path . Child ( " " ) , service . Spec . Cluster ip Path := spec for i , ip := range service . Spec . External I Ps { idx Path := ip if msgs := validation . Is Valid IP ( ip ) ; len ( msgs ) != 0 { for i := range msgs { all Errs = append ( all Errs , field . Invalid ( idx } else { all Errs = append ( all Errs , validate Non Special IP ( ip , idx if len ( service . Spec . Type ) == 0 { all Errs = append ( all Errs , field . Required ( spec } else if ! supported Service Type . Has ( string ( service . Spec . Type ) ) { all Errs = append ( all Errs , field . Not Supported ( spec Path . Child ( " " ) , service . Spec . Type , supported Service if service . Spec . Type == core . Service Type Load Balancer { ports Path := spec include Protocols := sets . New for i := range service . Spec . Ports { port Path := ports if ! supported Port Protocols . Has ( string ( service . Spec . Ports [ i ] . Protocol ) ) { all Errs = append ( all Errs , field . Invalid ( port } else { include if include Protocols . Len ( ) > 1 { all Errs = append ( all Errs , field . Invalid ( ports if service . Spec . Type == core . Service Type Cluster IP { ports Path := spec for i := range service . Spec . Ports { port Path := ports if service . Spec . Ports [ i ] . Node Port != 0 { all Errs = append ( all Errs , field . Forbidden ( port // Check for duplicate Node Ports, considering (protocol,port) pairs ports Path = spec node Ports := make ( map [ core . Service if port . Node port Path := ports var key core . Service key . Node Port = port . Node _ , found := node if found { all Errs = append ( all Errs , field . Duplicate ( port Path . Child ( " " ) , port . Node node // Check for duplicate Ports, considering (protocol,port) pairs ports Path = spec ports := make ( map [ core . Service for i , port := range service . Spec . Ports { port Path := ports key := core . Service if found { all Errs = append ( all Errs , field . Duplicate ( port // Validate Source Range field and annotation _ , ok := service . Annotations [ core . Annotation Load Balancer Source Ranges if len ( service . Spec . Load Balancer Source Ranges ) > 0 || ok { var field if len ( service . Spec . Load Balancer Source Ranges ) > 0 { field Path = spec val = fmt . Sprintf ( " " , service . Spec . Load Balancer Source } else { field Path = field . New Path ( " " , " " ) . Key ( core . Annotation Load Balancer Source Ranges val = service . Annotations [ core . Annotation Load Balancer Source Ranges if service . Spec . Type != core . Service Type Load Balancer { all Errs = append ( all Errs , field . Forbidden ( field _ , err := apiservice . Get Load Balancer Source if err != nil { all Errs = append ( all Errs , field . Invalid ( field all Errs = append ( all Errs , validate Service External Traffic Fields return all } 
func validate Service External Traffic Fields Value ( service * core . Service ) field . Error List { all Errs := field . Error // Check first class fields. if service . Spec . External Traffic Policy != " " && service . Spec . External Traffic Policy != core . Service External Traffic Policy Type Cluster && service . Spec . External Traffic Policy != core . Service External Traffic Policy Type Local { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) . Child ( " " ) , service . Spec . External Traffic Policy , fmt . Sprintf ( " " , core . Service External Traffic Policy Type Cluster , core . Service External Traffic Policy Type if service . Spec . Health Check Node Port < 0 { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) . Child ( " " ) , service . Spec . Health Check Node return all } 
func Validate Service External Traffic Fields Combination ( service * core . Service ) field . Error List { all Errs := field . Error if service . Spec . Type != core . Service Type Load Balancer && service . Spec . Type != core . Service Type Node Port && service . Spec . External Traffic Policy != " " { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , service . Spec . External Traffic if ! apiservice . Needs Health Check ( service ) && service . Spec . Health Check Node Port != 0 { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , service . Spec . Health Check Node return all } 
func Validate Service Update ( service , old Service * core . Service ) field . Error List { all Errs := Validate Object Meta Update ( & service . Object Meta , & old Service . Object Meta , field . New // Cluster IP should be immutable for services using it (every type other than External Name) // which do not have Cluster IP assigned yet (empty string value) if service . Spec . Type != core . Service Type External Name { if old Service . Spec . Type != core . Service Type External Name && old Service . Spec . Cluster IP != " " { all Errs = append ( all Errs , Validate Immutable Field ( service . Spec . Cluster IP , old Service . Spec . Cluster IP , field . New all Errs = append ( all Errs , Validate return all } 
func Validate Service Status Update ( service , old Service * core . Service ) field . Error List { all Errs := Validate Object Meta Update ( & service . Object Meta , & old Service . Object Meta , field . New all Errs = append ( all Errs , Validate Load Balancer Status ( & service . Status . Load Balancer , field . New return all } 
func Validate Replication Controller ( controller * core . Replication Controller ) field . Error List { all Errs := Validate Object Meta ( & controller . Object Meta , true , Validate Replication Controller Name , field . New all Errs = append ( all Errs , Validate Replication Controller Spec ( & controller . Spec , field . New return all } 
func Validate Replication Controller Update ( controller , old Controller * core . Replication Controller ) field . Error List { all Errs := Validate Object Meta Update ( & controller . Object Meta , & old Controller . Object Meta , field . New all Errs = append ( all Errs , Validate Replication Controller Spec ( & controller . Spec , field . New return all } 
func Validate Replication Controller Status Update ( controller , old Controller * core . Replication Controller ) field . Error List { all Errs := Validate Object Meta Update ( & controller . Object Meta , & old Controller . Object Meta , field . New all Errs = append ( all Errs , Validate Replication Controller Status ( controller . Status , field . New return all } 
func Validate Non Empty Selector ( selector Map map [ string ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error selector := labels . Set ( selector Map ) . As if selector . Empty ( ) { all Errs = append ( all Errs , field . Required ( fld return all } 
func Validate Pod Template Spec For RC ( template * core . Pod Template Spec , selector Map map [ string ] string , replicas int32 , fld Path * field . Path ) field . Error List { all Errs := field . Error if template == nil { all Errs = append ( all Errs , field . Required ( fld } else { selector := labels . Set ( selector Map ) . As if ! selector . Matches ( labels ) { all Errs = append ( all Errs , field . Invalid ( fld all Errs = append ( all Errs , Validate Pod Template Spec ( template , fld if replicas > 1 { all Errs = append ( all Errs , Validate Read Only Persistent Disks ( template . Spec . Volumes , fld // Restart Policy has already been first-order validated as per Validate Pod Template Spec(). if template . Spec . Restart Policy != core . Restart Policy Always { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " , " " ) , template . Spec . Restart Policy , [ ] string { string ( core . Restart Policy if template . Spec . Active Deadline Seconds != nil { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func Validate Taints In Node Annotations ( annotations map [ string ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error taints , err := helper . Get Taints From Node if err != nil { all Errs = append ( all Errs , field . Invalid ( fld Path , core . Taints Annotation return all if len ( taints ) > 0 { all Errs = append ( all Errs , validate Node Taints ( taints , fld Path . Child ( core . Taints Annotation return all } 
func validate Node Taints ( taints [ ] core . Taint , fld Path * field . Path ) field . Error List { all Errors := field . Error unique Taints := map [ core . Taint for i , curr Taint := range taints { idx Path := fld // validate the taint key all Errors = append ( all Errors , unversionedvalidation . Validate Label Name ( curr Taint . Key , idx // validate the taint value if errs := validation . Is Valid Label Value ( curr Taint . Value ) ; len ( errs ) != 0 { all Errors = append ( all Errors , field . Invalid ( idx Path . Child ( " " ) , curr // validate the taint effect all Errors = append ( all Errors , validate Taint Effect ( & curr Taint . Effect , false , idx // validate if taint is unique by <key, effect> if len ( unique Taints [ curr Taint . Effect ] ) > 0 && unique Taints [ curr Taint . Effect ] . Has ( curr Taint . Key ) { duplicated Error := field . Duplicate ( idx Path , curr duplicated all Errors = append ( all Errors , duplicated // add taint to existing Taints for uniqueness check if len ( unique Taints [ curr Taint . Effect ] ) == 0 { unique Taints [ curr unique Taints [ curr Taint . Effect ] . Insert ( curr return all } 
func Validate Node ( node * core . Node ) field . Error List { fld Path := field . New all Errs := Validate Object Meta ( & node . Object Meta , false , Validate Node Name , fld all Errs = append ( all Errs , Validate Node Specific Annotations ( node . Object Meta . Annotations , fld if len ( node . Spec . Taints ) > 0 { all Errs = append ( all Errs , validate Node Taints ( node . Spec . Taints , fld // Only validate spec. // All status fields are optional and can be updated later. // That said, if specified, we need to ensure they are valid. all Errs = append ( all Errs , Validate Node if len ( node . Spec . Pod CIDR ) != 0 { _ , err := Validate CIDR ( node . Spec . Pod if err != nil { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , node . Spec . Pod return all } 
func Validate Node Resources ( node * core . Node ) field . Error List { all Errs := field . Error // Validate resource quantities in capacity. huge Page Sizes := sets . New for k , v := range node . Status . Capacity { res Path := field . New all Errs = append ( all Errs , Validate Resource Quantity Value ( string ( k ) , v , res // track any huge page size that has a positive value if helper . Is Huge Page Resource Name ( k ) && v . Value ( ) > int64 ( 0 ) { huge Page if len ( huge Page Sizes ) > 1 { all Errs = append ( all Errs , field . Invalid ( res // Validate resource quantities in allocatable. huge Page Sizes = sets . New for k , v := range node . Status . Allocatable { res Path := field . New all Errs = append ( all Errs , Validate Resource Quantity Value ( string ( k ) , v , res // track any huge page size that has a positive value if helper . Is Huge Page Resource Name ( k ) && v . Value ( ) > int64 ( 0 ) { huge Page if len ( huge Page Sizes ) > 1 { all Errs = append ( all Errs , field . Invalid ( res return all } 
func Validate Node Update ( node , old Node * core . Node ) field . Error List { fld Path := field . New all Errs := Validate Object Meta Update ( & node . Object Meta , & old Node . Object Meta , fld all Errs = append ( all Errs , Validate Node Specific Annotations ( node . Object Meta . Annotations , fld // TODO: Enable the code once we have better core object.status update model. Currently, // anyone can update node status. // if !apiequality.Semantic.Deep Equal(node.Status, core.Node Status{}) { // all Errs = append(all Errs, field.Invalid("status", node.Status, "must be empty")) // } all Errs = append ( all Errs , Validate Node // Validate no duplicate addresses in node status. addresses := make ( map [ core . Node for i , address := range node . Status . Addresses { if _ , ok := addresses [ address ] ; ok { all Errs = append ( all Errs , field . Duplicate ( field . New if len ( old Node . Spec . Pod CIDR ) == 0 { // Allow the controller manager to assign a CIDR to a node if it doesn't have one. old Node . Spec . Pod CIDR = node . Spec . Pod } else { if old Node . Spec . Pod CIDR != node . Spec . Pod CIDR { all Errs = append ( all Errs , field . Forbidden ( field . New // Allow controller manager updating provider ID when not set if len ( old Node . Spec . Provider ID ) == 0 { old Node . Spec . Provider ID = node . Spec . Provider } else { if old Node . Spec . Provider ID != node . Spec . Provider ID { all Errs = append ( all Errs , field . Forbidden ( field . New if node . Spec . Config Source != nil { all Errs = append ( all Errs , validate Node Config Source Spec ( node . Spec . Config Source , field . New old Node . Spec . Config Source = node . Spec . Config if node . Status . Config != nil { all Errs = append ( all Errs , validate Node Config Status ( node . Status . Config , field . New old // TODO: move reset function to its own location // Ignore metadata changes now that they have been tested old Node . Object Meta = node . Object // Allow users to update capacity old // Allow users to unschedule node old // Clear status old // update taints if len ( node . Spec . Taints ) > 0 { all Errs = append ( all Errs , validate Node Taints ( node . Spec . Taints , fld old // We made allowed changes to old Node, and now we compare old Node to node. Any remaining differences indicate changes to protected fields. // TODO: Add a 'real' error type for this error and provide print actual diffs. if ! apiequality . Semantic . Deep Equal ( old Node , node ) { klog . V ( 4 ) . Infof ( " " , old all Errs = append ( all Errs , field . Forbidden ( field . New return all } 
func validate Node Config Source Spec ( source * core . Node Config Source , fld Path * field . Path ) field . Error List { all Errs := field . Error if source . Config all Errs = append ( all Errs , validate Config Map Node Config Source Spec ( source . Config Map , fld // add more subfields here in the future as they are added to Node Config Source // exactly one reference subfield must be non-nil if count != 1 { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func validate Config Map Node Config Source Spec ( source * core . Config Map Node Config Source , fld Path * field . Path ) field . Error List { all Errs := field . Error // uid and resource Version must not be set in spec if string ( source . UID ) != " " { all Errs = append ( all Errs , field . Forbidden ( fld if source . Resource Version != " " { all Errs = append ( all Errs , field . Forbidden ( fld return append ( all Errs , validate Config Map Node Config Source ( source , fld } 
func validate Node Config Status ( status * core . Node Config Status , fld Path * field . Path ) field . Error List { all Errs := field . Error if status . Assigned != nil { all Errs = append ( all Errs , validate Node Config Source Status ( status . Assigned , fld if status . Active != nil { all Errs = append ( all Errs , validate Node Config Source Status ( status . Active , fld if status . Last Known Good != nil { all Errs = append ( all Errs , validate Node Config Source Status ( status . Last Known Good , fld return all } 
func validate Node Config Source Status ( source * core . Node Config Source , fld Path * field . Path ) field . Error List { all Errs := field . Error if source . Config all Errs = append ( all Errs , validate Config Map Node Config Source Status ( source . Config Map , fld // add more subfields here in the future as they are added to Node Config Source // exactly one reference subfield must be non-nil if count != 1 { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func validate Config Map Node Config Source Status ( source * core . Config Map Node Config Source , fld Path * field . Path ) field . Error List { all Errs := field . Error // uid and resource Version must be set in status if string ( source . UID ) == " " { all Errs = append ( all Errs , field . Required ( fld if source . Resource Version == " " { all Errs = append ( all Errs , field . Required ( fld return append ( all Errs , validate Config Map Node Config Source ( source , fld } 
func validate Config Map Node Config Source ( source * core . Config Map Node Config Source , fld Path * field . Path ) field . Error List { all Errs := field . Error // validate target configmap namespace if source . Namespace == " " { all Errs = append ( all Errs , field . Required ( fld } else { for _ , msg := range Validate Name Func ( Validate Namespace Name ) ( source . Namespace , false ) { all Errs = append ( all Errs , field . Invalid ( fld // validate target configmap name if source . Name == " " { all Errs = append ( all Errs , field . Required ( fld } else { for _ , msg := range Validate Name Func ( Validate Config Map Name ) ( source . Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld // validate kubelet Config Key against rules for config Map key names if source . Kubelet Config Key == " " { all Errs = append ( all Errs , field . Required ( fld } else { for _ , msg := range validation . Is Config Map Key ( source . Kubelet Config Key ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , source . Kubelet Config return all } 
func validate Container Resource Name ( value string , fld Path * field . Path ) field . Error List { all Errs := validate Resource Name ( value , fld if len ( strings . Split ( value , " " ) ) == 1 { if ! helper . Is Standard Container Resource Name ( value ) { return append ( all Errs , field . Invalid ( fld } else if ! helper . Is Native Resource ( core . Resource Name ( value ) ) { if ! helper . Is Extended Resource Name ( core . Resource Name ( value ) ) { return append ( all Errs , field . Invalid ( fld return all } 
func Validate Resource Quota Resource Name ( value string , fld Path * field . Path ) field . Error List { all Errs := validate Resource Name ( value , fld if len ( strings . Split ( value , " " ) ) == 1 { if ! helper . Is Standard Quota Resource Name ( value ) { return append ( all Errs , field . Invalid ( fld Path , value , is Invalid Quota return all } 
func validate Limit Range Type Name ( value string , fld Path * field . Path ) field . Error List { all Errs := field . Error for _ , msg := range validation . Is Qualified Name ( value ) { all Errs = append ( all Errs , field . Invalid ( fld if len ( all Errs ) != 0 { return all if len ( strings . Split ( value , " " ) ) == 1 { if ! helper . Is Standard Limit Range Type ( value ) { return append ( all Errs , field . Invalid ( fld return all } 
func validate Limit Range Resource Name ( limit Type core . Limit Type , value string , fld Path * field . Path ) field . Error List { switch limit Type { case core . Limit Type Pod , core . Limit Type Container : return validate Container Resource Name ( value , fld default : return validate Resource Name ( value , fld } 
func Validate Service Account ( service Account * core . Service Account ) field . Error List { all Errs := Validate Object Meta ( & service Account . Object Meta , true , Validate Service Account Name , field . New return all } 
func Validate Secret Update ( new Secret , old Secret * core . Secret ) field . Error List { all Errs := Validate Object Meta Update ( & new Secret . Object Meta , & old Secret . Object Meta , field . New if len ( new Secret . Type ) == 0 { new Secret . Type = old all Errs = append ( all Errs , Validate Immutable Field ( new Secret . Type , old Secret . Type , field . New all Errs = append ( all Errs , Validate Secret ( new return all } 
func Validate Config Map ( cfg * core . Config Map ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Object Meta ( & cfg . Object Meta , true , Validate Config Map Name , field . New total for key , value := range cfg . Data { for _ , msg := range validation . Is Config Map Key ( key ) { all Errs = append ( all Errs , field . Invalid ( field . New // check if we have a duplicate key in the other bag if _ , is Value := cfg . Binary Data [ key ] ; is all Errs = append ( all Errs , field . Invalid ( field . New total for key , value := range cfg . Binary Data { for _ , msg := range validation . Is Config Map Key ( key ) { all Errs = append ( all Errs , field . Invalid ( field . New total if total Size > core . Max Secret Size { // pass back "" to indicate that the error refers to the whole object. all Errs = append ( all Errs , field . Too Long ( field . New Path ( " " ) , cfg , core . Max Secret return all } 
func Validate Config Map Update ( new Cfg , old Cfg * core . Config Map ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Object Meta Update ( & new Cfg . Object Meta , & old Cfg . Object Meta , field . New all Errs = append ( all Errs , Validate Config Map ( new return all } 
func Validate Resource Requirements ( requirements * core . Resource Requirements , fld Path * field . Path ) field . Error List { all Errs := field . Error lim Path := fld req Path := fld lim Contains CPU Or req Contains CPU Or lim Contains Huge req Contains Huge supported Qo S Compute Resources := sets . New String ( string ( core . Resource CPU ) , string ( core . Resource for resource Name , quantity := range requirements . Limits { fld Path := lim Path . Key ( string ( resource // Validate resource name. all Errs = append ( all Errs , validate Container Resource Name ( string ( resource Name ) , fld // Validate resource quantity. all Errs = append ( all Errs , Validate Resource Quantity Value ( string ( resource Name ) , quantity , fld if helper . Is Huge Page Resource Name ( resource Name ) { lim Contains Huge if supported Qo S Compute Resources . Has ( string ( resource Name ) ) { lim Contains CPU Or for resource Name , quantity := range requirements . Requests { fld Path := req Path . Key ( string ( resource // Validate resource name. all Errs = append ( all Errs , validate Container Resource Name ( string ( resource Name ) , fld // Validate resource quantity. all Errs = append ( all Errs , Validate Resource Quantity Value ( string ( resource Name ) , quantity , fld // Check that request <= limit. limit Quantity , exists := requirements . Limits [ resource if exists { // For non overcommitable resources, not only requests can't exceed limits, they also can't be lower, i.e. must be equal. if quantity . Cmp ( limit Quantity ) != 0 && ! helper . Is Overcommit Allowed ( resource Name ) { all Errs = append ( all Errs , field . Invalid ( req Path , quantity . String ( ) , fmt . Sprintf ( " " , resource } else if quantity . Cmp ( limit Quantity ) > 0 { all Errs = append ( all Errs , field . Invalid ( req Path , quantity . String ( ) , fmt . Sprintf ( " " , resource } else if ! helper . Is Overcommit Allowed ( resource Name ) { all Errs = append ( all Errs , field . Required ( lim if helper . Is Huge Page Resource Name ( resource Name ) { req Contains Huge if supported Qo S Compute Resources . Has ( string ( resource Name ) ) { req Contains CPU Or if ! lim Contains CPU Or Memory && ! req Contains CPU Or Memory && ( req Contains Huge Pages || lim Contains Huge Pages ) { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func validate Resource Quota Scopes ( resource Quota Spec * core . Resource Quota Spec , fld * field . Path ) field . Error List { all Errs := field . Error if len ( resource Quota Spec . Scopes ) == 0 { return all hard Limits := sets . New for k := range resource Quota Spec . Hard { hard fld scope Set := sets . New for _ , scope := range resource Quota Spec . Scopes { if ! helper . Is Standard Resource Quota Scope ( string ( scope ) ) { all Errs = append ( all Errs , field . Invalid ( fld Path , resource Quota for _ , k := range hard Limits . List ( ) { if helper . Is Standard Quota Resource Name ( k ) && ! helper . Is Resource Quota Scope Valid For Resource ( scope , k ) { all Errs = append ( all Errs , field . Invalid ( fld Path , resource Quota scope invalid Scope Pairs := [ ] sets . String { sets . New String ( string ( core . Resource Quota Scope Best Effort ) , string ( core . Resource Quota Scope Not Best Effort ) ) , sets . New String ( string ( core . Resource Quota Scope Terminating ) , string ( core . Resource Quota Scope Not for _ , invalid Scope Pair := range invalid Scope Pairs { if scope Set . Has All ( invalid Scope Pair . List ( ) ... ) { all Errs = append ( all Errs , field . Invalid ( fld Path , resource Quota return all } 
func validate Scoped Resource Selector Requirement ( resource Quota Spec * core . Resource Quota Spec , fld * field . Path ) field . Error List { all Errs := field . Error hard Limits := sets . New for k := range resource Quota Spec . Hard { hard fld scope Set := sets . New for _ , req := range resource Quota Spec . Scope Selector . Match Expressions { if ! helper . Is Standard Resource Quota Scope ( string ( req . Scope Name ) ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , req . Scope for _ , k := range hard Limits . List ( ) { if helper . Is Standard Quota Resource Name ( k ) && ! helper . Is Resource Quota Scope Valid For Resource ( req . Scope Name , k ) { all Errs = append ( all Errs , field . Invalid ( fld Path , resource Quota Spec . Scope switch req . Scope Name { case core . Resource Quota Scope Best Effort , core . Resource Quota Scope Not Best Effort , core . Resource Quota Scope Terminating , core . Resource Quota Scope Not Terminating : if req . Operator != core . Scope Selector Op Exists { all Errs = append ( all Errs , field . Invalid ( fld switch req . Operator { case core . Scope Selector Op In , core . Scope Selector Op Not In : if len ( req . Values ) == 0 { all Errs = append ( all Errs , field . Required ( fld case core . Scope Selector Op Exists , core . Scope Selector Op Does Not Exist : if len ( req . Values ) != 0 { all Errs = append ( all Errs , field . Invalid ( fld default : all Errs = append ( all Errs , field . Invalid ( fld scope Set . Insert ( string ( req . Scope invalid Scope Pairs := [ ] sets . String { sets . New String ( string ( core . Resource Quota Scope Best Effort ) , string ( core . Resource Quota Scope Not Best Effort ) ) , sets . New String ( string ( core . Resource Quota Scope Terminating ) , string ( core . Resource Quota Scope Not for _ , invalid Scope Pair := range invalid Scope Pairs { if scope Set . Has All ( invalid Scope Pair . List ( ) ... ) { all Errs = append ( all Errs , field . Invalid ( fld Path , resource Quota return all } 
func validate Scope Selector ( resource Quota Spec * core . Resource Quota Spec , fld * field . Path ) field . Error List { all Errs := field . Error if resource Quota Spec . Scope Selector == nil { return all all Errs = append ( all Errs , validate Scoped Resource Selector Requirement ( resource Quota return all } 
func Validate Resource Quota ( resource Quota * core . Resource Quota ) field . Error List { all Errs := Validate Object Meta ( & resource Quota . Object Meta , true , Validate Resource Quota Name , field . New all Errs = append ( all Errs , Validate Resource Quota Spec ( & resource Quota . Spec , field . New all Errs = append ( all Errs , Validate Resource Quota Status ( & resource Quota . Status , field . New return all } 
func Validate Resource Quota Update ( new Resource Quota , old Resource Quota * core . Resource Quota ) field . Error List { all Errs := Validate Object Meta Update ( & new Resource Quota . Object Meta , & old Resource Quota . Object Meta , field . New all Errs = append ( all Errs , Validate Resource Quota Spec ( & new Resource Quota . Spec , field . New // ensure scopes cannot change, and that resources are still valid for scope fld Path := field . New old Scopes := sets . New new Scopes := sets . New for _ , scope := range new Resource Quota . Spec . Scopes { new for _ , scope := range old Resource Quota . Spec . Scopes { old if ! old Scopes . Equal ( new Scopes ) { all Errs = append ( all Errs , field . Invalid ( fld Path , new Resource Quota . Spec . Scopes , field Immutable Error new Resource Quota . Status = old Resource return all } 
func validate Finalizer Name ( string Value string , fld Path * field . Path ) field . Error List { all Errs := apimachineryvalidation . Validate Finalizer Name ( string Value , fld all Errs = append ( all Errs , validate Kube Finalizer Name ( string Value , fld return all } 
func validate Kube Finalizer Name ( string Value string , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( strings . Split ( string Value , " " ) ) == 1 { if ! helper . Is Standard Finalizer Name ( string Value ) { return append ( all Errs , field . Invalid ( fld Path , string return all } 
func Validate Namespace Update ( new Namespace * core . Namespace , old Namespace * core . Namespace ) field . Error List { all Errs := Validate Object Meta Update ( & new Namespace . Object Meta , & old Namespace . Object Meta , field . New new Namespace . Spec . Finalizers = old new Namespace . Status = old return all } 
func Validate Namespace Status Update ( new Namespace , old Namespace * core . Namespace ) field . Error List { all Errs := Validate Object Meta Update ( & new Namespace . Object Meta , & old Namespace . Object Meta , field . New new Namespace . Spec = old if new Namespace . Deletion Timestamp . Is Zero ( ) { if new Namespace . Status . Phase != core . Namespace Active { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , new } else { if new Namespace . Status . Phase != core . Namespace Terminating { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " , " " ) , new return all } 
func Validate Endpoints ( endpoints * core . Endpoints ) field . Error List { all Errs := Validate Object Meta ( & endpoints . Object Meta , true , Validate Endpoints Name , field . New all Errs = append ( all Errs , Validate Endpoints Specific Annotations ( endpoints . Annotations , field . New all Errs = append ( all Errs , validate Endpoint Subsets ( endpoints . Subsets , field . New return all } 
func Validate Endpoints Update ( new Endpoints , old Endpoints * core . Endpoints ) field . Error List { all Errs := Validate Object Meta Update ( & new Endpoints . Object Meta , & old Endpoints . Object Meta , field . New all Errs = append ( all Errs , validate Endpoint Subsets ( new Endpoints . Subsets , field . New all Errs = append ( all Errs , Validate Endpoints Specific Annotations ( new Endpoints . Annotations , field . New return all } 
func Validate Security Context ( sc * core . Security Context , fld Path * field . Path ) field . Error List { all Errs := field . Error //this should only be true for testing since Security Context is defaulted by the core if sc == nil { return all if sc . Privileged != nil { if * sc . Privileged && ! capabilities . Get ( ) . Allow Privileged { all Errs = append ( all Errs , field . Forbidden ( fld if sc . Run As User != nil { for _ , msg := range validation . Is Valid User ID ( * sc . Run As User ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * sc . Run As if sc . Run As Group != nil { for _ , msg := range validation . Is Valid Group ID ( * sc . Run As Group ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , * sc . Run As if sc . Proc Mount != nil { if err := Validate Proc Mount Type ( fld Path . Child ( " " ) , * sc . Proc Mount ) ; err != nil { all Errs = append ( all if sc . Allow Privilege Escalation != nil && ! * sc . Allow Privilege Escalation { if sc . Privileged != nil && * sc . Privileged { all Errs = append ( all Errs , field . Invalid ( fld if sc . Capabilities != nil { for _ , cap := range sc . Capabilities . Add { if string ( cap ) == " " { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Load Balancer Status ( status * core . Load Balancer Status , fld Path * field . Path ) field . Error List { all Errs := field . Error for i , ingress := range status . Ingress { idx Path := fld if len ( ingress . IP ) > 0 { if is IP := ( net . Parse IP ( ingress . IP ) != nil ) ; ! is IP { all Errs = append ( all Errs , field . Invalid ( idx if len ( ingress . Hostname ) > 0 { for _ , msg := range validation . Is DNS1123Subdomain ( ingress . Hostname ) { all Errs = append ( all Errs , field . Invalid ( idx if is IP := ( net . Parse IP ( ingress . Hostname ) != nil ) ; is IP { all Errs = append ( all Errs , field . Invalid ( idx return all } 
func validate Volume Node Affinity ( node Affinity * core . Volume Node Affinity , fld Path * field . Path ) ( bool , field . Error List ) { all Errs := field . Error if node Affinity == nil { return false , all if node Affinity . Required != nil { all Errs = append ( all Errs , Validate Node Selector ( node Affinity . Required , fld } else { all Errs = append ( all Errs , field . Required ( fld return true , all } 
func Validate CIDR ( cidr string ) ( * net . IP Net , error ) { _ , net , err := net . Parse } 
func Validate Proc Mount Type ( fld Path * field . Path , proc Mount Type core . Proc Mount Type ) * field . Error { switch proc Mount Type { case core . Default Proc Mount , core . Unmasked Proc default : return field . Not Supported ( fld Path , proc Mount Type , [ ] string { string ( core . Default Proc Mount ) , string ( core . Unmasked Proc } 
func New Requirement ( key string , op selection . Operator , vals [ ] string ) ( * Requirement , error ) { if err := validate Label switch op { case selection . In , selection . Not case selection . Equals , selection . Double Equals , selection . Not case selection . Exists , selection . Does Not case selection . Greater Than , selection . Less for i := range vals { if _ , err := strconv . Parse for i := range vals { if err := validate Label return & Requirement { key : key , operator : op , str } 
func ( r * Requirement ) Matches ( ls Labels ) bool { switch r . operator { case selection . In , selection . Equals , selection . Double return r . has case selection . Not In , selection . Not return ! r . has case selection . Does Not case selection . Greater Than , selection . Less ls Value , err := strconv . Parse // There should be only one str Value in r.str Values, and can be converted to a integer. if len ( r . str Values ) != 1 { klog . V ( 10 ) . Infof ( " " , len ( r . str var r for i := range r . str Values { r Value , err = strconv . Parse Int ( r . str if err != nil { klog . V ( 10 ) . Infof ( " " , r . str return ( r . operator == selection . Greater Than && ls Value > r Value ) || ( r . operator == selection . Less Than && ls Value < r } 
for i := range r . str Values { ret . Insert ( r . str } 
if r . operator == selection . Does Not Exist { buffer . Write buffer . Write switch r . operator { case selection . Equals : buffer . Write case selection . Double Equals : buffer . Write case selection . Not Equals : buffer . Write case selection . In : buffer . Write case selection . Not In : buffer . Write case selection . Greater Than : buffer . Write case selection . Less Than : buffer . Write case selection . Exists , selection . Does Not switch r . operator { case selection . In , selection . Not In : buffer . Write if len ( r . str Values ) == 1 { buffer . Write String ( r . str } else { // only > 1 since == 0 prohibited by New Requirement // normalizes value order on output, without mutating the in-memory selector representation // also avoids normalization when it is not required, and ensures we do not mutate shared data buffer . Write String ( strings . Join ( safe Sort ( r . str switch r . operator { case selection . In , selection . Not In : buffer . Write } 
func safe Sort ( in [ ] string ) [ ] string { if sort . Strings Are } 
func ( lsel internal Selector ) Add ( reqs ... Requirement ) Selector { var sel internal sort . Sort ( By } 
func ( lsel internal } 
func ( lsel internal } 
} 
func ( l * Lexer ) scan Special Symbol ( ) ( Token , string ) { last Scanned Item := Scanned Special Symbol Loop : for { switch ch := l . read ( ) ; { case ch == 0 : break Special Symbol case is Special if token , ok := string2token [ string ( buffer ) ] ; ok { last Scanned Item = Scanned } else if last Scanned break Special Symbol break Special Symbol if last Scanned Item . tok == 0 { return Error return last Scanned Item . tok , last Scanned } 
func ( l * Lexer ) skip White Spaces ( ch byte ) byte { for { if ! is } 
func ( p * Parser ) lookahead ( context Parser Context ) ( Token , string ) { tok , lit := p . scanned Items [ p . position ] . tok , p . scanned if context == Values { switch tok { case In Token , Not In Token : tok = Identifier } 
p . scanned Items = append ( p . scanned Items , Scanned if token == End Of String } 
func ( p * Parser ) parse ( ) ( internal Selector , error ) { p . scan ( ) // init scanned var requirements internal switch tok { case Identifier Token , Does Not Exist Token : r , err := p . parse switch t { case End Of String case Comma if t2 != Identifier Token && t2 != Does Not Exist case End Of String } 
func ( p * Parser ) parse Operator ( ) ( op selection . Operator , err error ) { tok , lit := p . consume ( Key And switch tok { // Does Not Exist Token shouldn't be here because it's a unary operator, not a binary operator case In case Equals case Double Equals Token : op = selection . Double case Greater Than Token : op = selection . Greater case Less Than Token : op = selection . Less case Not In Token : op = selection . Not case Not Equals Token : op = selection . Not } 
func ( p * Parser ) parse if tok != Open Par switch tok { case Identifier Token , Comma Token : s , err := p . parse Identifiers if tok , _ = p . consume ( Values ) ; tok != Closed Par case Closed Par return sets . New } 
func ( p * Parser ) parse Identifiers List ( ) ( sets . String , error ) { s := sets . New switch tok { case Identifier switch tok2 { case Comma case Closed Par case Comma if tok2 == Closed Par Token { s . Insert ( " " ) // to handle ,) Double "" removed by String if tok2 == Comma s . Insert ( " " ) // to handle ,, Double "" removed by String } 
func ( p * Parser ) parse Exact Value ( ) ( sets . String , error ) { s := sets . New if tok == End Of String Token || tok == Comma if tok == Identifier } 
func Parse ( selector string ) ( Selector , error ) { parsed if err == nil { return parsed } 
func parse ( selector string ) ( internal sort . Sort ( By return internal } 
func Selector From Set ( ls Set ) Selector { if ls == nil || len ( ls ) == 0 { return internal var requirements internal for label , value := range ls { r , err := New } else { //TODO: double check errors when input comes from serialization? return internal // sort to have deterministic string representation sort . Sort ( By } 
func Selector From Validated Set ( ls Set ) Selector { if ls == nil || len ( ls ) == 0 { return internal var requirements internal for label , value := range ls { requirements = append ( requirements , Requirement { key : label , operator : selection . Equals , str // sort to have deterministic string representation sort . Sort ( By } 
func Sorted By Group And Version ( servers [ ] * API Service ) [ ] [ ] * API Service { servers By Group Priority Minimum := By Group Priority sort . Sort ( servers By Group Priority ret := [ ] [ ] * API for _ , curr := range servers By Group Priority Minimum { // check to see if we already have an entry for this group existing for j , group In Return := range ret { if group In Return [ 0 ] . Spec . Group == curr . Spec . Group { existing if existing Index >= 0 { ret [ existing Index ] = append ( ret [ existing sort . Sort ( By Version Priority ( ret [ existing ret = append ( ret , [ ] * API } 
func API Service Name To Group Version ( api Service Name string ) schema . Group Version { tokens := strings . Split N ( api Service return schema . Group } 
func New Local Available API Service Condition ( ) API Service Condition { return API Service Condition { Type : Available , Status : Condition True , Last Transition } 
func Get API Service Condition By Type ( api Service * API Service , condition Type API Service Condition Type ) * API Service Condition { for i := range api Service . Status . Conditions { if api Service . Status . Conditions [ i ] . Type == condition Type { return & api } 
func Set API Service Condition ( api Service * API Service , new Condition API Service Condition ) { existing Condition := Get API Service Condition By Type ( api Service , new if existing Condition == nil { api Service . Status . Conditions = append ( api Service . Status . Conditions , new if existing Condition . Status != new Condition . Status { existing Condition . Status = new existing Condition . Last Transition Time = new Condition . Last Transition existing Condition . Reason = new existing Condition . Message = new } 
func Is API Service Condition True ( api Service * API Service , condition Type API Service Condition Type ) bool { condition := Get API Service Condition By Type ( api Service , condition return condition != nil && condition . Status == Condition } 
func create User Strategy ( opts * policy . Run As User Strategy Options ) ( user . Run As User Strategy , error ) { switch opts . Rule { case policy . Run As User Strategy Must Run As : return user . New Must Run case policy . Run As User Strategy Must Run As Non Root : return user . New Run As Non case policy . Run As User Strategy Run As Any : return user . New Run As } 
func create Run As Group Strategy ( opts * policy . Run As Group Strategy Options ) ( group . Group Strategy , error ) { if opts == nil { return group . New Run As switch opts . Rule { case policy . Run As Group Strategy Must Run As : return group . New Must Run case policy . Run As Group Strategy Run As Any : return group . New Run As case policy . Run As Group Strategy May Run As : return group . New May Run } 
func create SE Linux Strategy ( opts * policy . SE Linux Strategy Options ) ( selinux . SE Linux Strategy , error ) { switch opts . Rule { case policy . SE Linux Strategy Must Run As : return selinux . New Must Run case policy . SE Linux Strategy Run As Any : return selinux . New Run As } 
func create App Armor Strategy ( psp * policy . Pod Security Policy ) ( apparmor . Strategy , error ) { return apparmor . New } 
func create Seccomp Strategy ( psp * policy . Pod Security Policy ) ( seccomp . Strategy , error ) { return seccomp . New } 
func create FS Group Strategy ( opts * policy . FS Group Strategy Options ) ( group . Group Strategy , error ) { switch opts . Rule { case policy . FS Group Strategy Run As Any : return group . New Run As case policy . FS Group Strategy May Run As : return group . New May Run case policy . FS Group Strategy Must Run As : return group . New Must Run } 
func create Supplemental Group Strategy ( opts * policy . Supplemental Groups Strategy Options ) ( group . Group Strategy , error ) { switch opts . Rule { case policy . Supplemental Groups Strategy Run As Any : return group . New Run As case policy . Supplemental Groups Strategy May Run As : return group . New May Run case policy . Supplemental Groups Strategy Must Run As : return group . New Must Run } 
func create Capabilities Strategy ( default Add Caps , required Drop Caps , allowed Caps [ ] corev1 . Capability ) ( capabilities . Strategy , error ) { return capabilities . New Default Capabilities ( default Add Caps , required Drop Caps , allowed } 
func create Sysctls Strategy ( safe Whitelist , allowed Unsafe Sysctls , forbidden Sysctls [ ] string ) sysctl . Sysctls Strategy { return sysctl . New Must Match Patterns ( safe Whitelist , allowed Unsafe Sysctls , forbidden } 
func ( c * Wardle V1alpha1Client ) REST return c . rest } 
func Firewall To G Cloud Create Cmd ( fw * compute . Firewall , project ID string ) string { args := firewall To Gcloud Args ( fw , project return fmt . Sprintf ( " " , fw . Name , get Name From } 
func Firewall To G Cloud Update Cmd ( fw * compute . Firewall , project ID string ) string { args := firewall To Gcloud Args ( fw , project } 
func canonicalize Instance } 
func last Component ( s string ) string { last Slash := strings . Last if last Slash != - 1 { s = s [ last } 
func Get GCE Region ( zone string ) ( string , error ) { ix := strings . Last } 
func split Provider ID ( provider ID string ) ( project , zone , instance string , err error ) { matches := provider IDRE . Find String Submatch ( provider } 
func handle Alpha Network Tier Get Error ( err error ) ( string , error ) { if is Forbidden ( err ) { // Network tier is still an Alpha feature in GCP, and not every project // is whitelisted to access the API. If we cannot access the API, just // assume the tier is premium. return cloud . Network Tier Default . To GCE } 
func contains CIDR ( outer , inner * net . IP Net ) bool { return outer . Contains ( first IP In Range ( inner ) ) && outer . Contains ( last IP In } 
func first IP In Range ( ip Net * net . IP Net ) net . IP { return ip Net . IP . Mask ( ip } 
func last IP In Range ( cidr * net . IP } 
func subnets In CIDR ( subnets [ ] * compute . Subnetwork , cidr * net . IP for _ , subnet := range subnets { _ , subnet Range , err := net . Parse CIDR ( subnet . Ip Cidr if err != nil { return nil , fmt . Errorf ( " " , subnet . Ip Cidr if contains CIDR ( cidr , subnet } 
func New From Tokens ( tokens map [ string ] * user . Default } 
func Is Not Registered _ , ok := err . ( * not Registered } 
func Is Missing _ , ok := err . ( * missing Kind } 
func Is Missing _ , ok := err . ( * missing Version } 
func New Strict Decoding Error ( message string , data string ) error { return & strict Decoding } 
func Is Strict Decoding _ , ok := err . ( * strict Decoding } 
func New Unstructured Negotiated Serializer ( ) runtime . Negotiated Serializer { return unstructured Negotiated Serializer { scheme : scheme , typer : New Unstructured Object Typer ( ) , creator : New Unstructured } 
func ( g * Cloud ) Get Global Backend Service ( name string ) ( * compute . Backend Service , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric v , err := g . c . Backend Services ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Get Beta Global Backend Service ( name string ) ( * computebeta . Backend Service , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric Context With Version ( " " , " " , compute Beta v , err := g . c . Beta Backend Services ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Get Alpha Global Backend Service ( name string ) ( * computealpha . Backend Service , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric Context With Version ( " " , " " , compute Alpha v , err := g . c . Alpha Backend Services ( ) . Get ( ctx , meta . Global } 
func ( g * Cloud ) Delete Global Backend Service ( name string ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric return mc . Observe ( g . c . Backend Services ( ) . Delete ( ctx , meta . Global } 
func ( g * Cloud ) Create Global Backend Service ( bg * compute . Backend Service ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric return mc . Observe ( g . c . Backend Services ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) Create Beta Global Backend Service ( bg * computebeta . Backend Service ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric Context With Version ( " " , " " , compute Beta return mc . Observe ( g . c . Beta Backend Services ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) Create Alpha Global Backend Service ( bg * computealpha . Backend Service ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric Context With Version ( " " , " " , compute Alpha return mc . Observe ( g . c . Alpha Backend Services ( ) . Insert ( ctx , meta . Global } 
func ( g * Cloud ) List Global Backend Services ( ) ( [ ] * compute . Backend Service , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric v , err := g . c . Backend } 
func ( g * Cloud ) Get Global Backend Service Health ( name string , instance Group Link string ) ( * compute . Backend Service Group Health , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric group Ref := & compute . Resource Group Reference { Group : instance Group v , err := g . c . Backend Services ( ) . Get Health ( ctx , meta . Global Key ( name ) , group } 
func ( g * Cloud ) Get Region Backend Service ( name , region string ) ( * compute . Backend Service , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric v , err := g . c . Region Backend Services ( ) . Get ( ctx , meta . Regional } 
func ( g * Cloud ) Delete Region Backend Service ( name , region string ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric return mc . Observe ( g . c . Region Backend Services ( ) . Delete ( ctx , meta . Regional } 
func ( g * Cloud ) Create Region Backend Service ( bg * compute . Backend Service , region string ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric return mc . Observe ( g . c . Region Backend Services ( ) . Insert ( ctx , meta . Regional } 
func ( g * Cloud ) List Region Backend Services ( region string ) ( [ ] * compute . Backend Service , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric v , err := g . c . Region Backend } 
func ( g * Cloud ) Get Regional Backend Service Health ( name , region string , instance Group Link string ) ( * compute . Backend Service Group Health , error ) { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric ref := & compute . Resource Group Reference { Group : instance Group v , err := g . c . Region Backend Services ( ) . Get Health ( ctx , meta . Regional } 
func ( g * Cloud ) Set Security Policy For Beta Global Backend Service ( backend Service Name string , security Policy Reference * computebeta . Security Policy Reference ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric Context With Version ( " " , " " , compute Beta return mc . Observe ( g . c . Beta Backend Services ( ) . Set Security Policy ( ctx , meta . Global Key ( backend Service Name ) , security Policy } 
func ( g * Cloud ) Set Security Policy For Alpha Global Backend Service ( backend Service Name string , security Policy Reference * computealpha . Security Policy Reference ) error { ctx , cancel := cloud . Context With Call mc := new Backend Service Metric Context With Version ( " " , " " , compute Alpha return mc . Observe ( g . c . Alpha Backend Services ( ) . Set Security Policy ( ctx , meta . Global Key ( backend Service Name ) , security Policy } 
func in Active List ( sj batchv1beta1 . Cron } 
func get Parent UID From Job ( j batchv1 . Job ) ( types . UID , bool ) { controller Ref := metav1 . Get Controller if controller if controller return controller } 
func group Jobs By Parent ( js [ ] batchv1 . Job ) map [ types . UID ] [ ] batchv1 . Job { jobs By for _ , job := range js { parent UID , found := get Parent UID From jobs By Sj [ parent UID ] = append ( jobs By Sj [ parent return jobs By } 
func get Recent Unmet Schedule Times ( sj batchv1beta1 . Cron sched , err := cron . Parse var earliest if sj . Status . Last Schedule Time != nil { earliest Time = sj . Status . Last Schedule } else { // If none found, then this is either a recently created scheduled Job, // or the active/completed info was somehow lost (contract for status // in kubernetes says it may need to be recreated), or that we have // started a job, but have not noticed it yet (distributed systems can // have arbitrary delays). In any case, use the creation time of the // Cron Job as last known start time. earliest Time = sj . Object Meta . Creation if sj . Spec . Starting Deadline Seconds != nil { // Controller is not going to schedule anything below this point scheduling Deadline := now . Add ( - time . Second * time . Duration ( * sj . Spec . Starting Deadline if scheduling Deadline . After ( earliest Time ) { earliest Time = scheduling if earliest for t := sched . Next ( earliest // An object might miss several starts. For example, if // controller gets wedged on friday at 5:01pm when everyone has // gone home, and someone comes in on tuesday AM and discovers // the problem and restarts the controller, then all the hourly // jobs, more than 80 of them for one hourly scheduled Job, should // all start running with no further intervention (if the scheduled Job // allows concurrency and late starts). // // However, if there is a bug somewhere, or incorrect clock // on controller's server or apiservers (for setting creation } 
func get Job From Template ( sj * batchv1beta1 . Cron Job , scheduled Time time . Time ) ( * batchv1 . Job , error ) { labels := copy Labels ( & sj . Spec . Job annotations := copy Annotations ( & sj . Spec . Job // We want job names for a given nominal start time to have a deterministic name to avoid the same job being created twice name := fmt . Sprintf ( " " , sj . Name , get Time Hash ( scheduled job := & batchv1 . Job { Object Meta : metav1 . Object Meta { Labels : labels , Annotations : annotations , Name : name , Owner References : [ ] metav1 . Owner Reference { * metav1 . New Controller Ref ( sj , controller if err := legacyscheme . Scheme . Convert ( & sj . Spec . Job } 
func Is Job Finished ( j * batchv1 . Job ) bool { is Finished , _ := get Finished return is } 
func New Nested Pending Operations ( exponential Back Off On Error bool ) Nested Pending Operations { g := & nested Pending Operations { operations : [ ] operation { } , exponential Back Off On Error : exponential Back Off On g . cond = sync . New } 
func ( grm * nested Pending Operations ) is Operation Exists ( volume Name v1 . Unique Volume Name , pod Name types . Unique Pod Name ) ( bool , int ) { // If volume Name is empty, operation can be executed concurrently if volume Name == Empty Unique Volume for previous Op Index , previous Op := range grm . operations { if previous Op . volume Name != volume if previous Op . pod Name != Empty Unique Pod Name && pod Name != Empty Unique Pod Name && previous Op . pod Name != pod // Match return true , previous Op } 
func New Cgroup Notifier ( path , attribute string , threshold int64 ) ( Cgroup epfd , err = unix . Epoll return & linux Cgroup } 
func wait ( epfd , eventfd int , timeout time . Duration ) ( bool , error ) { events := make ( [ ] unix . Epoll Event , num Fd timeout n , err := unix . Epoll Wait ( epfd , events , timeout if n > num Fd } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Priority Class ) ( nil ) , ( * scheduling . Priority Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Priority Class_To_scheduling_Priority Class ( a . ( * v1 . Priority Class ) , b . ( * scheduling . Priority if err := s . Add Generated Conversion Func ( ( * scheduling . Priority Class ) ( nil ) , ( * v1 . Priority Class ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheduling_Priority Class_To_v1_Priority Class ( a . ( * scheduling . Priority Class ) , b . ( * v1 . Priority if err := s . Add Generated Conversion Func ( ( * v1 . Priority Class List ) ( nil ) , ( * scheduling . Priority Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Priority Class List_To_scheduling_Priority Class List ( a . ( * v1 . Priority Class List ) , b . ( * scheduling . Priority Class if err := s . Add Generated Conversion Func ( ( * scheduling . Priority Class List ) ( nil ) , ( * v1 . Priority Class List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_scheduling_Priority Class List_To_v1_Priority Class List ( a . ( * scheduling . Priority Class List ) , b . ( * v1 . Priority Class } 
func Convert_v1_Priority Class_To_scheduling_Priority Class ( in * v1 . Priority Class , out * scheduling . Priority Class , s conversion . Scope ) error { return auto Convert_v1_Priority Class_To_scheduling_Priority } 
func Convert_scheduling_Priority Class_To_v1_Priority Class ( in * scheduling . Priority Class , out * v1 . Priority Class , s conversion . Scope ) error { return auto Convert_scheduling_Priority Class_To_v1_Priority } 
func Convert_v1_Priority Class List_To_scheduling_Priority Class List ( in * v1 . Priority Class List , out * scheduling . Priority Class List , s conversion . Scope ) error { return auto Convert_v1_Priority Class List_To_scheduling_Priority Class } 
func Convert_scheduling_Priority Class List_To_v1_Priority Class List ( in * scheduling . Priority Class List , out * v1 . Priority Class List , s conversion . Scope ) error { return auto Convert_scheduling_Priority Class List_To_v1_Priority Class } 
func Convert_v1alpha1_Resource Quota Controller Configuration_To_config_Resource Quota Controller Configuration ( in * v1alpha1 . Resource Quota Controller Configuration , out * config . Resource Quota Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Resource Quota Controller Configuration_To_config_Resource Quota Controller } 
func Convert_config_Resource Quota Controller Configuration_To_v1alpha1_Resource Quota Controller Configuration ( in * config . Resource Quota Controller Configuration , out * v1alpha1 . Resource Quota Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Resource Quota Controller Configuration_To_v1alpha1_Resource Quota Controller } 
func ( s * Deprecated Insecure Serving if s . Bind Port < 0 || s . Bind Port > 65335 { errors = append ( errors , fmt . Errorf ( " " , s . Bind } 
func ( s * Deprecated Insecure Serving Options ) Add Unqualified Flags ( fs * pflag . Flag fs . IP Var ( & s . Bind Address , " " , s . Bind fs . Mark fs . Int Var ( & s . Bind Port , " " , s . Bind fs . Mark } 
func ( s * Deprecated Insecure Serving Options ) Apply To ( c * * server . Deprecated Insecure Serving if s . Bind listen := Create if s . Listen Func != nil { listen = s . Listen addr := net . Join Host Port ( s . Bind Address . String ( ) , fmt . Sprintf ( " " , s . Bind s . Listener , s . Bind Port , err = listen ( s . Bind * c = & server . Deprecated Insecure Serving } 
func ( s * Deprecated Insecure Serving Options With Loopback ) Apply To ( insecure Serving Info * * server . Deprecated Insecure Serving Info , loopback Client Config * * rest . Config ) error { if s == nil || s . Deprecated Insecure Serving Options == nil || insecure Serving if err := s . Deprecated Insecure Serving Options . Apply To ( insecure Serving if * insecure Serving Info == nil || loopback Client secure Loopback Client Config , err := ( * insecure Serving Info ) . New Loopback Client switch { // if we failed and there's no fallback loopback client config, we need to fail case err != nil && * loopback Client // if we failed, but we already have a fallback loopback client config (usually insecure), allow it case err != nil && * loopback Client Config != nil : default : * loopback Client Config = secure Loopback Client } 
func New Control Plane Join Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Short : " " , Example : control Plane Join Example , Phases : [ ] workflow . Phase { { Name : " " , Short : " " , Inherit Flags : get Control Plane Join Phase Flags ( " " ) , Run All Siblings : true , } , new Etcd Local Subphase ( ) , new Update Status Subphase ( ) , new Mark Control Plane } 
func Make Regexp } 
func ( f * Filter Server ) Handler For ( delegate http . Handler ) * Filter } 
func extract Host ( header string ) ( host string ) { host , _ , err := net . Split Host } 
func make Upgrade Transport ( config * rest . Config , keepalive time . Duration ) ( proxy . Upgrade Request Round Tripper , error ) { transport Config , err := config . Transport tls Config , err := transport . TLS Config For ( transport rt := utilnet . Set Old Transport Defaults ( & http . Transport { TLS Client Config : tls Config , Dial Context : ( & net . Dialer { Timeout : 30 * time . Second , Keep Alive : keepalive , } ) . Dial upgrader , err := transport . HTTP Wrappers For Config ( transport Config , proxy . Mirror return proxy . New Upgrade Request Round } 
func New Server ( filebase string , api Proxy Prefix string , static Prefix string , filter * Filter if ! strings . Has transport , err := rest . Transport upgrade Transport , err := make Upgrade proxy := proxy . New Upgrade Aware proxy . Upgrade Transport = upgrade proxy . Use Request proxy if filter != nil { proxy Server = filter . Handler For ( proxy if ! strings . Has Prefix ( api Proxy Prefix , " " ) { proxy Server = strip Leave Slash ( api Proxy Prefix , proxy mux := http . New Serve mux . Handle ( api Proxy Prefix , proxy if filebase != " " { // Require user to explicitly request this behavior rather than // serving their working directory by default. mux . Handle ( static Prefix , new File Handler ( static } 
} 
func strip Leave Slash ( prefix string , h http . Handler ) http . Handler { return http . Handler Func ( func ( w http . Response Writer , req * http . Request ) { p := strings . Trim if len ( p ) >= len ( req . URL . Path ) { http . Not h . Serve } 
func rollbacker ( rest Client Getter genericclioptions . REST Client Getter , mapping * meta . REST Mapping ) ( kubectl . Rollbacker , error ) { client Config , err := rest Client Getter . To REST external , err := kubernetes . New For Config ( client return kubectl . Rollbacker For ( mapping . Group Version Kind . Group } 
func Drop Disabled Fields ( psp Spec , old PSP Spec * policy . Pod Security Policy Spec ) { if ! utilfeature . Default Feature Gate . Enabled ( features . Proc Mount Type ) && ! allowed Proc Mount Types In Use ( old PSP Spec ) { psp Spec . Allowed Proc Mount if ! utilfeature . Default Feature Gate . Enabled ( features . Run As Group ) && ( old PSP Spec == nil || old PSP Spec . Run As Group == nil ) { psp Spec . Run As if ! utilfeature . Default Feature Gate . Enabled ( features . Sysctls ) && ! sysctls In Use ( old PSP Spec ) { psp Spec . Allowed Unsafe psp Spec . Forbidden if ! utilfeature . Default Feature Gate . Enabled ( features . CSI Inline Volume ) { psp Spec . Allowed CSI if ! utilfeature . Default Feature Gate . Enabled ( features . Runtime Class ) && ( old PSP Spec == nil || old PSP Spec . Runtime Class == nil ) { psp Spec . Runtime } 
func ( c * Fake Cluster Role Bindings ) Get ( name string , options v1 . Get Options ) ( result * rbacv1 . Cluster Role Binding , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( clusterrolebindings Resource , name ) , & rbacv1 . Cluster Role return obj . ( * rbacv1 . Cluster Role } 
func ( c * Fake Cluster Role Bindings ) List ( opts v1 . List Options ) ( result * rbacv1 . Cluster Role Binding List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( clusterrolebindings Resource , clusterrolebindings Kind , opts ) , & rbacv1 . Cluster Role Binding label , _ , _ := testing . Extract From List list := & rbacv1 . Cluster Role Binding List { List Meta : obj . ( * rbacv1 . Cluster Role Binding List ) . List for _ , item := range obj . ( * rbacv1 . Cluster Role Binding } 
func New Cmd Kustomize ( streams genericclioptions . IO Streams ) * cobra . Command { var o kustomize cmd := & cobra . Command { Use : " " , Short : i18n . T ( " " ) , Long : kustomize Long , Example : kustomize Example , Run return kustomize . Run Kustomize Build ( streams . Out , fs . Make Real FS ( ) , o . kustomization } 
func ( o * kustomize if len ( args ) == 0 { o . kustomization } else { o . kustomization } 
func New Inter Pod Affinity Priority ( info predicates . Node Info , node Lister algorithm . Node Lister , pod Lister algorithm . Pod Lister , hard Pod Affinity Weight int32 ) Priority Function { inter Pod Affinity := & Inter Pod Affinity { info : info , node Lister : node Lister , pod Lister : pod Lister , hard Pod Affinity Weight : hard Pod Affinity return inter Pod Affinity . Calculate Inter Pod Affinity } 
func ( ipa * Inter Pod Affinity ) Calculate Inter Pod Affinity Priority ( pod * v1 . Pod , node Name To Info map [ string ] * schedulernodeinfo . Node Info , nodes [ ] * v1 . Node ) ( schedulerapi . Host Priority has Affinity Constraints := affinity != nil && affinity . Pod has Anti Affinity Constraints := affinity != nil && affinity . Pod Anti // priority Map stores the mapping from node name to so-far computed score of // the node. pm := new Pod Affinity Priority all Node Names := make ( [ ] string , 0 , len ( node Name To lazy Init := has Affinity Constraints || has Anti Affinity for name := range node Name To Info { all Node Names = append ( all Node // if pod has affinity defined, or target node has affinity Pods if lazy Init || len ( node Name To Info [ name ] . Pods With // convert the topology key based weights to the node name based weights var max Count , min process Pod := func ( existing Pod * v1 . Pod ) error { existing Pod Node , err := ipa . info . Get Node Info ( existing Pod . Spec . Node if err != nil { if apierrors . Is Not Found ( err ) { klog . Errorf ( " " , existing Pod . Spec . Node existing Pod Affinity := existing existing Has Affinity Constraints := existing Pod Affinity != nil && existing Pod Affinity . Pod existing Has Anti Affinity Constraints := existing Pod Affinity != nil && existing Pod Affinity . Pod Anti if has Affinity Constraints { // For every soft pod affinity term of <pod>, if <existing Pod> matches the term, // increment <pm.counts> for every node in the cluster with the same <term.Topology Key> // value as that of <existing Pods>`s node by the term`s weight. terms := affinity . Pod Affinity . Preferred During Scheduling Ignored During pm . process Terms ( terms , pod , existing Pod , existing Pod if has Anti Affinity Constraints { // For every soft pod anti-affinity term of <pod>, if <existing Pod> matches the term, // decrement <pm.counts> for every node in the cluster with the same <term.Topology Key> // value as that of <existing Pod>`s node by the term`s weight. terms := affinity . Pod Anti Affinity . Preferred During Scheduling Ignored During pm . process Terms ( terms , pod , existing Pod , existing Pod if existing Has Affinity Constraints { // For every hard pod affinity term of <existing Pod>, if <pod> matches the term, // increment <pm.counts> for every node in the cluster with the same <term.Topology Key> // value as that of <existing Pod>'s node by the constant <ipa.hard Pod Affinity Weight> if ipa . hard Pod Affinity Weight > 0 { terms := existing Pod Affinity . Pod Affinity . Required During Scheduling Ignored During // TODO: Uncomment this block when implement Required During Scheduling Required During Execution. //if len(existing Pod Affinity.Pod Affinity.Required During Scheduling Required During Execution) != 0 { // terms = append(terms, existing Pod Affinity.Pod Affinity.Required During Scheduling Required During Execution...) //} for _ , term := range terms { pm . process Term ( & term , existing Pod , pod , existing Pod Node , int64 ( ipa . hard Pod Affinity // For every soft pod affinity term of <existing Pod>, if <pod> matches the term, // increment <pm.counts> for every node in the cluster with the same <term.Topology Key> // value as that of <existing Pod>'s node by the term's weight. terms := existing Pod Affinity . Pod Affinity . Preferred During Scheduling Ignored During pm . process Terms ( terms , existing Pod , pod , existing Pod if existing Has Anti Affinity Constraints { // For every soft pod anti-affinity term of <existing Pod>, if <pod> matches the term, // decrement <pm.counts> for every node in the cluster with the same <term.Topology Key> // value as that of <existing Pod>'s node by the term's weight. terms := existing Pod Affinity . Pod Anti Affinity . Preferred During Scheduling Ignored During pm . process Terms ( terms , existing Pod , pod , existing Pod process Node := func ( i int ) { node Info := node Name To Info [ all Node if node Info . Node ( ) != nil { if has Affinity Constraints || has Anti Affinity Constraints { // We need to process all the pods. for _ , existing Pod := range node Info . Pods ( ) { if err := process Pod ( existing Pod ) ; err != nil { pm . set } else { // The pod doesn't have any constraints - we need to check only existing // ones that have some. for _ , existing Pod := range node Info . Pods With Affinity ( ) { if err := process Pod ( existing Pod ) ; err != nil { pm . set workqueue . Parallelize Until ( context . TODO ( ) , 16 , len ( all Node Names ) , process if pm . first Error != nil { return nil , pm . first if * pm . counts [ node . Name ] > max Count { max if * pm . counts [ node . Name ] < min Count { min // calculate final priority score for each node result := make ( schedulerapi . Host Priority max Min Diff := max Count - min for _ , node := range nodes { f if max Min Diff > 0 && pm . counts [ node . Name ] != nil { f Score = float64 ( schedulerapi . Max Priority ) * ( float64 ( * pm . counts [ node . Name ] - min Count ) / float64 ( max Count - min result = append ( result , schedulerapi . Host Priority { Host : node . Name , Score : int ( f if klog . V ( 10 ) { klog . Infof ( " " , pod . Name , node . Name , int ( f } 
func get } 
func get Params ( generic for key , value := range generic Params { str Val , is if ! is params [ key ] = str } 
func get Args ( generic val , found := generic if found { var is args , is if ! is delete ( generic } 
func populate Resource List V1 ( spec string ) ( v1 . Resource result := v1 . Resource resource for _ , resource Statement := range resource Statements { parts := strings . Split ( resource if len ( parts ) != 2 { return nil , fmt . Errorf ( " " , resource resource Name := v1 . Resource resource Quantity , err := resource . Parse result [ resource Name ] = resource } 
func Handle Resource Requirements V1 ( params map [ string ] string ) ( v1 . Resource Requirements , error ) { result := v1 . Resource limits , err := populate Resource List requests , err := populate Resource List } 
func make Pod Spec ( params map [ string ] string , name string ) ( * v1 . Pod Spec , error ) { stdin , err := generate . Get tty , err := generate . Get resource Requirements , err := Handle Resource Requirements spec := v1 . Pod Spec { Service Account Name : params [ " " ] , Containers : [ ] v1 . Container { { Name : name , Image : params [ " " ] , Stdin : stdin , TTY : tty , Resources : resource } 
func update Pod Containers ( params map [ string ] string , args [ ] string , envs [ ] v1 . Env Var , image Pull Policy v1 . Pull Policy , pod Spec * v1 . Pod Spec ) error { if len ( args ) > 0 { command , err := generate . Get if command { pod } else { pod if len ( envs ) > 0 { pod if len ( image Pull Policy ) > 0 { // image Pull Policy should be valid here since we have verified it before. pod Spec . Containers [ 0 ] . Image Pull Policy = image Pull } 
func parse Envs ( env Array [ ] string ) ( [ ] v1 . Env Var , error ) { envs := make ( [ ] v1 . Env Var , 0 , len ( env for _ , env := range env if len ( validation . Is Env Var env Var := v1 . Env envs = append ( envs , env } 
func New Cmd Config Use Context ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { options := & use Context Options { config Access : config cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Aliases : [ ] string { " " } , Long : `Sets the current-context in a kubeconfig file` , Example : use Context Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check fmt . Fprintf ( out , " \n " , options . context } 
func Get Container Ports ( pods ... * v1 . Pod ) [ ] * v1 . Container Port { var ports [ ] * v1 . Container } 
func Get Pod // When priority of a running pod is nil, it means it was created at a time // that there was no global default priority class and the priority class // name of the pod was empty. So, we resolve to the static default priority. return scheduling . Default Priority When No Default Class } 
func Get Pod Start Time ( pod * v1 . Pod ) * metav1 . Time { if pod . Status . Start Time != nil { return pod . Status . Start // Should not reach here as the start time of a running time should not be nil // Return current timestamp as the default value. // This will not affect the calculation of earliest timestamp of all the pods on one node, // because current timestamp is always after the Start } 
func Get Earliest Pod Start earliest Pod Start Time := Get Pod Start highest Priority := Get Pod for _ , pod := range victims . Pods { if Get Pod Priority ( pod ) == highest Priority { if Get Pod Start Time ( pod ) . Before ( earliest Pod Start Time ) { earliest Pod Start Time = Get Pod Start } else if Get Pod Priority ( pod ) > highest Priority { highest Priority = Get Pod earliest Pod Start Time = Get Pod Start return earliest Pod Start } 
func More Important Pod ( pod1 , pod2 interface { } ) bool { p1 := Get Pod p2 := Get Pod return Get Pod Start Time ( pod1 . ( * v1 . Pod ) ) . Before ( Get Pod Start } 
func ( s * role Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Role , err error ) { err = cache . List } 
func ( s * role Lister ) Roles ( namespace string ) Role Namespace Lister { return role Namespace } 
func ( s role Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Role , err error ) { err = cache . List All By } 
func New Plugin Initializer ( authentication Info Resolver Wrapper webhook . Authentication Info Resolver Wrapper , service Resolver webhook . Service Resolver , ) * Plugin Initializer { return & Plugin Initializer { authentication Info Resolver Wrapper : authentication Info Resolver Wrapper , service Resolver : service } 
func ( i * Plugin Initializer ) Initialize ( plugin admission . Interface ) { if wants , ok := plugin . ( Wants Service Resolver ) ; ok { wants . Set Service Resolver ( i . service if wants , ok := plugin . ( Wants Authentication Info Resolver Wrapper ) ; ok { if i . authentication Info Resolver Wrapper != nil { wants . Set Authentication Info Resolver Wrapper ( i . authentication Info Resolver } 
func ( in * Namespace Controller Configuration ) Deep Copy Into ( out * Namespace Controller out . Namespace Sync Period = in . Namespace Sync } 
func ( in * Namespace Controller Configuration ) Deep Copy ( ) * Namespace Controller out := new ( Namespace Controller in . Deep Copy } 
func New Allocator CIDR Range ( cidr * net . IP Net , allocator Factory allocator . Allocator Factory ) * Range { max := Range base := big For range r := Range { net : cidr , base : base . Add ( base , big . New r . alloc = allocator Factory ( r . max , range } 
func New CIDR Range ( cidr * net . IP Net ) * Range { return New Allocator CIDR Range ( cidr , func ( max int , range Spec string ) allocator . Interface { return allocator . New Allocation Map ( max , range } 
func New From Snapshot ( snap * api . Range Allocation ) ( * Range , error ) { _ , ipnet , err := net . Parse r := New CIDR } 
if ! ok { return & Err Not In if ! allocated { return Err } 
func ( r * Range ) Allocate Next ( ) ( net . IP , error ) { offset , ok , err := r . alloc . Allocate if ! ok { return nil , Err return add IP } 
} 
func ( r * Range ) For Each ( fn func ( net . IP ) ) { r . alloc . For Each ( func ( offset int ) { ip , _ := Get Indexed } 
} 
func ( r * Range ) Restore ( net * net . IP Net , data [ ] byte ) error { if ! net . IP . Equal ( r . net . IP ) || net . Mask . String ( ) != r . net . Mask . String ( ) { return Err Mismatched } 
offset := calculate IP } 
func big For return big . New Int ( 0 ) . Set } 
func add IP Offset ( base * big . Int , offset int ) net . IP { return net . IP ( big . New Int ( 0 ) . Add ( base , big . New } 
func calculate IP Offset ( base * big . Int , ip net . IP ) int { return int ( big . New Int ( 0 ) . Sub ( big For } 
func Range Size ( subnet * net . IP // For I } 
func Get Indexed IP ( subnet * net . IP Net , index int ) ( net . IP , error ) { ip := add IP Offset ( big For } 
func ( c * Fake Controller Revisions ) List ( opts v1 . List Options ) ( result * appsv1 . Controller Revision List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( controllerrevisions Resource , controllerrevisions Kind , c . ns , opts ) , & appsv1 . Controller Revision label , _ , _ := testing . Extract From List list := & appsv1 . Controller Revision List { List Meta : obj . ( * appsv1 . Controller Revision List ) . List for _ , item := range obj . ( * appsv1 . Controller Revision } 
func ( c * Fake Controller Revisions ) Create ( controller Revision * appsv1 . Controller Revision ) ( result * appsv1 . Controller Revision , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( controllerrevisions Resource , c . ns , controller Revision ) , & appsv1 . Controller return obj . ( * appsv1 . Controller } 
func ( c * Fake Controller Revisions ) Update ( controller Revision * appsv1 . Controller Revision ) ( result * appsv1 . Controller Revision , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( controllerrevisions Resource , c . ns , controller Revision ) , & appsv1 . Controller return obj . ( * appsv1 . Controller } 
func ( c * Second Example V1Client ) REST return c . rest } 
func ( in * API Service Spec ) Deep Copy Into ( out * API Service * out = new ( Service if in . CA Bundle != nil { in , out := & in . CA Bundle , & out . CA } 
func ( in By Group Priority Minimum ) Deep Copy Into ( out * By Group Priority * out = make ( By Group Priority * out = new ( API ( * in ) . Deep Copy } 
func ( in By Group Priority Minimum ) Deep Copy ( ) By Group Priority out := new ( By Group Priority in . Deep Copy } 
func ( in By Version Priority ) Deep Copy ( ) By Version out := new ( By Version in . Deep Copy } 
func New Filtered Role Binding Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers , tweak List Options internalinterfaces . Tweak List Options Func ) cache . Shared Index Informer { return cache . New Shared Index Informer ( & cache . List Watch { List Func : func ( options metav1 . List Options ) ( runtime . Object , error ) { if tweak List Options != nil { tweak List return client . Rbac V1 ( ) . Role } , Watch Func : func ( options metav1 . List Options ) ( watch . Interface , error ) { if tweak List Options != nil { tweak List return client . Rbac V1 ( ) . Role } , } , & rbacv1 . Role Binding { } , resync } 
func Perform Post Upgrade Tasks ( client clientset . Interface , cfg * kubeadmapi . Init Configuration , new K8s Ver * version . Version , dry // Upload currently used configuration to the cluster // Note: This is done right in the beginning of cluster initialization; as we might want to make other phases // depend on centralized information from this source in the future if err := uploadconfig . Upload // Create the new, version-branched kubelet Component Config Config Map if err := kubeletphase . Create Config Map ( cfg . Cluster Configuration . Component Configs . Kubelet , cfg . Kubernetes // Write the new kubelet config down to disk and the env file if needed if err := write Kubelet Config Files ( client , cfg , new K8s Ver , dry // Annotate the node with the crisocket information, sourced either from the Init Configuration struct or // --cri-socket. // TODO: In the future we want to use something more official like Node Status or similar for detecting this properly if err := patchnodephase . Annotate CRI Socket ( client , cfg . Node Registration . Name , cfg . Node Registration . CRI // Create/update RBAC rules that makes the bootstrap tokens able to post CS Rs if err := nodebootstraptoken . Allow Bootstrap Tokens To Post CS // Create/update RBAC rules that makes the bootstrap tokens able to get their CS Rs approved automatically if err := nodebootstraptoken . Auto Approve Node Bootstrap // Create/update RBAC rules that makes the nodes to rotate certificates and get their CS Rs approved automatically if err := nodebootstraptoken . Auto Approve Node Certificate // TODO: Is this needed to do here? I think that updating cluster info should probably be separate from a normal upgrade // Create the cluster-info Config Map with the associated RBAC rules // if err := clusterinfo.Create Bootstrap Config Map If Not Exists(client, kubeadmconstants.Get Admin Kube Config Path()); err != nil { // return err //} // Create/update RBAC rules that makes the cluster-info Config Map reachable if err := clusterinfo . Create Cluster Info RBAC // Upgrade kube-dns/Core DNS and kube-proxy if err := dns . Ensure DNS Addon ( & cfg . Cluster // Remove the old DNS deployment if a new DNS service is now used (kube-dns to Core DNS or vice versa) if err := remove Old DNS Deployment If Another DNS Is Used ( & cfg . Cluster Configuration , client , dry if err := proxy . Ensure Proxy Addon ( & cfg . Cluster Configuration , & cfg . Local API return errorsutil . New } 
func Get Kubelet Dir ( dry Run bool ) ( string , error ) { if dry Run { return kubeadmconstants . Create Temp Dir For return kubeadmconstants . Kubelet Run } 
func move Files ( files map [ string ] string ) error { files To for from , to := range files { if err := os . Rename ( from , to ) ; err != nil { return rollback Files ( files To files To } 
func rollback Files ( files map [ string ] string , original Err error ) error { errs := [ ] error { original return errors . Errorf ( " " , files , errorsutil . New } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Lease List ) ( nil ) , ( * coordination . Lease List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Lease List_To_coordination_Lease List ( a . ( * v1 . Lease List ) , b . ( * coordination . Lease if err := s . Add Generated Conversion Func ( ( * coordination . Lease List ) ( nil ) , ( * v1 . Lease List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_coordination_Lease List_To_v1_Lease List ( a . ( * coordination . Lease List ) , b . ( * v1 . Lease if err := s . Add Generated Conversion Func ( ( * v1 . Lease Spec ) ( nil ) , ( * coordination . Lease Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Lease Spec_To_coordination_Lease Spec ( a . ( * v1 . Lease Spec ) , b . ( * coordination . Lease if err := s . Add Generated Conversion Func ( ( * coordination . Lease Spec ) ( nil ) , ( * v1 . Lease Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_coordination_Lease Spec_To_v1_Lease Spec ( a . ( * coordination . Lease Spec ) , b . ( * v1 . Lease } 
func Convert_v1_Lease_To_coordination_Lease ( in * v1 . Lease , out * coordination . Lease , s conversion . Scope ) error { return auto } 
func Convert_coordination_Lease_To_v1_Lease ( in * coordination . Lease , out * v1 . Lease , s conversion . Scope ) error { return auto } 
func Convert_v1_Lease List_To_coordination_Lease List ( in * v1 . Lease List , out * coordination . Lease List , s conversion . Scope ) error { return auto Convert_v1_Lease List_To_coordination_Lease } 
func Convert_coordination_Lease List_To_v1_Lease List ( in * coordination . Lease List , out * v1 . Lease List , s conversion . Scope ) error { return auto Convert_coordination_Lease List_To_v1_Lease } 
func Convert_v1_Lease Spec_To_coordination_Lease Spec ( in * v1 . Lease Spec , out * coordination . Lease Spec , s conversion . Scope ) error { return auto Convert_v1_Lease Spec_To_coordination_Lease } 
func Convert_coordination_Lease Spec_To_v1_Lease Spec ( in * coordination . Lease Spec , out * v1 . Lease Spec , s conversion . Scope ) error { return auto Convert_coordination_Lease Spec_To_v1_Lease } 
func get Version ( last Seen Binary Version * string ) error { // Create the get request for the etcd version endpoint. req , err := http . New Request ( " " , etcd Version Scrape // Obtain Etcd Version from the JSON response. var version Etcd if err := json . New // Return without updating the version if it stayed the same since last time. if * last Seen Binary Version == version . Binary // Delete the metric for the previous version. if * last Seen Binary Version != " " { deleted := etcd Version . Delete ( prometheus . Labels { " " : * last Seen Binary // Record the new version in a metric. etcd Version . With ( prometheus . Labels { " " : version . Binary * last Seen Binary Version = version . Binary } 
func get Version Periodically ( stop Ch <- chan struct { } ) { last Seen Binary for { if err := get Version ( & last Seen Binary select { case <- stop case <- time . After ( scrape } 
func scrape Metrics ( ) ( map [ string ] * dto . Metric Family , error ) { req , err := http . New Request ( " " , etcd Metrics Scrape // Parse the metrics in text format to a Metric Family struct. var text Parser expfmt . Text return text Parser . Text To Metric } 
func ( s * audit Sink Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Audit Sink , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1alpha1 . Audit } 
func ( s * audit Sink Lister ) Get ( name string ) ( * v1alpha1 . Audit Sink , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1alpha1 . Audit } 
func nil Safe } 
func bounds Safe } 
func keys } 
} 
func get // Verify the types of the values match if reflect . Type Of ( last ) . Kind ( ) != reflect . Type return reflect . Type } 
func get Field Meta ( s proto . Schema , name string ) ( apply . Field Meta Impl , error ) { m := apply . Field Meta if s != nil { ext := s . Get if ! ok { return apply . Field Meta if len ( strategies ) > 2 { return apply . Field Meta // For lists, choose the strategy for this type, not the subtype m . Merge if ! ok { return apply . Field Meta m . Merge Keys = apply . Merge } 
func get Common Group Version Kind ( recorded , local , remote map [ string ] interface { } ) ( schema . Group Version Kind , error ) { recorded GVK , err := get Group Version if err != nil { return schema . Group Version local GVK , err := get Group Version if err != nil { return schema . Group Version remote GVK , err := get Group Version if err != nil { return schema . Group Version if ! reflect . Deep Equal ( recorded GVK , local GVK ) || ! reflect . Deep Equal ( local GVK , remote GVK ) { return schema . Group Version Kind { } , fmt . Errorf ( " " , recorded GVK , local GVK , remote return recorded } 
func get Group Version Kind ( config map [ string ] interface { } ) ( schema . Group Version Kind , error ) { gvk := schema . Group Version } 
func New CSV ( path string ) ( * Token record tokens := make ( map [ string ] * user . Default reader := csv . New reader . Fields Per record if record [ 0 ] == " " { klog . Warningf ( " " , path , record obj := & user . Default if _ , exist := tokens [ record [ 0 ] ] ; exist { klog . Warningf ( " " , path , record return & Token } 
func New Shared Informer Factory ( client kubernetes . Interface , default Resync time . Duration ) Shared Informer Factory { return New Shared Informer Factory With Options ( client , default } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Test Type { } , & Test Type scheme . Add Known Types ( Scheme Group metav1 . Add To Group Version ( scheme , Scheme Group } 
func New Remote Runtime Service ( endpoint string , connection Timeout time . Duration ) ( internalapi . Runtime addr , dailer , err := util . Get Address And ctx , cancel := context . With Timeout ( context . Background ( ) , connection conn , err := grpc . Dial Context ( ctx , addr , grpc . With Insecure ( ) , grpc . With Dialer ( dailer ) , grpc . With Default Call Options ( grpc . Max Call Recv Msg Size ( max Msg return & Remote Runtime Service { timeout : connection Timeout , runtime Client : runtimeapi . New Runtime Service Client ( conn ) , log Reduction : logreduction . New Log Reduction ( identical Error } 
func ( r * Remote Runtime Service ) Version ( api Version string ) ( * runtimeapi . Version Response , error ) { ctx , cancel := get Context With typed Version , err := r . runtime Client . Version ( ctx , & runtimeapi . Version Request { Version : api if typed Version . Version == " " || typed Version . Runtime Name == " " || typed Version . Runtime Api Version == " " || typed Version . Runtime Version == " " { return nil , fmt . Errorf ( " " , * typed return typed } 
func ( r * Remote Runtime Service ) Run Pod Sandbox ( config * runtimeapi . Pod Sandbox Config , runtime Handler string ) ( string , error ) { // Use 2 times longer timeout for sandbox operation (4 mins by default) // TODO: Make the pod sandbox timeout configurable. ctx , cancel := get Context With resp , err := r . runtime Client . Run Pod Sandbox ( ctx , & runtimeapi . Run Pod Sandbox Request { Config : config , Runtime Handler : runtime if resp . Pod Sandbox Id == " " { error Message := fmt . Sprintf ( " " , config . Get klog . Errorf ( " " , error return " " , errors . New ( error return resp . Pod Sandbox } 
func ( r * Remote Runtime Service ) Stop Pod Sandbox ( pod Sand Box ID string ) error { ctx , cancel := get Context With _ , err := r . runtime Client . Stop Pod Sandbox ( ctx , & runtimeapi . Stop Pod Sandbox Request { Pod Sandbox Id : pod Sand Box if err != nil { klog . Errorf ( " " , pod Sand Box } 
func ( r * Remote Runtime Service ) Pod Sandbox Status ( pod Sand Box ID string ) ( * runtimeapi . Pod Sandbox Status , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . Pod Sandbox Status ( ctx , & runtimeapi . Pod Sandbox Status Request { Pod Sandbox Id : pod Sand Box if resp . Status != nil { if err := verify Sandbox } 
func ( r * Remote Runtime Service ) List Pod Sandbox ( filter * runtimeapi . Pod Sandbox Filter ) ( [ ] * runtimeapi . Pod Sandbox , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . List Pod Sandbox ( ctx , & runtimeapi . List Pod Sandbox } 
func ( r * Remote Runtime Service ) Create Container ( pod Sand Box ID string , config * runtimeapi . Container Config , sandbox Config * runtimeapi . Pod Sandbox Config ) ( string , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . Create Container ( ctx , & runtimeapi . Create Container Request { Pod Sandbox Id : pod Sand Box ID , Config : config , Sandbox Config : sandbox if err != nil { klog . Errorf ( " " , pod Sand Box if resp . Container Id == " " { error Message := fmt . Sprintf ( " " , config . Get klog . Errorf ( " " , error return " " , errors . New ( error return resp . Container } 
func ( r * Remote Runtime Service ) Start Container ( container ID string ) error { ctx , cancel := get Context With _ , err := r . runtime Client . Start Container ( ctx , & runtimeapi . Start Container Request { Container Id : container if err != nil { klog . Errorf ( " " , container } 
func ( r * Remote Runtime Service ) Stop Container ( container ctx , cancel := get Context With r . log Reduction . Clear ID ( container _ , err := r . runtime Client . Stop Container ( ctx , & runtimeapi . Stop Container Request { Container Id : container if err != nil { klog . Errorf ( " " , container } 
func ( r * Remote Runtime Service ) Remove Container ( container ID string ) error { ctx , cancel := get Context With r . log Reduction . Clear ID ( container _ , err := r . runtime Client . Remove Container ( ctx , & runtimeapi . Remove Container Request { Container Id : container if err != nil { klog . Errorf ( " " , container } 
func ( r * Remote Runtime Service ) List Containers ( filter * runtimeapi . Container Filter ) ( [ ] * runtimeapi . Container , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . List Containers ( ctx , & runtimeapi . List Containers } 
func ( r * Remote Runtime Service ) Container Status ( container ID string ) ( * runtimeapi . Container Status , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . Container Status ( ctx , & runtimeapi . Container Status Request { Container Id : container if err != nil { // Don't spam the log with endless messages about the same failure. if r . log Reduction . Should Message Be Printed ( err . Error ( ) , container ID ) { klog . Errorf ( " " , container r . log Reduction . Clear ID ( container if resp . Status != nil { if err := verify Container Status ( resp . Status ) ; err != nil { klog . Errorf ( " " , container } 
func ( r * Remote Runtime Service ) Update Container Resources ( container ID string , resources * runtimeapi . Linux Container Resources ) error { ctx , cancel := get Context With _ , err := r . runtime Client . Update Container Resources ( ctx , & runtimeapi . Update Container Resources Request { Container Id : container if err != nil { klog . Errorf ( " " , container } 
func ( r * Remote Runtime Service ) Exec Sync ( container var cancel context . Cancel if timeout != 0 { // Use timeout + default timeout (2 minutes) as timeout to leave some time for // the runtime to do cleanup. ctx , cancel = get Context With } else { ctx , cancel = get Context With timeout req := & runtimeapi . Exec Sync Request { Container Id : container ID , Cmd : cmd , Timeout : timeout resp , err := r . runtime Client . Exec if err != nil { klog . Errorf ( " " , container if resp . Exit Code != 0 { err = utilexec . Code Exit Error { Err : fmt . Errorf ( " " , strings . Join ( cmd , " " ) , resp . Exit Code , resp . Stderr ) , Code : int ( resp . Exit } 
func ( r * Remote Runtime Service ) Exec ( req * runtimeapi . Exec Request ) ( * runtimeapi . Exec Response , error ) { ctx , cancel := get Context With resp , err := r . runtime if err != nil { klog . Errorf ( " " , req . Container if resp . Url == " " { error klog . Errorf ( " " , error return nil , errors . New ( error } 
func ( r * Remote Runtime Service ) Attach ( req * runtimeapi . Attach Request ) ( * runtimeapi . Attach Response , error ) { ctx , cancel := get Context With resp , err := r . runtime if err != nil { klog . Errorf ( " " , req . Container if resp . Url == " " { error klog . Errorf ( " " , error return nil , errors . New ( error } 
func ( r * Remote Runtime Service ) Port Forward ( req * runtimeapi . Port Forward Request ) ( * runtimeapi . Port Forward Response , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . Port if err != nil { klog . Errorf ( " " , req . Pod Sandbox if resp . Url == " " { error klog . Errorf ( " " , error return nil , errors . New ( error } 
func ( r * Remote Runtime Service ) Update Runtime Config ( runtime Config * runtimeapi . Runtime Config ) error { ctx , cancel := get Context With // Response doesn't contain anything of interest. This translates to an // Event notification to the network plugin, which can't fail, so we're // really looking to surface destination unreachable. _ , err := r . runtime Client . Update Runtime Config ( ctx , & runtimeapi . Update Runtime Config Request { Runtime Config : runtime } 
func ( r * Remote Runtime Service ) Status ( ) ( * runtimeapi . Runtime Status , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . Status ( ctx , & runtimeapi . Status if resp . Status == nil || len ( resp . Status . Conditions ) < 2 { error klog . Errorf ( " " , error return nil , errors . New ( error } 
func ( r * Remote Runtime Service ) Container Stats ( container ID string ) ( * runtimeapi . Container Stats , error ) { ctx , cancel := get Context With resp , err := r . runtime Client . Container Stats ( ctx , & runtimeapi . Container Stats Request { Container Id : container if err != nil { if r . log Reduction . Should Message Be Printed ( err . Error ( ) , container ID ) { klog . Errorf ( " " , container r . log Reduction . Clear ID ( container return resp . Get } 
func ( o * Deprecated Controller Options ) Add Flags ( fs * pflag . Flag fs . Float32Var ( & o . Deleting Pods fs . Mark fs . Int32Var ( & o . Deleting Pods fs . Mark fs . Int32Var ( & o . Register Retry Count , " " , o . Register Retry fs . Mark } 
func ( o * Deprecated Controller Options ) Apply To ( cfg * kubectrlmgrconfig . Deprecated Controller cfg . Deleting Pods QPS = o . Deleting Pods cfg . Deleting Pods Burst = o . Deleting Pods cfg . Register Retry Count = o . Register Retry } 
func ( o * Deprecated Controller } 
func Is Deletion Candidate ( obj metav1 . Object , finalizer string ) bool { return obj . Get Deletion Timestamp ( ) != nil && slice . Contains String ( obj . Get } 
func TLS Config For ( c * Config ) ( * tls . Config , error ) { if ! ( c . Has CA ( ) || c . Has Cert Auth ( ) || c . Has Cert Callback ( ) || c . TLS . Insecure || len ( c . TLS . Server if c . Has if err := load TLS tls Config := & tls . Config { // Can't use SS Lv3 because of POODLE and BEAST // Can't use TL Sv1.0 because of POODLE and BEAST using CBC cipher // Can't use TL Sv1.1 because of RC4 cipher usage Min Version : tls . Version TLS12 , Insecure Skip Verify : c . TLS . Insecure , Server Name : c . TLS . Server if c . Has CA ( ) { tls Config . Root C As = root Cert Pool ( c . TLS . CA var static if c . Has Cert Auth ( ) { // If key/cert were provided, verify them before setting up // tls Config.Get Client Certificate. cert , err := tls . X509Key Pair ( c . TLS . Cert Data , c . TLS . Key static if c . Has Cert Auth ( ) || c . Has Cert Callback ( ) { tls Config . Get Client Certificate = func ( * tls . Certificate Request Info ) ( * tls . Certificate , error ) { // Note: static key/cert data always take precedence over cert // callback. if static Cert != nil { return static if c . Has Cert Callback ( ) { cert , err := c . TLS . Get // Get // Both c.TLS.Cert Data/Key Data were unset and Get return tls } 
func data From Slice Or if len ( file ) > 0 { file Data , err := ioutil . Read return file } 
func root Cert Pool ( ca Data [ ] byte ) * x509 . Cert Pool { // What we really want is a copy of x509.system Roots Pool, but that isn't exposed. It's difficult to build (see the go // code for a look at the platform specific insanity), so we'll use the fact that Root C As == nil gives us the system values // It doesn't allow trusting either/or, but hopefully that won't be an issue if len ( ca // if we have ca Data, use it cert Pool := x509 . New Cert cert Pool . Append Certs From PEM ( ca return cert } 
func Wrappers ( fns ... Wrapper Func ) Wrapper return func ( rt http . Round Tripper ) http . Round } 
func Context Canceller ( ctx context . Context , err error ) Wrapper Func { return func ( rt http . Round Tripper ) http . Round Tripper { return & context } 
func ( _ Real } 
func get Azure Cloud Provider ( cloud Provider cloudprovider . Interface ) ( azure Cloud Provider , error ) { azure Cloud Provider , ok := cloud if ! ok || azure Cloud Provider == nil { return nil , fmt . Errorf ( " " , cloud return azure Cloud } 
func get Default Addresses ( bind Address string ) ( default Healthz Address , default Metrics Address string ) { if net . Parse IP ( bind } 
func ( c * Storage V1Client ) REST return c . rest } 
func Loopback Host Port ( bind Address string ) ( string , string , error ) { host , port , err := net . Split Host Port ( bind if err != nil { // should never happen return " " , " " , fmt . Errorf ( " " , bind is I Pv6 := net . Parse // Get ip of local interface, but fall back to "localhost". // Note that "localhost" is resolved with the external nameserver first with Go's stdlib. // So if localhost.<yoursearchdomain> resolves, we don't get a 127.0.0.1 as expected. addrs , err := net . Interface if err == nil { for _ , address := range addrs { if ipnet , ok := address . ( * net . IP Net ) ; ok && ipnet . IP . Is Loopback ( ) && is I } 
func New Must Run As ( ranges [ ] policy . ID Range ) ( Group return & must Run } 
func ( s * must Run } 
func ( s * must Run As ) Generate } 
func ( s * must Run As ) Validate ( fld Path * field . Path , _ * api . Pod , groups [ ] int64 ) field . Error List { all Errs := field . Error if len ( groups ) == 0 && len ( s . ranges ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld all Errs = append ( all Errs , Validate Groups In Ranges ( fld return all } 
func New Kube Static Pod Path Manager ( real Dir , temp Dir , backup Dir , backup Etcd Dir string , keep Manifest Dir , keep Etcd Dir bool ) Static Pod Path Manager { return & Kube Static Pod Path Manager { real Manifest Dir : real Dir , temp Manifest Dir : temp Dir , backup Manifest Dir : backup Dir , backup Etcd Dir : backup Etcd Dir , keep Manifest Dir : keep Manifest Dir , keep Etcd Dir : keep Etcd } 
func New Kube Static Pod Path Manager Using Temp Dirs ( real Manifest Dir string , save Manifests Dir , save Etcd Dir bool ) ( Static Pod Path Manager , error ) { upgraded Manifests Dir , err := constants . Create Temp Dir For backup Manifests Dir , err := constants . Create Timestamp Dir For backup Etcd Dir , err := constants . Create Timestamp Dir For return New Kube Static Pod Path Manager ( real Manifest Dir , upgraded Manifests Dir , backup Manifests Dir , backup Etcd Dir , save Manifests Dir , save Etcd } 
func ( spm * Kube Static Pod Path Manager ) Move File ( old Path , new Path string ) error { return os . Rename ( old Path , new } 
func ( spm * Kube Static Pod Path Manager ) Real Manifest Path ( component string ) string { return constants . Get Static Pod Filepath ( component , spm . real Manifest } 
func ( spm * Kube Static Pod Path Manager ) Temp Manifest Path ( component string ) string { return constants . Get Static Pod Filepath ( component , spm . temp Manifest } 
func ( spm * Kube Static Pod Path Manager ) Backup Manifest Path ( component string ) string { return constants . Get Static Pod Filepath ( component , spm . backup Manifest } 
func ( spm * Kube Static Pod Path Manager ) Cleanup if err := os . Remove All ( spm . Temp Manifest if ! spm . keep Manifest Dir { if err := os . Remove All ( spm . Backup Manifest if ! spm . keep Etcd Dir { if err := os . Remove All ( spm . Backup Etcd return utilerrors . New } 
func perform Etcd Static Pod Upgrade ( renew Certs bool , client clientset . Interface , waiter apiclient . Waiter , path Mgr Static Pod Path Manager , cfg * kubeadmapi . Init Configuration , recover Manifests map [ string ] string , old Etcd Client , new Etcd Client etcdutil . Cluster // Checking health state of etcd before proceeding with the upgrade _ , err := old Etcd Client . Get Cluster // Backing up etcd data store backup Etcd Dir := path Mgr . Backup Etcd running Etcd Dir := cfg . Etcd . Local . Data if err := util . Copy Dir ( running Etcd Dir , backup Etcd // Need to check currently used version and version from constants, if differs then upgrade desired Etcd Version , err := constants . Etcd Supported Version ( cfg . Kubernetes // gets the etcd version of the local/stacked etcd member running on the current machine current Etcd Versions , err := old Etcd Client . Get Cluster current Etcd Version Str , ok := current Etcd Versions [ etcdutil . Get Client URL ( & cfg . Local API current Etcd Version , err := version . Parse Semantic ( current Etcd Version if err != nil { return true , errors . Wrapf ( err , " " , current Etcd Version // Comparing current etcd version with desired to catch the same version or downgrade condition and fail on them. if desired Etcd Version . Less Than ( current Etcd Version ) { return false , errors . Errorf ( " " , cfg . Kubernetes Version , desired Etcd Version . String ( ) , current Etcd // For the case when desired etcd version is the same as current etcd version if strings . Compare ( desired Etcd Version . String ( ) , current Etcd before Etcd Pod Hash , err := waiter . Wait For Static Pod Single Hash ( cfg . Node // Write the updated etcd static Pod manifest into the temporary directory, at this point no etcd change // has occurred in any aspects. if err := etcdphase . Create Local Etcd Static Pod Manifest File ( path Mgr . Temp Manifest Dir ( ) , cfg . Node Registration . Name , & cfg . Cluster Configuration , & cfg . Local API retry // Perform etcd upgrade using common to all control plane components function if err := upgrade Component ( constants . Etcd , renew Certs , waiter , path Mgr , cfg , before Etcd Pod Hash , recover if _ , err := old Etcd Client . Wait For Cluster Available ( retries , retry if err := rollback Etcd Data ( cfg , path Mgr ) ; err != nil { // Even copying back datastore failed, no options for recovery left, bailing out return true , errors . Errorf ( " " , err , backup Etcd if _ , err := old Etcd Client . Wait For Cluster Available ( retries , retry // Nothing else left to try to recover etcd cluster return true , errors . Wrapf ( err , " " , backup Etcd // Initialize the new etcd client if it wasn't pre-initialized if new Etcd Client == nil { etcd Client , err := etcdutil . New From Cluster ( client , cfg . Certificates new Etcd Client = etcd if _ , err = new Etcd Client . Wait For Cluster Available ( retries , retry // Despite the fact that upgrade if err := rollback Etcd Data ( cfg , path Mgr ) ; err != nil { // Even copying back datastore failed, no options for recovery left, bailing out return true , errors . Wrapf ( err , " " , backup Etcd rollback Old Manifests ( recover Manifests , err , path // rollback Old if _ , err := old Etcd Client . Wait For Cluster Available ( retries , retry // Nothing else left to try to recover etcd cluster return true , errors . Wrapf ( err , " " , backup Etcd } 
func Static Pod Control Plane ( client clientset . Interface , waiter apiclient . Waiter , path Mgr Static Pod Path Manager , cfg * kubeadmapi . Init Configuration , etcd Upgrade , renew Certs bool , old Etcd Client , new Etcd Client etcdutil . Cluster Interrogator ) error { recover var is External before Pod Hash Map , err := waiter . Wait For Static Pod Control Plane Hashes ( cfg . Node if old Etcd Client == nil { if cfg . Etcd . External != nil { // External etcd is External etcd Client , err := etcdutil . New ( cfg . Etcd . External . Endpoints , cfg . Etcd . External . CA File , cfg . Etcd . External . Cert File , cfg . Etcd . External . Key old Etcd Client = etcd // Since etcd is managed externally, the new etcd client will be the same as the old client if new Etcd Client == nil { new Etcd Client = etcd } else { // etcd Static Pod etcd Client , err := etcdutil . New From Cluster ( client , cfg . Certificates old Etcd Client = etcd // etcd upgrade is done prior to other control plane components if ! is External Etcd && etcd // Perform etcd upgrade using common to all control plane components function fatal , err := perform Etcd Static Pod Upgrade ( renew Certs , client , waiter , path Mgr , cfg , recover Manifests , old Etcd Client , new Etcd // Write the updated static Pod manifests into the temporary directory fmt . Printf ( " \n " , path Mgr . Temp Manifest err = controlplanephase . Create Init Static Pod Manifest Files ( path Mgr . Temp Manifest for _ , component := range constants . Control Plane Components { if err = upgrade Component ( component , renew Certs , waiter , path Mgr , cfg , before Pod Hash Map [ component ] , recover // Remove the temporary directories used on a best-effort (don't fail if the calls error out) // The calls are set here by design; we should _not_ use "defer" above as that would remove the directories // even in the "fail and rollback" case, where we want the directories preserved for the user. return path Mgr . Cleanup } 
func rollback Old Manifests ( old Manifests map [ string ] string , orig Err error , path Mgr Static Pod Path Manager , restore Etcd bool ) error { errs := [ ] error { orig for component , backup Path := range old Manifests { // Will restore etcd manifest only if it was explicitly requested by setting restore Etcd to True if component == constants . Etcd && ! restore // Where we should put back the backed up manifest real Manifest Path := path Mgr . Real Manifest // Move the backup manifest back into the manifests directory err := path Mgr . Move File ( backup Path , real Manifest // Let the user know there were problems, but we tried to recover return errors . Wrap ( utilerrors . New } 
func rollback Etcd Data ( cfg * kubeadmapi . Init Configuration , path Mgr Static Pod Path Manager ) error { backup Etcd Dir := path Mgr . Backup Etcd running Etcd Dir := cfg . Etcd . Local . Data if err := util . Copy Dir ( backup Etcd Dir , running Etcd Dir ) ; err != nil { // Let the user know there we're problems, but we tried to reover return errors . Wrapf ( err , " " , backup Etcd } 
func renew Certs By Component ( cfg * kubeadmapi . Init Configuration , component string ) error { // if the cluster is using a local etcd if cfg . Etcd . Local != nil { if component == constants . Etcd || component == constants . Kube API Server { // try to load the etcd CA ca Cert , ca Key , err := certsphase . Load Certificate Authority ( cfg . Certificates Dir , certsphase . Kubeadm Cert Etcd CA . Base // create a renewer for certificates signed by etcd CA renewer := renewal . New File Renewal ( ca Cert , ca // then, if upgrading the etcd component, renew all the certificates signed by etcd CA and used // by etcd itself (the etcd-server, the etcd-peer and the etcd-healthcheck-client certificate) if component == constants . Etcd { for _ , cert := range [ ] * certsphase . Kubeadm Cert { & certsphase . Kubeadm Cert Etcd Server , & certsphase . Kubeadm Cert Etcd Peer , & certsphase . Kubeadm Cert Etcd Healthcheck , } { fmt . Printf ( " \n " , cert . Base if err := renewal . Renew Existing Cert ( cfg . Certificates Dir , cert . Base // if upgrading the apiserver component, renew the certificate signed by etcd CA and used // by the apiserver (the apiserver-etcd-client certificate) if component == constants . Kube API Server { cert := certsphase . Kubeadm Cert Etcd API fmt . Printf ( " \n " , cert . Base if err := renewal . Renew Existing Cert ( cfg . Certificates Dir , cert . Base if component == constants . Kube API Server { // Checks if an external CA is provided by the user (when the CA Cert is present but the CA Key is not) // if not, then CA is managed by kubeadm, so it is possible to renew all the certificates signed by ca // and used the apis server (the apiserver certificate and the apiserver-kubelet-client certificate) external CA , _ := certsphase . Using External CA ( & cfg . Cluster if ! external CA { // try to load ca ca Cert , ca Key , err := certsphase . Load Certificate Authority ( cfg . Certificates Dir , certsphase . Kubeadm Cert Root CA . Base if err != nil { return errors . Wrapf ( err , " " , constants . Kube API // create a renewer for certificates signed by CA renewer := renewal . New File Renewal ( ca Cert , ca // renew the certificates for _ , cert := range [ ] * certsphase . Kubeadm Cert { & certsphase . Kubeadm Cert API Server , & certsphase . Kubeadm Cert Kubelet Client , } { fmt . Printf ( " \n " , cert . Base if err := renewal . Renew Existing Cert ( cfg . Certificates Dir , cert . Base // Checks if an external Front-Proxy CA is provided by the user (when the Front-Proxy CA Cert is present but the Front-Proxy CA Key is not) // if not, then Front-Proxy CA is managed by kubeadm, so it is possible to renew all the certificates signed by ca // and used the apis server (the front-proxy-client certificate) external Front Proxy CA , _ := certsphase . Using External Front Proxy CA ( & cfg . Cluster if ! external Front Proxy CA { // try to load front-proxy-ca ca Cert , ca Key , err := certsphase . Load Certificate Authority ( cfg . Certificates Dir , certsphase . Kubeadm Cert Front Proxy CA . Base if err != nil { return errors . Wrapf ( err , " " , constants . Kube API // create a renewer for certificates signed by Front-Proxy CA renewer := renewal . New File Renewal ( ca Cert , ca // renew the certificates cert := certsphase . Kubeadm Cert Front Proxy fmt . Printf ( " \n " , cert . Base if err := renewal . Renew Existing Cert ( cfg . Certificates Dir , cert . Base } 
func New Cmd Scale ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Scale Options ( io valid cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : scale Long , Example : scale Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( o . Run } , Valid Args : valid o . Record Flags . Add o . Print Flags . Add cmd . Flags ( ) . String Var cmd . Flags ( ) . Bool cmd . Flags ( ) . String Var ( & o . Resource Version , " " , o . Resource cmd . Flags ( ) . Int Var ( & o . Current Replicas , " " , o . Current cmd . Flags ( ) . Int cmd . Mark Flag cmd . Flags ( ) . Duration cmdutil . Add Filename Option Flags ( cmd , & o . Filename } 
func ( o * Scale Options ) Run Scale ( ) error { r := o . builder . Unstructured ( ) . Continue On Error ( ) . Namespace Param ( o . namespace ) . Default Namespace ( ) . Filename Param ( o . enforce Namespace , & o . Filename Options ) . Resource Type Or Name Args ( o . All , o . args ... ) . Flatten ( ) . Label Selector if len ( o . Resource // only set a precondition if the user has requested one. A nil precondition means we can do a blind update, so // we avoid a Scale GET that may or may not succeed var precondition * kubectl . Scale if o . Current Replicas != - 1 || len ( o . Resource Version ) > 0 { precondition = & kubectl . Scale Precondition { Size : o . Current Replicas , Resource Version : o . Resource retry := kubectl . New Retry var wait For Replicas * kubectl . Retry if o . Timeout != 0 { wait For Replicas = kubectl . New Retry mapping := info . Resource if mapping . Resource . Group Resource ( ) == ( schema . Group Resource { Group : " " , Resource : " " } ) { // go down the legacy jobs path. This can be removed in 3.14 For now, contain it. fmt . Fprintf ( o . Err if err := Scale Job ( info , o . client Set . Batch V1 ( ) , uint ( o . Replicas ) , precondition , retry , wait For } else { if err := o . scaler . Scale ( info . Namespace , info . Name , uint ( o . Replicas ) , precondition , retry , wait For Replicas , mapping . Resource . Group // if the recorder makes a change, compute and create another patch if merge Patch , err := o . Recorder . Make Record Merge } else if len ( merge Patch ) > 0 { client , err := o . unstructured Client For helper := resource . New if _ , err := helper . Patch ( info . Namespace , info . Name , types . Merge Patch Type , merge return o . Print } 
func New Bootstrap Token Phase ( ) workflow . Phase { return workflow . Phase { Name : " " , Aliases : [ ] string { " " } , Short : " " , Example : bootstrap Token Examples , Long : bootstrap Token Long Desc , Inherit Flags : [ ] string { options . Cfg Path , options . Kubeconfig Path , options . Skip Token Print , } , Run : run Bootstrap } 
func ( g * Cloud ) Compute Services ( ) * Services { return & Services { g . service , g . service Alpha , g . service } 
func new GCE Cloud ( config io . Reader ) ( gce Cloud * Cloud , err error ) { var cloud Config * Cloud var config File * Config if config != nil { config File , err = read klog . Infof ( " " , config cloud Config , err = generate Cloud Config ( config return Create GCE Cloud ( cloud } 
func Create GCE Cloud ( config * Cloud Config ) ( * Cloud , error ) { // Remove any pre-release version and build metadata from the semver, // leaving only the MAJOR.MINOR.PATCH portion. See http://semver.org/. version := strings . Trim Left ( strings . Split ( strings . Split ( version . Get ( ) . Git // Create a user-agent header append string to supply to the Google API // clients, to identify Kubernetes as the origin of the GCP API calls. user // Use Project ID for Network Project ID, if it wasn't explicitly set. if config . Network Project ID == " " { config . Network Project ID = config . Project client , err := new Oauth Client ( config . Token service . User Agent = user client , err = new Oauth Client ( config . Token service service Beta . User Agent = user client , err = new Oauth Client ( config . Token service service Alpha . User Agent = user // Expect override api endpoint to always be v1 api and follows the same pattern as prod. // Generate alpha and beta api endpoints based on override v1 api endpoint. // For example, // staging API endpoint: https://www.googleapis.com/compute/staging_v1/ if config . API Endpoint != " " { service . Base Path = fmt . Sprintf ( " " , config . API service Beta . Base Path = fmt . Sprintf ( " " , strings . Replace ( config . API service Alpha . Base Path = fmt . Sprintf ( " " , strings . Replace ( config . API container container Service . User Agent = user if config . Container API Endpoint != " " { container Service . Base Path = config . Container API tpu Service , err := new TPU // Project ID and.Network Project ID may be project number or name. proj ID , net Proj ID := try Convert To Project Names ( config . Project ID , config . Network Project on XPN := proj ID != net Proj var network var subnet var is Legacy if config . Network URL != " " { network URL = config . Network } else if config . Network Name != " " { network URL = gce Network URL ( config . API Endpoint , net Proj ID , config . Network } else { // Other consumers may use the cloudprovider without utilizing the wrapped GCE API functions // or functions requiring network/subnetwork UR if config . Subnetwork URL != " " { subnet URL = config . Subnetwork } else if config . Subnetwork Name != " " { subnet URL = gce Subnetwork URL ( config . API Endpoint , net Proj ID , config . Region , config . Subnetwork } else { // Determine the type of network and attempt to discover the correct subnet for AUTO mode. // Gracefully fail because kubelet calls Create GCE Cloud without any config, and minions // lack the proper credentials for API calls. if network Name := last Component ( network URL ) ; network if n , err = get Network ( service , net Proj ID , network Name ) ; err != nil { klog . Warningf ( " " , network } else { switch type Of Network ( n ) { case net Type Legacy : klog . Infof ( " " , network is Legacy case net Type Custom : klog . Warningf ( " " , network case net Type Auto : subnet URL , err = determine Subnet URL ( service , net Proj ID , network if err != nil { klog . Warningf ( " " , network } else { klog . Infof ( " " , subnet if len ( config . Managed Zones ) == 0 { config . Managed Zones , err = get Zones For Region ( service , config . Project if len ( config . Managed Zones ) > 1 { klog . Infof ( " " , config . Managed operation Poll Rate Limiter := flowcontrol . New Token Bucket Rate gce := & Cloud { service : service , service Alpha : service Alpha , service Beta : service Beta , container Service : container Service , tpu Service : tpu Service , project ID : proj ID , network Project ID : net Proj ID , on XPN : on XPN , region : config . Region , regional : config . Regional , local Zone : config . Zone , managed Zones : config . Managed Zones , network URL : network URL , is Legacy Network : is Legacy Network , subnetwork URL : subnet URL , secondary Range Name : config . Secondary Range Name , node Tags : config . Node Tags , node Instance Prefix : config . Node Instance Prefix , use Metadata Server : config . Use Metadata Server , operation Poll Rate Limiter : operation Poll Rate Limiter , Alpha Feature Gate : config . Alpha Feature Gate , node gce . manager = & gce Service gce . s = & cloud . Service { GA : service , Alpha : service Alpha , Beta : service Beta , Project Router : & gce Project Router { gce } , Rate Limiter : & gce Rate gce . c = cloud . New } 
func ( g * Cloud ) Set Rate Limiter ( rl cloud . Rate Limiter ) { if rl != nil { g . s . Rate } 
func determine Subnet URL ( service * compute . Service , network Project ID , network Name , region string ) ( string , error ) { subnets , err := list Subnetworks Of Network ( service , network Project ID , network auto Subnets , err := subnets In CIDR ( subnets , auto Subnet IP if len ( auto if len ( auto return auto Subnets [ 0 ] . Self } 
func ( g * Cloud ) Initialize ( client Builder cloudprovider . Controller Client Builder , stop <- chan struct { } ) { g . client Builder = client g . client = client Builder . Client Or if g . On XPN ( ) { g . event Broadcaster = record . New g . event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : g . client . Core g . event Recorder = g . event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event go g . watch Cluster } 
func is Project Number ( id Or Number string ) bool { _ , err := strconv . Parse Uint ( id Or } 
func get Region In URL ( url Str string ) string { fields := strings . Split ( url } 
func get Network ( svc * compute . Service , network Project ID , network ID string ) ( * compute . Network , error ) { return svc . Networks . Get ( network Project ID , network } 
func list Subnetworks Of Network ( svc * compute . Service , network Project ID , network err := svc . Subnetworks . List ( network Project ID , region ) . Filter ( fmt . Sprintf ( " " , network ID ) ) . Pages ( context . Background ( ) , func ( res * compute . Subnetwork } 
func get Project ID ( svc * compute . Service , project Number Or ID string ) ( string , error ) { proj , err := svc . Projects . Get ( project Number Or } 
func ( g * Cloud ) Get Network Endpoint Group ( name string , zone string ) ( * computebeta . Network Endpoint Group , error ) { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric v , err := g . c . Beta Network Endpoint Groups ( ) . Get ( ctx , meta . Zonal } 
func ( g * Cloud ) List Network Endpoint Group ( zone string ) ( [ ] * computebeta . Network Endpoint Group , error ) { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric negs , err := g . c . Beta Network Endpoint } 
func ( g * Cloud ) Aggregated List Network Endpoint Group ( ) ( map [ string ] [ ] * computebeta . Network Endpoint Group , error ) { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric // TODO: filter for the region the cluster is in. all , err := g . c . Beta Network Endpoint Groups ( ) . Aggregated ret := map [ string ] [ ] * computebeta . Network Endpoint for key , by ret [ zone ] = append ( ret [ zone ] , by } 
func ( g * Cloud ) Create Network Endpoint Group ( neg * computebeta . Network Endpoint Group , zone string ) error { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric return mc . Observe ( g . c . Beta Network Endpoint Groups ( ) . Insert ( ctx , meta . Zonal } 
func ( g * Cloud ) Delete Network Endpoint Group ( name string , zone string ) error { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric return mc . Observe ( g . c . Beta Network Endpoint Groups ( ) . Delete ( ctx , meta . Zonal } 
func ( g * Cloud ) Attach Network Endpoints ( name , zone string , endpoints [ ] * computebeta . Network Endpoint ) error { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric req := & computebeta . Network Endpoint Groups Attach Endpoints Request { Network return mc . Observe ( g . c . Beta Network Endpoint Groups ( ) . Attach Network Endpoints ( ctx , meta . Zonal } 
func ( g * Cloud ) List Network Endpoints ( name , zone string , show Health Status bool ) ( [ ] * computebeta . Network Endpoint With Health Status , error ) { ctx , cancel := cloud . Context With Call mc := new Network Endpoint Group Metric health if show Health Status { health req := & computebeta . Network Endpoint Groups List Endpoints Request { Health Status : health l , err := g . c . Beta Network Endpoint Groups ( ) . List Network Endpoints ( ctx , meta . Zonal } 
func new Stateful Sets ( c * Apps V1beta1Client , namespace string ) * stateful Sets { return & stateful Sets { client : c . REST } 
func ( c * stateful Sets ) Update ( stateful Set * v1beta1 . Stateful Set ) ( result * v1beta1 . Stateful Set , err error ) { result = & v1beta1 . Stateful err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( stateful Set . Name ) . Body ( stateful } 
func ( s * mutating Webhook Configuration Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Mutating Webhook Configuration , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Mutating Webhook } 
func ( s * mutating Webhook Configuration Lister ) Get ( name string ) ( * v1beta1 . Mutating Webhook Configuration , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1beta1 . Mutating Webhook } 
func Describer ( rest Client Getter genericclioptions . REST Client Getter , mapping * meta . REST Mapping ) ( describe . Describer , error ) { client Config , err := rest Client Getter . To REST // try to get a describer if describer , ok := Describer For ( mapping . Group Version Kind . Group Kind ( ) , client // if this is a kind we don't have a describer for yet, go generic if possible if generic Describer , ok := Generic Describer For ( mapping , client Config ) ; ok { return generic // otherwise return an unregistered error return nil , fmt . Errorf ( " " , mapping . Group Version } 
func Describer For ( kind schema . Group Kind , client Config * rest . Config ) ( describe . Describer , bool ) { describers , err := describer Map ( client } 
func Generic Describer For ( mapping * meta . REST Mapping , client Config * rest . Config ) ( describe . Describer , bool ) { // used to fetch the resource dynamic Client , err := dynamic . New For Config ( client // used to get events for the resource client Set , err := clientset . New For Config ( client events Client := client Set . Core return & generic Describer { mapping , dynamic Client , events } 
func Describe Limit Ranges ( limit Ranges * corev1 . Limit Range List , w Prefix Writer ) { if len ( limit for _ , limit Range := range limit Ranges . Items { describe Limit Range Spec ( limit } 
func Describe Resource Quotas ( quotas * corev1 . Resource Quota List , w Prefix sort . Sort ( Sortable Resource for _ , scope := range scopes { help Text := help Text For Resource Quota Scope ( corev1 . Resource Quota if len ( help Text ) > 0 { w . Write ( LEVEL_0 , " \n " , help resources := make ( [ ] corev1 . Resource sort . Sort ( Sortable Resource for _ , resource := range resources { hard used w . Write ( LEVEL_0 , " \t \t \n " , string ( resource ) , used Quantity . String ( ) , hard } 
func print Node Selector Terms Multiline With Indent ( w Prefix Writer , indent Level int , title , inner Indent string , reqs [ ] corev1 . Node Selector Requirement ) { w . Write ( indent Level , " " , title , inner if len ( reqs ) == 0 { w . Write for i , req := range reqs { if i != 0 { w . Write ( indent Level , " " , inner expr Str := fmt . Sprintf ( " " , req . Key , strings . To if len ( req . Values ) > 0 { expr Str = fmt . Sprintf ( " " , expr w . Write ( LEVEL_0 , " \n " , expr } 
func Describe Probe ( probe * corev1 . Probe ) string { attrs := fmt . Sprintf ( " " , probe . Initial Delay Seconds , probe . Timeout Seconds , probe . Period Seconds , probe . Success Threshold , probe . Failure case probe . HTTP url . Scheme = strings . To Lower ( string ( probe . HTTP if len ( probe . HTTP Get . Port . String ( ) ) > 0 { url . Host = net . Join Host Port ( probe . HTTP Get . Host , probe . HTTP } else { url . Host = probe . HTTP url . Path = probe . HTTP case probe . TCP Socket != nil : return fmt . Sprintf ( " " , probe . TCP Socket . Host , probe . TCP } 
func Env Value Retriever ( pod * corev1 . Pod ) Env Var Resolver Func { return func ( e corev1 . Env Var ) string { gv , err := schema . Parse Group Version ( e . Value From . Field Ref . API gvk := gv . With internal Field Path , _ , err := scheme . Scheme . Convert Field Label ( gvk , e . Value From . Field Ref . Field value From , err := fieldpath . Extract Field Path As String ( pod , internal Field return value } 
func describe Ingress Annotations ( w Prefix } 
func new Err No return describe . Err No } 
func ( d * Describers ) Describe Object ( exact interface { } , extra ... interface { } ) ( string , error ) { exact Type := reflect . Type fns , ok := d . search Fns [ exact if ! ok { return " " , new Err No Describer ( exact if len ( extra ) == 0 { for _ , type Fn := range fns { if len ( type Fn . Extra ) == 0 { return type type for _ , t := range type for _ , obj := range extra { types = append ( types , reflect . Type for _ , type Fn := range fns { if type Fn . Matches ( types ) { return type return " " , new Err No Describer ( append ( [ ] reflect . Type { exact } 
func ( d * Describers ) Add ( fns ... interface { } ) error { for _ , fn := range fns { fv := reflect . Value num In := ft . Num if num if ft . Num types := make ( [ ] reflect . Type , 0 , num for i := 0 ; i < num if ft . Out ( 0 ) != reflect . Type var for Error // This convolution is necessary, otherwise Type Of picks up on the fact // that for Error Type is nil. error Type := reflect . Type Of ( & for Error if ft . Out ( 1 ) != error if d . search Fns == nil { d . search Fns = make ( map [ reflect . Type ] [ ] type fns := d . search fn := type d . search } 
func ( fn type // reorder the items in array types and fn.Extra // convert the type into string and sort them, check if they are matched var for i := range fn . Extra { var for i := range types { if _ , found := var } 
func ( fn type Func ) Describe ( exact interface { } , extra ... interface { } ) ( string , error ) { values := [ ] reflect . Value { reflect . Value for _ , obj := range extra { values = append ( values , reflect . Value if ! out [ 1 ] . Is } 
func print Labels Multiline ( w Prefix Writer , title string , labels map [ string ] string ) { print Labels Multiline With Indent ( w , " " , title , " \t " , labels , sets . New } 
func print Labels Multiline With Indent ( w Prefix Writer , initial Indent , title , inner Indent string , labels map [ string ] string , skip sets . String ) { w . Write ( LEVEL_0 , " " , initial Indent , title , inner if labels == nil || len ( labels ) == 0 { w . Write if len ( keys ) == 0 { w . Write for i , key := range keys { if i != 0 { w . Write ( LEVEL_0 , " " , initial w . Write ( LEVEL_0 , " " , inner } 
func print Node Taints Multiline ( w Prefix Writer , title string , taints [ ] corev1 . Taint ) { print Taints Multiline With } 
func print Taints Multiline With Indent ( w Prefix Writer , initial Indent , title , inner Indent string , taints [ ] corev1 . Taint ) { w . Write ( LEVEL_0 , " " , initial Indent , title , inner if taints == nil || len ( taints ) == 0 { w . Write // to print taints in the sorted order sort . Slice ( taints , func ( i , j int ) bool { cmp return cmp Key ( taints [ i ] ) < cmp for i , taint := range taints { if i != 0 { w . Write ( LEVEL_0 , " " , initial w . Write ( LEVEL_0 , " " , inner w . Write ( LEVEL_0 , " \n " , taint . To } 
func print Pods Multiline ( w Prefix Writer , title string , pods [ ] corev1 . Pod ) { print Pods Multiline With } 
func print Pods Multiline With Indent ( w Prefix Writer , initial Indent , title , inner Indent string , pods [ ] corev1 . Pod ) { w . Write ( LEVEL_0 , " " , initial Indent , title , inner if pods == nil || len ( pods ) == 0 { w . Write // to print pods in the sorted order sort . Slice ( pods , func ( i , j int ) bool { cmp return cmp Key ( pods [ i ] ) < cmp for i , pod := range pods { if i != 0 { w . Write ( LEVEL_0 , " " , initial w . Write ( LEVEL_0 , " " , inner } 
func print Pod Tolerations Multiline ( w Prefix Writer , title string , tolerations [ ] corev1 . Toleration ) { print Tolerations Multiline With } 
func print Tolerations Multiline With Indent ( w Prefix Writer , initial Indent , title , inner Indent string , tolerations [ ] corev1 . Toleration ) { w . Write ( LEVEL_0 , " " , initial Indent , title , inner if tolerations == nil || len ( tolerations ) == 0 { w . Write for i , toleration := range tolerations { if i != 0 { w . Write ( LEVEL_0 , " " , initial w . Write ( LEVEL_0 , " " , inner if toleration . Toleration Seconds != nil { w . Write ( LEVEL_0 , " " , * toleration . Toleration } 
func Sorted Resource Names ( list corev1 . Resource List ) [ ] corev1 . Resource Name { resources := make ( [ ] corev1 . Resource sort . Sort ( Sortable Resource } 
func print Annotations Multiline With Filter ( w Prefix Writer , title string , annotations map [ string ] string , skip sets . String ) { print Annotations Multiline With } 
func print Annotations Multiline ( w Prefix Writer , title string , annotations map [ string ] string ) { print Annotations Multiline With Indent ( w , " " , title , " \t " , annotations , sets . New } 
func print Annotations Multiline With Indent ( w Prefix Writer , initial Indent , title , inner Indent string , annotations map [ string ] string , skip sets . String ) { w . Write ( LEVEL_0 , " " , initial Indent , title , inner if len ( annotations ) == 0 { w . Write if len ( annotations ) == 0 { w . Write indent := initial Indent + inner value := strings . Trim if ( len ( value ) + len ( key ) + 2 ) > max Annotation for _ , s := range strings . Split ( value , " \n " ) { w . Write ( LEVEL_0 , " \n " , indent , shorten ( s , max Annotation } 
func translate Timestamp Since ( timestamp metav1 . Time ) string { if timestamp . Is return duration . Human } 
func format if ! more { host Port := net . Join Host list = append ( list , host } 
func find Node Roles ( node * corev1 . Node ) [ ] string { roles := sets . New for k , v := range node . Labels { switch { case strings . Has Prefix ( k , describe . Label Node Role Prefix ) : if role := strings . Trim Prefix ( k , describe . Label Node Role case k == describe . Node Label } 
func load Balancer Status Stringer ( s corev1 . Load Balancer result := sets . New if ! wide && len ( r ) > describe . Load Balancer Width { r = r [ 0 : ( describe . Load Balancer } 
func ( cc * Controller ) sync Config Source ( client clientset . Interface , event Client v1core . Events Getter , node Name string ) { select { case <- cc . pending Config cc . poke Config Source // get the latest Node.Spec.Config Source from the informer source , err := latest Node Config Source ( cc . node Informer . Get Store ( ) , node if err != nil { cc . config Status . Set Error Override ( fmt . Sprintf ( status . Sync Error Fmt , status . Internal syncerr = fmt . Errorf ( " " , status . Internal if updated , reason , err := cc . reset Config ( ) ; err != nil { reason = fmt . Sprintf ( status . Sync Error cc . config Status . Set Error } else if updated { restart For New Config ( event Client , node // TODO(mtaufen): It would be nice if we could check the payload's metadata before (re)downloading the whole payload // we at least try pulling the latest configmap out of the local informer store. // construct the interface that can dynamically dispatch the correct Download, etc. methods for the given source type remote , reason , err := checkpoint . New Remote Config if err != nil { reason = fmt . Sprintf ( status . Sync Error cc . config Status . Set Error // "download" source, either from informer's in-memory store or directly from the API server, if the informer doesn't have a copy payload , reason , err := cc . download Config if err != nil { reason = fmt . Sprintf ( status . Sync Error cc . config Status . Set Error // save a checkpoint for the payload, if one does not already exist if reason , err := cc . save Config Checkpoint ( remote , payload ) ; err != nil { reason = fmt . Sprintf ( status . Sync Error cc . config Status . Set Error // update the local, persistent record of assigned config if updated , reason , err := cc . set Assigned Config ( remote ) ; err != nil { reason = fmt . Sprintf ( status . Sync Error cc . config Status . Set Error } else if updated { restart For New Config ( event Client , node // If we get here: // - there is no need to restart to use new config // - there was no error trying to sync configuration // - if, previously, there was an error trying to sync configuration, we need to clear that error from the status cc . config Status . Set Error } 
func ( cc * Controller ) download Config Payload ( client clientset . Interface , source checkpoint . Remote Config if cc . remote Config Source Informer != nil { store = cc . remote Config Source Informer . Get } 
func ( cc * Controller ) set Assigned Config ( source checkpoint . Remote Config Source ) ( bool , string , error ) { assigned , err := cc . checkpoint if err != nil { return false , status . Internal if err := cc . checkpoint Store . Set Assigned ( source ) ; err != nil { return false , status . Internal return ! checkpoint . Equal Remote Config } 
func ( cc * Controller ) reset Config ( ) ( bool , string , error ) { updated , err := cc . checkpoint if err != nil { return false , status . Internal } 
func restart For New Config ( event Client v1core . Events Getter , node Name string , source checkpoint . Remote Config Source ) { message := Local Event if source != nil { message = fmt . Sprintf ( Remote Event Message Fmt , source . API Path ( ) , source . UID ( ) , source . Resource Version ( ) , source . Kubelet // we directly log and send the event, instead of using the event recorder, // because the event recorder won't flush its queue before we exit (we'd lose the event) event := make Event ( node Name , apiv1 . Event Type Normal , Kubelet Config Changed Event klog . V ( 3 ) . Infof ( " " , event . Involved if _ , err := event Client . Events ( apiv1 . Namespace } 
func make Event ( node Name , eventtype , reason , message string ) * apiv1 . Event { const component // NOTE(mtaufen): This is consistent with pkg/kubelet/kubelet.go. Even though setting the node // name as the UID looks strange, it appears to be conventional for events sent by the Kubelet. ref := apiv1 . Object Reference { Kind : " " , Name : node Name , UID : types . UID ( node if namespace == " " { namespace = metav1 . Namespace return & apiv1 . Event { Object Meta : metav1 . Object Meta { Name : fmt . Sprintf ( " " , ref . Name , t . Unix Nano ( ) ) , Namespace : namespace , } , Involved Object : ref , Reason : reason , Message : message , First Timestamp : t , Last Timestamp : t , Count : 1 , Type : eventtype , Source : apiv1 . Event Source { Component : component Kubelet , Host : string ( node } 
a . lock . R defer a . lock . R if ! a . bootstrapped { retries = a . bootstrap for count := 0 ; count < retries ; count ++ { if count > 0 { a . lock . R a . lock . R if a . ready { return a . merged if a . last Err != nil { return nil , a . last return nil , Err Not } 
func Create Default Audit Log Policy ( policy File string ) error { policy := auditv1 . Policy { Type Meta : metav1 . Type Meta { API Version : auditv1 . Scheme Group Version . String ( ) , Kind : " " , } , Rules : [ ] auditv1 . Policy Rule { { Level : auditv1 . Level return write Policy To Disk ( policy } 
func ( nfs * Named Flag Sets ) Flag Set ( name string ) * pflag . Flag Set { if nfs . Flag Sets == nil { nfs . Flag Sets = map [ string ] * pflag . Flag if _ , ok := nfs . Flag Sets [ name ] ; ! ok { nfs . Flag Sets [ name ] = pflag . New Flag Set ( name , pflag . Exit On return nfs . Flag } 
func Print Sections ( w io . Writer , fss Named Flag Sets , cols int ) { for _ , name := range fss . Order { fs := fss . Flag if ! fs . Has wide FS := pflag . New Flag Set ( " " , pflag . Exit On wide FS . Add Flag wide fmt . Fprintf ( & buf , " \n \n \n " , strings . To Upper ( name [ : 1 ] ) + name [ 1 : ] , wide FS . Flag Usages } 
func New Cmd Config Unset ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { options := & unset Options { config Access : config cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : unset Long , Example : unset Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check } 
func ( b * portworx Volume Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func ( b * portworx Volume Mounter ) Set Up At ( dir string , fs Group * int64 ) error { not Mnt , err := b . mounter . Is Likely Not Mount klog . Infof ( " " , dir , ! not if err != nil && ! os . Is Not if ! not attach attach Options [ attach Context attach Options [ attach Host Key ] = b . plugin . host . Get Host if _ , err := b . manager . Attach Volume ( b , attach klog . V ( 4 ) . Infof ( " " , b . volume if err := os . Mkdir if err := b . manager . Mount if ! b . read Only { volume . Set Volume Ownership ( b , fs klog . Infof ( " " , b . volume } 
func ( c * portworx Volume Unmounter ) Tear Down if err := c . manager . Unmount // Call Portworx Detach Volume. if err := c . manager . Detach } 
func ( f * Print Flags ) Set Kind ( kind schema . Group Kind ) { f . Human Readable Flags . Set } 
func ( f * Print Flags ) Allowed Formats ( ) [ ] string { formats := f . JSON Yaml Print Flags . Allowed formats = append ( formats , f . Name Print Flags . Allowed formats = append ( formats , f . Template Flags . Allowed formats = append ( formats , f . Custom Columns Flags . Allowed formats = append ( formats , f . Human Readable Flags . Allowed } 
func ( f * Print Flags ) Use Open API Columns ( api openapi . Resources , mapping * meta . REST Mapping ) error { // Found openapi metadata for this resource schema := api . Lookup Resource ( mapping . Group Version columns , found := openapi . Get Print Columns ( schema . Get parts := strings . Split allow Missing f . Output f . Template Flags . Template f . Template Flags . Allow Missing Keys = & allow Missing } 
func ( f * Print Flags ) To Printer ( ) ( printers . Resource Printer , error ) { output if f . Output Format != nil { output Format = * f . Output no if f . No Headers != nil { no Headers = * f . No f . Human Readable Flags . No Headers = no f . Custom Columns Flags . No Headers = no // for "get.go" we want to support a --template argument given, even when no --output format is provided if f . Template Flags . Template Argument != nil && len ( * f . Template Flags . Template Argument ) > 0 && len ( output Format ) == 0 { output if p , err := f . Template Flags . To Printer ( output Format ) ; ! genericclioptions . Is No Compatible Printer if f . Template Flags . Template Argument != nil { f . Custom Columns Flags . Template Argument = * f . Template Flags . Template if p , err := f . JSON Yaml Print Flags . To Printer ( output Format ) ; ! genericclioptions . Is No Compatible Printer if p , err := f . Human Readable Flags . To Printer ( output Format ) ; ! genericclioptions . Is No Compatible Printer if p , err := f . Custom Columns Flags . To Printer ( output Format ) ; ! genericclioptions . Is No Compatible Printer if p , err := f . Name Print Flags . To Printer ( output Format ) ; ! genericclioptions . Is No Compatible Printer return nil , genericclioptions . No Compatible Printer Error { Output Format : & output Format , Allowed Formats : f . Allowed } 
func ( f * Print Flags ) Add Flags ( cmd * cobra . Command ) { f . JSON Yaml Print Flags . Add f . Name Print Flags . Add f . Template Flags . Add f . Human Readable Flags . Add f . Custom Columns Flags . Add if f . Output Format != nil { cmd . Flags ( ) . String Var P ( f . Output Format , " " , " " , * f . Output if f . No Headers != nil { cmd . Flags ( ) . Bool Var ( f . No Headers , " " , * f . No } 
func New Get Print Flags ( ) * Print Flags { output no return & Print Flags { Output Format : & output Format , No Headers : & no Headers , JSON Yaml Print Flags : genericclioptions . New JSON Yaml Print Flags ( ) , Name Print Flags : genericclioptions . New Name Print Flags ( " " ) , Template Flags : genericclioptions . New Kube Template Print Flags ( ) , Human Readable Flags : New Human Print Flags ( ) , Custom Columns Flags : New Custom Columns Print } 
func ( in * Cron Job ) Deep Copy Into ( out * Cron out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Cron Job ) Deep Copy ( ) * Cron out := new ( Cron in . Deep Copy } 
func ( in * Cron Job ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Cron Job List ) Deep Copy Into ( out * Cron Job out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Cron for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Cron Job List ) Deep Copy ( ) * Cron Job out := new ( Cron Job in . Deep Copy } 
func ( in * Cron Job List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Cron Job Spec ) Deep Copy Into ( out * Cron Job if in . Starting Deadline Seconds != nil { in , out := & in . Starting Deadline Seconds , & out . Starting Deadline in . Job Template . Deep Copy Into ( & out . Job if in . Successful Jobs History Limit != nil { in , out := & in . Successful Jobs History Limit , & out . Successful Jobs History if in . Failed Jobs History Limit != nil { in , out := & in . Failed Jobs History Limit , & out . Failed Jobs History } 
func ( in * Cron Job Spec ) Deep Copy ( ) * Cron Job out := new ( Cron Job in . Deep Copy } 
func ( in * Cron Job Status ) Deep Copy ( ) * Cron Job out := new ( Cron Job in . Deep Copy } 
func ( in * Job Template ) Deep Copy Into ( out * Job out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Template . Deep Copy } 
func ( in * Job Template ) Deep Copy ( ) * Job out := new ( Job in . Deep Copy } 
func ( in * Job Template ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Job Template Spec ) Deep Copy Into ( out * Job Template in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Job Template Spec ) Deep Copy ( ) * Job Template out := new ( Job Template in . Deep Copy } 
func Validate Policy Rule ( rule rbac . Policy Rule , is Namespaced bool , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( rule . Verbs ) == 0 { all Errs = append ( all Errs , field . Required ( fld if len ( rule . Non Resource UR Ls ) > 0 { if is Namespaced { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , rule . Non Resource UR if len ( rule . API Groups ) > 0 || len ( rule . Resources ) > 0 || len ( rule . Resource Names ) > 0 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , rule . Non Resource UR return all if len ( rule . API Groups ) == 0 { all Errs = append ( all Errs , field . Required ( fld if len ( rule . Resources ) == 0 { all Errs = append ( all Errs , field . Required ( fld return all } 
func Validate Role Binding Subject ( subject rbac . Subject , is Namespaced bool , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( subject . Name ) == 0 { all Errs = append ( all Errs , field . Required ( fld switch subject . Kind { case rbac . Service Account Kind : if len ( subject . Name ) > 0 { for _ , msg := range validation . Validate Service Account Name ( subject . Name , false ) { all Errs = append ( all Errs , field . Invalid ( fld if len ( subject . API Group ) > 0 { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , subject . API if ! is Namespaced && len ( subject . Namespace ) == 0 { all Errs = append ( all Errs , field . Required ( fld case rbac . User Kind : // TODO(ericchiang): What other restrictions on user name are there? if subject . API Group != rbac . Group Name { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , subject . API Group , [ ] string { rbac . Group case rbac . Group Kind : // TODO(ericchiang): What other restrictions on group name are there? if subject . API Group != rbac . Group Name { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , subject . API Group , [ ] string { rbac . Group default : all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , subject . Kind , [ ] string { rbac . Service Account Kind , rbac . User Kind , rbac . Group return all } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1beta1 . Daemon Set { } , func ( obj interface { } ) { Set Object Defaults_Daemon Set ( obj . ( * v1beta1 . Daemon scheme . Add Type Defaulting Func ( & v1beta1 . Daemon Set List { } , func ( obj interface { } ) { Set Object Defaults_Daemon Set List ( obj . ( * v1beta1 . Daemon Set scheme . Add Type Defaulting Func ( & v1beta1 . Deployment { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1beta1 . Deployment List { } , func ( obj interface { } ) { Set Object Defaults_Deployment List ( obj . ( * v1beta1 . Deployment scheme . Add Type Defaulting Func ( & v1beta1 . Network Policy { } , func ( obj interface { } ) { Set Object Defaults_Network Policy ( obj . ( * v1beta1 . Network scheme . Add Type Defaulting Func ( & v1beta1 . Network Policy List { } , func ( obj interface { } ) { Set Object Defaults_Network Policy List ( obj . ( * v1beta1 . Network Policy scheme . Add Type Defaulting Func ( & v1beta1 . Pod Security Policy { } , func ( obj interface { } ) { Set Object Defaults_Pod Security Policy ( obj . ( * v1beta1 . Pod Security scheme . Add Type Defaulting Func ( & v1beta1 . Pod Security Policy List { } , func ( obj interface { } ) { Set Object Defaults_Pod Security Policy List ( obj . ( * v1beta1 . Pod Security Policy scheme . Add Type Defaulting Func ( & v1beta1 . Replica Set { } , func ( obj interface { } ) { Set Object Defaults_Replica Set ( obj . ( * v1beta1 . Replica scheme . Add Type Defaulting Func ( & v1beta1 . Replica Set List { } , func ( obj interface { } ) { Set Object Defaults_Replica Set List ( obj . ( * v1beta1 . Replica Set } 
func get Current Version if err := windows . Reg Open Key Ex ( windows . HKEY_LOCAL_MACHINE , windows . String To UTF16Ptr ( `SOFTWARE\\Microsoft\\Windows NT\\Current defer windows . Reg Close if err := windows . Reg Query Value Ex ( h , windows . String To return windows . UTF16To } 
func get Version Revision ( ) ( uint16 , error ) { revision String , err := get Current Version revision , err := windows . UTF16From String ( revision } 
func get Kernel Version ( ) ( string , error ) { // Get Current Build Number. build Number , err := get Current Version // Get Current Major Version Number. major Version Number String , err := get Current Version major Version Number , err := windows . UTF16From String ( major Version Number // Get Current Minor Version Number. minor Version Number String , err := get Current Version minor Version Number , err := windows . UTF16From String ( minor Version Number // Get UBR. revision , err := get Version return fmt . Sprintf ( " " , major Version Number [ 0 ] , minor Version Number [ 0 ] , build } 
func get OS Image Version ( ) ( string , error ) { product Name , err := get Current Version return product } 
func Register ( pvc Lister corelisters . Persistent Volume Claim Lister , pv Lister corelisters . Persistent Volume Lister , pod Lister corelisters . Pod Lister , asw cache . Actual State Of World , dsw cache . Desired State Of World , plugin Mgr * volume . Volume Plugin Mgr ) { register Metrics . Do ( func ( ) { prometheus . Must Register ( new Attach Detach State Collector ( pvc Lister , pod Lister , pv Lister , asw , dsw , plugin prometheus . Must Register ( forced Detach Metric } 
func Handle Plugin Error ( plugin string , err error , impacted ... * auditinternal . Event ) { // Count the error. error Counter . With Label for _ , ev := range impacted { msg = msg + Event } 
func New File To Print ( real Path , print Path string ) File To Print { return File To Print { Real Path : real Path , Print Path : print } 
func Print Dry Run File ( file Name , real Dir , print Dir string , w io . Writer ) error { return Print Dry Run Files ( [ ] File To Print { New File To Print ( filepath . Join ( real Dir , file Name ) , filepath . Join ( print Dir , file } 
func Print Dry Run Files ( files [ ] File To for _ , file := range files { if len ( file . Real file Bytes , err := ioutil . Read File ( file . Real // Make it possible to fake the path of the file; i.e. you may want to tell the user // "Here is what would be written to /etc/kubernetes/admin.conf", although you wrote it to /tmp/kubeadm-dryrun/admin.conf and are loading it from there // Fall back to the "real" path if Print Path is not set output File Path := file . Print if len ( output File Path ) == 0 { output File Path = file . Real fmt . Fprintf ( w , " \n " , output File apiclient . Print Bytes With Line Prefix ( w , file return errorsutil . New } 
func ( w * Waiter ) Wait For Pods With Label ( kv Label string ) error { fmt . Printf ( " n" , v Label, etav1. N amespace } 
func ( w * Waiter ) Wait For Pod To Disappear ( pod Name string ) error { fmt . Printf ( " n" , od Name, etav1. N amespace } 
func ( w * Waiter ) Wait For Healthy Kubelet ( _ time . Duration , healthz Endpoint string ) error { fmt . Printf ( " \n " , healthz } 
func ( w * Waiter ) Wait For Static Pod Control Plane Hashes ( _ string ) ( map [ string ] string , error ) { return map [ string ] string { constants . Kube API Server : " " , constants . Kube Controller Manager : " " , constants . Kube } 
func ( w * Waiter ) Wait For Static Pod Single } 
cc := sanitized Config ( Copy if cc . Bearer Token != " " { cc . Bearer if cc . Auth Config Persister != nil { cc . Auth Config Persister = sanitized Auth Config Persister { cc . Auth Config } 
func ( c TLS Client Config ) String ( ) string { cc := sanitized TLS Client Config { Insecure : c . Insecure , Server Name : c . Server Name , Cert File : c . Cert File , Key File : c . Key File , CA File : c . CA File , Cert Data : c . Cert Data , Key Data : c . Key Data , CA Data : c . CA // Explicitly mark non-empty credential fields as redacted. if len ( cc . Cert Data ) != 0 { cc . Cert if len ( cc . Key Data ) != 0 { cc . Key } 
func REST Client For ( config * Config ) ( * REST Client , error ) { if config . Group if config . Negotiated if config . QPS == 0.0 { qps = Default if config . Burst == 0 { burst = Default base URL , versioned API Path , err := default Server Url transport , err := Transport var http if transport != http . Default Transport { http if config . Timeout > 0 { http return New REST Client ( base URL , versioned API Path , config . Content Config , qps , burst , config . Rate Limiter , http } 
func Unversioned REST Client For ( config * Config ) ( * REST Client , error ) { if config . Negotiated base URL , versioned API Path , err := default Server Url transport , err := Transport var http if transport != http . Default Transport { http if config . Timeout > 0 { http version Config := config . Content if version Config . Group Version == nil { v := metav1 . Scheme Group version Config . Group return New REST Client ( base URL , versioned API Path , version Config , config . QPS , config . Burst , config . Rate Limiter , http } 
func Set Kubernetes Defaults ( config * Config ) error { if len ( config . User Agent ) == 0 { config . User Agent = Default Kubernetes User } 
func adjust } 
func adjust seg := strings . Split } 
func build User } 
func Default Kubernetes User Agent ( ) string { return build User Agent ( adjust Command ( os . Args [ 0 ] ) , adjust Version ( version . Get ( ) . Git Version ) , gruntime . GOOS , gruntime . GOARCH , adjust Commit ( version . Get ( ) . Git } 
func In Cluster Config ( ) ( * Config , error ) { const ( token root CA if len ( host ) == 0 || len ( port ) == 0 { return nil , Err Not In token , err := ioutil . Read File ( token tls Client Config := TLS Client if _ , err := certutil . New Pool ( root CA File ) ; err != nil { klog . Errorf ( " " , root CA } else { tls Client Config . CA File = root CA return & Config { // TODO: switch to using cluster DNS. Host : " " + net . Join Host Port ( host , port ) , TLS Client Config : tls Client Config , Bearer Token : string ( token ) , Bearer Token File : token } 
func Anonymous Client Config ( config * Config ) * Config { // copy only known safe fields return & Config { Host : config . Host , API Path : config . API Path , Content Config : config . Content Config , TLS Client Config : TLS Client Config { Insecure : config . Insecure , Server Name : config . Server Name , CA File : config . TLS Client Config . CA File , CA Data : config . TLS Client Config . CA Data , } , Rate Limiter : config . Rate Limiter , User Agent : config . User } 
func Copy Config ( config * Config ) * Config { return & Config { Host : config . Host , API Path : config . API Path , Content Config : config . Content Config , Username : config . Username , Password : config . Password , Bearer Token : config . Bearer Token , Bearer Token File : config . Bearer Token File , Impersonate : Impersonation Config { Groups : config . Impersonate . Groups , Extra : config . Impersonate . Extra , User Name : config . Impersonate . User Name , } , Auth Provider : config . Auth Provider , Auth Config Persister : config . Auth Config Persister , Exec Provider : config . Exec Provider , TLS Client Config : TLS Client Config { Insecure : config . TLS Client Config . Insecure , Server Name : config . TLS Client Config . Server Name , Cert File : config . TLS Client Config . Cert File , Key File : config . TLS Client Config . Key File , CA File : config . TLS Client Config . CA File , Cert Data : config . TLS Client Config . Cert Data , Key Data : config . TLS Client Config . Key Data , CA Data : config . TLS Client Config . CA Data , } , User Agent : config . User Agent , Transport : config . Transport , Wrap Transport : config . Wrap Transport , QPS : config . QPS , Burst : config . Burst , Rate Limiter : config . Rate } 
func New LRU Expire Cache With Clock ( max Size int , clock Clock ) * LRU Expire Cache { cache , err := lru . New ( max return & LRU Expire } 
func ( c * LRU Expire c . cache . Add ( key , & cache } 
func ( c * LRU Expire if c . clock . Now ( ) . After ( e . ( * cache Entry ) . expire return e . ( * cache } 
func ( c * LRU Expire } 
func ( c * LRU Expire } 
func Register ( ) { register Metrics . Do ( func ( ) { prometheus . Must Register ( HTTP prometheus . Must Register ( HTTP Requests prometheus . Must Register ( HTTP Inflight } 
func ( mounter * quobyte Mounter ) Set Up ( fs Group * int64 ) error { plugin Dir := mounter . plugin . host . Get Plugin Dir ( utilstrings . Escape Qualified Name ( quobyte Plugin return mounter . Set Up At ( plugin Dir , fs } 
func ( quobyte Volume * quobyte ) Get Path ( ) string { user := quobyte group := quobyte // Quobyte has only one mount in the Plugin Dir where all Volumes are mounted // The Quobyte client does a fixed-user mapping plugin Dir := quobyte Volume . plugin . host . Get Plugin Dir ( utilstrings . Escape Qualified Name ( quobyte Plugin return filepath . Join ( plugin Dir , fmt . Sprintf ( " " , user , group , quobyte } 
func parse API Config ( plugin * quobyte Plugin , params map [ string ] string ) ( * quobyte API Config , error ) { var api Server , secret secret delete for k , v := range params { switch gostrings . To Lower ( k ) { case " " : secret delete Keys = append ( delete case " " : secret delete Keys = append ( delete case " " : api delete Keys = append ( delete if len ( api secret Map , err := util . Get Secret For PV ( secret Namespace , secret Name , quobyte Plugin Name , plugin . host . Get Kube cfg := & quobyte API Config { quobyte API Server : api if cfg . quobyte User , ok = secret Map [ " " ] ; ! ok { return nil , fmt . Errorf ( " \" \" " , secret Namespace , secret if cfg . quobyte Password , ok = secret Map [ " " ] ; ! ok { return nil , fmt . Errorf ( " \" \" " , secret Namespace , secret } 
func get Listener For Port ( existing Listeners [ ] listeners . Listener , port v1 . Service Port ) * listeners . Listener { for _ , l := range existing Listeners { if listeners . Protocol ( l . Protocol ) == to Listeners Protocol ( port . Protocol ) && l . Protocol } 
func get Pool By Listener ID ( client * gophercloud . Service Client , loadbalancer ID string , listener ID string ) ( * v2pools . Pool , error ) { listener err := v2pools . List ( client , v2pools . List Opts { Loadbalancer ID : loadbalancer ID } ) . Each Page ( func ( page pagination . Page ) ( bool , error ) { pools List , err := v2pools . Extract for _ , p := range pools List { for _ , l := range p . Listeners { if l . ID == listener ID { listener Pools = append ( listener if len ( listener Pools ) > 1 { return false , Err Multiple if err != nil { if is Not Found ( err ) { return nil , Err Not if len ( listener Pools ) == 0 { return nil , Err Not } else if len ( listener Pools ) > 1 { return nil , Err Multiple return & listener } 
func member Exists ( members [ ] v2pools . Member , addr string , port int ) bool { for _ , member := range members { if member . Address == addr && member . Protocol } 
func ( lbaas * Lbaas V2 ) Get Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service ) ( * v1 . Load Balancer Status , bool , error ) { load Balancer Name := lbaas . Get Load Balancer Name ( ctx , cluster loadbalancer , err := get Loadbalancer By Name ( lbaas . lb , load Balancer if err == Err Not status := & v1 . Load Balancer port ID := loadbalancer . Vip Port if port ID != " " { float IP , err := get Floating IP By Port ID ( lbaas . network , port if err != nil && err != Err Not Found { return nil , false , fmt . Errorf ( " " , port if float IP != nil { status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : float IP . Floating } else { status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : loadbalancer . Vip } 
func ( lbaas * Lbaas V2 ) Get Load Balancer Name ( ctx context . Context , cluster Name string , service * v1 . Service ) string { // TODO: replace Default Load Balancer Name to generate more meaningful loadbalancer names. return cloudprovider . Default Load Balancer } 
func node Address For if len ( addrs ) == 0 { return " " , Err No Address allowed Addr Types := [ ] v1 . Node Address Type { v1 . Node Internal IP , v1 . Node External for _ , allowed Addr Type := range allowed Addr Types { for _ , addr := range addrs { if addr . Type == allowed Addr return " " , Err No Address } 
func get String From Service Annotation ( service * v1 . Service , annotation Key string , default Setting string ) string { klog . V ( 4 ) . Infof ( " " , service , annotation Key , default if annotation Value , ok := service . Annotations [ annotation Key ] ; ok { //if there is an annotation for this setting, set the "setting" var to it // annotation Value can be empty, it is working as designed // it makes possible for instance provisioning loadbalancer without floatingip klog . V ( 4 ) . Infof ( " " , annotation Key , annotation return annotation //if there is no annotation, set "settings" var to the value from cloud config klog . V ( 4 ) . Infof ( " " , annotation Key , default return default } 
func get Subnet ID For LB ( compute * gophercloud . Service Client , node v1 . Node ) ( string , error ) { ip Address , err := node Address For instance ID := node . Spec . Provider if ind := strings . Last Index ( instance ID , " " ) ; ind >= 0 { instance ID = instance interfaces , err := get Attached Interfaces By ID ( compute , instance for _ , intf := range interfaces { for _ , fixed IP := range intf . Fixed I Ps { if fixed IP . IP Address == ip Address { return fixed IP . Subnet return " " , Err Not } 
func get Node Security Group ID For LB ( compute * gophercloud . Service Client , network * gophercloud . Service Client , nodes [ ] * v1 . Node ) ( [ ] string , error ) { sec Group Names := sets . New for _ , node := range nodes { node Name := types . Node srv , err := get Server By Name ( compute , node // use the first node-security-groups // case 0: node1:SG1 node2:SG1 return SG1 // case 1: node1:SG1 node2:SG2 return SG1,SG2 // case 2: node1:SG1,SG2 node2:SG3,SG4 return SG1,SG3 // case 3: node1:SG1,SG2 node2:SG2,SG3 return SG1,SG2 sec Group Names . Insert ( srv . Security sec Group I Ds := make ( [ ] string , sec Group for i , name := range sec Group Names . List ( ) { sec Group ID , err := groups . ID From sec Group I Ds [ i ] = sec Group return sec Group I } 
func is Security Group Not Found ( err error ) bool { err Type := reflect . Type err Type Slice := strings . Split ( err err Type if len ( err Type Slice ) != 0 { err Type Value = err Type Slice [ len ( err Type if err Type } 
func get Floating Network ID For LB ( client * gophercloud . Service Client ) ( string , error ) { var floating Network type Network With External external . Network External err := networks . List ( client , networks . List Opts { } ) . Each Page ( func ( page pagination . Page ) ( bool , error ) { var external Network [ ] Network With External err := networks . Extract Networks Into ( page , & external for _ , external Net := range external Network { if external Net . External { floating Network Ids = append ( floating Network Ids , external if len ( floating Network Ids ) > 1 { return false , Err Multiple if err != nil { if is Not Found ( err ) { return " " , Err Not if err == Err Multiple return floating Network if len ( floating Network Ids ) == 0 { return " " , Err Not return floating Network } 
func ( lbaas * Lbaas V2 ) Ensure Load Balancer ( ctx context . Context , cluster Name string , api Service * v1 . Service , nodes [ ] * v1 . Node ) ( * v1 . Load Balancer Status , error ) { klog . V ( 4 ) . Infof ( " " , cluster Name , api Service . Namespace , api Service . Name , api Service . Spec . Load Balancer IP , api Service . Spec . Ports , nodes , api if len ( nodes ) == 0 { return nil , fmt . Errorf ( " " , api Service . Namespace , api lbaas . opts . Subnet ID = get String From Service Annotation ( api Service , Service Annotation Load Balancer Subnet ID , lbaas . opts . Subnet if len ( lbaas . opts . Subnet ID ) == 0 { // Get Subnet ID automatically. // The LB needs to be configured with instance addresses on the same subnet, so get Subnet ID by one node. subnet ID , err := get Subnet ID For if err != nil { klog . Warningf ( " " , api Service . Namespace , api return nil , fmt . Errorf ( " " + " " , api Service . Namespace , api lbaas . opts . Subnet ID = subnet ports := api floating Pool := get String From Service Annotation ( api Service , Service Annotation Load Balancer Floating Network ID , lbaas . opts . Floating Network if len ( floating floating Pool , err = get Floating Network ID For if err != nil { klog . Warningf ( " " , api Service . Namespace , api var internal internal := get String From Service Annotation ( api Service , Service Annotation Load Balancer internal case " " : if len ( floating Pool ) != 0 { klog . V ( 4 ) . Infof ( " " , floating internal // Check for TCP protocol on each port // TODO: Convert all error messages to use an event recorder for _ , port := range ports { if port . Protocol != v1 . Protocol source Ranges , err := servicehelpers . Get Load Balancer Source Ranges ( api if err != nil { return nil , fmt . Errorf ( " " , api Service . Namespace , api if ! servicehelpers . Is Allow All ( source Ranges ) && ! lbaas . opts . Manage Security affinity := api Service . Spec . Session var persistence * v2pools . Session switch affinity { case v1 . Service Affinity case v1 . Service Affinity Client IP : persistence = & v2pools . Session name := lbaas . Get Load Balancer Name ( ctx , cluster Name , api loadbalancer , err := get Loadbalancer By if err != nil { if err != Err Not loadbalancer , err = lbaas . create Load Balancer ( api Service , name , internal provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning lbmethod := v2pools . LB Method ( lbaas . opts . LB if lbmethod == " " { lbmethod = v2pools . LB Method Round old Listeners , err := get Listeners By Load Balancer for port Index , port := range ports { listener := get Listener For Port ( old listener , err = listeners . Create ( lbaas . lb , listeners . Create Opts { Name : fmt . Sprintf ( " " , name , port Index ) , Protocol : listeners . Protocol ( port . Protocol ) , Protocol Port : int ( port . Port ) , Loadbalancer provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning // After all ports have been processed, remaining listeners are removed as obsolete. // Pop valid listeners. old Listeners = pop Listener ( old pool , err := get Pool By Listener if err != nil && err != Err Not pool , err = v2pools . Create ( lbaas . lb , v2pools . Create Opts { Name : fmt . Sprintf ( " " , name , port Index ) , Protocol : v2pools . Protocol ( port . Protocol ) , LB Method : lbmethod , Listener provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning members , err := get Members By Pool if err != nil && ! is Not for _ , node := range nodes { addr , err := node Address For if err != nil { if err == Err Not if ! member Exists ( members , addr , int ( port . Node _ , err := v2pools . Create Member ( lbaas . lb , pool . ID , v2pools . Create Member Opts { Name : fmt . Sprintf ( " " , name , port Index , node . Name ) , Protocol Port : int ( port . Node Port ) , Address : addr , Subnet ID : lbaas . opts . Subnet provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning } else { // After all members have been processed, remaining members are deleted as obsolete. members = pop Member ( members , addr , int ( port . Node err := v2pools . Delete Member ( lbaas . lb , pool . ID , member . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning monitor ID := pool . Monitor if monitor ID == " " && lbaas . opts . Create monitor , err := v2monitors . Create ( lbaas . lb , v2monitors . Create Opts { Name : fmt . Sprintf ( " " , name , port Index ) , Pool ID : pool . ID , Type : string ( port . Protocol ) , Delay : int ( lbaas . opts . Monitor Delay . Duration . Seconds ( ) ) , Timeout : int ( lbaas . opts . Monitor Timeout . Duration . Seconds ( ) ) , Max Retries : int ( lbaas . opts . Monitor Max provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning monitor } else if lbaas . opts . Create if monitor ID != " " { klog . V ( 4 ) . Infof ( " " , pool . ID , monitor // All remaining listeners are obsolete, delete for _ , listener := range old // get pool for listener pool , err := get Pool By Listener if err != nil && err != Err Not if pool != nil { // get and delete monitor monitor ID := pool . Monitor if monitor ID != " " { klog . V ( 4 ) . Infof ( " " , monitor err = v2monitors . Delete ( lbaas . lb , monitor ID ) . Extract if err != nil && ! is Not Found ( err ) { return nil , fmt . Errorf ( " " , monitor provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning // get and delete pool members members , err := get Members By Pool if err != nil && ! is Not err := v2pools . Delete Member ( lbaas . lb , pool . ID , member . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning // delete pool err = v2pools . Delete ( lbaas . lb , pool . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning // delete listener err = listeners . Delete ( lbaas . lb , listener . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return nil , fmt . Errorf ( " " , provisioning port ID := loadbalancer . Vip Port float IP , err := get Floating IP By Port ID ( lbaas . network , port if err != nil && err != Err Not Found { return nil , fmt . Errorf ( " " , port if float IP == nil && floating Pool != " " && ! internal Annotation { klog . V ( 4 ) . Infof ( " " , loadbalancer . ID , port float IP Opts := floatingips . Create Opts { Floating Network ID : floating Pool , Port ID : port load Balancer IP := api Service . Spec . Load Balancer if load Balancer IP != " " { float IP Opts . Floating IP = load Balancer float IP , err = floatingips . Create ( lbaas . network , float IP if err != nil { return nil , fmt . Errorf ( " " , float IP status := & v1 . Load Balancer if float IP != nil { status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : float IP . Floating } else { status . Ingress = [ ] v1 . Load Balancer Ingress { { IP : loadbalancer . Vip if lbaas . opts . Manage Security Groups { err := lbaas . ensure Security Group ( cluster Name , api if err != nil { // cleanup what was created so far _ = lbaas . Ensure Load Balancer Deleted ( ctx , cluster Name , api } 
func ( lbaas * Lbaas V2 ) ensure Security Group ( cluster Name string , api Service * v1 . Service , nodes [ ] * v1 . Node , loadbalancer * loadbalancers . Load if len ( lbaas . opts . Node Security Group I Ds ) == 0 { lbaas . opts . Node Security Group I Ds , err = get Node Security Group ID For if err != nil { return fmt . Errorf ( " " , api Service . Namespace , api klog . V ( 4 ) . Infof ( " " , lbaas . opts . Node Security Group I Ds , api Service . Namespace , api // get service ports ports := api // get service source ranges source Ranges , err := servicehelpers . Get Load Balancer Source Ranges ( api if err != nil { return fmt . Errorf ( " " , api Service . Namespace , api // ensure security group for LB lb Sec Group Name := get Security Group Name ( api lb Sec Group ID , err := groups . ID From Name ( lbaas . network , lb Sec Group if err != nil { // If the security group of LB not exist, create it later if is Security Group Not Found ( err ) { lb Sec Group } else { return fmt . Errorf ( " " , lb Sec Group if len ( lb Sec Group ID ) == 0 { // create security group lb Sec Group Create Opts := groups . Create Opts { Name : get Security Group Name ( api Service ) , Description : fmt . Sprintf ( " " , api Service . Namespace , api Service . Name , cluster lb Sec Group , err := groups . Create ( lbaas . network , lb Sec Group Create if err != nil { return fmt . Errorf ( " " , api Service . Namespace , api lb Sec Group ID = lb Sec //add rule in security group for _ , port := range ports { for _ , source Range := range source Ranges . String Slice ( ) { ethertype := rules . Ether network , _ , err := net . Parse CIDR ( source if err != nil { return fmt . Errorf ( " " , source if network . To4 ( ) == nil { ethertype = rules . Ether lb Sec Group Rule Create Opts := rules . Create Opts { Direction : rules . Dir Ingress , Port Range Max : int ( port . Port ) , Port Range Min : int ( port . Port ) , Protocol : to Rule Protocol ( port . Protocol ) , Remote IP Prefix : source Range , Sec Group ID : lb Sec Group . ID , Ether _ , err = rules . Create ( lbaas . network , lb Sec Group Rule Create if err != nil { return fmt . Errorf ( " " , lb Sec lb Sec Group Rule Create Opts := rules . Create Opts { Direction : rules . Dir Ingress , Port Range Max : 4 , // ICMP: Code - Values for ICMP "Destination Unreachable: Fragmentation Needed and Don't Fragment was Set" Port Range Min : 3 , // ICMP: Type Protocol : rules . Protocol ICMP , Remote IP Prefix : " " , // The Fragmentation packet can come from anywhere along the path back to the source Range - we need to all this from all Sec Group ID : lb Sec Group . ID , Ether Type : rules . Ether _ , err = rules . Create ( lbaas . network , lb Sec Group Rule Create if err != nil { return fmt . Errorf ( " " , lb Sec lb Sec Group Rule Create Opts = rules . Create Opts { Direction : rules . Dir Ingress , Port Range Max : 0 , // ICMP: Code - Values for ICMP "Packet Too Big" Port Range Min : 2 , // ICMP: Type Protocol : rules . Protocol ICMP , Remote IP Prefix : " " , // The Fragmentation packet can come from anywhere along the path back to the source Range - we need to all this from all Sec Group ID : lb Sec Group . ID , Ether Type : rules . Ether _ , err = rules . Create ( lbaas . network , lb Sec Group Rule Create if err != nil { return fmt . Errorf ( " " , lb Sec // get security groups of port port ID := loadbalancer . Vip Port port , err := get Port By ID ( lbaas . network , port for _ , port Security Groups := range port . Security Groups { if port Security Groups == lb Sec // update loadbalancer vip port if ! found { port . Security Groups = append ( port . Security Groups , lb Sec update Opts := neutronports . Update Opts { Security Groups : & port . Security res := neutronports . Update ( lbaas . network , port ID , update if res . Err != nil { msg := fmt . Sprintf ( " " , port ID , api Service . Namespace , api // ensure rules for every node security group for _ , port := range ports { for _ , node Security Group ID := range lbaas . opts . Node Security Group I Ds { opts := rules . List Opts { Direction : string ( rules . Dir Ingress ) , Sec Group ID : node Security Group ID , Remote Group ID : lb Sec Group ID , Port Range Max : int ( port . Node Port ) , Port Range Min : int ( port . Node sec Group Rules , err := get Security Group if err != nil && ! is Not Found ( err ) { msg := fmt . Sprintf ( " " , lb Sec Group ID , node Security Group if len ( sec Group // Add the rules in the Node Security Group err = create Node Security Group ( lbaas . network , node Security Group ID , int ( port . Node Port ) , port . Protocol , lb Sec Group if err != nil { return fmt . Errorf ( " " , api Service . Namespace , api } 
func ( lbaas * Lbaas V2 ) Update Load Balancer ( ctx context . Context , cluster Name string , service * v1 . Service , nodes [ ] * v1 . Node ) error { load Balancer Name := lbaas . Get Load Balancer Name ( ctx , cluster klog . V ( 4 ) . Infof ( " " , cluster Name , load Balancer lbaas . opts . Subnet ID = get String From Service Annotation ( service , Service Annotation Load Balancer Subnet ID , lbaas . opts . Subnet if len ( lbaas . opts . Subnet ID ) == 0 && len ( nodes ) > 0 { // Get Subnet ID automatically. // The LB needs to be configured with instance addresses on the same subnet, so get Subnet ID by one node. subnet ID , err := get Subnet ID For lbaas . opts . Subnet ID = subnet loadbalancer , err := get Loadbalancer By Name ( lbaas . lb , load Balancer if loadbalancer == nil { return fmt . Errorf ( " " , load Balancer // Get all listeners for this loadbalancer, by "port key". type port var listener I lb Listeners := make ( map [ port all Listeners , err := get Listeners By Load Balancer if err != nil { return fmt . Errorf ( " " , load Balancer for _ , l := range all Listeners { key := port Key { Protocol : listeners . Protocol ( l . Protocol ) , Port : l . Protocol lb listener I Ds = append ( listener I // Get all pools for this loadbalancer, by listener ID. lb for _ , listener ID := range listener I Ds { pool , err := get Pool By Listener ID ( lbaas . lb , loadbalancer . ID , listener if err != nil { return fmt . Errorf ( " " , listener lb Pools [ listener for _ , node := range nodes { addr , err := node Address For // Check for adding/removing members associated with each port for port Index , port := range ports { // Get listener associated with this port listener , ok := lb Listeners [ port Key { Protocol : to Listeners if ! ok { return fmt . Errorf ( " " , load Balancer // Get pool associated with this listener pool , ok := lb if ! ok { return fmt . Errorf ( " " , load Balancer // Find existing pool members (by address) for this port get Members , err := get Members By Pool for _ , member := range get // Add any new members for this port for addr , node := range addrs { if _ , ok := members [ addr ] ; ok && members [ addr ] . Protocol Port == int ( port . Node _ , err := v2pools . Create Member ( lbaas . lb , pool . ID , v2pools . Create Member Opts { Name : fmt . Sprintf ( " " , loadbalancer . Name , port Index , node . Name ) , Address : addr , Protocol Port : int ( port . Node Port ) , Subnet ID : lbaas . opts . Subnet provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return fmt . Errorf ( " " , provisioning // Remove any old members for this port for _ , member := range members { if _ , ok := addrs [ member . Address ] ; ok && member . Protocol Port == int ( port . Node err = v2pools . Delete Member ( lbaas . lb , pool . ID , member . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return fmt . Errorf ( " " , provisioning if lbaas . opts . Manage Security Groups { err := lbaas . update Security Group ( cluster } 
func ( lbaas * Lbaas V2 ) update Security Group ( cluster Name string , api Service * v1 . Service , nodes [ ] * v1 . Node , loadbalancer * loadbalancers . Load Balancer ) error { original Node Security Group I Ds := lbaas . opts . Node Security Group I lbaas . opts . Node Security Group I Ds , err = get Node Security Group ID For if err != nil { return fmt . Errorf ( " " , api Service . Namespace , api klog . V ( 4 ) . Infof ( " " , lbaas . opts . Node Security Group I Ds , api Service . Namespace , api original := sets . New String ( original Node Security Group I current := sets . New String ( lbaas . opts . Node Security Group I // Generate Name lb Sec Group Name := get Security Group Name ( api lb Sec Group ID , err := groups . ID From Name ( lbaas . network , lb Sec Group if err != nil { return fmt . Errorf ( " " , lb Sec Group ports := api for _ , port := range ports { for removal := range removals { // Delete the rules in the Node Security Group opts := rules . List Opts { Direction : string ( rules . Dir Ingress ) , Sec Group ID : removal , Remote Group ID : lb Sec Group ID , Port Range Max : int ( port . Node Port ) , Port Range Min : int ( port . Node sec Group Rules , err := get Security Group if err != nil && ! is Not Found ( err ) { return fmt . Errorf ( " " , lb Sec Group for _ , rule := range sec Group if res . Err != nil && ! is Not for _ , node Security Group ID := range lbaas . opts . Node Security Group I Ds { opts := rules . List Opts { Direction : string ( rules . Dir Ingress ) , Sec Group ID : node Security Group ID , Remote Group ID : lb Sec Group ID , Port Range Max : int ( port . Node Port ) , Port Range Min : int ( port . Node sec Group Rules , err := get Security Group if err != nil && ! is Not Found ( err ) { return fmt . Errorf ( " " , lb Sec Group ID , node Security Group if len ( sec Group // Add the rules in the Node Security Group err = create Node Security Group ( lbaas . network , node Security Group ID , int ( port . Node Port ) , port . Protocol , lb Sec Group if err != nil { return fmt . Errorf ( " " , api Service . Namespace , api } 
func ( lbaas * Lbaas V2 ) Ensure Load Balancer Deleted ( ctx context . Context , cluster Name string , service * v1 . Service ) error { load Balancer Name := lbaas . Get Load Balancer Name ( ctx , cluster klog . V ( 4 ) . Infof ( " " , cluster Name , load Balancer loadbalancer , err := get Loadbalancer By Name ( lbaas . lb , load Balancer if err != nil && err != Err Not if loadbalancer . Vip Port ID != " " { port ID := loadbalancer . Vip Port floating IP , err := get Floating IP By Port ID ( lbaas . network , port if err != nil && err != Err Not if floating IP != nil { err = floatingips . Delete ( lbaas . network , floating IP . ID ) . Extract if err != nil && ! is Not // get all listeners associated with this loadbalancer listener List , err := get Listeners By Load Balancer // get all pools (and health monitors) associated with this loadbalancer var pool I var monitor I for _ , listener := range listener List { pool , err := get Pool By Listener if err != nil && err != Err Not if pool != nil { pool I Ds = append ( pool I // If create-monitor of cloud-config is false, pool has not monitor. if pool . Monitor ID != " " { monitor I Ds = append ( monitor I Ds , pool . Monitor // delete all monitors for _ , monitor ID := range monitor I Ds { err := v2monitors . Delete ( lbaas . lb , monitor ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return fmt . Errorf ( " " , provisioning // delete all members and pools for _ , pool ID := range pool I Ds { // get members for current pool members List , err := get Members By Pool ID ( lbaas . lb , pool if err != nil && ! is Not Found ( err ) { return fmt . Errorf ( " " , pool // delete all members for this pool for _ , member := range members List { err := v2pools . Delete Member ( lbaas . lb , pool ID , member . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return fmt . Errorf ( " " , provisioning // delete pool err = v2pools . Delete ( lbaas . lb , pool ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return fmt . Errorf ( " " , provisioning // delete all listeners for _ , listener := range listener List { err := listeners . Delete ( lbaas . lb , listener . ID ) . Extract if err != nil && ! is Not provisioning Status , err := wait Loadbalancer Active Provisioning if err != nil { return fmt . Errorf ( " " , provisioning // delete loadbalancer err = loadbalancers . Delete ( lbaas . lb , loadbalancer . ID ) . Extract if err != nil && ! is Not err = wait Loadbalancer // Delete the Security Group if lbaas . opts . Manage Security Groups { err := lbaas . Ensure Security Group Deleted ( cluster } 
func ( lbaas * Lbaas V2 ) Ensure Security Group Deleted ( cluster Name string , service * v1 . Service ) error { // Generate Name lb Sec Group Name := get Security Group lb Sec Group ID , err := groups . ID From Name ( lbaas . network , lb Sec Group if err != nil { if is Security Group Not return fmt . Errorf ( " " , lb Sec Group lb Sec Group := groups . Delete ( lbaas . network , lb Sec Group if lb Sec Group . Err != nil && ! is Not Found ( lb Sec Group . Err ) { return lb Sec if len ( lbaas . opts . Node Security Group I Ds ) == 0 { // Just happen when nodes have not Security Group, or should not happen // Update Load Balancer and Ensure Load Balancer can set lbaas.opts.Node Security Group I Ds when it is empty // And service controller call Update Load Balancer to set lbaas.opts.Node Security Group I } else { // Delete the rules in the Node Security Group for _ , node Security Group ID := range lbaas . opts . Node Security Group I Ds { opts := rules . List Opts { Sec Group ID : node Security Group ID , Remote Group ID : lb Sec Group sec Group Rules , err := get Security Group if err != nil && ! is Not Found ( err ) { msg := fmt . Sprintf ( " " , lb Sec Group ID , node Security Group for _ , rule := range sec Group if res . Err != nil && ! is Not } 
func ( c * Fake Volume Attachments ) Update ( volume Attachment * v1beta1 . Volume Attachment ) ( result * v1beta1 . Volume Attachment , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( volumeattachments Resource , volume Attachment ) , & v1beta1 . Volume return obj . ( * v1beta1 . Volume } 
func ( c * Fake Volume Attachments ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( volumeattachments Resource , name ) , & v1beta1 . Volume } 
func new Local Subject Access Reviews ( c * Authorization V1Client , namespace string ) * local Subject Access Reviews { return & local Subject Access Reviews { client : c . REST } 
func ( d * Unstructured Object Typer ) Object Kinds ( obj runtime . Object ) ( gvks [ ] schema . Group Version Kind , unversioned Type bool , err error ) { if _ , ok := obj . ( runtime . Unstructured ) ; ok { gvk := obj . Get Object Kind ( ) . Group Version if len ( gvk . Kind ) == 0 { return nil , false , runtime . New Missing Kind if len ( gvk . Version ) == 0 { return nil , false , runtime . New Missing Version return [ ] schema . Group Version return nil , false , runtime . New Not Registered Err For Type ( " " , reflect . Type } 
func ( s * network Policy Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Network Policy , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Network } 
func New Server ( config Config , runtime Runtime ) ( Server , error ) { s := & server { config : config , runtime : & cri Adapter { runtime } , cache : new Request if s . config . Base URL == nil { s . config . Base if s . config . TLS Config != nil { s . config . Base ws := & restful . Web handler restful . Route } { { " " , s . serve Exec } , { " " , s . serve Attach } , { " " , s . serve Port // If serving relative to a base path, set that here. path Prefix := path . Dir ( s . config . Base for _ , e := range endpoints { for _ , method := range [ ] string { " " , " " } { ws . Route ( ws . Method ( method ) . Path ( path . Join ( path handler := restful . New s . server = & http . Server { Addr : s . config . Addr , Handler : s . handler , TLS Config : s . config . TLS } 
func Build Auth ( node Name types . Node Name , client clientset . Interface , config kubeletconfig . Kubelet Configuration ) ( server . Auth Interface , error ) { // Get clients, if provided var ( token Client authenticationclient . Token Review sar Client authorizationclient . Subject Access Review if client != nil && ! reflect . Value Of ( client ) . Is Nil ( ) { token Client = client . Authentication V1beta1 ( ) . Token sar Client = client . Authorization V1beta1 ( ) . Subject Access authenticator , err := Build Authn ( token attributes := server . New Node Authorizer Attributes Getter ( node authorizer , err := Build Authz ( sar return server . New Kubelet } 
func Build Authn ( client authenticationclient . Token Review Interface , authn kubeletconfig . Kubelet Authentication ) ( authenticator . Request , error ) { authenticator Config := authenticatorfactory . Delegating Authenticator Config { Anonymous : authn . Anonymous . Enabled , Cache TTL : authn . Webhook . Cache TTL . Duration , Client CA File : authn . X509 . Client CA authenticator Config . Token Access Review authenticator , _ , err := authenticator } 
func Build Authz ( client authorizationclient . Subject Access Review Interface , authz kubeletconfig . Kubelet Authorization ) ( authorizer . Authorizer , error ) { switch authz . Mode { case kubeletconfig . Kubelet Authorization Mode Always Allow : return authorizerfactory . New Always Allow case kubeletconfig . Kubelet Authorization Mode authorizer Config := authorizerfactory . Delegating Authorizer Config { Subject Access Review Client : client , Allow Cache TTL : authz . Webhook . Cache Authorized TTL . Duration , Deny Cache TTL : authz . Webhook . Cache Unauthorized return authorizer } 
func New Protocol Port ( protocol string , port int32 ) * Protocol Port { pp := & Protocol if len ( pp . Protocol ) == 0 { pp . Protocol = string ( v1 . Protocol } 
func ( h Host Port pp := New Protocol if _ , ok := h [ ip ] ; ! ok { h [ ip ] = map [ Protocol } 
func ( h Host Port pp := New Protocol } 
func ( h Host Port } 
func ( h Host Port Info ) Check pp := New Protocol // If ip is 0.0.0.0 check all IP's (protocol, port) pair if ip == Default Bind All Host // If ip isn't 0.0.0.0, only check IP and 0.0.0.0's (protocol, port) pair for _ , key := range [ ] string { Default Bind All Host } 
func ( h Host Port Info ) sanitize ( ip , protocol * string ) { if len ( * ip ) == 0 { * ip = Default Bind All Host if len ( * protocol ) == 0 { * protocol = string ( v1 . Protocol } 
func New Cloud CIDR Allocator ( client clientset . Interface , cloud cloudprovider . Interface , node Informer informers . Node Informer ) ( CIDR event Broadcaster := record . New recorder := event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : client . Core gce if ! ok { err := fmt . Errorf ( " " , cloud . Provider ca := & cloud CIDR Allocator { client : client , cloud : gce Cloud , node Lister : node Informer . Lister ( ) , nodes Synced : node Informer . Informer ( ) . Has Synced , node Update Channel : make ( chan string , cidr Update Queue Size ) , recorder : recorder , nodes In Processing : map [ string ] * node Processing node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : nodeutil . Create Add Node Handler ( ca . Allocate Or Occupy CIDR ) , Update Func : nodeutil . Create Update Node Handler ( func ( _ , new Node * v1 . Node ) error { if new Node . Spec . Pod CIDR == " " { return ca . Allocate Or Occupy CIDR ( new // Even if Pod CIDR is assigned, but Network Unavailable condition is // set to true, we need to process the node to set the condition. network Unavailable Taint := & v1 . Taint { Key : schedulerapi . Taint Node Network Unavailable , Effect : v1 . Taint Effect No _ , cond := nodeutil . Get Node Condition ( & new Node . Status , v1 . Node Network if cond == nil || cond . Status != v1 . Condition False || utiltaints . Taint Exists ( new Node . Spec . Taints , network Unavailable Taint ) { return ca . Allocate Or Occupy CIDR ( new } ) , Delete Func : nodeutil . Create Delete Node Handler ( ca . Release klog . V ( 0 ) . Infof ( " " , cloud . Provider } 
func ( ca * cloud CIDR Allocator ) Allocate Or Occupy if ! ca . insert Node To ca . node Update } 
func ( ca * cloud CIDR Allocator ) update CIDR Allocation ( node Name string ) error { node , err := ca . node Lister . Get ( node if err != nil { if errors . Is Not klog . Errorf ( " " , node cidrs , err := ca . cloud . Alias Ranges ( types . Node Name ( node if err != nil { nodeutil . Record Node Status if len ( cidrs ) == 0 { nodeutil . Record Node Status _ , cidr , err := net . Parse pod if node . Spec . Pod CIDR == pod CIDR { klog . V ( 4 ) . Infof ( " " , node . Name , pod // We don't return here, in order to set the Network Unavailable condition later below. } else { if node . Spec . Pod CIDR != " " { klog . Errorf ( " " , node . Name , node . Spec . Pod CIDR , pod // We fall through and set the CIDR despite this error. This // implements the same logic as implemented in the // range for i := 0 ; i < cidr Update Retries ; i ++ { if err = utilnode . Patch Node CIDR ( ca . client , types . Node Name ( node . Name ) , pod CIDR ) ; err == nil { klog . Infof ( " " , node . Name , pod if err != nil { nodeutil . Record Node Status klog . Errorf ( " " , node . Name , pod err = utilnode . Set Node Condition ( ca . client , types . Node Name ( node . Name ) , v1 . Node Condition { Type : v1 . Node Network Unavailable , Status : v1 . Condition False , Reason : " " , Message : " " , Last Transition } 
func Get Chain Lines ( table Table , save [ ] byte ) map [ Chain ] [ ] byte { chains table read // find beginning of table for read Index < len ( save ) { line , n := read Line ( read read if bytes . Has Prefix ( line , table // parse table lines for read Index < len ( save ) { line , n := read Line ( read read if bytes . Has Prefix ( line , commit } else if line [ 0 ] == ':' && len ( line ) > 1 { // We assume that the <line> contains space - chain lines have 3 fields, // space delimited. If there is no space, this line will panic. chain := Chain ( line [ 1 : bytes . Index ( line , space chains return chains } 
func Is Qualified if len ( prefix ) == 0 { errs = append ( errs , " " + Empty } else if msgs := Is DNS1123Subdomain ( prefix ) ; len ( msgs ) != 0 { errs = append ( errs , prefix default : return append ( errs , " " + Regex Error ( qualified Name Err Msg , qualified Name if len ( name ) == 0 { errs = append ( errs , " " + Empty } else if len ( name ) > qualified Name Max Length { errs = append ( errs , " " + Max Len Error ( qualified Name Max if ! qualified Name Regexp . Match String ( name ) { errs = append ( errs , " " + Regex Error ( qualified Name Err Msg , qualified Name } 
func Is Fully Qualified Name ( fld Path * field . Path , name string ) field . Error List { var all Errors field . Error if len ( name ) == 0 { return append ( all Errors , field . Required ( fld if errs := Is DNS1123Subdomain ( name ) ; len ( errs ) > 0 { return append ( all Errors , field . Invalid ( fld if len ( strings . Split ( name , " " ) ) < 3 { return append ( all Errors , field . Invalid ( fld return all } 
func Is Valid Label if len ( value ) > Label Value Max Length { errs = append ( errs , Max Len Error ( Label Value Max if ! label Value Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( label Value Err Msg , label Value } 
func Is if len ( value ) > DNS1123Label Max Length { errs = append ( errs , Max Len Error ( DNS1123Label Max if ! dns1123Label Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( dns1123Label Err Msg , dns1123Label } 
func Is if len ( value ) > DNS1123Subdomain Max Length { errs = append ( errs , Max Len Error ( DNS1123Subdomain Max if ! dns1123Subdomain Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( dns1123Subdomain Error Msg , dns1123Subdomain } 
func Is if len ( value ) > DNS1035Label Max Length { errs = append ( errs , Max Len Error ( DNS1035Label Max if ! dns1035Label Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( dns1035Label Err Msg , dns1035Label } 
func Is Wildcard DNS1123Subdomain ( value string ) [ ] string { wildcard DNS1123Subdomain Regexp := regexp . Must Compile ( " " + wildcard DNS1123Subdomain if len ( value ) > DNS1123Subdomain Max Length { errs = append ( errs , Max Len Error ( DNS1123Subdomain Max if ! wildcard DNS1123Subdomain Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( wildcard DNS1123Subdomain Err Msg , wildcard DNS1123Subdomain } 
func Is C Identifier ( value string ) [ ] string { if ! c Identifier Regexp . Match String ( value ) { return [ ] string { Regex Error ( identifier Err Msg , c Identifier } 
func Is In return [ ] string { Inclusive Range } 
func Is Valid Group ID ( gid int64 ) [ ] string { if min Group ID <= gid && gid <= max Group return [ ] string { Inclusive Range Error ( min Group ID , max Group } 
func Is Valid User ID ( uid int64 ) [ ] string { if min User ID <= uid && uid <= max User return [ ] string { Inclusive Range Error ( min User ID , max User } 
func Is Valid Port if len ( port ) > 15 { errs = append ( errs , Max Len if ! port Name Charset Regex . Match if ! port Name One Letter Regexp . Match } 
func Is Valid IP ( value string ) [ ] string { if net . Parse } 
func Is Valid Percent ( percent string ) [ ] string { if ! percent Regexp . Match String ( percent ) { return [ ] string { Regex Error ( percent Err Msg , percent } 
func Is HTTP Header Name ( value string ) [ ] string { if ! http Header Name Regexp . Match String ( value ) { return [ ] string { Regex Error ( http Header Name Err Msg , http Header Name } 
func Is Env Var if ! env Var Name Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( env Var Name Fmt Err Msg , env Var Name errs = append ( errs , has Ch Dir } 
func Is Config Map if len ( value ) > DNS1123Subdomain Max Length { errs = append ( errs , Max Len Error ( DNS1123Subdomain Max if ! config Map Key Regexp . Match String ( value ) { errs = append ( errs , Regex Error ( config Map Key Err Msg , config Map Key errs = append ( errs , has Ch Dir } 
func Regex } 
func Is Valid Socket ip , port , err := net . Split Host port errs = append ( errs , Is Valid Port Num ( port errs = append ( errs , Is Valid } 
func New Exec Mounter ( exec mount . Exec , wrapped mount . Interface ) mount . Interface { return & exec Mounter { wrapped } 
func ( m * exec Mounter ) Mount ( source string , target string , fstype string , options [ ] string ) error { bind , bind Opts , bind Remount Opts := mount . Is if bind { err := m . do Exec Mount ( source , target , fstype , bind return m . do Exec Mount ( source , target , fstype , bind Remount return m . do Exec } 
func ( m * exec Mounter ) do Exec mount Args := mount . Make Mount output , err := m . exec . Run ( " " , mount klog . V ( 5 ) . Infof ( " " , mount } 
func ( m * exec Mounter ) Unmount ( target string ) error { output if err == nil { klog . V ( 5 ) . Infof ( " " , target , string ( output } else { klog . V ( 5 ) . Infof ( " " , target , err , string ( output } 
func ( m * exec Mounter ) Is Likely Not Mount Point ( file string ) ( bool , error ) { return m . wrapped Mounter . Is Likely Not Mount } 
func ( m * exec Mounter ) Device Opened ( pathname string ) ( bool , error ) { return m . wrapped Mounter . Device } 
func ( m * exec Mounter ) Path Is Device ( pathname string ) ( bool , error ) { return m . wrapped Mounter . Path Is } 
func ( m * exec Mounter ) Get Device Name From Mount ( mount Path , plugin Mount Dir string ) ( string , error ) { return m . wrapped Mounter . Get Device Name From Mount ( mount Path , plugin Mount } 
func New Deployment Controller ( d Informer appsinformers . Deployment Informer , rs Informer appsinformers . Replica Set Informer , pod Informer coreinformers . Pod Informer , client clientset . Interface ) ( * Deployment Controller , error ) { event Broadcaster := record . New event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : client . Core if client != nil && client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { if err := metrics . Register Metric And Track Rate Limiter Usage ( " " , client . Core V1 ( ) . REST Client ( ) . Get Rate dc := & Deployment Controller { client : client , event Recorder : event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event Source { Component : " " } ) , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate dc . rs Control = controller . Real RS Control { Kube Client : client , Recorder : dc . event d Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : dc . add Deployment , Update Func : dc . update Deployment , // This will enter the sync loop and no-op, because the deployment has been deleted from the store. Delete Func : dc . delete rs Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : dc . add Replica Set , Update Func : dc . update Replica Set , Delete Func : dc . delete Replica pod Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Delete Func : dc . delete dc . sync Handler = dc . sync dc . enqueue dc . d Lister = d dc . rs Lister = rs dc . pod Lister = pod dc . d Lister Synced = d Informer . Informer ( ) . Has dc . rs Lister Synced = rs Informer . Informer ( ) . Has dc . pod Lister Synced = pod Informer . Informer ( ) . Has } 
func ( dc * Deployment Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer dc . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , dc . d Lister Synced , dc . rs Lister Synced , dc . pod Lister for i := 0 ; i < workers ; i ++ { go wait . Until ( dc . worker , time . Second , stop <- stop } 
func ( dc * Deployment Controller ) add Replica Set ( obj interface { } ) { rs := obj . ( * apps . Replica if rs . Deletion Timestamp != nil { // On a restart of the controller manager, it's possible for an object to // show up in a state that is already pending deletion. dc . delete Replica // If it has a Controller Ref, that's all that matters. if controller Ref := metav1 . Get Controller Of ( rs ) ; controller Ref != nil { d := dc . resolve Controller Ref ( rs . Namespace , controller dc . enqueue // Otherwise, it's an orphan. Get a list of all matching Deployments and sync // them to see if anyone wants to adopt it. ds := dc . get Deployments For Replica for _ , d := range ds { dc . enqueue } 
func ( dc * Deployment Controller ) get Deployments For Replica Set ( rs * apps . Replica Set ) [ ] * apps . Deployment { deployments , err := dc . d Lister . Get Deployments For Replica // Because all Replica Set's belonging to a deployment should have a unique label key, // there should never be more than one deployment returned by the above method. // If that happens we should probably dynamically repair the situation by ultimately // trying to clean up one of the controllers, for now we just return the older one if len ( deployments ) > 1 { // Controller } 
func ( dc * Deployment Controller ) update Replica Set ( old , cur interface { } ) { cur RS := cur . ( * apps . Replica old RS := old . ( * apps . Replica if cur RS . Resource Version == old RS . Resource Version { // Periodic resync will send update events for all known replica sets. // Two different versions of the same replica set will always have different R cur Controller Ref := metav1 . Get Controller Of ( cur old Controller Ref := metav1 . Get Controller Of ( old controller Ref Changed := ! reflect . Deep Equal ( cur Controller Ref , old Controller if controller Ref Changed && old Controller Ref != nil { // The Controller Ref was changed. Sync the old controller, if any. if d := dc . resolve Controller Ref ( old RS . Namespace , old Controller Ref ) ; d != nil { dc . enqueue // If it has a Controller Ref, that's all that matters. if cur Controller Ref != nil { d := dc . resolve Controller Ref ( cur RS . Namespace , cur Controller klog . V ( 4 ) . Infof ( " " , cur dc . enqueue // Otherwise, it's an orphan. If anything changed, sync matching controllers // to see if anyone wants to adopt it now. label Changed := ! reflect . Deep Equal ( cur RS . Labels , old if label Changed || controller Ref Changed { ds := dc . get Deployments For Replica Set ( cur klog . V ( 4 ) . Infof ( " " , cur for _ , d := range ds { dc . enqueue } 
func ( dc * Deployment Controller ) delete Replica Set ( obj interface { } ) { rs , ok := obj . ( * apps . Replica // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the Replica Set // changed labels the new deployment will not be woken up till the periodic resync. if ! ok { tombstone , ok := obj . ( cache . Deleted Final State if ! ok { utilruntime . Handle rs , ok = tombstone . Obj . ( * apps . Replica if ! ok { utilruntime . Handle controller Ref := metav1 . Get Controller if controller d := dc . resolve Controller Ref ( rs . Namespace , controller dc . enqueue } 
func ( dc * Deployment Controller ) delete // When a delete is dropped, the relist will notice a pod in the store not // in the list, leading to the insertion of a tombstone object which contains // the deleted key/value. Note that this value might be stale. If the Pod // changed labels the new deployment will not be woken up till the periodic resync. if ! ok { tombstone , ok := obj . ( cache . Deleted Final State if ! ok { utilruntime . Handle if ! ok { utilruntime . Handle if d := dc . get Deployment For Pod ( pod ) ; d != nil && d . Spec . Strategy . Type == apps . Recreate Deployment Strategy Type { // Sync if this Deployment now has no more Pods. rs List , err := util . List Replica Sets ( d , util . Rs List From Client ( dc . client . Apps pod Map , err := dc . get Pod Map For Deployment ( d , rs num for _ , pod List := range pod Map { num Pods += len ( pod if num Pods == 0 { dc . enqueue } 
func ( dc * Deployment Controller ) enqueue After ( deployment * apps . Deployment , after time . Duration ) { key , err := controller . Key if err != nil { utilruntime . Handle dc . queue . Add } 
func ( dc * Deployment Controller ) get Deployment For Pod ( pod * v1 . Pod ) * apps . Deployment { // Find the owning replica set var rs * apps . Replica controller Ref := metav1 . Get Controller if controller if controller Ref . Kind != apps . Scheme Group Version . With rs , err = dc . rs Lister . Replica Sets ( pod . Namespace ) . Get ( controller if err != nil || rs . UID != controller Ref . UID { klog . V ( 4 ) . Infof ( " " , controller // Now find the Deployment that owns that Replica Set. controller Ref = metav1 . Get Controller if controller return dc . resolve Controller Ref ( rs . Namespace , controller } 
func ( dc * Deployment Controller ) resolve Controller Ref ( namespace string , controller Ref * metav1 . Owner Reference ) * apps . Deployment { // We can't look up by UID, so look up by Name and then verify UID. // Don't even try to look up by Name if it's the wrong Kind. if controller Ref . Kind != controller d , err := dc . d Lister . Deployments ( namespace ) . Get ( controller if d . UID != controller Ref . UID { // The controller we found with this Name is not the same one that the // Controller } 
func ( dc * Deployment Controller ) get Replica Sets For Deployment ( d * apps . Deployment ) ( [ ] * apps . Replica Set , error ) { // List all Replica Sets to find those we own but that no longer match our // selector. They will be orphaned by Claim Replica Sets(). rs List , err := dc . rs Lister . Replica deployment Selector , err := metav1 . Label Selector As // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing Replica Sets (see #42639). can Adopt Func := controller . Recheck Deletion Timestamp ( func ( ) ( metav1 . Object , error ) { fresh , err := dc . client . Apps V1 ( ) . Deployments ( d . Namespace ) . Get ( d . Name , metav1 . Get cm := controller . New Replica Set Controller Ref Manager ( dc . rs Control , d , deployment Selector , controller Kind , can Adopt return cm . Claim Replica Sets ( rs } 
func ( dc * Deployment Controller ) get Pod Map For Deployment ( d * apps . Deployment , rs List [ ] * apps . Replica Set ) ( map [ types . UID ] * v1 . Pod List , error ) { // Get all Pods that potentially belong to this Deployment. selector , err := metav1 . Label Selector As pods , err := dc . pod // Group Pods by their controller (if it's in rs List). pod Map := make ( map [ types . UID ] * v1 . Pod List , len ( rs for _ , rs := range rs List { pod Map [ rs . UID ] = & v1 . Pod for _ , pod := range pods { // Do not ignore inactive Pods because Recreate Deployments need to verify that no // Pods from older versions are running before spinning up new Pods. controller Ref := metav1 . Get Controller if controller // Only append if we care about this UID. if pod List , ok := pod Map [ controller Ref . UID ] ; ok { pod List . Items = append ( pod return pod } 
func ( dc * Deployment Controller ) sync Deployment ( key string ) error { start klog . V ( 4 ) . Infof ( " " , key , start defer func ( ) { klog . V ( 4 ) . Infof ( " " , key , time . Since ( start namespace , name , err := cache . Split Meta Namespace deployment , err := dc . d if errors . Is Not // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment . Deep everything := metav1 . Label if reflect . Deep Equal ( d . Spec . Selector , & everything ) { dc . event Recorder . Eventf ( d , v1 . Event Type if d . Status . Observed Generation < d . Generation { d . Status . Observed dc . client . Apps V1 ( ) . Deployments ( d . Namespace ) . Update // List Replica Sets owned by this Deployment, while reconciling Controller Ref // through adoption/orphaning. rs List , err := dc . get Replica Sets For // List all Pods owned by this Deployment, grouped by their Replica Set. // Current uses of the pod Map are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. pod Map , err := dc . get Pod Map For Deployment ( d , rs if d . Deletion Timestamp != nil { return dc . sync Status Only ( d , rs // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progress Deadline Seconds. if err = dc . check Paused if d . Spec . Paused { return dc . sync ( d , rs // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won't proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if get Rollback To ( d ) != nil { return dc . rollback ( d , rs scaling Event , err := dc . is Scaling Event ( d , rs if scaling Event { return dc . sync ( d , rs switch d . Spec . Strategy . Type { case apps . Recreate Deployment Strategy Type : return dc . rollout Recreate ( d , rs List , pod case apps . Rolling Update Deployment Strategy Type : return dc . rollout Rolling ( d , rs } 
func New Config ( codecs serializer . Codec Factory ) * Config { return & Config { Serializer : codecs , Build Handler Chain Func : Default Build Handler Chain , Handler Chain Wait Group : new ( utilwaitgroup . Safe Wait Group ) , Legacy API Group Prefixes : sets . New String ( Default Legacy API Prefix ) , Disabled Post Start Hooks : sets . New String ( ) , Healthz Checks : [ ] healthz . Healthz Checker { healthz . Ping Healthz , healthz . Log Healthz } , Enable Index : true , Enable Discovery : true , Enable Profiling : true , Enable Metrics : true , Max Requests In Flight : 400 , Max Mutating Requests In Flight : 200 , Request Timeout : time . Duration ( 60 ) * time . Second , Min Request Timeout : 1800 , // 10MB is the recommended maximum client request size in bytes // the etcd server should accept. See // https://github.com/etcd-io/etcd/blob/release-3.3/etcdserver/server.go#L90. // A request body might be encoded in json, and is converted to // proto when persisted in etcd. Assuming the upper bound of // the size ratio is 10:1, we set 100MB as the largest size // increase the "copy" operations in a json patch may cause. JSON Patch Max Copy Bytes : int64 ( 100 * 1024 * 1024 ) , // 10MB is the recommended maximum client request size in bytes // the etcd server should accept. See // https://github.com/etcd-io/etcd/blob/release-3.3/etcdserver/server.go#L90. // A request body might be encoded in json, and is converted to // proto when persisted in etcd. Assuming the upper bound of // the size ratio is 10:1, we set 100MB as the largest request // body size to be accepted and decoded in a write request. Max Request Body Bytes : int64 ( 100 * 1024 * 1024 ) , Enable API Response Compression : utilfeature . Default Feature Gate . Enabled ( features . API Response Compression ) , // Default to treating watch as a long-running operation // Generic API servers have no inherent long-running subresources Long Running Func : genericfilters . Basic Long Running Request Check ( sets . New String ( " " ) , sets . New } 
func ( c * Config ) Complete ( informers informers . Shared Informer Factory ) Completed Config { if len ( c . External Address ) == 0 && c . Public Address != nil { c . External Address = c . Public // if there is no port, and we listen on one securely, use that one if _ , _ , err := net . Split Host Port ( c . External Address ) ; err != nil { if c . Secure _ , port , err := c . Secure Serving . Host c . External Address = net . Join Host Port ( c . External if c . Open API Config != nil { if c . Open API Config . Security Definitions != nil { // Setup Open API security: all AP Is will have the same authentication for now. c . Open API Config . Default for k := range * c . Open API Config . Security for _ , k := range keys { c . Open API Config . Default Security = append ( c . Open API Config . Default if c . Open API Config . Common Responses == nil { c . Open API Config . Common if _ , exists := c . Open API Config . Common Responses [ http . Status Unauthorized ] ; ! exists { c . Open API Config . Common Responses [ http . Status Unauthorized ] = spec . Response { Response Props : spec . Response // make sure we populate info, and info.version, if not manually set if c . Open API Config . Info == nil { c . Open API if c . Open API Config . Info . Version == " " { if c . Version != nil { c . Open API } else { c . Open API if c . Discovery Addresses == nil { c . Discovery Addresses = discovery . Default Addresses { Default Address : c . External Authorize Client Bearer Token ( c . Loopback Client if c . Request Info Resolver == nil { c . Request Info Resolver = New Request Info return Completed Config { & completed } 
func ( c completed Config ) New ( name string , delegation Target Delegation Target ) ( * Generic API if c . Loopback Client handler Chain Builder := func ( handler http . Handler ) http . Handler { return c . Build Handler Chain api Server Handler := New API Server Handler ( name , c . Serializer , handler Chain Builder , delegation Target . Unprotected s := & Generic API Server { discovery Addresses : c . Discovery Addresses , Loopback Client Config : c . Loopback Client Config , legacy API Group Prefixes : c . Legacy API Group Prefixes , admission Control : c . Admission Control , Serializer : c . Serializer , Audit Backend : c . Audit Backend , Authorizer : c . Authorization . Authorizer , delegation Target : delegation Target , Handler Chain Wait Group : c . Handler Chain Wait Group , min Request Timeout : time . Duration ( c . Min Request Timeout ) * time . Second , Shutdown Timeout : c . Request Timeout , Secure Serving Info : c . Secure Serving , External Address : c . External Address , Handler : api Server Handler , listed Path Provider : api Server Handler , open API Config : c . Open API Config , post Start Hooks : map [ string ] post Start Hook Entry { } , pre Shutdown Hooks : map [ string ] pre Shutdown Hook Entry { } , disabled Post Start Hooks : c . Disabled Post Start Hooks , healthz Checks : c . Healthz Checks , Discovery Group Manager : discovery . New Root AP Is Handler ( c . Discovery Addresses , c . Serializer ) , enable API Response Compression : c . Enable API Response Compression , max Request Body Bytes : c . Max Request Body for { if c . JSON Patch Max Copy existing := atomic . Load Int64 ( & jsonpatch . Accumulated Copy Size if existing > 0 && existing < c . JSON Patch Max Copy if atomic . Compare And Swap Int64 ( & jsonpatch . Accumulated Copy Size Limit , existing , c . JSON Patch Max Copy for k , v := range delegation Target . Post Start Hooks ( ) { s . post Start for k , v := range delegation Target . Pre Shutdown Hooks ( ) { s . pre Shutdown generic Api Server Hook if c . Shared Informer Factory != nil && ! s . is Post Start Hook Registered ( generic Api Server Hook Name ) { err := s . Add Post Start Hook ( generic Api Server Hook Name , func ( context Post Start Hook Context ) error { c . Shared Informer Factory . Start ( context . Stop for _ , delegate Check := range delegation Target . Healthz for _ , existing Check := range c . Healthz Checks { if existing Check . Name ( ) == delegate s . healthz Checks = append ( s . healthz Checks , delegate s . listed Path Provider = routes . Listed Path Providers { s . listed Path Provider , delegation install // use the Unprotected Handler from the delegation target to ensure that we don't attempt to double authenticator, authorize, // or some other part of the filter chain in delegation cases. if delegation Target . Unprotected Handler ( ) == nil && c . Enable Index { s . Handler . Non Go Restful Mux . Not Found Handler ( routes . Index Lister { Status Code : http . Status Not Found , Path Provider : s . listed Path } 
func Authorize Client Bearer Token ( loopback * restclient . Config , authn * Authentication Info , authz * Authorization Info ) { if loopback == nil || len ( loopback . Bearer privileged Loopback Token := loopback . Bearer var uid = uuid . New tokens := make ( map [ string ] * user . Default tokens [ privileged Loopback Token ] = & user . Default Info { Name : user . API Server User , UID : uid , Groups : [ ] string { user . System Privileged token Authenticator := authenticatorfactory . New From authn . Authenticator = authenticatorunion . New ( token token Authorizer := authorizerfactory . New Privileged Groups ( user . System Privileged authz . Authorizer = authorizerunion . New ( token } 
func ( f * shared Informer Factory ) For Resource ( resource schema . Group Version Resource ) ( Generic Informer , error ) { switch resource { // Group=example.apiserver.code-generator.k8s.io, Version=internal Version case example . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group Resource ( ) , informer : f . Example ( ) . Internal Version ( ) . Test // Group=example.test.apiserver.code-generator.k8s.io, Version=internal Version case example2 . Scheme Group Version . With Resource ( " " ) : return & generic Informer { resource : resource . Group Resource ( ) , informer : f . Second Example ( ) . Internal Version ( ) . Test } 
func New GRPC Service ( endpoint string , call addr , err := parse connection , err := grpc . Dial ( addr , grpc . With Insecure ( ) , grpc . With Default Call Options ( grpc . Fail Fast ( false ) ) , grpc . With Dialer ( func ( string , time . Duration ) ( net . Conn , error ) { // Ignoring addr and timeout arguments: // addr - comes from the closure // timeout - is ignored since we are connecting in a non-blocking configuration c , err := net . Dial Timeout ( unix kms Client := kmsapi . New Key Management Service return & g RPC Service { kms Client : kms Client , connection : connection , call Timeout : call } 
func parse if u . Scheme != unix // Linux abstract namespace socket - no physical file required // Warning: Linux Abstract sockets have not concept of ACL (unlike traditional file based sockets). // However, Linux Abstract sockets are subject to Linux networking namespace, so will only be accessible to // containers within the same pod (unless host networking is used). if strings . Has Prefix ( u . Path , " " ) { return strings . Trim } 
func ( g * g RPC Service ) Decrypt ( cipher [ ] byte ) ( [ ] byte , error ) { ctx , cancel := context . With Timeout ( context . Background ( ) , g . call if err := g . check API request := & kmsapi . Decrypt Request { Cipher : cipher , Version : kmsapi response , err := g . kms } 
func ( g * g RPC Service ) Encrypt ( plain [ ] byte ) ( [ ] byte , error ) { ctx , cancel := context . With Timeout ( context . Background ( ) , g . call if err := g . check API request := & kmsapi . Encrypt Request { Plain : plain , Version : kmsapi response , err := g . kms } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Deployment { } , & Deployment List { } , & Deployment Rollback { } , & Replication Controller Dummy { } , & Scale { } , & Daemon Set List { } , & Daemon Set { } , & Ingress { } , & Ingress List { } , & Replica Set { } , & Replica Set List { } , & Pod Security Policy { } , & Pod Security Policy List { } , & Network Policy { } , & Network Policy // Add the watch version that applies metav1 . Add To Group Version ( scheme , Scheme Group } 
func ( m * manager ) remove Worker ( pod UID types . UID , container Name string , probe Type probe Type ) { m . worker defer m . worker delete ( m . workers , probe Key { pod UID , container Name , probe } 
func ( m * manager ) worker Count ( ) int { m . worker Lock . R defer m . worker Lock . R } 
func ( * default Auth Loader ) Load Auth ( path string ) ( * clientauth . Info , error ) { return clientauth . Load From } 
func ( a * Prompting Auth auth . User , err = prompt For auth . Password , err = prompt For } 
func ( m * Colon Separated Multimap String kv := strings . Split k := strings . Trim v := strings . Trim } 
func ( m * Colon Separated Multimap String // stable sort by keys, order of values should be preserved sort . Slice } 
func REST In Peace ( storage rest . Standard Storage , err error ) rest . Standard } 
func Parse Image Name ( image string ) ( string , string , string , error ) { named , err := dockerref . Parse Normalized repo To // If no tag was specified, use the default "latest". if len ( tag ) == 0 && len ( digest ) == 0 { tag = Default Image return repo To } 
func ( o * Resource Quota Controller Options ) Add Flags ( fs * pflag . Flag fs . Duration Var ( & o . Resource Quota Sync Period . Duration , " " , o . Resource Quota Sync fs . Int32Var ( & o . Concurrent Resource Quota Syncs , " " , o . Concurrent Resource Quota } 
func ( o * Resource Quota Controller Options ) Apply To ( cfg * resourcequotaconfig . Resource Quota Controller cfg . Resource Quota Sync Period = o . Resource Quota Sync cfg . Concurrent Resource Quota Syncs = o . Concurrent Resource Quota } 
func ( o * Resource Quota Controller } 
func ( md * metrics Stat FS ) Get if md . path == " " { return metrics , New No Path Defined err := md . get Fs } 
func ( md * metrics Stat FS ) get Fs Info ( metrics * Metrics ) error { available , capacity , usage , inodes , inodes Free , inodes Used , err := fs . Fs if err != nil { return New Fs Info Failed metrics . Available = resource . New Quantity ( available , resource . Binary metrics . Capacity = resource . New Quantity ( capacity , resource . Binary metrics . Used = resource . New Quantity ( usage , resource . Binary metrics . Inodes = resource . New Quantity ( inodes , resource . Binary metrics . Inodes Free = resource . New Quantity ( inodes Free , resource . Binary metrics . Inodes Used = resource . New Quantity ( inodes Used , resource . Binary } 
func Selectable Fields ( obj * wardle . Fischer ) fields . Set { return generic . Object Meta Fields Set ( & obj . Object } 
func Get return labels . Set ( apiserver . Object Meta . Labels ) , Selectable } 
func New REST ( opts Getter generic . REST Options Getter ) * REST { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Secret { } } , New List Func : func ( ) runtime . Object { return & api . Secret List { } } , Predicate Func : secret . Matcher , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : secret . Strategy , Update Strategy : secret . Strategy , Delete Strategy : secret . Strategy , Export Strategy : secret . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts Getter , Attr Func : secret . Get Attrs , Trigger Func : secret . Secret Name Trigger if err := store . Complete With } 
func ( r * none Endpoint Reconciler ) Reconcile Endpoints ( service Name string , ip net . IP , endpoint Ports [ ] corev1 . Endpoint Port , reconcile } 
func ( r * none Endpoint Reconciler ) Remove Endpoints ( service Name string , ip net . IP , endpoint Ports [ ] corev1 . Endpoint } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Ingress { } , & Ingress // Add the watch version that applies metav1 . Add To Group Version ( scheme , Scheme Group } 
func New Cmd Create Service ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { cmd := & cobra . Command { Use : " " , Aliases : [ ] string { " " } , Short : i18n . T ( " " ) , Long : " " , Run : cmdutil . Default Sub Command Run ( io Streams . Err cmd . Add Command ( New Cmd Create Service Cluster IP ( f , io cmd . Add Command ( New Cmd Create Service Node Port ( f , io cmd . Add Command ( New Cmd Create Service Load Balancer ( f , io cmd . Add Command ( New Cmd Create Service External Name ( f , io } 
func New Cmd Create Service Cluster IP ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Service Cluster IP Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : service Cluster IP Long , Example : service Cluster IP Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Service Cluster IP Generator add Port } 
func ( o * Service Cluster IP Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Service Cluster IP Generator V1Name : generator = & generateversioned . Service Common Generator V1 { Name : name , TCP : cmdutil . Get Flag String Slice ( cmd , " " ) , Type : v1 . Service Type Cluster IP , Cluster IP : cmdutil . Get Flag default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func New Cmd Create Service Node Port ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Service Node Port Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : service Node Port Long , Example : service Node Port Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Service Node Port Generator add Port } 
func ( o * Service Node Port Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Service Node Port Generator V1Name : generator = & generateversioned . Service Common Generator V1 { Name : name , TCP : cmdutil . Get Flag String Slice ( cmd , " " ) , Type : v1 . Service Type Node Port , Cluster IP : " " , Node Port : cmdutil . Get Flag default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func New Cmd Create Service Load Balancer ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Service Load Balancer Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : service Load Balancer Long , Example : service Load Balancer Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Service Load Balancer Generator add Port } 
func New Cmd Create Service External Name ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Service External Name Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : service External Name Long , Example : service External Name Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Service External Name Generator add Port cmd . Mark Flag } 
func ( o * Service External Name Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Service External Name Generator V1Name : generator = & generateversioned . Service Common Generator V1 { Name : name , Type : v1 . Service Type External Name , External Name : cmdutil . Get Flag String ( cmd , " " ) , Cluster default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func Add To Scheme ( scheme * runtime . Scheme ) { utilruntime . Must ( kubeadm . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( v1beta2 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1beta2 . Scheme Group } 
func New REST Client ( base URL * url . URL , versioned API Path string , config Content Config , max QPS float32 , max Burst int , rate Limiter flowcontrol . Rate Limiter , client * http . Client ) ( * REST Client , error ) { base := * base if ! strings . Has base . Raw if config . Group Version == nil { config . Group Version = & schema . Group if len ( config . Content Type ) == 0 { config . Content serializers , err := create var throttle flowcontrol . Rate if max QPS > 0 && rate Limiter == nil { throttle = flowcontrol . New Token Bucket Rate Limiter ( max QPS , max } else if rate Limiter != nil { throttle = rate return & REST Client { base : & base , versioned API Path : versioned API Path , content Config : config , serializers : * serializers , create Backoff Mgr : read Exp Backoff } 
func ( c * REST Client ) Get Rate Limiter ( ) flowcontrol . Rate } 
func read Exp Backoff Config ( ) Backoff Manager { backoff Base := os . Getenv ( env Backoff backoff Duration := os . Getenv ( env Backoff backoff Base Int , err Base := strconv . Parse Int ( backoff backoff Duration Int , err Duration := strconv . Parse Int ( backoff if err Base != nil || err Duration != nil { return & No return & URL Backoff { Backoff : flowcontrol . New Back Off ( time . Duration ( backoff Base Int ) * time . Second , time . Duration ( backoff Duration } 
func create Serializers ( config Content Config ) ( * Serializers , error ) { media Types := config . Negotiated Serializer . Supported Media content Type := config . Content media Type , _ , err := mime . Parse Media Type ( content info , ok := runtime . Serializer Info For Media Type ( media Types , media if ! ok { if len ( content Type ) != 0 || len ( media Types ) == 0 { return nil , fmt . Errorf ( " " , content info = media internal GV := schema . Group Versions { { Group : config . Group Version . Group , Version : runtime . API Version Internal , } , // always include the legacy group as a decoding target to handle non-error `Status` return types { Group : " " , Version : runtime . API Version s := & Serializers { Encoder : config . Negotiated Serializer . Encoder For Version ( info . Serializer , * config . Group Version ) , Decoder : config . Negotiated Serializer . Decoder To Version ( info . Serializer , internal GV ) , Renegotiated Decoder : func ( content Type string , params map [ string ] string ) ( runtime . Decoder , error ) { info , ok := runtime . Serializer Info For Media Type ( media Types , content if ! ok { return nil , fmt . Errorf ( " " , content return config . Negotiated Serializer . Decoder To Version ( info . Serializer , internal if info . Stream Serializer != nil { s . Streaming Serializer = info . Stream s . Framer = info . Stream } 
func ( c * REST Client ) Verb ( verb string ) * Request { backoff := c . create Backoff if c . Client == nil { return New Request ( nil , verb , c . base , c . versioned API Path , c . content return New Request ( c . Client , verb , c . base , c . versioned API Path , c . content } 
func ( c * REST Client ) Patch ( pt types . Patch Type ) * Request { return c . Verb ( " " ) . Set } 
func ( s * local Subject Access Review Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Local Subject Access Review , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Local Subject Access } 
func ( s * local Subject Access Review Lister ) Local Subject Access Reviews ( namespace string ) Local Subject Access Review Namespace Lister { return local Subject Access Review Namespace } 
func ( s local Subject Access Review Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Local Subject Access Review , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1 . Local Subject Access } 
func New Event Informer ( client kubernetes . Interface , namespace string , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Event Informer ( client , namespace , resync } 
func add Known Types ( scheme * runtime . Scheme ) error { // TODO this gets cleaned up when the types are fixed scheme . Add Known Types ( Scheme Group Version , & Lease { } , & Lease } 
func New HTTP Extender ( config * schedulerapi . Extender Config ) ( algorithm . Scheduler Extender , error ) { if config . HTTP Timeout . Nanoseconds ( ) == 0 { config . HTTP Timeout = time . Duration ( Default Extender transport , err := make client := & http . Client { Transport : transport , Timeout : config . HTTP managed Resources := sets . New for _ , r := range config . Managed Resources { managed return & HTTP Extender { extender URL : config . URL Prefix , preempt Verb : config . Preempt Verb , filter Verb : config . Filter Verb , prioritize Verb : config . Prioritize Verb , bind Verb : config . Bind Verb , weight : config . Weight , client : client , node Cache Capable : config . Node Cache Capable , managed Resources : managed } 
func ( h * HTTP Extender ) Process Preemption ( pod * v1 . Pod , node To Victims map [ * v1 . Node ] * schedulerapi . Victims , node Name To Info map [ string ] * schedulernodeinfo . Node Info , ) ( map [ * v1 . Node ] * schedulerapi . Victims , error ) { var ( result schedulerapi . Extender Preemption args * schedulerapi . Extender Preemption if ! h . Supports Preemption ( ) { return nil , fmt . Errorf ( " " , h . extender if h . node Cache Capable { // If extender has cached node info, pass Node Name To Meta Victims in args. node Name To Meta Victims := convert To Node Name To Meta Victims ( node To args = & schedulerapi . Extender Preemption Args { Pod : pod , Node Name To Meta Victims : node Name To Meta } else { node Name To Victims := convert To Node Name To Victims ( node To args = & schedulerapi . Extender Preemption Args { Pod : pod , Node Name To Victims : node Name To if err := h . send ( h . preempt // Extender will always return Node Name To Meta Victims. // So let's convert it to Node To Victims by using Node Name To Info. new Node To Victims , err := h . convert To Node To Victims ( result . Node Name To Meta Victims , node Name To // Do not override node To Victims return new Node To } 
func ( h * HTTP Extender ) convert To Node To Victims ( node Name To Meta Victims map [ string ] * schedulerapi . Meta Victims , node Name To Info map [ string ] * schedulernodeinfo . Node Info , ) ( map [ * v1 . Node ] * schedulerapi . Victims , error ) { node To for node Name , meta Victims := range node Name To Meta for _ , meta Pod := range meta Victims . Pods { pod , err := h . convert Pod UID To Pod ( meta Pod , node Name , node Name To node To Victims [ node Name To Info [ node return node To } 
func ( h * HTTP Extender ) convert Pod UID To Pod ( meta Pod * schedulerapi . Meta Pod , node Name string , node Name To Info map [ string ] * schedulernodeinfo . Node Info ) ( * v1 . Pod , error ) { var node Info * schedulernodeinfo . Node if node Info , ok := node Name To Info [ node Name ] ; ok { for _ , pod := range node Info . Pods ( ) { if string ( pod . UID ) == meta return nil , fmt . Errorf ( " " , h . extender URL , meta Pod , node return nil , fmt . Errorf ( " " , h . extender URL , node } 
func convert To Node Name To Meta Victims ( node To Victims map [ * v1 . Node ] * schedulerapi . Victims , ) map [ string ] * schedulerapi . Meta Victims { node Name To Victims := map [ string ] * schedulerapi . Meta for node , victims := range node To Victims { meta Victims := & schedulerapi . Meta Victims { Pods : [ ] * schedulerapi . Meta for _ , pod := range victims . Pods { meta Pod := & schedulerapi . Meta meta Victims . Pods = append ( meta Victims . Pods , meta node Name To Victims [ node . Get Name ( ) ] = meta return node Name To } 
func convert To Node Name To Victims ( node To Victims map [ * v1 . Node ] * schedulerapi . Victims , ) map [ string ] * schedulerapi . Victims { node Name To for node , victims := range node To Victims { node Name To Victims [ node . Get return node Name To } 
func ( h * HTTP Extender ) Filter ( pod * v1 . Pod , nodes [ ] * v1 . Node , node Name To Info map [ string ] * schedulernodeinfo . Node Info , ) ( [ ] * v1 . Node , schedulerapi . Failed Nodes Map , error ) { var ( result schedulerapi . Extender Filter node List * v1 . Node node node args * schedulerapi . Extender if h . filter Verb == " " { return nodes , schedulerapi . Failed Nodes if h . node Cache Capable { node Name for _ , node := range nodes { node Name Slice = append ( node Name node Names = & node Name } else { node List = & v1 . Node for _ , node := range nodes { node List . Items = append ( node args = & schedulerapi . Extender Args { Pod : pod , Nodes : node List , Node Names : node if err := h . send ( h . filter if h . node Cache Capable && result . Node Names != nil { node Result = make ( [ ] * v1 . Node , 0 , len ( * result . Node for i := range * result . Node Names { node Result = append ( node Result , node Name To Info [ ( * result . Node } else if result . Nodes != nil { node for i := range result . Nodes . Items { node Result = append ( node return node Result , result . Failed } 
func ( h * HTTP Extender ) Prioritize ( pod * v1 . Pod , nodes [ ] * v1 . Node ) ( * schedulerapi . Host Priority List , int , error ) { var ( result schedulerapi . Host Priority node List * v1 . Node node args * schedulerapi . Extender if h . prioritize Verb == " " { result := schedulerapi . Host Priority for _ , node := range nodes { result = append ( result , schedulerapi . Host if h . node Cache Capable { node Name for _ , node := range nodes { node Name Slice = append ( node Name node Names = & node Name } else { node List = & v1 . Node for _ , node := range nodes { node List . Items = append ( node args = & schedulerapi . Extender Args { Pod : pod , Nodes : node List , Node Names : node if err := h . send ( h . prioritize } 
func ( h * HTTP Extender ) Bind ( binding * v1 . Binding ) error { var result schedulerapi . Extender Binding if ! h . Is req := & schedulerapi . Extender Binding Args { Pod Name : binding . Name , Pod Namespace : binding . Namespace , Pod if err := h . send ( h . bind } 
func ( h * HTTP url := strings . Trim Right ( h . extender req , err := http . New Request ( " " , url , bytes . New if resp . Status Code != http . Status OK { return fmt . Errorf ( " " , action , url , resp . Status return json . New } 
func ( h * HTTP Extender ) Is Interested ( pod * v1 . Pod ) bool { if h . managed if h . has Managed if h . has Managed Resources ( pod . Spec . Init } 
func ( runner * runner ) Get DNS Suffix Search List ( ) ( [ ] string , error ) { // Parse the DNS suffix search list from ipconfig output // ipconfig /all on Windows displays the entry of DNS suffix search list // An example output contains: // // DNS Suffix Search List. . . . . . : example1.com // example2.com // // TODO: this does not work when the label is localized suffix return suffix out , err := runner . exec . Command ( cmd Ipconfig , cmd Default for i , line := range lines { if trimmed := strings . Trim Space ( line ) ; strings . Has Prefix ( trimmed , dns Suffix Search Lis Label ) { if parts := strings . Split ( trimmed , " " ) ; len ( parts ) > 1 { if trimmed := strings . Trim Space ( parts [ 1 ] ) ; trimmed != " " { suffix List = append ( suffix List , strings . Trim for j := i + 1 ; j < len ( lines ) ; j ++ { if trimmed := strings . Trim Space ( lines [ j ] ) ; trimmed != " " && ! strings . Contains ( trimmed , " " ) { suffix List = append ( suffix } else { klog . V ( 1 ) . Infof ( " " , cmd Ipconfig , cmd Default return suffix } 
func Undecorated Storage ( config * storagebackend . Config , resource Prefix string , key Func func ( obj runtime . Object ) ( string , error ) , new Func func ( ) runtime . Object , new List Func func ( ) runtime . Object , get Attrs Func storage . Attr Func , trigger storage . Trigger Publisher Func ) ( storage . Interface , factory . Destroy Func ) { return New Raw } 
func New Raw Storage ( config * storagebackend . Config ) ( storage . Interface , factory . Destroy } 
func ( prometheus Metrics Provider ) New Deprecated Depth Metric ( name string ) workqueue . Gauge Metric { depth := prometheus . New Gauge ( prometheus . Gauge } 
func New Request ( client HTTP Client , verb string , base URL * url . URL , versioned API Path string , content Content Config , serializers Serializers , backoff Backoff Manager , throttle flowcontrol . Rate backoff = & No path if base URL != nil { path Prefix = path . Join ( path Prefix , base r := & Request { client : client , verb : verb , base URL : base URL , path Prefix : path . Join ( path Prefix , versioned API Path ) , content : content , serializers : serializers , backoff switch { case len ( content . Accept Content Types ) > 0 : r . Set Header ( " " , content . Accept Content case len ( content . Content Type ) > 0 : r . Set Header ( " " , content . Content } 
r . path Prefix = path . Join ( r . path } 
} 
if msgs := Is Valid Path Segment } 
func ( r * Request ) Back Off ( manager Backoff Manager ) * Request { if manager == nil { r . backoff Mgr = & No r . backoff } 
func ( r * Request ) Throttle ( limiter flowcontrol . Rate } 
func ( r * Request ) Name ( resource if len ( resource if len ( r . resource Name ) != 0 { r . err = fmt . Errorf ( " " , r . resource Name , resource if msgs := Is Valid Path Segment Name ( resource Name ) ; len ( msgs ) != 0 { r . err = fmt . Errorf ( " " , resource r . resource Name = resource } 
if r . namespace if msgs := Is Valid Path Segment r . namespace } 
func ( r * Request ) Namespace If } 
func ( r * Request ) Abs r . path Prefix = path . Join ( r . base if len ( segments ) == 1 && ( len ( r . base URL . Path ) > 1 || len ( segments [ 0 ] ) > 1 ) && strings . Has Suffix ( segments [ 0 ] , " " ) { // preserve any trailing slashes for legacy behavior r . path } 
func ( r * Request ) Request r . path } 
func ( r * Request ) Param ( param return r . set Param ( param } 
func ( r * Request ) Versioned Params ( obj runtime . Object , codec runtime . Parameter Codec ) * Request { return r . Specifically Versioned Params ( obj , codec , * r . content . Group } 
} 
switch t := obj . ( type ) { case string : data , err := ioutil . Read glog r . body = bytes . New case [ ] byte : glog r . body = bytes . New case runtime . Object : // callers may pass typed interface pointers, therefore we must check nil with reflection if reflect . Value Of ( t ) . Is glog r . body = bytes . New r . Set Header ( " " , r . content . Content } 
} 
func ( r Request ) final URL Template ( ) url . URL { new for k := range r . params { new r . params = new group if r . URL ( ) != nil && r . base URL != nil && strings . Contains ( r . URL ( ) . Path , r . base URL . Path ) { group Index += len ( strings . Split ( r . base if group const Core Group const Named Group is Core Group := segments [ group Index ] == Core Group is Named Group := segments [ group Index ] == Named Group if is Core Group { // checking the case of core group with /api/v1/... format index = group } else if is Named Group { // checking the case of named group with /apis/apps/v1/... format index = group url . Raw //switch seg Length := len(segments) - index; seg } 
func ( r * Request ) Watch ( ) ( watch . Interface , error ) { return r . Watch With Specific Decoders ( func ( body io . Read Closer ) streaming . Decoder { framer := r . serializers . Framer . New Frame return streaming . New Decoder ( framer , r . serializers . Streaming } 
func ( r * Request ) Watch With Specific Decoders ( wrapper Decoder Fn func ( io . Read Closer ) streaming . Decoder , embedded if r . serializers . Framer == nil { return nil , fmt . Errorf ( " " , r . content . Content req , err := http . New if r . ctx != nil { req = req . With if client == nil { client = http . Default r . backoff Mgr . Sleep ( r . backoff Mgr . Calculate update URL if r . base URL != nil { if err != nil { r . backoff Mgr . Update Backoff ( r . base } else { r . backoff Mgr . Update Backoff ( r . base URL , err , resp . Status if err != nil { // The watch stream mechanism handles many common partial data errors, so closed // connections can be retried in many cases. if net . Is Probable EOF ( err ) { return watch . New Empty if resp . Status Code != http . Status if result := r . transform return nil , fmt . Errorf ( " " , url , resp . Status wrapper Decoder := wrapper Decoder return watch . New Stream Watcher ( restclientwatch . New Decoder ( wrapper Decoder , embedded Decoder ) , // use 500 to indicate that the cause of the error is unknown - other error codes // are more specific to HTTP interactions, and set a reason errors . New Client Error Reporter ( http . Status Internal Server } 
func update URL if req . base URL != nil { url = req . base // Errors can be arbitrary strings. Unbound label cardinality is not suitable for a metric // system so we just report them as `<error>`. if err != nil { metrics . Request } else { //Metrics for failure codes metrics . Request Result . Increment ( strconv . Itoa ( resp . Status } 
func ( r * Request ) Stream ( ) ( io . Read r . try req , err := http . New if r . ctx != nil { req = req . With if client == nil { client = http . Default r . backoff Mgr . Sleep ( r . backoff Mgr . Calculate update URL if r . base URL != nil { if err != nil { r . backoff Mgr . Update } else { r . backoff Mgr . Update Backoff ( r . URL ( ) , err , resp . Status switch { case ( resp . Status Code >= 200 ) && ( resp . Status result := r . transform if err == nil { err = fmt . Errorf ( " " , result . status } 
defer func ( ) { metrics . Request Latency . Observe ( r . verb , r . final URL // TODO: added to catch programmer errors (invoking operations with an object with an empty namespace) if ( r . verb == " " || r . verb == " " || r . verb == " " ) && r . namespace Set && len ( r . resource if ( r . verb == " " ) && r . namespace if client == nil { client = http . Default // Right now we make about ten retry attempts if we get a Retry-After response. max req , err := http . New var cancel Fn context . Cancel r . ctx , cancel Fn = context . With defer cancel if r . ctx != nil { req = req . With r . backoff Mgr . Sleep ( r . backoff Mgr . Calculate if retries > 0 { // We are retrying the request that we already send to apiserver // at least once before. // This request should also be throttled with the client-internal throttler. r . try update URL if err != nil { r . backoff Mgr . Update } else { r . backoff Mgr . Update Backoff ( r . URL ( ) , err , resp . Status if err != nil { // "Connection reset by peer" is usually a transient error. // Thus in case of "GET" operations, we simply retry it. // We are not automatically retrying "write" operations, as // they are not idempotent. if ! net . Is Connection // For the purpose of retry, we set the artificial "retry-after" response. // TODO: Should we clean the original response if it exists? resp = & http . Response { Status Code : http . Status Internal Server Error , Header : http . Header { " " : [ ] string { " " } } , Body : ioutil . Nop Closer ( bytes . New done := func ( ) bool { // Ensure the response body is fully read and closed // before we reconnect, so that we reuse the same TCP // connection. defer func ( ) { const max Body Slurp if resp . Content Length <= max Body Slurp Size { io . Copy ( ioutil . Discard , & io . Limited Reader { R : resp . Body , N : max Body Slurp if seconds , wait := check Wait ( resp ) ; wait && retries < max r . backoff } 
func ( r * Request ) Do ( ) Result { r . try err := r . request ( func ( req * http . Request , resp * http . Response ) { result = r . transform } 
func ( r * Request ) Do Raw ( ) ( [ ] byte , error ) { r . try err := r . request ( func ( req * http . Request , resp * http . Response ) { result . body , result . err = ioutil . Read glog if resp . Status Code < http . Status OK || resp . Status Code > http . Status Partial Content { result . err = r . transform Unstructured Response } 
func ( r * Request ) transform if resp . Body != nil { data , err := ioutil . Read case http2 . Stream stream return Result { err : stream unexpected return Result { err : unexpected glog // verify the content type is accurate content if len ( content Type ) > 0 && ( decoder == nil || ( len ( r . content . Content Type ) > 0 && content Type != r . content . Content Type ) ) { media Type , params , err := mime . Parse Media Type ( content if err != nil { return Result { err : errors . New Internal decoder , err = r . serializers . Renegotiated Decoder ( media if err != nil { // if we fail to negotiate a decoder, treat this as an unstructured error switch { case resp . Status Code == http . Status Switching Protocols : // no-op, we've been upgraded case resp . Status Code < http . Status OK || resp . Status Code > http . Status Partial Content : return Result { err : r . transform Unstructured Response return Result { body : body , content Type : content Type , status Code : resp . Status switch { case resp . Status Code == http . Status Switching Protocols : // no-op, we've been upgraded case resp . Status Code < http . Status OK || resp . Status Code > http . Status Partial Content : // calculate an unstructured error from the response which the Result object may use if the caller // did not return a structured error. retry After , _ := retry After err := r . new Unstructured Response Error ( body , is Text Response ( resp ) , resp . Status Code , req . Method , retry return Result { body : body , content Type : content Type , status Code : resp . Status return Result { body : body , content Type : content Type , status Code : resp . Status } 
func truncate } 
func glog Body ( prefix string , body [ ] byte ) { if klog . V ( 8 ) { if bytes . Index } ) != - 1 { klog . Infof ( " \n " , prefix , truncate } else { klog . Infof ( " " , prefix , truncate } 
func ( r * Request ) transform Unstructured Response Error ( resp * http . Response , req * http . Request , body [ ] byte ) error { if body == nil && resp . Body != nil { if data , err := ioutil . Read All ( & io . Limited Reader { R : resp . Body , N : max Unstructured Response Text retry After , _ := retry After return r . new Unstructured Response Error ( body , is Text Response ( resp ) , resp . Status Code , req . Method , retry } 
func ( r * Request ) new Unstructured Response Error ( body [ ] byte , is Text Response bool , status Code int , method string , retry After int ) error { // cap the amount of output we create if len ( body ) > max Unstructured Response Text Bytes { body = body [ : max Unstructured Response Text if is Text Response { message = strings . Trim var group Resource schema . Group if len ( r . resource ) > 0 { group Resource . Group = r . content . Group group return errors . New Generic Server Response ( status Code , method , group Resource , r . resource Name , message , retry } 
func is Text Response ( resp * http . Response ) bool { content if len ( content media , _ , err := mime . Parse Media Type ( content return strings . Has } 
if r . decoder == nil { return nil , fmt . Errorf ( " " , r . content switch t := out . ( type ) { case * metav1 . Status : // any status besides Status Success is considered an error. if t . Status != metav1 . Status Success { return nil , errors . From } 
func ( r Result ) Status Code ( status Code * int ) Result { * status Code = r . status } 
if r . decoder == nil { return fmt . Errorf ( " " , r . content if len ( r . body ) == 0 { return fmt . Errorf ( " " , r . status Code , r . content // if a different object is returned, see if it is Status and avoid double decoding // the object. switch t := out . ( type ) { case * metav1 . Status : // any status besides Status Success is considered an error. if t . Status != metav1 . Status Success { return errors . From } 
func ( r Result ) Was Created ( was Created * bool ) Result { * was Created = r . status Code == http . Status } 
func ( r Result ) Error ( ) error { // if we have received an unexpected server error, and we have a body and decoder, we can try to extract // a Status object. if r . err == nil || ! errors . Is Unexpected Server // attempt to convert the body into a Status object // to be backwards compatible with old servers that do not return a version, default to "v1" out , _ , err := r . decoder . Decode ( r . body , & schema . Group Version switch t := out . ( type ) { case * metav1 . Status : // because we default the kind, we *must* check for Status Failure if t . Status == metav1 . Status Failure { return errors . From } 
func Is Valid Path Segment Name ( name string ) [ ] string { for _ , illegal Name := range Name May Not Be { if name == illegal Name { return [ ] string { fmt . Sprintf ( `may not be '%s'` , illegal for _ , illegal Content := range Name May Not Contain { if strings . Contains ( name , illegal Content ) { errors = append ( errors , fmt . Sprintf ( `may not contain '%s'` , illegal } 
func Is Valid Path Segment for _ , illegal Content := range Name May Not Contain { if strings . Contains ( name , illegal Content ) { errors = append ( errors , fmt . Sprintf ( `may not contain '%s'` , illegal } 
func ( f * JSON Path Print Flags ) To Printer ( template Format string ) ( printers . Resource Printer , error ) { if ( f . Template Argument == nil || len ( * f . Template Argument ) == 0 ) && len ( template Format ) == 0 { return nil , No Compatible Printer Error { Options : f , Output Format : & template template if f . Template Argument == nil || len ( * f . Template Argument ) == 0 { for format := range json if strings . Has Prefix ( template Format , format ) { template Value = template template } else { template Value = * f . Template if _ , supported Format := json Formats [ template Format ] ; ! supported Format { return nil , No Compatible Printer Error { Output Format : & template Format , Allowed Formats : f . Allowed if len ( template if template Format == " " { data , err := ioutil . Read File ( template if err != nil { return nil , fmt . Errorf ( " \n " , template template p , err := printers . New JSON Path Printer ( template if err != nil { return nil , fmt . Errorf ( " \n " , template allow Missing if f . Allow Missing Keys != nil { allow Missing Keys = * f . Allow Missing p . Allow Missing Keys ( allow Missing } 
func ( f * JSON Path Print Flags ) Add Flags ( c * cobra . Command ) { if f . Template Argument != nil { c . Flags ( ) . String Var ( f . Template Argument , " " , * f . Template c . Mark Flag if f . Allow Missing Keys != nil { c . Flags ( ) . Bool Var ( f . Allow Missing Keys , " " , * f . Allow Missing } 
func New JSON Path Print Flags ( template Value string , allow Missing Keys bool ) * JSON Path Print Flags { return & JSON Path Print Flags { Template Argument : & template Value , Allow Missing Keys : & allow Missing } 
func New Client With Options ( c REST Client , transforms ... Request Transform ) REST return & client } 
func Validate Lease ( lease * coordination . Lease ) field . Error List { all Errs := validation . Validate Object Meta ( & lease . Object Meta , true , validation . Name Is DNS Subdomain , field . New all Errs = append ( all Errs , Validate Lease Spec ( & lease . Spec , field . New return all } 
func Validate Lease Update ( lease , old Lease * coordination . Lease ) field . Error List { all Errs := validation . Validate Object Meta Update ( & lease . Object Meta , & old Lease . Object Meta , field . New all Errs = append ( all Errs , Validate Lease Spec ( & lease . Spec , field . New return all } 
func Validate Lease Spec ( spec * coordination . Lease Spec , fld Path * field . Path ) field . Error List { all Errs := field . Error if spec . Lease Duration Seconds != nil && * spec . Lease Duration Seconds <= 0 { fld := fld all Errs = append ( all Errs , field . Invalid ( fld , spec . Lease Duration if spec . Lease Transitions != nil && * spec . Lease Transitions < 0 { fld := fld all Errs = append ( all Errs , field . Invalid ( fld , spec . Lease return all } 
func New Cmd Config Delete Context ( out , err Out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : " " , Example : delete Context Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check Err ( run Delete Context ( out , err Out , config } 
func fake Cluster ID ( cluster ID string ) Cluster ID { return Cluster ID { cluster ID : & cluster ID , store : cache . New } 
func New Fake GCE Cloud ( vals Test Cluster Values ) * Cloud { client := & http . Client { Transport : & fake Round gce := & Cloud { region : vals . Region , service : service , managed Zones : [ ] string { vals . Zone Name } , project ID : vals . Project ID , network Project ID : vals . Project ID , Cluster ID : fake Cluster ID ( vals . Cluster c := cloud . New Mock GCE ( & gce Project } 
func New REST ( opts Getter generic . REST Options Getter ) * REST { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Pod Template { } } , New List Func : func ( ) runtime . Object { return & api . Pod Template List { } } , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : podtemplate . Strategy , Update Strategy : podtemplate . Strategy , Delete Strategy : podtemplate . Strategy , Export Strategy : podtemplate . Strategy , Return Deleted Object : true , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts if err := store . Complete With } 
func convert To Discovery API Group ( api Services [ ] * apiregistrationapi . API Service ) * metav1 . API Group { api Services By Group := apiregistrationapi . Sorted By Group And Version ( api var discovery Group * metav1 . API for _ , api Service := range api Services By Group { // the first API Service which is valid becomes the default if discovery Group == nil { discovery Group = & metav1 . API Group { Name : api Service . Spec . Group , Preferred Version : metav1 . Group Version For Discovery { Group Version : api Service . Spec . Group + " " + api Service . Spec . Version , Version : api discovery Group . Versions = append ( discovery Group . Versions , metav1 . Group Version For Discovery { Group Version : api Service . Spec . Group + " " + api Service . Spec . Version , Version : api return discovery } 
func Default Kube Proxy Configuration ( internalcfg * kubeadmapi . Cluster Configuration ) { externalproxycfg := & kubeproxyconfigv1alpha1 . Kube Proxy // Do a roundtrip to the external version for defaulting if internalcfg . Component Configs . Kube Proxy != nil { Scheme . Convert ( internalcfg . Component Configs . Kube if externalproxycfg . Cluster CIDR == " " && internalcfg . Networking . Pod Subnet != " " { externalproxycfg . Cluster CIDR = internalcfg . Networking . Pod if externalproxycfg . Client Connection . Kubeconfig == " " { externalproxycfg . Client Connection . Kubeconfig = Kubeproxy Kube Config File if internalcfg . Component Configs . Kube Proxy == nil { internalcfg . Component Configs . Kube Proxy = & kubeproxyconfig . Kube Proxy // TODO: Figure out how to handle errors in defaulting code // Go back to the internal version Scheme . Convert ( externalproxycfg , internalcfg . Component Configs . Kube } 
func Default Kubelet Configuration ( internalcfg * kubeadmapi . Cluster Configuration ) { externalkubeletcfg := & kubeletconfigv1beta1 . Kubelet // Do a roundtrip to the external version for defaulting if internalcfg . Component Configs . Kubelet != nil { Scheme . Convert ( internalcfg . Component if externalkubeletcfg . Static Pod Path == " " { externalkubeletcfg . Static Pod Path = kubeadmapiv1beta2 . Default Manifests if externalkubeletcfg . Cluster DNS == nil { dns IP , err := constants . Get DNSIP ( internalcfg . Networking . Service if err != nil { externalkubeletcfg . Cluster DNS = [ ] string { kubeadmapiv1beta2 . Default Cluster } else { externalkubeletcfg . Cluster DNS = [ ] string { dns if externalkubeletcfg . Cluster Domain == " " { externalkubeletcfg . Cluster Domain = internalcfg . Networking . DNS // Enforce security-related kubelet options // Require all clients to the kubelet API to have client certs signed by the cluster CA externalkubeletcfg . Authentication . X509 . Client CA File = filepath . Join ( internalcfg . Certificates Dir , constants . CA Cert externalkubeletcfg . Authentication . Anonymous . Enabled = utilpointer . Bool // On every client request to the kubelet API, execute a webhook (Subject Access Review request) to the API server // and ask it whether the client is authorized to access the kubelet API externalkubeletcfg . Authorization . Mode = kubeletconfigv1beta1 . Kubelet Authorization Mode // Let clients using other authentication methods like Service Account tokens also access the kubelet API externalkubeletcfg . Authentication . Webhook . Enabled = utilpointer . Bool // Disable the readonly port of the kubelet, in order to not expose unnecessary information externalkubeletcfg . Read Only // Enables client certificate rotation for the kubelet externalkubeletcfg . Rotate // Serve a /healthz webserver on localhost:10248 that kubeadm can talk to externalkubeletcfg . Healthz Bind externalkubeletcfg . Healthz Port = utilpointer . Int32Ptr ( constants . Kubelet Healthz if internalcfg . Component Configs . Kubelet == nil { internalcfg . Component Configs . Kubelet = & kubeletconfig . Kubelet // TODO: Figure out how to handle errors in defaulting code // Go back to the internal version Scheme . Convert ( externalkubeletcfg , internalcfg . Component } 
func ( r * crd Handler ) remove Dead Storage ( ) { all Custom Resource Definitions , err := r . crd if err != nil { utilruntime . Handle r . custom Storage defer r . custom Storage storage Map := r . custom Storage . Load ( ) . ( crd Storage // Copy because we cannot write to storage Map without a race // as it is used without locking elsewhere storage Map2 := storage for uid , s := range storage for _ , crd := range all Custom Resource for _ , storage := range s . storages { // destroy only the main storage. Those for the subresources share cacher and etcd clients. storage . Custom Resource . Destroy delete ( storage r . custom Storage . Store ( storage } 
func ( r * crd Handler ) Get Custom Resource Lister Collection Deleter ( crd * apiextensions . Custom Resource Definition ) ( finalizer . Lister Collection Deleter , error ) { info , err := r . get Or Create Serving Info return info . storages [ info . storage Version ] . Custom } 
func ( in crd Storage Map ) clone ( ) crd Storage out := make ( crd Storage } 
func ( cc * Certificate Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle defer cc . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , cc . csrs for i := 0 ; i < workers ; i ++ { go wait . Until ( cc . worker , time . Second , stop <- stop } 
func ( cc * Certificate Controller ) process Next Work Item ( ) bool { c defer cc . queue . Done ( c if err := cc . sync Func ( c Key . ( string ) ) ; err != nil { cc . queue . Add Rate Limited ( c if _ , ignorable := err . ( ignorable Error ) ; ! ignorable { utilruntime . Handle Error ( fmt . Errorf ( " " , c } else { klog . V ( 4 ) . Infof ( " " , c cc . queue . Forget ( c } 
func ( cc * Certificate Controller ) sync Func ( key string ) error { start defer func ( ) { klog . V ( 4 ) . Infof ( " " , key , time . Since ( start csr , err := cc . csr if errors . Is Not // need to operate on a copy so we don't mutate the csr in the shared cache csr = csr . Deep } 
func Ignorable Error ( s string , args ... interface { } ) ignorable Error { return ignorable } 
func ( in * Custom Resource Column Definition ) Deep Copy ( ) * Custom Resource Column out := new ( Custom Resource Column in . Deep Copy } 
func ( in * Custom Resource Conversion ) Deep Copy Into ( out * Custom Resource if in . Webhook Client Config != nil { in , out := & in . Webhook Client Config , & out . Webhook Client * out = new ( Webhook Client ( * in ) . Deep Copy if in . Conversion Review Versions != nil { in , out := & in . Conversion Review Versions , & out . Conversion Review } 
func ( in * Custom Resource Conversion ) Deep Copy ( ) * Custom Resource out := new ( Custom Resource in . Deep Copy } 
func ( in * Custom Resource Definition ) Deep Copy Into ( out * Custom Resource out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Custom Resource Definition ) Deep Copy ( ) * Custom Resource out := new ( Custom Resource in . Deep Copy } 
func ( in * Custom Resource Definition ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Custom Resource Definition Condition ) Deep Copy Into ( out * Custom Resource Definition in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Custom Resource Definition Condition ) Deep Copy ( ) * Custom Resource Definition out := new ( Custom Resource Definition in . Deep Copy } 
func ( in * Custom Resource Definition List ) Deep Copy Into ( out * Custom Resource Definition out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Custom Resource for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Custom Resource Definition List ) Deep Copy ( ) * Custom Resource Definition out := new ( Custom Resource Definition in . Deep Copy } 
func ( in * Custom Resource Definition List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Custom Resource Definition Names ) Deep Copy Into ( out * Custom Resource Definition if in . Short Names != nil { in , out := & in . Short Names , & out . Short } 
func ( in * Custom Resource Definition Names ) Deep Copy ( ) * Custom Resource Definition out := new ( Custom Resource Definition in . Deep Copy } 
func ( in * Custom Resource Definition Spec ) Deep Copy Into ( out * Custom Resource Definition in . Names . Deep Copy * out = new ( Custom Resource ( * in ) . Deep Copy * out = new ( Custom Resource ( * in ) . Deep Copy * out = make ( [ ] Custom Resource Definition for i := range * in { ( * in ) [ i ] . Deep Copy if in . Additional Printer Columns != nil { in , out := & in . Additional Printer Columns , & out . Additional Printer * out = make ( [ ] Custom Resource Column * out = new ( Custom Resource ( * in ) . Deep Copy } 
func ( in * Custom Resource Definition Spec ) Deep Copy ( ) * Custom Resource Definition out := new ( Custom Resource Definition in . Deep Copy } 
func ( in * Custom Resource Definition Status ) Deep Copy Into ( out * Custom Resource Definition * out = make ( [ ] Custom Resource Definition for i := range * in { ( * in ) [ i ] . Deep Copy in . Accepted Names . Deep Copy Into ( & out . Accepted if in . Stored Versions != nil { in , out := & in . Stored Versions , & out . Stored } 
func ( in * Custom Resource Definition Status ) Deep Copy ( ) * Custom Resource Definition out := new ( Custom Resource Definition in . Deep Copy } 
func ( in * Custom Resource Definition Version ) Deep Copy Into ( out * Custom Resource Definition * out = new ( Custom Resource ( * in ) . Deep Copy * out = new ( Custom Resource ( * in ) . Deep Copy if in . Additional Printer Columns != nil { in , out := & in . Additional Printer Columns , & out . Additional Printer * out = make ( [ ] Custom Resource Column } 
func ( in * Custom Resource Definition Version ) Deep Copy ( ) * Custom Resource Definition out := new ( Custom Resource Definition in . Deep Copy } 
func ( in * Custom Resource Subresource Scale ) Deep Copy Into ( out * Custom Resource Subresource if in . Label Selector Path != nil { in , out := & in . Label Selector Path , & out . Label Selector } 
func ( in * Custom Resource Subresource Scale ) Deep Copy ( ) * Custom Resource Subresource out := new ( Custom Resource Subresource in . Deep Copy } 
func ( in * Custom Resource Subresource Status ) Deep Copy ( ) * Custom Resource Subresource out := new ( Custom Resource Subresource in . Deep Copy } 
func ( in * Custom Resource Subresources ) Deep Copy Into ( out * Custom Resource * out = new ( Custom Resource Subresource * out = new ( Custom Resource Subresource ( * in ) . Deep Copy } 
func ( in * Custom Resource Subresources ) Deep Copy ( ) * Custom Resource out := new ( Custom Resource in . Deep Copy } 
func ( in * Custom Resource Validation ) Deep Copy Into ( out * Custom Resource if in . Open APIV3Schema != nil { in , out := & in . Open APIV3Schema , & out . Open * out = ( * in ) . Deep } 
func ( in * Custom Resource Validation ) Deep Copy ( ) * Custom Resource out := new ( Custom Resource in . Deep Copy } 
func ( in * External Documentation ) Deep Copy ( ) * External out := new ( External in . Deep Copy } 
func ( in JSON Schema Definitions ) Deep Copy ( ) JSON Schema out := new ( JSON Schema in . Deep Copy } 
func ( in JSON Schema Dependencies ) Deep Copy ( ) JSON Schema out := new ( JSON Schema in . Deep Copy } 
func ( in * JSON Schema Props ) Deep Copy Into ( out * JSON Schema Props ) { clone := in . Deep } 
func ( in * JSON Schema Props Or Array ) Deep Copy Into ( out * JSON Schema Props Or * out = ( * in ) . Deep if in . JSON Schemas != nil { in , out := & in . JSON Schemas , & out . JSON * out = make ( [ ] JSON Schema for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * JSON Schema Props Or Array ) Deep Copy ( ) * JSON Schema Props Or out := new ( JSON Schema Props Or in . Deep Copy } 
func ( in * JSON Schema Props Or Bool ) Deep Copy Into ( out * JSON Schema Props Or * out = ( * in ) . Deep } 
func ( in * JSON Schema Props Or Bool ) Deep Copy ( ) * JSON Schema Props Or out := new ( JSON Schema Props Or in . Deep Copy } 
func ( in * JSON Schema Props Or String Array ) Deep Copy Into ( out * JSON Schema Props Or String * out = ( * in ) . Deep } 
func ( in * JSON Schema Props Or String Array ) Deep Copy ( ) * JSON Schema Props Or String out := new ( JSON Schema Props Or String in . Deep Copy } 
func ( c * Storage V1alpha1Client ) REST return c . rest } 
func ( b * azure File Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func get User Key ( attr admission . Attributes ) string { user Info := attr . Get User if user return user Info . Get } 
func get Source And Object Key ( attr admission . Attributes ) string { object := attr . Get return strings . Join ( [ ] string { event . Source . Component , event . Source . Host , event . Involved Object . Kind , event . Involved Object . Namespace , event . Involved Object . Name , string ( event . Involved Object . UID ) , event . Involved Object . API } 
func New Cmd Create Service Account ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Service Account Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Aliases : [ ] string { " " } , Short : i18n . T ( " " ) , Long : service Account Long , Example : service Account Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Service Account V1Generator } 
func ( o * Service Account Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Service Account V1Generator Name : generator = & generateversioned . Service Account Generator default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func ( h * hostport Syncer ) open Hostports ( pod Hostport Mapping * Pod Port Mapping ) error { var ret for _ , port := range pod Hostport Mapping . Port Mappings { if port . Host // We do not open host ports for SCTP ports, as we agreed in the Support of SCTP KEP if port . Protocol == v1 . Protocol hp := hostport { port : port . Host Port , protocol : strings . To socket , err := h . port if err != nil { ret Err = fmt . Errorf ( " " , port . Host Port , get Pod Full Name ( pod Hostport // If encounter any error, close all hostports that just got opened. if ret Err != nil { for hp , socket := range ports { if err := socket . Close ( ) ; err != nil { klog . Errorf ( " " , hp . port , get Pod Full Name ( pod Hostport return ret for host Port , socket := range ports { h . host Port Map [ host } 
func gather All Hostports ( active Pod Port Mappings [ ] * Pod Port Mapping ) ( map [ * Port Mapping ] target Pod , error ) { pod Hostport Map := make ( map [ * Port Mapping ] target for _ , pm := range active Pod Port Mappings { if pm . IP . To4 ( ) == nil { return nil , fmt . Errorf ( " " , get Pod Full // should not handle hostports for hostnetwork pods if pm . Host for _ , port := range pm . Port Mappings { if port . Host Port != 0 { pod Hostport Map [ port ] = target Pod { pod Full Name : get Pod Full Name ( pm ) , pod return pod Hostport } 
func write Line ( buf * bytes . Buffer , words ... string ) { buf . Write } 
func hostport Chain Name ( pm * Port Mapping , pod Full Name string ) utiliptables . Chain { hash := sha256 . Sum256 ( [ ] byte ( strconv . Itoa ( int ( pm . Host Port ) ) + string ( pm . Protocol ) + pod Full encoded := base32 . Std Encoding . Encode To return utiliptables . Chain ( kube Hostport Chain } 
func ( h * hostport Syncer ) Open Pod Hostports And Sync ( new Port Mapping * Pod Port Mapping , nat Interface Name string , active Pod Port Mappings [ ] * Pod Port Mapping ) error { // try to open pod host port if specified if err := h . open Hostports ( new Port for _ , pm := range active Pod Port Mappings { if pm . Namespace == new Port Mapping . Namespace && pm . Name == new Port if ! found { active Pod Port Mappings = append ( active Pod Port Mappings , new Port return h . Sync Hostports ( nat Interface Name , active Pod Port } 
func ( h * hostport Syncer ) Sync Hostports ( nat Interface Name string , active Pod Port Mappings [ ] * Pod Port hostport Pod Map , err := gather All Hostports ( active Pod Port // Ensure KUBE-HOSTPORTS chains ensure Kube Hostport Chains ( h . iptables , nat Interface // Get iptables-save output so we can check for existing chains and rules. // This will be a map of chain name to chain with rules as stored in iptables-save/iptables-restore existing NAT iptables Data := bytes . New err = h . iptables . Save Into ( utiliptables . Table NAT , iptables } else { // otherwise parse the output existing NAT Chains = utiliptables . Get Chain Lines ( utiliptables . Table NAT , iptables nat Chains := bytes . New nat Rules := bytes . New write Line ( nat // Make sure we keep stats for the top-level chains, if they existed // (which most should have because we created them above). if chain , ok := existing NAT Chains [ kube Hostports Chain ] ; ok { write Bytes Line ( nat } else { write Line ( nat Chains , utiliptables . Make Chain Line ( kube Hostports // Accumulate NAT chains to keep. active NAT for port , target := range hostport Pod Map { protocol := strings . To hostport Chain := hostport Chain Name ( port , target . pod Full if chain , ok := existing NAT Chains [ hostport Chain ] ; ok { write Bytes Line ( nat } else { write Line ( nat Chains , utiliptables . Make Chain Line ( hostport active NAT Chains [ hostport // Redirect to hostport chain args := [ ] string { " " , string ( kube Hostports Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s hostport %d"` , target . pod Full Name , port . Host Port ) , " " , protocol , " " , protocol , " " , fmt . Sprintf ( " " , port . Host Port ) , " " , string ( hostport write Line ( nat // Assuming kubelet is syncing iptables KUBE-MARK-MASQ chain // If the request comes from the pod that is serving the hostport, then SNAT args = [ ] string { " " , string ( hostport Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s hostport %d"` , target . pod Full Name , port . Host Port ) , " " , target . pod IP , " " , string ( iptablesproxy . Kube Mark Masq write Line ( nat // Create hostport chain to DNAT traffic to final destination // IP Tables will maintained the stats for this chain args = [ ] string { " " , string ( hostport Chain ) , " " , " " , " " , fmt . Sprintf ( `"%s hostport %d"` , target . pod Full Name , port . Host Port ) , " " , protocol , " " , protocol , " " , " " , fmt . Sprintf ( " " , target . pod IP , port . Container write Line ( nat // Delete chains no longer in use. for chain := range existing NAT Chains { if ! active NAT Chains [ chain ] { chain if ! strings . Has Prefix ( chain String , kube Hostport Chain // We must (as per iptables) write a chain-line for it, which has // the nice effect of flushing the chain. Then we can remove the // chain. write Bytes Line ( nat Chains , existing NAT write Line ( nat Rules , " " , chain write Line ( nat nat Lines := append ( nat Chains . Bytes ( ) , nat klog . V ( 3 ) . Infof ( " " , nat err = h . iptables . Restore All ( nat Lines , utiliptables . No Flush Tables , utiliptables . Restore h . cleanup Hostport Map ( hostport Pod } 
func ( h * hostport Syncer ) cleanup Hostport Map ( container Port Map map [ * Port Mapping ] target Pod ) { // compute hostports that are supposed to be open current for container Port := range container Port Map { hp := hostport { port : container Port . Host Port , protocol : strings . To Lower ( string ( container current // close and delete obsolete hostports for hp , socket := range h . host Port Map { if _ , ok := current delete ( h . host Port } 
func Add Handlers ( h printers . Print Handler ) { pod Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Pod Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Pod Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Pod Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Pod Spec { } . Swagger h . Table Handler ( pod Column Definitions , print Pod h . Table Handler ( pod Column Definitions , print pod Template Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( pod Template Column Definitions , print Pod h . Table Handler ( pod Template Column Definitions , print Pod Template pod Disruption Budget Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( pod Disruption Budget Column Definitions , print Pod Disruption h . Table Handler ( pod Disruption Budget Column Definitions , print Pod Disruption Budget replication Controller Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Replication Controller Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Replication Controller Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Replication Controller Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Replication Controller Spec { } . Swagger h . Table Handler ( replication Controller Column Definitions , print Replication h . Table Handler ( replication Controller Column Definitions , print Replication Controller replica Set Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Replica Set Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Replica Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Replica Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : extensionsv1beta1 . Replica Set Spec { } . Swagger h . Table Handler ( replica Set Column Definitions , print Replica h . Table Handler ( replica Set Column Definitions , print Replica Set daemon Set Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Daemon Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Daemon Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Daemon Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Daemon Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Daemon Set Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Pod Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : extensionsv1beta1 . Daemon Set Spec { } . Swagger h . Table Handler ( daemon Set Column Definitions , print Daemon h . Table Handler ( daemon Set Column Definitions , print Daemon Set job Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : batchv1 . Job Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : batchv1 . Job Spec { } . Swagger h . Table Handler ( job Column Definitions , print h . Table Handler ( job Column Definitions , print Job cron Job Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : batchv1beta1 . Cron Job Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : batchv1beta1 . Cron Job Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : batchv1beta1 . Cron Job Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : batchv1beta1 . Cron Job Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : batchv1 . Job Spec { } . Swagger h . Table Handler ( cron Job Column Definitions , print Cron h . Table Handler ( cron Job Column Definitions , print Cron Job service Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Service Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Service Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Service Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Service Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Service Spec { } . Swagger h . Table Handler ( service Column Definitions , print h . Table Handler ( service Column Definitions , print Service ingress Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( ingress Column Definitions , print h . Table Handler ( ingress Column Definitions , print Ingress stateful Set Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( stateful Set Column Definitions , print Stateful h . Table Handler ( stateful Set Column Definitions , print Stateful Set endpoint Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Endpoints { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( endpoint Column Definitions , print h . Table Handler ( endpoint Column Definitions , print Endpoints node Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Node System Info { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Node Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Node Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Node System Info { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Node System Info { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Node System Info { } . Swagger h . Table Handler ( node Column Definitions , print h . Table Handler ( node Column Definitions , print Node event Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Event { } . Involved Object . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Event { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Format : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( event Column Definitions , print h . Table Handler ( event Column Definitions , print Event namespace Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( namespace Column Definitions , print h . Table Handler ( namespace Column Definitions , print Namespace secret Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Secret { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Secret { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( secret Column Definitions , print h . Table Handler ( secret Column Definitions , print Secret service Account Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Service Account { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( service Account Column Definitions , print Service h . Table Handler ( service Account Column Definitions , print Service Account persistent Volume Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Persistent Volume Spec { } . Swagger h . Table Handler ( persistent Volume Column Definitions , print Persistent h . Table Handler ( persistent Volume Column Definitions , print Persistent Volume persistent Volume Claim Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Claim Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Claim Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Claim Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Persistent Volume Claim Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : apiv1 . Persistent Volume Claim Spec { } . Swagger h . Table Handler ( persistent Volume Claim Column Definitions , print Persistent Volume h . Table Handler ( persistent Volume Claim Column Definitions , print Persistent Volume Claim component Status Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( component Status Column Definitions , print Component h . Table Handler ( component Status Column Definitions , print Component Status deployment Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Deployment Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Deployment Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : " " } , { Name : " " , Type : " " , Priority : 1 , Description : extensionsv1beta1 . Deployment Spec { } . Swagger h . Table Handler ( deployment Column Definitions , print h . Table Handler ( deployment Column Definitions , print Deployment horizontal Pod Autoscaler Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : autoscalingv2beta1 . Horizontal Pod Autoscaler Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : autoscalingv2beta1 . Horizontal Pod Autoscaler Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : autoscalingv2beta1 . Horizontal Pod Autoscaler Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : autoscalingv2beta1 . Horizontal Pod Autoscaler Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : autoscalingv2beta1 . Horizontal Pod Autoscaler Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( horizontal Pod Autoscaler Column Definitions , print Horizontal Pod h . Table Handler ( horizontal Pod Autoscaler Column Definitions , print Horizontal Pod Autoscaler config Map Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : apiv1 . Config Map { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( config Map Column Definitions , print Config h . Table Handler ( config Map Column Definitions , print Config Map pod Security Policy Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : policyv1beta1 . Pod Security Policy Spec { } . Swagger h . Table Handler ( pod Security Policy Column Definitions , print Pod Security h . Table Handler ( pod Security Policy Column Definitions , print Pod Security Policy network Policy Column Definitioins := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : extensionsv1beta1 . Network Policy Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( network Policy Column Definitioins , print Network h . Table Handler ( network Policy Column Definitioins , print Network Policy role Bindings Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : rbacv1beta1 . Role Binding { } . Swagger h . Table Handler ( role Bindings Column Definitions , print Role h . Table Handler ( role Bindings Column Definitions , print Role Binding cluster Role Bindings Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Priority : 1 , Description : rbacv1beta1 . Cluster Role Binding { } . Swagger h . Table Handler ( cluster Role Bindings Column Definitions , print Cluster Role h . Table Handler ( cluster Role Bindings Column Definitions , print Cluster Role Binding certificate Signing Request Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : certificatesv1beta1 . Certificate Signing Request Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : certificatesv1beta1 . Certificate Signing Request Status { } . Swagger h . Table Handler ( certificate Signing Request Column Definitions , print Certificate Signing h . Table Handler ( certificate Signing Request Column Definitions , print Certificate Signing Request lease Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : coordinationv1 . Lease Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( lease Column Definitions , print h . Table Handler ( lease Column Definitions , print Lease storage Class Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : storagev1 . Storage Class { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( storage Class Column Definitions , print Storage h . Table Handler ( storage Class Column Definitions , print Storage Class status Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Description : metav1 . Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Status { } . Swagger h . Table Handler ( status Column Definitions , print controller Revision Column Definition := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : appsv1beta1 . Controller Revision { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( controller Revision Column Definition , print Controller h . Table Handler ( controller Revision Column Definition , print Controller Revision resource Quota Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( resource Quota Column Definitions , print Resource h . Table Handler ( resource Quota Column Definitions , print Resource Quota priority Class Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : schedulingv1 . Priority Class { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : schedulingv1 . Priority Class { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( priority Class Column Definitions , print Priority h . Table Handler ( priority Class Column Definitions , print Priority Class runtime Class Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( runtime Class Column Definitions , print Runtime h . Table Handler ( runtime Class Column Definitions , print Runtime Class volume Attachment Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Format : " " , Description : storagev1 . Volume Attachment Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : storagev1 . Volume Attachment Source { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : storagev1 . Volume Attachment Spec { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : storagev1 . Volume Attachment Status { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Table Handler ( volume Attachment Column Definitions , print Volume h . Table Handler ( volume Attachment Column Definitions , print Volume Attachment Add Default } 
func Add Default Handlers ( h printers . Print Handler ) { // types without defined columns object Meta Column Definitions := [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : metav1 . Object Meta { } . Swagger Doc ( ) [ " " ] } , { Name : " " , Type : " " , Description : metav1 . Object Meta { } . Swagger h . Default Table Handler ( object Meta Column Definitions , print Object } 
func translate Timestamp Until ( timestamp metav1 . Time ) string { if timestamp . Is return duration . Human } 
func print Replication Controller ( obj * api . Replication Controller , options printers . Print Options ) ( [ ] metav1beta1 . Table Row , error ) { row := metav1beta1 . Table Row { Object : runtime . Raw desired current ready Replicas := obj . Status . Ready row . Cells = append ( row . Cells , obj . Name , int64 ( desired Replicas ) , int64 ( current Replicas ) , int64 ( ready Replicas ) , translate Timestamp Since ( obj . Creation if options . Wide { names , images := layout Container row . Cells = append ( row . Cells , names , images , labels . Format return [ ] metav1beta1 . Table } 
func get Node External IP ( node * api . Node ) string { for _ , address := range node . Status . Addresses { if address . Type == api . Node External } 
func get Node Internal IP ( node * api . Node ) string { for _ , address := range node . Status . Addresses { if address . Type == api . Node Internal } 
func find Node Roles ( node * api . Node ) [ ] string { roles := sets . New for k , v := range node . Labels { switch { case strings . Has Prefix ( k , label Node Role Prefix ) : if role := strings . Trim Prefix ( k , label Node Role case k == node Label } 
func print Event List ( list * api . Event List , options printers . Print Options ) ( [ ] metav1beta1 . Table Row , error ) { rows := make ( [ ] metav1beta1 . Table for i := range list . Items { r , err := print } 
func print Role Binding List ( list * rbac . Role Binding List , options printers . Print Options ) ( [ ] metav1beta1 . Table Row , error ) { rows := make ( [ ] metav1beta1 . Table for i := range list . Items { r , err := print Role } 
func print Cluster Role Binding List ( list * rbac . Cluster Role Binding List , options printers . Print Options ) ( [ ] metav1beta1 . Table Row , error ) { rows := make ( [ ] metav1beta1 . Table for i := range list . Items { r , err := print Cluster Role } 
func layout Container Cells ( containers [ ] api . Container ) ( names string , images string ) { var names var images for i , container := range containers { names Buffer . Write images Buffer . Write if i != len ( containers ) - 1 { names Buffer . Write images Buffer . Write return names Buffer . String ( ) , images } 
func format Event Source ( es api . Event Source ) string { Event Source if len ( es . Host ) > 0 { Event Source String = append ( Event Source return strings . Join ( Event Source } 
func format Resource Name ( kind schema . Group Kind , name string , with Kind bool ) string { if ! with return strings . To } 
func ( b * flocker Volume Mounter ) Set Up ( fs Group * int64 ) error { return b . Set Up At ( b . Get Path ( ) , fs } 
func ( p * flocker Plugin ) new Flocker Client ( host IP string ) ( * flockerapi . Client , error ) { host := env . Get Env As String Or Fallback ( " " , default port , err := env . Get Env As Int Or Fallback ( " " , default ca Cert Path := env . Get Env As String Or Fallback ( " " , default CA Cert key Path := env . Get Env As String Or Fallback ( " " , default Client Key cert Path := env . Get Env As String Or Fallback ( " " , default Client Cert c , err := flockerapi . New Client ( host , port , host IP , ca Cert Path , key Path , cert } 
func ( b * flocker Volume Mounter ) Set Up At ( dir string , fs if b . flocker Client == nil { b . flocker Client , err = b . new Flocker dataset UUID , err := b . Get Dataset if err != nil { return fmt . Errorf ( " " , b . dataset dataset State , err := b . flocker Client . Get Dataset State ( dataset if err != nil { return fmt . Errorf ( " " , dataset primary UUID , err := b . flocker Client . Get Primary if dataset State . Primary != primary UUID { if err := b . update Dataset Primary ( dataset UUID , primary _ , err := b . flocker Client . Get Dataset State ( dataset if err != nil { return fmt . Errorf ( " " , dataset // TODO: handle failed mounts here. not Mnt , err := b . mounter . Is Likely Not Mount klog . V ( 4 ) . Infof ( " " , dir , ! not Mnt , err , dataset UUID , b . read if err != nil && ! os . Is Not if ! not if err := os . Mkdir if b . read global Flocker Path := make Global Flocker Path ( dataset err = b . mounter . Mount ( global Flocker if err != nil { not Mnt , mnt Err := b . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! not Mnt { if mnt Err = b . mounter . Unmount ( dir ) ; mnt Err != nil { klog . Errorf ( " " , mnt not Mnt , mnt Err := b . mounter . Is Likely Not Mount if mnt Err != nil { klog . Errorf ( " " , mnt if ! not if ! b . read Only { volume . Set Volume Ownership ( b , fs } 
func ( b * flocker Volume Mounter ) update Dataset Primary ( dataset UUID string , primary UUID string ) error { // We need to update the primary and wait for it to be ready _ , err := b . flocker Client . Update Primary For Dataset ( primary UUID , dataset timeout Chan := time . New Timer ( timeout Waiting For defer timeout tick Chan := time . New Ticker ( ticker Waiting For defer tick for { if s , err := b . flocker Client . Get Dataset State ( dataset UUID ) ; err == nil && s . Primary == primary select { case <- timeout Chan . C : return fmt . Errorf ( " \n " , dataset UUID , primary case <- tick } 
func attachable Pod For Object ( rest Client Getter genericclioptions . REST Client client Config , err := rest Client Getter . To REST clientset , err := corev1client . New For Config ( client namespace , selector , err := Selectors For sort By := func ( pods [ ] * corev1 . Pod ) sort . Interface { return sort . Reverse ( podutils . Active pod , _ , err := Get First Pod ( clientset , namespace , selector . String ( ) , timeout , sort } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & Custom Resource Definition { } , func ( obj interface { } ) { Set Object Defaults_Custom Resource Definition ( obj . ( * Custom Resource scheme . Add Type Defaulting Func ( & Custom Resource Definition List { } , func ( obj interface { } ) { Set Object Defaults_Custom Resource Definition List ( obj . ( * Custom Resource Definition } 
func ( c * Fake Cluster Role Bindings ) Create ( cluster Role Binding * v1beta1 . Cluster Role Binding ) ( result * v1beta1 . Cluster Role Binding , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( clusterrolebindings Resource , cluster Role Binding ) , & v1beta1 . Cluster Role return obj . ( * v1beta1 . Cluster Role } 
func ( c * Fake Cluster Role Bindings ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( clusterrolebindings Resource , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Cluster Role Binding } 
func ( c * Fake Cluster Role Bindings ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Cluster Role Binding , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( clusterrolebindings Resource , name , pt , data , subresources ... ) , & v1beta1 . Cluster Role return obj . ( * v1beta1 . Cluster Role } 
func ( c * Fake Namespaces ) Get ( name string , options v1 . Get Options ) ( result * corev1 . Namespace , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( namespaces } 
func ( c * Fake Namespaces ) List ( opts v1 . List Options ) ( result * corev1 . Namespace List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( namespaces Resource , namespaces Kind , opts ) , & corev1 . Namespace label , _ , _ := testing . Extract From List list := & corev1 . Namespace List { List Meta : obj . ( * corev1 . Namespace List ) . List for _ , item := range obj . ( * corev1 . Namespace } 
func ( c * Fake Namespaces ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( namespaces } 
func ( c * Fake Namespaces ) Create ( namespace * corev1 . Namespace ) ( result * corev1 . Namespace , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( namespaces } 
func ( c * Fake Namespaces ) Update ( namespace * corev1 . Namespace ) ( result * corev1 . Namespace , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( namespaces } 
func ( c * Fake Namespaces ) Update Status ( namespace * corev1 . Namespace ) ( * corev1 . Namespace , error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Subresource Action ( namespaces } 
func ( c * Fake Namespaces ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( namespaces } 
func ( c * Fake Namespaces ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * corev1 . Namespace , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( namespaces } 
func ( d * Visiting Depth First ) Walk ( g Visitable g . Visit From ( t , func ( n graph . Node ) ( should Continue bool ) { if d . Edge Filter != nil && ! d . Edge } 
func ( d * Visiting Depth } 
func ( d * Visiting Depth } 
func maybe Wrap For Connection Upgrades ( rest Config * restclient . Config , rt http . Round Tripper , req * http . Request ) ( http . Round Tripper , bool , error ) { if ! httpstream . Is Upgrade tls Config , err := restclient . TLS Config For ( rest follow Redirects := utilfeature . Default Feature Gate . Enabled ( genericfeatures . Streaming Proxy require Same Host Redirects := utilfeature . Default Feature Gate . Enabled ( genericfeatures . Validate Proxy upgrade Round Tripper := spdy . New Round Tripper ( tls Config , follow Redirects , require Same Host wrapped RT , err := restclient . HTTP Wrappers For Config ( rest Config , upgrade Round return wrapped } 
func ( r * responder ) Object ( status Code int , obj runtime . Object ) { responsewriters . Write Raw JSON ( status } 
func ( r * proxy Handler ) update API Service ( api Service * apiregistrationapi . API Service ) { if api Service . Spec . Service == nil { r . handling Info . Store ( proxy Handling new Info := proxy Handling Info { name : api Service . Name , rest Config : & restclient . Config { TLS Client Config : restclient . TLS Client Config { Insecure : api Service . Spec . Insecure Skip TLS Verify , Server Name : api Service . Spec . Service . Name + " " + api Service . Spec . Service . Namespace + " " , Cert Data : r . proxy Client Cert , Key Data : r . proxy Client Key , CA Data : api Service . Spec . CA Bundle , } , } , service Name : api Service . Spec . Service . Name , service Namespace : api Service . Spec . Service . Namespace , service Port : api Service . Spec . Service . Port , service Available : apiregistrationapi . Is API Service Condition True ( api if r . proxy Transport != nil && r . proxy Transport . Dial Context != nil { new Info . rest Config . Dial = r . proxy Transport . Dial new Info . proxy Round Tripper , new Info . transport Building Error = restclient . Transport For ( new Info . rest if new Info . transport Building Error != nil { klog . Warning ( new Info . transport Building r . handling Info . Store ( new } 
func ( s * node Lister ) List ( selector labels . Selector ) ( ret [ ] * v1 . Node , err error ) { err = cache . List } 
func ( s * node Lister ) Get ( name string ) ( * v1 . Node , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not } 
func New } 
func Int Key Set ( the Map interface { } ) Int { v := reflect . Value Of ( the for _ , key Value := range v . Map Keys ( ) { ret . Insert ( key } 
} 
} 
} 
func ( s Int ) Has } 
func ( s Int ) Difference ( s2 Int ) Int { result := New } 
func ( s1 Int ) Union ( s2 Int ) Int { result := New } 
result := New } 
func ( s1 Int ) Equal ( s2 Int ) bool { return len ( s1 ) == len ( s2 ) && s1 . Is } 
func ( s Int ) List ( ) [ ] int { res := make ( sortable Slice Of } 
func ( s Int ) Unsorted } 
func ( s Int ) Pop var zero return zero } 
func Get V Sphere ( ) ( * V Sphere , error ) { cfg , err := get V Sphere vs , err := new Controller } 
func get Accessible Datastores ( ctx context . Context , node Vm Detail * Node Details , node Manager * Node Manager ) ( [ ] * vclib . Datastore Info , error ) { accessible Datastores , err := node Vm Detail . vm . Get All Accessible if err != nil { // Check if the node VM is not found which indicates that the node info in the node manager is stale. // If so, rediscover the node and retry. if vclib . Is Managed Object Not Found Error ( err ) { klog . V ( 4 ) . Infof ( " " , err , node Vm Detail . Node err = node Manager . Rediscover Node ( convert To K8s Type ( node Vm Detail . Node if err == nil { klog . V ( 4 ) . Infof ( " " , node Vm Detail . Node node Info , err := node Manager . Get Node Info ( convert To K8s Type ( node Vm Detail . Node if err != nil { klog . V ( 4 ) . Infof ( " " , err , node Vm accessible Datastores , err = node Info . vm . Get All Accessible if err != nil { klog . V ( 4 ) . Infof ( " " , err , node Vm } else { klog . V ( 4 ) . Infof ( " " , err , node Vm } else { klog . V ( 4 ) . Infof ( " " , err , node Vm return accessible } 
func get Shared Datastores In K8S Cluster ( ctx context . Context , dc * vclib . Datacenter , node Manager * Node Manager ) ( [ ] * vclib . Datastore Info , error ) { node Vm Details , err := node Manager . Get Node if len ( node Vm Details ) == 0 { msg := fmt . Sprintf ( " " , node Vm var shared Datastores [ ] * vclib . Datastore for _ , node Vm Detail := range node Vm Details { klog . V ( 9 ) . Infof ( " " , node Vm Detail . Node accessible Datastores , err := get Accessible Datastores ( ctx , & node Vm Detail , node if err != nil { if err == vclib . Err No VM Found { klog . V ( 9 ) . Infof ( " " , node Vm Detail . Node if len ( shared Datastores ) == 0 { shared Datastores = accessible } else { shared Datastores = intersect ( shared Datastores , accessible if len ( shared Datastores ) == 0 { return nil , fmt . Errorf ( " " , node Vm klog . V ( 9 ) . Infof ( " " , shared shared Datastores , err = get Datastores For Endpoint VC ( ctx , dc , shared klog . V ( 9 ) . Infof ( " " , shared return shared } 
func get Most Free Datastore Name ( ctx context . Context , client * vim25 . Client , ds Info List [ ] * vclib . Datastore Info ) ( string , error ) { var cur cur for i , ds Info := range ds Info List { ds Free Space := ds Info . Info . Get Datastore Info ( ) . Free if ds Free Space > cur Max { cur Max = ds Free return ds Info List [ index ] . Info . Get Datastore } 
func get Datastores For Endpoint VC ( ctx context . Context , dc * vclib . Datacenter , shared Ds Infos [ ] * vclib . Datastore Info ) ( [ ] * vclib . Datastore Info , error ) { var datastores [ ] * vclib . Datastore all Ds Info Map , err := dc . Get All for _ , shared Ds Info := range shared Ds Infos { ds Info , ok := all Ds Info Map [ shared Ds if ok { datastores = append ( datastores , ds } else { klog . V ( 4 ) . Infof ( " " , shared Ds } 
func ( vs * V Sphere ) clean Up Dummy V Ms ( dummy VM Prefix string ) { // Create context ctx , cancel := context . With for { time . Sleep ( Clean Up Dummy VM Routine vsi , err := vs . get V Sphere Instance For Server ( vs . cfg . Workspace . V Center dc , err := vclib . Get // Get the folder reference for global working directory where the dummy VM needs to be created. vm Folder , err := dc . Get Folder By // A write lock is acquired to make sure the clean Up routine doesn't delete any VM's created by ongoing PVC requests. clean Up Dummy V Ms := func ( ) { clean Up Dummy VM defer clean Up Dummy VM err = diskmanagers . Clean Up Dummy V Ms ( ctx , vm clean Up Dummy V } 
func getcanonical Volume Path ( ctx context . Context , dc * vclib . Datacenter , volume Path string ) ( string , error ) { var folder var folder canonical Volume Path := volume ds Path Obj , err := vclib . Get Datastore Path Obj From VM Disk Path ( volume ds Path := strings . Split ( strings . Trim Space ( ds Path if len ( ds Path ) <= 1 { return canonical Volume datastore := ds Path ds Folder := ds folder Name ID Map , datastore Exists := datastore Folder ID if datastore Exists { folder ID , folder Exists = folder Name ID Map [ ds // Get the datastore folder ID if datastore or folder doesn't exist in datastore Folder ID Map if ! datastore Exists || ! folder Exists { if ! vclib . Is Valid UUID ( ds Folder ) { dummy Disk Vol Path := " " + datastore + " " + ds Folder + " " + Dummy Disk // Querying a non-existent dummy disk on the datastore folder. // It would fail and return an folder ID in the error message. _ , err := dc . Get Virtual Disk Page83Data ( ctx , dummy Disk Vol canonical Volume Path , err = get Path From File Not disk Path := vclib . Get Path From VM Disk Path ( canonical Volume if disk Path == " " { return " " , fmt . Errorf ( " " , canonical Volume folder ID = strings . Split ( strings . Trim Space ( disk setdatastore Folder ID Map ( datastore Folder ID Map , datastore , ds Folder , folder canonical Volume Path = strings . Replace ( volume Path , ds Folder , folder return canonical Volume } 
func get Path From File Not Found ( err error ) ( string , error ) { if soap . Is Soap Fault ( err ) { fault := soap . To Soap f , ok := fault . Vim Fault ( ) . ( types . File Not } 
func ( vs * V Sphere ) convert Vol Paths To Device Paths ( ctx context . Context , node Volumes map [ k8stypes . Node Name ] [ ] string ) ( map [ k8stypes . Node Name ] [ ] string , error ) { vm Volumes := make ( map [ k8stypes . Node for node Name , vol Paths := range node Volumes { node Info , err := vs . node Manager . Get Node Info ( node _ , err = vs . get V Sphere Instance For Server ( node Info . vc for i , vol Path := range vol Paths { device Vol Path , err := convert Vol Path To Device Path ( ctx , node Info . data Center , vol if err != nil { klog . Errorf ( " " , vol Path , device Vol vol Paths [ i ] = device Vol vm Volumes [ node Name ] = vol return vm } 
func ( vs * V Sphere ) check Disk Attached ( ctx context . Context , nodes [ ] k8stypes . Node Name , node Volumes map [ k8stypes . Node Name ] [ ] string , attached map [ string ] map [ string ] bool , retry bool ) ( [ ] k8stypes . Node Name , error ) { var nodes To Retry [ ] k8stypes . Node var vm List [ ] * vclib . Virtual var node Info Node for _ , node Name := range nodes { node Info , err = vs . node Manager . Get Node Info ( node if err != nil { return nodes To vm List = append ( vm List , node // Making sure session is valid _ , err = vs . get V Sphere Instance For Server ( node Info . vc if err != nil { return nodes To // If any of the nodes are not present property collector query will fail for entire operation vm Mo List , err := node Info . data Center . Get VM Mo List ( ctx , vm if err != nil { if vclib . Is Managed Object Not Found Error ( err ) && ! retry { klog . V ( 4 ) . Infof ( " " , nodes , vm // Property Collector Query failed // Verify Volume Paths per VM for _ , node Name := range nodes { node Info , err := vs . node Manager . Get Node Info ( node if err != nil { return nodes To devices , err := node Info . vm . Virtual if err != nil { if vclib . Is Managed Object Not Found Error ( err ) { klog . V ( 4 ) . Infof ( " " , node Name , node nodes To Retry = append ( nodes To Retry , node return nodes To klog . V ( 4 ) . Infof ( " " , node Name , node vclib . Verify Volume Paths For VM Devices ( devices , node Volumes [ node Name ] , convert To String ( node return nodes To vm Mo Map := make ( map [ string ] mo . Virtual for _ , vm Mo := range vm Mo List { if vm Mo . Config == nil { klog . Errorf ( " " , vm klog . V ( 9 ) . Infof ( " " , vm Mo . Name , strings . To Lower ( vm vm Mo Map [ strings . To Lower ( vm Mo . Config . Uuid ) ] = vm klog . V ( 9 ) . Infof ( " " , vm Mo for _ , node Name := range nodes { node , err := vs . node Manager . Get Node ( node if err != nil { return nodes To node UUID , err := Get Node return nodes To node UUID = strings . To Lower ( node klog . V ( 9 ) . Infof ( " " , node Name , node UUID , vm Mo vclib . Verify Volume Paths For VM ( vm Mo Map [ node UUID ] , node Volumes [ node Name ] , convert To String ( node return nodes To } 
func New Cmd Token ( out io . Writer , err W io . Writer ) * cobra . Command { var kube Config var dry token ` ) , // Without this callback, if a user runs just the "token" // command without a subcommand, or with an invalid subcommand, // cobra will print usage information, but still exit cleanly. // We want to return an error code in these cases so that the // user knows that their command was invalid. Run E : cmdutil . Sub Cmd Run options . Add Kube Config Flag ( token Cmd . Persistent Flags ( ) , & kube Config token Cmd . Persistent Flags ( ) . Bool Var ( & dry Run , options . Dry Run , dry cfg := & kubeadmapiv1beta2 . Init var cfg var print Join bto := options . New Bootstrap Token create Cmd := & cobra . Command { Use : " " , Disable Flags In Use ` ) , Run : func ( token Cmd * cobra . Command , args [ ] string ) { if len ( args ) > 0 { bto . Token err := validation . Validate Mixed Arguments ( token kubeadmutil . Check err = bto . Apply kubeadmutil . Check kube Config File = cmdutil . Get Kube Config Path ( kube Config client , err := get Clientset ( kube Config File , dry kubeadmutil . Check err = Run Create Token ( out , client , cfg Path , cfg , print Join Command , kube Config kubeadmutil . Check options . Add Config Flag ( create Cmd . Flags ( ) , & cfg create Cmd . Flags ( ) . Bool Var ( & print Join bto . Add TTL Flag With Name ( create bto . Add Usages Flag ( create bto . Add Groups Flag ( create bto . Add Description Flag ( create token Cmd . Add Command ( create token Cmd . Add Command ( New Cmd Token list ` ) , Run : func ( token Cmd * cobra . Command , args [ ] string ) { kube Config File = cmdutil . Get Kube Config Path ( kube Config client , err := get Clientset ( kube Config File , dry kubeadmutil . Check err = Run List Tokens ( out , err kubeadmutil . Check token Cmd . Add Command ( list delete Cmd := & cobra . Command { Use : " " , Disable Flags In Use ` ) , Run : func ( token Cmd * cobra . Command , args [ ] string ) { if len ( args ) < 1 { kubeadmutil . Check Err ( errors . Errorf ( " " , bootstrapapi . Bootstrap Token ID kube Config File = cmdutil . Get Kube Config Path ( kube Config client , err := get Clientset ( kube Config File , dry kubeadmutil . Check err = Run Delete kubeadmutil . Check token Cmd . Add Command ( delete return token } 
func New Cmd Token ` ) , Run : func ( cmd * cobra . Command , args [ ] string ) { err := Run Generate kubeadmutil . Check } 
func Run Create Token ( out io . Writer , client clientset . Interface , cfg Path string , cfg * kubeadmapiv1beta2 . Init Configuration , print Join Command bool , kube Config File string ) error { // Kubernetes Version is not used, but we set it explicitly to avoid the lookup // of the version from the internet when executing Load Or Default Init Configuration phaseutil . Set Kubernetes Version ( & cfg . Cluster internalcfg , err := configutil . Load Or Default Init Configuration ( cfg if err := tokenphase . Create New Tokens ( client , internalcfg . Bootstrap // if --print-join-command was specified, print a machine-readable full `kubeadm join` command // otherwise, just print the token if print Join Command { skip Token join Command , err := cmdutil . Get Join Worker Command ( kube Config File , internalcfg . Bootstrap Tokens [ 0 ] . Token . String ( ) , skip Token join Command = strings . Replace All ( join join Command = strings . Replace All ( join fmt . Fprintln ( out , join } else { fmt . Fprintln ( out , internalcfg . Bootstrap } 
func Run Generate token , err := bootstraputil . Generate Bootstrap } 
func Run List Tokens ( out io . Writer , err token Selector := fields . Selector From Set ( map [ string ] string { // TODO: We hard-code "type" here until `field_constants.go` that is // currently in `pkg/apis/core/` exists in the external API, i.e. // k8s.io/api/v1. Should be v1.Secret Type Field " " : string ( bootstrapapi . Secret Type Bootstrap list Options := metav1 . List Options { Field Selector : token secrets , err := client . Core V1 ( ) . Secrets ( metav1 . Namespace System ) . List ( list w := tabwriter . New for _ , secret := range secrets . Items { // Get the Bootstrap Token struct representation from the Secret object token , err := kubeadmapi . Bootstrap Token From if err != nil { fmt . Fprintf ( err // Get the human-friendly string representation for the token human Friendly Token Output := human Readable Bootstrap fmt . Fprintln ( w , human Friendly Token } 
func Run Delete Tokens ( out io . Writer , client clientset . Interface , token I Ds Or Tokens [ ] string ) error { for _ , token ID Or Token := range token I Ds Or Tokens { // Assume this is a token id and try to parse it token ID := token ID Or klog . V ( 1 ) . Infof ( " " , token ID Or if ! bootstraputil . Is Valid Bootstrap Token ID ( token ID Or Token ) { // Okay, the full token with both id and secret was probably passed. Parse it and extract the ID only bts , err := kubeadmapiv1beta2 . New Bootstrap Token String ( token ID Or if err != nil { return errors . Errorf ( " " , token ID Or Token , bootstrapapi . Bootstrap Token ID Pattern , bootstrapapi . Bootstrap Token ID token token Secret Name := bootstraputil . Bootstrap Token Secret Name ( token klog . V ( 1 ) . Infof ( " " , token if err := client . Core V1 ( ) . Secrets ( metav1 . Namespace System ) . Delete ( token Secret Name , nil ) ; err != nil { return errors . Wrapf ( err , " " , token fmt . Fprintf ( out , " \n " , token } 
func ( m * mapper ) info For name , _ := metadata namespace , _ := metadata resource Version , _ := metadata Accessor . Resource ret := & Info { Source : source , Namespace : namespace , Name : name , Resource Version : resource if m . local Fn == nil || ! m . local Fn ( ) { rest Mapper , err := m . rest Mapper mapping , err := rest Mapper . REST Mapping ( gvk . Group client , err := m . client Fn ( gvk . Group } 
func ( m * mapper ) info For Object ( obj runtime . Object , typer runtime . Object Typer , preferred GV Ks [ ] schema . Group Version Kind ) ( * Info , error ) { group Version Kinds , _ , err := typer . Object if err != nil { return nil , fmt . Errorf ( " " , reflect . Type gvk := group Version if len ( group Version Kinds ) > 1 && len ( preferred GV Ks ) > 0 { gvk = preferred Object Kind ( group Version Kinds , preferred GV name , _ := metadata namespace , _ := metadata resource Version , _ := metadata Accessor . Resource ret := & Info { Namespace : namespace , Name : name , Resource Version : resource if m . local Fn == nil || ! m . local Fn ( ) { rest Mapper , err := m . rest Mapper mapping , err := rest Mapper . REST Mapping ( gvk . Group client , err := m . client Fn ( gvk . Group } 
func preferred Object Kind ( possibilities [ ] schema . Group Version Kind , preferences [ ] schema . Group Version Kind ) schema . Group Version // Group Kind match for _ , priority := range preferences { for _ , possibility := range possibilities { if possibility . Group Kind ( ) == priority . Group } 
func ( g * Gen Scheme ) Filter ( c * generator . Context , t * types . Type ) bool { ret := ! g . scheme g . scheme } 
func New Token Cleaner ( cl clientset . Interface , secrets coreinformers . Secret Informer , options Token Cleaner Options ) ( * Token Cleaner , error ) { e := & Token Cleaner { client : cl , secret Lister : secrets . Lister ( ) , secret Synced : secrets . Informer ( ) . Has Synced , token Secret Namespace : options . Token Secret Namespace , queue : workqueue . New Named Rate Limiting Queue ( workqueue . Default Controller Rate if cl . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { if err := metrics . Register Metric And Track Rate Limiter Usage ( " " , cl . Core V1 ( ) . REST Client ( ) . Get Rate secrets . Informer ( ) . Add Event Handler With Resync Period ( cache . Filtering Resource Event Handler { Filter Func : func ( obj interface { } ) bool { switch t := obj . ( type ) { case * v1 . Secret : return t . Type == bootstrapapi . Secret Type Bootstrap Token && t . Namespace == e . token Secret default : utilruntime . Handle } , Handler : cache . Resource Event Handler Funcs { Add Func : e . enqueue Secrets , Update Func : func ( old Secret , new Secret interface { } ) { e . enqueue Secrets ( new Secret ) } , } , } , options . Secret } 
func ( tc * Token Cleaner ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle defer tc . queue . Shut if ! controller . Wait For Cache Sync ( " " , stop Ch , tc . secret go wait . Until ( tc . worker , 10 * time . Second , stop <- stop } 
func ( tc * Token Cleaner ) process Next Work if err := tc . sync Func ( key . ( string ) ) ; err != nil { tc . queue . Add Rate utilruntime . Handle } 
func New CSR Cleaner Controller ( csr Client csrclient . Certificate Signing Request Interface , csr Informer certificatesinformers . Certificate Signing Request Informer , ) * CSR Cleaner Controller { return & CSR Cleaner Controller { csr Client : csr Client , csr Lister : csr } 
func ( ccc * CSR Cleaner Controller ) Run ( workers int , stop Ch <- chan struct { } ) { defer utilruntime . Handle for i := 0 ; i < workers ; i ++ { go wait . Until ( ccc . worker , polling Interval , stop <- stop } 
func ( ccc * CSR Cleaner Controller ) worker ( ) { csrs , err := ccc . csr } 
func is Issued Expired ( csr * capi . Certificate Signing Request ) ( bool , error ) { is Expired , err := is for _ , c := range csr . Status . Conditions { if c . Type == capi . Certificate Approved && is Issued ( csr ) && is } 
func is Pending Past Deadline ( csr * capi . Certificate Signing Request ) bool { // If there are no Conditions on the status, the CSR will appear via // `kubectl` as `Pending`. if len ( csr . Status . Conditions ) == 0 && is Older Than ( csr . Creation Timestamp , pending Expiration ) { klog . Infof ( " " , csr . Name , pending } 
func is Denied Past Deadline ( csr * capi . Certificate Signing Request ) bool { for _ , c := range csr . Status . Conditions { if c . Type == capi . Certificate Denied && is Older Than ( c . Last Update Time , denied Expiration ) { klog . Infof ( " " , csr . Name , denied } 
func is Issued Past Deadline ( csr * capi . Certificate Signing Request ) bool { for _ , c := range csr . Status . Conditions { if c . Type == capi . Certificate Approved && is Issued ( csr ) && is Older Than ( c . Last Update Time , approved Expiration ) { klog . Infof ( " " , csr . Name , approved } 
func is Older Than ( t metav1 . Time , d time . Duration ) bool { return ! t . Is } 
func is Expired ( csr * capi . Certificate Signing certs , err := x509 . Parse return time . Now ( ) . After ( certs [ 0 ] . Not } 
func New Subject Options ( streams genericclioptions . IO Streams ) * Subject Options { return & Subject Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , IO } 
func New Cmd Subject ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := New Subject cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : subject Long , Example : subject Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( o . Run ( add o . Print Flags . Add cmdutil . Add Filename Option Flags ( cmd , & o . Filename cmd . Flags ( ) . Bool cmd . Flags ( ) . String Var cmd . Flags ( ) . Bool cmdutil . Add Dry Run cmd . Flags ( ) . String Array cmd . Flags ( ) . String Array cmd . Flags ( ) . String Array Var ( & o . Service Accounts , " " , o . Service cmdutil . Add Include Uninitialized } 
func ( o * Subject if len ( o . Users ) == 0 && len ( o . Groups ) == 0 && len ( o . Service for _ , sa := range o . Service for _ , info := range o . Infos { _ , ok := info . Object . ( * rbacv1 . Cluster Role } 
func ( o * Subject Options ) Run ( fn update Subjects ) error { patches := Calculate Patches ( o . Infos , scheme . Default JSON for _ , user := range sets . New String ( o . Users ... ) . List ( ) { subject := rbacv1 . Subject { Kind : rbacv1 . User Kind , API Group : rbacv1 . Group for _ , group := range sets . New String ( o . Groups ... ) . List ( ) { subject := rbacv1 . Subject { Kind : rbacv1 . Group Kind , API Group : rbacv1 . Group for _ , sa := range sets . New String ( o . Service subject := rbacv1 . Subject { Kind : rbacv1 . Service Account transformed , err := update Subject For if transformed && err == nil { // TODO: switch Update Pod Spec For Object to work on v1.Pod Spec return runtime . Encode ( scheme . Default JSON all name := info . Object if patch . Err != nil { all Errs = append ( all //no changes if string ( patch . Patch ) == " " || len ( patch . Patch ) == 0 { all Errs = append ( all if o . Local || o . Dry Run { if err := o . Print Obj ( info . Object , o . Out ) ; err != nil { all Errs = append ( all actual , err := resource . New Helper ( info . Client , info . Mapping ) . Patch ( info . Namespace , info . Name , types . Strategic Merge Patch if err != nil { all Errs = append ( all if err := o . Print Obj ( actual , o . Out ) ; err != nil { all Errs = append ( all return utilerrors . New Aggregate ( all } 
func update Subject For Object ( obj runtime . Object , subjects [ ] rbacv1 . Subject , fn update Subjects ) ( bool , error ) { switch t := obj . ( type ) { case * rbacv1 . Role case * rbacv1 . Cluster Role } 
func new Replica Sets ( c * Apps V1beta2Client , namespace string ) * replica Sets { return & replica Sets { client : c . REST } 
func ( c * replica Sets ) Create ( replica Set * v1beta2 . Replica Set ) ( result * v1beta2 . Replica Set , err error ) { result = & v1beta2 . Replica err = c . client . Post ( ) . Namespace ( c . ns ) . Resource ( " " ) . Body ( replica } 
func ( c * replica Sets ) Update ( replica Set * v1beta2 . Replica Set ) ( result * v1beta2 . Replica Set , err error ) { result = & v1beta2 . Replica err = c . client . Put ( ) . Namespace ( c . ns ) . Resource ( " " ) . Name ( replica Set . Name ) . Body ( replica } 
func Make CSR ( private Key interface { } , subject * pkix . Name , dns SA Ns [ ] string , ip SA Ns [ ] net . IP ) ( csr [ ] byte , err error ) { template := & x509 . Certificate Request { Subject : * subject , DNS Names : dns SA Ns , IP Addresses : ip SA return Make CSR From Template ( private } 
func Make CSR From Template ( private Key interface { } , template * x509 . Certificate t . Signature Algorithm = sig Type ( private csr DER , err := x509 . Create Certificate Request ( cryptorand . Reader , & t , private csr Pem Block := & pem . Block { Type : Certificate Request Block Type , Bytes : csr return pem . Encode To Memory ( csr Pem } 
func apply OOM Score Adj ( pid int , oom Score var pid if pid == 0 { pid } else { pid max oom Score Adj Path := path . Join ( " " , pid value := strconv . Itoa ( oom Score klog . V ( 4 ) . Infof ( " " , oom Score Adj for i := 0 ; i < max Tries ; i ++ { err = ioutil . Write File ( oom Score Adj if err != nil { if os . Is Not Exist ( err ) { klog . V ( 2 ) . Infof ( " " , oom Score Adj return os . Err Not if err != nil { klog . V ( 2 ) . Infof ( " " , oom Score Adj } 
func ( oom Adjuster * OOM Adjuster ) apply OOM Score Adj Container ( cgroup Name string , oom Score Adj , max Tries int ) error { adjusted Process for i := 0 ; i < max Tries ; i ++ { continue pid List , err := oom Adjuster . pid Lister ( cgroup if err != nil { if os . Is Not Exist ( err ) { // Nothing to do since the container doesn't exist anymore. return os . Err Not continue klog . V ( 10 ) . Infof ( " " , cgroup } else if len ( pid continue } else { for _ , pid := range pid List { if ! adjusted Process if err = oom Adjuster . Apply OOM Score Adj ( pid , oom Score Adj ) ; err == nil { adjusted Process } else if err == os . Err Not continue if ! continue } 
func new Scale Set ( az * Cloud ) ( VM ss := & scale Set { Cloud : az , availability Set : new Availability ss . node Name To Scale Set Mapping Cache , err = ss . new Node Name To Scale Set Mapping ss . availability Set Nodes Cache , err = ss . new Availability Set Nodes ss . vmss Cache , err = ss . new Vmss ss . vmss VM Cache , err = ss . new Vmss VM } 
func ( ss * scale Set ) get Vmss VM ( node Name string ) ( ss Name , instance ID string , vm compute . Virtual Machine Scale Set VM , err error ) { instance ID , err = get Scale Set VM Instance ID ( node if err != nil { return ss Name , instance ss Name , err = ss . get Scale Set Name By Node Name ( node if err != nil { return ss Name , instance if ss Name == " " { return " " , " " , vm , cloudprovider . Instance Not resource Group , err := ss . Get Node Resource Group ( node klog . V ( 4 ) . Infof ( " " , ss Name , instance ID , node key := build Vmss Cache Key ( resource Group , ss . make Vmss VM Name ( ss Name , instance cached VM , err := ss . vmss VM if err != nil { return ss Name , instance if cached VM == nil { klog . Errorf ( " " , node return ss Name , instance ID , vm , cloudprovider . Instance Not return ss Name , instance ID , * ( cached VM . ( * compute . Virtual Machine Scale Set } 
func ( ss * scale Set ) Get Power Status By Node Name ( name string ) ( power State string , err error ) { _ , _ , vm , err := ss . get Vmss if err != nil { return power if vm . Instance View != nil && vm . Instance View . Statuses != nil { statuses := * vm . Instance if strings . Has Prefix ( state , vm Power State Prefix ) { return strings . Trim Prefix ( state , vm Power State } 
func ( ss * scale Set ) get Vmss VM By Instance ID ( resource Group , scale Set Name , instance ID string ) ( vm compute . Virtual Machine Scale Set VM , err error ) { vm Name := ss . make Vmss VM Name ( scale Set Name , instance key := build Vmss Cache Key ( resource Group , vm cached VM , err := ss . vmss VM if cached VM == nil { klog . Errorf ( " " , scale Set Name , instance return vm , cloudprovider . Instance Not return * ( cached VM . ( * compute . Virtual Machine Scale Set } 
func ( ss * scale Set ) Get Instance ID By Node Name ( name string ) ( string , error ) { managed By AS , err := ss . is Node Managed By Availability if managed By AS { // vm is managed by availability set. return ss . availability Set . Get Instance ID By Node _ , _ , vm , err := ss . get Vmss resource converted Resource ID , err := convert Resource Group Name To Lower ( resource return converted Resource } 
func ( ss * scale Set ) Get Node Name By Provider ID ( provider ID string ) ( types . Node Name , error ) { // Node Name is not part of provider ID for vmss instances. scale Set Name , err := extract Scale Set Name By Provider ID ( provider if err != nil { klog . V ( 4 ) . Infof ( " " , provider return ss . availability Set . Get Node Name By Provider ID ( provider resource Group , err := extract Resource Group By Provider ID ( provider if err != nil { return " " , fmt . Errorf ( " " , provider instance ID , err := get Last Segment ( provider if err != nil { klog . V ( 4 ) . Infof ( " " , provider return ss . availability Set . Get Node Name By Provider ID ( provider vm , err := ss . get Vmss VM By Instance ID ( resource Group , scale Set Name , instance if vm . Os Profile != nil && vm . Os Profile . Computer Name != nil { node Name := strings . To Lower ( * vm . Os Profile . Computer return types . Node Name ( node } 
func ( ss * scale Set ) Get Instance Type By Node Name ( name string ) ( string , error ) { managed By AS , err := ss . is Node Managed By Availability if managed By AS { // vm is managed by availability set. return ss . availability Set . Get Instance Type By Node _ , _ , vm , err := ss . get Vmss } 
func ( ss * scale Set ) Get Zone By Node Name ( name string ) ( cloudprovider . Zone , error ) { managed By AS , err := ss . is Node Managed By Availability if managed By AS { // vm is managed by availability set. return ss . availability Set . Get Zone By Node _ , _ , vm , err := ss . get Vmss var failure zone failure Domain = ss . make Zone ( zone } else if vm . Instance View != nil && vm . Instance View . Platform Fault Domain != nil { // Availability zone is not used for the node, falling back to fault domain. failure Domain = strconv . Itoa ( int ( * vm . Instance View . Platform Fault return cloudprovider . Zone { Failure Domain : failure } 
func ( ss * scale Set ) Get IP By Node Name ( node Name string ) ( string , string , error ) { nic , err := ss . Get Primary Interface ( node if err != nil { klog . Errorf ( " " , node Name , node ip Config , err := get Primary IP if err != nil { klog . Errorf ( " " , node internal IP := * ip Config . Private IP public if ip Config . Public IP Address != nil && ip Config . Public IP Address . ID != nil { pip ID := * ip Config . Public IP pip Name , err := get Last Segment ( pip if err != nil { return " " , " " , fmt . Errorf ( " " , node Name , pip resource Group , err := ss . Get Node Resource Group ( node pip , exists Pip , err := ss . get Public IP Address ( resource Group , pip if exists Pip { public IP = * pip . IP return internal IP , public } 
func ( az * Cloud ) get Vmss Machine ID ( resource Group , scale Set Name , instance ID string ) string { return fmt . Sprintf ( vmss Machine ID Template , az . Subscription ID , strings . To Lower ( resource Group ) , scale Set Name , instance } 
func get Scale Set VM Instance ID ( machine Name string ) ( string , error ) { name Length := len ( machine if name Length < 6 { return " " , Error Not Vmss instance ID , err := strconv . Parse Uint ( machine Name [ name if err != nil { return " " , Error Not Vmss return fmt . Sprintf ( " " , instance } 
func extract Scale Set Name By Provider ID ( provider ID string ) ( string , error ) { matches := scale Set Name RE . Find String Submatch ( provider if len ( matches ) != 2 { return " " , Error Not Vmss } 
func extract Resource Group By Provider ID ( provider ID string ) ( string , error ) { matches := resource Group RE . Find String Submatch ( provider if len ( matches ) != 2 { return " " , Error Not Vmss } 
func ( ss * scale Set ) list Scale Sets ( resource ctx , cancel := get Context With all Scale Sets , err := ss . Virtual Machine Scale Sets Client . List ( ctx , resource ss Names := make ( [ ] string , len ( all Scale for i := range all Scale Sets { ss Names [ i ] = * ( all Scale return ss } 
func ( ss * scale Set ) list Scale Set V Ms ( scale Set Name , resource Group string ) ( [ ] compute . Virtual Machine Scale Set ctx , cancel := get Context With all V Ms , err := ss . Virtual Machine Scale Set V Ms Client . List ( ctx , resource Group , scale Set Name , " " , " " , string ( compute . Instance return all V } 
func ( ss * scale Set ) get Agent Pool Scale Sets ( nodes [ ] * v1 . Node ) ( * [ ] string , error ) { agent Pool Scale for nx := range nodes { if is Master if ss . Should Node Excluded From Load node ss Name , err := ss . get Scale Set Name By Node Name ( node if ss Name == " " { klog . V ( 3 ) . Infof ( " " , node * agent Pool Scale Sets = append ( * agent Pool Scale Sets , ss return agent Pool Scale } 
func ( ss * scale Set ) Get VM Set Names ( service * v1 . Service , nodes [ ] * v1 . Node ) ( vm Set Names * [ ] string , err error ) { has Mode , is Auto , service VM Set Names := get Service Load Balancer if ! has Mode { // no mode specified in service annotation default to Primary Scale Set Name. scale Set Names := & [ ] string { ss . Config . Primary Scale Set return scale Set scale Set Names , err := ss . get Agent Pool Scale if len ( * scale Set // sort the list to have deterministic selection sort . Strings ( * scale Set if ! is Auto { if service VM Set Names == nil || len ( service VM Set for sasx := range service VM Set Names { for asx := range * scale Set Names { if strings . Equal Fold ( ( * scale Set Names ) [ asx ] , service VM Set service VM Set Names [ sasx ] = ( * scale Set if ! found { klog . Errorf ( " " , service VM Set return nil , fmt . Errorf ( " " , service VM Set vm Set Names = & service VM Set return vm Set } 
func extract Resource Group By VMSS Nic ID ( nic ID string ) ( string , error ) { matches := vmss IP Configuration RE . Find String Submatch ( nic if len ( matches ) != 4 { return " " , fmt . Errorf ( " " , nic } 
func ( ss * scale Set ) Get Primary Interface ( node Name string ) ( network . Interface , error ) { managed By AS , err := ss . is Node Managed By Availability Set ( node if managed By AS { // vm is managed by availability set. return ss . availability Set . Get Primary Interface ( node ss Name , instance ID , vm , err := ss . get Vmss VM ( node if err != nil { // VM is availability set, but not cached yet in availability Set Nodes Cache. if err == Error Not Vmss Instance { return ss . availability Set . Get Primary Interface ( node klog . Errorf ( " " , node Name , node primary Interface ID , err := ss . get Primary Interface if err != nil { klog . Errorf ( " " , node nic Name , err := get Last Segment ( primary Interface if err != nil { klog . Errorf ( " " , node Name , primary Interface resource Group , err := extract Resource Group By VMSS Nic ID ( primary Interface ctx , cancel := get Context With nic , err := ss . Interfaces Client . Get Virtual Machine Scale Set Network Interface ( ctx , resource Group , ss Name , instance ID , nic if err != nil { klog . Errorf ( " " , node Name , resource Group , ss Name , nic } 
func ( ss * scale Set ) get Primary Network Interface Configuration ( network Configurations [ ] compute . Virtual Machine Scale Set Network Configuration , node Name string ) ( * compute . Virtual Machine Scale Set Network Configuration , error ) { if len ( network Configurations ) == 1 { return & network for idx := range network Configurations { network Config := & network if network Config . Primary != nil && * network Config . Primary == true { return network return nil , fmt . Errorf ( " " , node } 
func ( ss * scale Set ) Ensure Host In Pool ( service * v1 . Service , node Name types . Node Name , backend Pool ID string , vm Set Name string , is Internal bool ) error { klog . V ( 3 ) . Infof ( " " , node Name , vm Set Name , backend Pool vm Name := map Node Name To VM Name ( node ss Name , instance ID , vm , err := ss . get Vmss VM ( vm // Check scale set name: // - For basic SKU load balancer, return nil if the node's scale set is mismatched with vm Set Name. // - For standard SKU load balancer, backend could belong to multiple VMSS, so we // don't check vm Set for it. if vm Set Name != " " && ! ss . use Standard Load Balancer ( ) && ! strings . Equal Fold ( vm Set Name , ss Name ) { klog . V ( 3 ) . Infof ( " " , vm Name , vm Set // Find primary network interface configuration. network Interface Configurations := * vm . Network Profile Configuration . Network Interface primary Network Interface Configuration , err := ss . get Primary Network Interface Configuration ( network Interface Configurations , vm // Find primary IP configuration. primary IP Configuration , err := ss . get Primary IP Config For Scale Set ( primary Network Interface Configuration , vm // Update primary IP configuration's Load Balancer Backend Address Pools. found new Backend Pools := [ ] compute . Sub if primary IP Configuration . Load Balancer Backend Address Pools != nil { new Backend Pools = * primary IP Configuration . Load Balancer Backend Address for _ , existing Pool := range new Backend Pools { if strings . Equal Fold ( backend Pool ID , * existing Pool . ID ) { found // The backend Pool ID has already been found from existing Load Balancer Backend Address Pools. if found if ss . use Standard Load Balancer ( ) && len ( new Backend Pools ) > 0 { // Although standard load balancer supports backends from multiple scale // sets, the same network interface couldn't be added to more than one load balancer of // the same type. Omit those nodes (e.g. masters) so Azure ARM won't complain // about this. new Backend Pools I Ds := make ( [ ] string , 0 , len ( new Backend for _ , pool := range new Backend Pools { if pool . ID != nil { new Backend Pools I Ds = append ( new Backend Pools I is Same LB , old LB Name , err := is Backend Pool On Same LB ( backend Pool ID , new Backend Pools I if ! is Same LB { klog . V ( 4 ) . Infof ( " " , node Name , old LB // Compose a new vmss VM with added backend Pool ID. new Backend Pools = append ( new Backend Pools , compute . Sub Resource { ID : to . String Ptr ( backend Pool primary IP Configuration . Load Balancer Backend Address Pools = & new Backend new VM := compute . Virtual Machine Scale Set VM { Sku : vm . Sku , Location : vm . Location , Virtual Machine Scale Set VM Properties : & compute . Virtual Machine Scale Set VM Properties { Hardware Profile : vm . Hardware Profile , Network Profile Configuration : & compute . Virtual Machine Scale Set VM Network Profile Configuration { Network Interface Configurations : & network Interface // Get the node resource group. node Resource Group , err := ss . Get Node Resource Group ( vm // Invalidate the cache since we would update it. key := build Vmss Cache Key ( node Resource Group , ss . make Vmss VM Name ( ss Name , instance defer ss . vmss VM // Update vmss VM with backoff. ctx , cancel := get Context With klog . V ( 2 ) . Infof ( " " , vm Name , backend Pool resp , err := ss . Virtual Machine Scale Set V Ms Client . Update ( ctx , node Resource Group , ss Name , instance ID , new if ss . Cloud Provider Backoff && should Retry HTTP Request ( resp , err ) { klog . V ( 2 ) . Infof ( " " , vm Name , backend Pool retry Err := ss . Update Vmss VM With Retry ( node Resource Group , ss Name , instance ID , new if retry Err != nil { err = retry klog . Errorf ( " " , vm Name , backend Pool } 
func ( ss * scale Set ) Ensure Hosts In Pool ( service * v1 . Service , nodes [ ] * v1 . Node , backend Pool ID string , vm Set Name string , is Internal bool ) error { host for _ , node := range nodes { local Node if ss . use Standard Load Balancer ( ) && ss . exclude Master Nodes From Standard LB ( ) && is Master Node ( node ) { klog . V ( 4 ) . Infof ( " " , local Node Name , backend Pool if ss . Should Node Excluded From Load Balancer ( node ) { klog . V ( 4 ) . Infof ( " " , local Node f := func ( ) error { // VMAS nodes should also be added to the SLB backends. if ss . use Standard Load Balancer ( ) { // Check whether the node is VMAS virtual machine. managed By AS , err := ss . is Node Managed By Availability Set ( local Node if err != nil { klog . Errorf ( " " , local Node if managed By AS { return ss . availability Set . Ensure Host In Pool ( service , types . Node Name ( local Node Name ) , backend Pool ID , vm Set Name , is err := ss . Ensure Host In Pool ( service , types . Node Name ( local Node Name ) , backend Pool ID , vm Set Name , is if err != nil { return fmt . Errorf ( " " , get Service Name ( service ) , backend Pool host Updates = append ( host errs := utilerrors . Aggregate Goroutines ( host } 
func ( ss * scale Set ) ensure Backend Pool Deleted From Node ( service * v1 . Service , node Name , backend Pool ID string ) error { ss Name , instance ID , vm , err := ss . get Vmss VM ( node // Find primary network interface configuration. network Interface Configurations := * vm . Network Profile Configuration . Network Interface primary Network Interface Configuration , err := ss . get Primary Network Interface Configuration ( network Interface Configurations , node // Find primary IP configuration.4 primary IP Configuration , err := ss . get Primary IP Config For Scale Set ( primary Network Interface Configuration , node if primary IP Configuration . Load Balancer Backend Address Pools == nil || len ( * primary IP Configuration . Load Balancer Backend Address // Construct new load Balancer Backend Address Pools and remove backend Address Pools from primary IP configuration. existing Backend Pools := * primary IP Configuration . Load Balancer Backend Address new Backend Pools := [ ] compute . Sub found for i := len ( existing Backend Pools ) - 1 ; i >= 0 ; i -- { cur Pool := existing Backend if strings . Equal Fold ( backend Pool ID , * cur Pool . ID ) { klog . V ( 10 ) . Infof ( " " , backend Pool ID , node found new Backend Pools = append ( existing Backend Pools [ : i ] , existing Backend // Pool not found, assume it has been already removed. if ! found // Compose a new vmss VM with added backend Pool ID. primary IP Configuration . Load Balancer Backend Address Pools = & new Backend new VM := compute . Virtual Machine Scale Set VM { Sku : vm . Sku , Location : vm . Location , Virtual Machine Scale Set VM Properties : & compute . Virtual Machine Scale Set VM Properties { Hardware Profile : vm . Hardware Profile , Network Profile Configuration : & compute . Virtual Machine Scale Set VM Network Profile Configuration { Network Interface Configurations : & network Interface // Get the node resource group. node Resource Group , err := ss . Get Node Resource Group ( node // Invalidate the cache since we would update it. key := build Vmss Cache Key ( node Resource Group , ss . make Vmss VM Name ( ss Name , instance defer ss . vmss VM // Update vmss VM with backoff. ctx , cancel := get Context With klog . V ( 2 ) . Infof ( " " , node Name , backend Pool resp , err := ss . Virtual Machine Scale Set V Ms Client . Update ( ctx , node Resource Group , ss Name , instance ID , new if ss . Cloud Provider Backoff && should Retry HTTP Request ( resp , err ) { klog . V ( 2 ) . Infof ( " " , node Name , backend Pool retry Err := ss . Update Vmss VM With Retry ( node Resource Group , ss Name , instance ID , new if retry Err != nil { err = retry klog . Errorf ( " " , node Name , backend Pool if err != nil { klog . Errorf ( " " , node Name , backend Pool } else { klog . V ( 2 ) . Infof ( " " , node Name , backend Pool } 
func ( ss * scale Set ) get Node Name By IP Configuration ID ( ip Configuration ID string ) ( string , error ) { matches := vmss IP Configuration RE . Find String Submatch ( ip Configuration if len ( matches ) != 4 { klog . V ( 4 ) . Infof ( " " , ip Configuration return " " , Error Not Vmss resource scale Set instance vm , err := ss . get Vmss VM By Instance ID ( resource Group , scale Set Name , instance if vm . Os Profile != nil && vm . Os Profile . Computer Name != nil { return strings . To Lower ( * vm . Os Profile . Computer } 
func ( ss * scale Set ) Ensure Backend Pool Deleted ( service * v1 . Service , backend Pool ID , vm Set Name string , backend Address Pools * [ ] network . Backend Address Pool ) error { // Returns nil if backend address pools already deleted. if backend Address ip Configuration I for _ , backend Pool := range * backend Address Pools { if strings . Equal Fold ( * backend Pool . ID , backend Pool ID ) && backend Pool . Backend IP Configurations != nil { for _ , ip Conf := range * backend Pool . Backend IP Configurations { if ip ip Configuration I Ds = append ( ip Configuration I Ds , * ip host Updates := make ( [ ] func ( ) error , 0 , len ( ip Configuration I for i := range ip Configuration I Ds { ip Configuration ID := ip Configuration I f := func ( ) error { if scale Set Name , err := extract Scale Set Name By Provider ID ( ip Configuration ID ) ; err == nil { // Only remove nodes belonging to specified vm Set to basic LB backends. if ! ss . use Standard Load Balancer ( ) && ! strings . Equal Fold ( scale Set Name , vm Set node Name , err := ss . get Node Name By IP Configuration ID ( ip Configuration if err != nil { if err == Error Not Vmss klog . Errorf ( " " , ip Configuration err = ss . ensure Backend Pool Deleted From Node ( service , node Name , backend Pool if err != nil { return fmt . Errorf ( " " , backend Pool ID , node host Updates = append ( host errs := utilerrors . Aggregate Goroutines ( host } 
func New Indexer Informer Watcher ( lw cache . Lister Watcher , obj done w := watch . New Proxy t := new indexer , informer := cache . New Indexer Informer ( lw , obj Type , 0 , cache . Resource Event Handler Funcs { Add Func : func ( obj interface { } ) { go t . Wait For Ticket ( t . Get Ticket ( ) , func ( ) { select { case ch <- watch . Event { Type : watch . Added , Object : obj . ( runtime . Object ) , } : case <- w . Stop } , Update Func : func ( old , new interface { } ) { go t . Wait For Ticket ( t . Get Ticket ( ) , func ( ) { select { case ch <- watch . Event { Type : watch . Modified , Object : new . ( runtime . Object ) , } : case <- w . Stop } , Delete Func : func ( obj interface { } ) { go t . Wait For Ticket ( t . Get Ticket ( ) , func ( ) { stale Obj , stale := obj . ( cache . Deleted Final State if stale { // We have no means of passing the additional information down using watch API based on watch.Event // but the caller can filter such objects by checking if metadata.deletion Timestamp is set obj = stale select { case ch <- watch . Event { Type : watch . Deleted , Object : obj . ( runtime . Object ) , } : case <- w . Stop go func ( ) { defer close ( done informer . Run ( w . Stop return indexer , informer , w , done } 
func ( e Empty Element ) Merge ( v Strategy ) ( Result , error ) { return v . Merge } 
func ( m * Manager Impl ) Start ( active Pods Active Pods Func , sources Ready config . Sources m . active Pods = active m . sources Ready = sources // Loads in allocated Devices information from disk. err := m . read socket os . Mkdir // Removes all stale sockets in m.socketdir. Device plugins can monitor // this and use it as a signal to re-register with the new Kubelet. if err := m . remove s , err := net . Listen ( " " , socket if err != nil { klog . Errorf ( err Listen m . server = grpc . New Server ( [ ] grpc . Server pluginapi . Register Registration klog . V ( 2 ) . Infof ( " " , socket } 
func ( m * Manager Impl ) Get Watcher Handler ( ) watcher . Plugin return watcher . Plugin } 
func ( m * Manager Impl ) Validate Plugin ( plugin Name string , endpoint string , versions [ ] string , found In Deprecated Dir bool ) error { klog . V ( 2 ) . Infof ( " " , plugin if ! m . is Version Compatible With if ! v1helper . Is Extended Resource Name ( v1 . Resource Name ( plugin Name ) ) { return fmt . Errorf ( " " , fmt . Sprintf ( err Invalid Resource Name , plugin } 
func ( m * Manager Impl ) Register Plugin ( plugin Name string , endpoint string , versions [ ] string ) error { klog . V ( 2 ) . Infof ( " " , plugin e , err := new Endpoint Impl ( endpoint , plugin options , err := e . client . Get Device Plugin m . register Endpoint ( plugin go m . run Endpoint ( plugin } 
func ( m * Manager Impl ) De Register Plugin ( plugin // Note: This will mark the resource unhealthy as per the behavior // in run Endpoint if e I , ok := m . endpoints [ plugin Name ] ; ok { e } 
func ( m * Manager Impl ) Allocate ( node * schedulernodeinfo . Node Info , attrs * lifecycle . Pod Admit err := m . allocate Pod // quick return if no plugin Resources requested if _ , pod Require Device Plugin Resource := m . pod Devices [ string ( pod . UID ) ] ; ! pod Require Device Plugin m . sanitize Node } 
func ( m * Manager Impl ) Register ( ctx context . Context , r * pluginapi . Register Request ) ( * pluginapi . Empty , error ) { klog . Infof ( " " , r . Resource metrics . Device Plugin Registration Count . With Label Values ( r . Resource metrics . Deprecated Device Plugin Registration Count . With Label Values ( r . Resource var version for _ , v := range pluginapi . Supported Versions { if r . Version == v { version if ! version Compatible { error String := fmt . Sprintf ( err Unsupported Version , r . Version , pluginapi . Supported klog . Infof ( " " , r . Resource Name , error return & pluginapi . Empty { } , fmt . Errorf ( error if ! v1helper . Is Extended Resource Name ( v1 . Resource Name ( r . Resource Name ) ) { error String := fmt . Sprintf ( err Invalid Resource Name , r . Resource klog . Infof ( " " , error return & pluginapi . Empty { } , fmt . Errorf ( error // TODO: for now, always accepts newest device plugin. Later may consider to // add some policies here, e.g., verify whether an old device plugin with the // same resource name is still alive to determine whether we want to accept // the new registration. go m . add } 
func ( m * Manager for _ , e I := range m . endpoints { e } 
func ( m * Manager Impl ) Get Capacity ( ) ( v1 . Resource List , v1 . Resource List , [ ] string ) { needs Update var capacity = v1 . Resource var allocatable = v1 . Resource deleted Resources := sets . New for resource Name , devices := range m . healthy Devices { e I , ok := m . endpoints [ resource if ( ok && e I . e . stop Grace Period Expired ( ) ) || ! ok { // The resources contained in endpoints and (un)healthy delete ( m . endpoints , resource delete ( m . healthy Devices , resource deleted Resources . Insert ( resource needs Update } else { capacity [ v1 . Resource Name ( resource Name ) ] = * resource . New Quantity ( int64 ( devices . Len ( ) ) , resource . Decimal allocatable [ v1 . Resource Name ( resource Name ) ] = * resource . New Quantity ( int64 ( devices . Len ( ) ) , resource . Decimal for resource Name , devices := range m . unhealthy Devices { e I , ok := m . endpoints [ resource if ( ok && e I . e . stop Grace Period delete ( m . endpoints , resource delete ( m . unhealthy Devices , resource deleted Resources . Insert ( resource needs Update } else { capacity Count := capacity [ v1 . Resource Name ( resource unhealthy Count := * resource . New Quantity ( int64 ( devices . Len ( ) ) , resource . Decimal capacity Count . Add ( unhealthy capacity [ v1 . Resource Name ( resource Name ) ] = capacity if needs Update Checkpoint { m . write return capacity , allocatable , deleted Resources . Unsorted } 
func ( m * Manager Impl ) write registered for resource , devices := range m . healthy Devices { registered Devs [ resource ] = devices . Unsorted data := checkpoint . New ( m . pod Devices . to Checkpoint Data ( ) , registered err := m . checkpoint Manager . Create Checkpoint ( kubelet Device Manager if err != nil { return fmt . Errorf ( " " , kubelet Device Manager } 
func ( m * Manager Impl ) read Checkpoint ( ) error { registered dev Entries := make ( [ ] checkpoint . Pod Devices cp := checkpoint . New ( dev Entries , registered err := m . checkpoint Manager . Get Checkpoint ( kubelet Device Manager if err != nil { if err == errors . Err Checkpoint Not Found { klog . Warningf ( " " , kubelet Device Manager pod Devices , registered Devs := cp . Get m . pod Devices . from Checkpoint Data ( pod m . allocated Devices = m . pod for resource := range registered Devs { // During start up, creates empty healthy Devices list so that the resource capacity // will stay zero till the corresponding device plugin re-registers. m . healthy Devices [ resource ] = sets . New m . unhealthy Devices [ resource ] = sets . New m . endpoints [ resource ] = endpoint Info { e : new Stopped Endpoint } 
func ( m * Manager Impl ) update Allocated Devices ( active Pods [ ] * v1 . Pod ) { if ! m . sources Ready . All active Pod Uids := sets . New for _ , pod := range active Pods { active Pod allocated Pod Uids := m . pod pods To Be Removed := allocated Pod Uids . Difference ( active Pod if len ( pods To Be klog . V ( 3 ) . Infof ( " " , pods To Be m . pod Devices . delete ( pods To Be // Regenerated allocated Devices after we update pod allocation information. m . allocated Devices = m . pod } 
func ( m * Manager Impl ) devices To Allocate ( pod UID , cont Name , resource string , required int , reusable // Gets list of devices that have already been allocated. // This can happen if a container restarts for example. devices := m . pod Devices . container Devices ( pod UID , cont if devices != nil { klog . V ( 3 ) . Infof ( " " , resource , cont Name , pod // A pod's resource is not expected to change once admitted by the API server, // so just fail loudly here. We can revisit this part if this no longer holds. if needed != 0 { return nil , fmt . Errorf ( " " , pod UID , cont klog . V ( 3 ) . Infof ( " " , needed , resource , pod UID , cont // Needs to allocate additional devices. if _ , ok := m . healthy devices = sets . New // Allocates from reusable Devices list first. for device := range reusable // Needs to allocate additional devices. if m . allocated Devices [ resource ] == nil { m . allocated Devices [ resource ] = sets . New // Gets Devices in use. devices In Use := m . allocated // Gets a list of available devices. available := m . healthy Devices [ resource ] . Difference ( devices In allocated := available . Unsorted // Updates m.allocated Devices with allocated devices to prevent them // from being allocated to other pods/containers, given that we are // not holding lock during the rpc call. for _ , device := range allocated { m . allocated } 
func ( m * Manager Impl ) allocate Container Resources ( pod * v1 . Pod , container * v1 . Container , devices To Reuse map [ string ] sets . String ) error { pod cont allocated Devices if ! m . is Device Plugin // Updates allocated Devices to garbage collect any stranded resources // before doing the device plugin allocation. if ! allocated Devices Updated { m . update Allocated Devices ( m . active allocated Devices alloc Devices , err := m . devices To Allocate ( pod UID , cont Name , resource , needed , devices To if alloc Devices == nil || len ( alloc start RPC // Manager.Allocate involves RPC calls to device plugin, which // could be heavy-weight. Therefore we want to perform this operation outside // mutex lock. Note if Allocate call fails, we may leave container resources // partially allocated for the failed container. We rely on update Allocated e m . allocated Devices = m . pod devs := alloc Devices . Unsorted // TODO: refactor this part of code to just append a Container Allocation Request // in a passed in Allocate resp , err := e metrics . Device Plugin Allocation Duration . With Label Values ( resource ) . Observe ( metrics . Since In Seconds ( start RPC metrics . Deprecated Device Plugin Allocation Latency . With Label Values ( resource ) . Observe ( metrics . Since In Microseconds ( start RPC if err != nil { // In case of allocation failure, we want to restore m.allocated Devices // to the actual allocated state from m.pod m . allocated Devices = m . pod if len ( resp . Container // Update internal cached pod m . pod Devices . insert ( pod UID , cont Name , resource , alloc Devices , resp . Container // Checkpoints device to container allocation information. return m . write } 
func ( m * Manager Impl ) Get Device Run Container Options ( pod * v1 . Pod , container * v1 . Container ) ( * Device Run Container Options , error ) { pod cont needs Re if ! m . is Device Plugin err := m . call Pre Start Container If Needed ( pod UID , cont // This is a device plugin resource yet we don't have cached // resource state. This is likely due to a race during node // restart. We re-issue allocate request to cover this race. if m . pod Devices . container Devices ( pod UID , cont Name , resource ) == nil { needs Re if needs Re Allocate { klog . V ( 2 ) . Infof ( " " , pod m . allocate Pod return m . pod Devices . device Run Container } 
func ( m * Manager Impl ) call Pre Start Container If Needed ( pod UID , cont e if e I . opts == nil || ! e I . opts . Pre Start devices := m . pod Devices . container Devices ( pod UID , cont return fmt . Errorf ( " " , pod UID , cont devs := devices . Unsorted klog . V ( 4 ) . Infof ( " " , cont Name , pod _ , err := e I . e . pre Start } 
func ( m * Manager Impl ) sanitize Node Allocatable ( node * schedulernodeinfo . Node Info ) { var new Allocatable allocatable Resource := node . Allocatable if allocatable Resource . Scalar Resources == nil { allocatable Resource . Scalar Resources = make ( map [ v1 . Resource for resource , devices := range m . allocated quant , ok := allocatable Resource . Scalar Resources [ v1 . Resource // Needs to update node Info.Allocatable Resource to make sure // Node Info.allocatable Resource at least equal to the capacity already allocated. if new Allocatable Resource == nil { new Allocatable Resource = allocatable new Allocatable Resource . Scalar Resources [ v1 . Resource if new Allocatable Resource != nil { node . Set Allocatable Resource ( new Allocatable } 
func ( m * Manager Impl ) Get Devices ( pod UID , container Name string ) [ ] * podresourcesapi . Container return m . pod Devices . get Container Devices ( pod UID , container } 
func New Namespace Keyed Indexer And Reflector ( lw Lister Watcher , expected Type interface { } , resync Period time . Duration ) ( indexer Indexer , reflector * Reflector ) { indexer = New Indexer ( Meta Namespace Key Func , Indexers { " " : Meta Namespace Index reflector = New Reflector ( lw , expected Type , indexer , resync } 
func New Named Reflector ( name string , lw Lister Watcher , expected Type interface { } , store Store , resync Period time . Duration ) * Reflector { r := & Reflector { name : name , lister Watcher : lw , store : store , expected Type : reflect . Type Of ( expected Type ) , period : time . Second , resync Period : resync Period , clock : & clock . Real } 
func ( r * Reflector ) Run ( stop Ch <- chan struct { } ) { klog . V ( 3 ) . Infof ( " " , r . expected Type , r . resync wait . Until ( func ( ) { if err := r . List And Watch ( stop Ch ) ; err != nil { utilruntime . Handle } , r . period , stop } 
func ( r * Reflector ) resync Chan ( ) ( <- chan time . Time , func ( ) bool ) { if r . resync Period == 0 { return never Exit // The cleanup function is required: imagine the scenario where watches // always fail so we end up listing frequently. Then, if we don't // manually stop the timer, we could end up with many timers active // concurrently. t := r . clock . New Timer ( r . resync } 
func ( r * Reflector ) List And Watch ( stop Ch <- chan struct { } ) error { klog . V ( 3 ) . Infof ( " " , r . expected var resource // Explicitly set "0" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1 . List Options { Resource if err := func ( ) error { init defer init Trace . Log If list panic go func ( ) { defer func ( ) { if r := recover ( ) ; r != nil { panic // Attempt to gather list in chunks, if supported by lister Watcher, if not, the first // list request will return the full response. pager := pager . New ( pager . Simple Page Func ( func ( opts metav1 . List Options ) ( runtime . Object , error ) { return r . lister if r . Watch List Page Size != 0 { pager . Page Size = r . Watch List Page close ( list select { case <- stop case r := <- panic case <- list if err != nil { return fmt . Errorf ( " " , r . name , r . expected init list Meta Interface , err := meta . List resource Version = list Meta Interface . Get Resource init items , err := meta . Extract init if err := r . sync With ( items , resource init r . set Last Sync Resource Version ( resource init cancel defer close ( cancel go func ( ) { resync Ch , cleanup := r . resync for { select { case <- resync Ch : case <- stop case <- cancel if r . Should Resync == nil || r . Should resync Ch , cleanup = r . resync for { // give the stop Ch a chance to stop the loop, even in case of continue statements further down on errors select { case <- stop timeout Seconds := int64 ( min Watch options = metav1 . List Options { Resource Version : resource Version , // We want to avoid situations of hanging watchers. Stop any wachers that do not // receive any events within the timeout window. Timeout Seconds : & timeout Seconds , // To reduce load on kube-apiserver on watch restarts, you may enable watch bookmarks. // Reflector doesn't assume bookmarks are returned at all (if the server do not support // watch bookmarks, it will ignore this field). // Disabled in Alpha release of watch bookmarks feature. Allow Watch w , err := r . lister if err != nil { switch err { case io . EOF : // watch closed normally case io . Err Unexpected EOF : klog . V ( 1 ) . Infof ( " " , r . name , r . expected default : utilruntime . Handle Error ( fmt . Errorf ( " " , r . name , r . expected // If this is "connection refused" error, it means that most likely apiserver is not responsive. // It doesn't make sense to re-list all objects because most likely we will be able to restart // watch where we ended. // If that's the case wait and resend watch request. if url Error , ok := err . ( * url . Error ) ; ok { if op Error , ok := url Error . Err . ( * net . Op Error ) ; ok { if errno , ok := op if err := r . watch Handler ( w , & resource Version , resyncerrc , stop Ch ) ; err != nil { if err != error Stop Requested { klog . Warningf ( " " , r . name , r . expected } 
func ( r * Reflector ) sync With ( items [ ] runtime . Object , resource return r . store . Replace ( found , resource } 
func ( r * Reflector ) watch Handler ( w watch . Interface , resource Version * string , errc chan error , stop event loop : for { select { case <- stop Ch : return error Stop case event , ok := <- w . Result if event . Type == watch . Error { return apierrs . From if e , a := r . expected Type , reflect . Type Of ( event . Object ) ; e != nil && e != a { utilruntime . Handle if err != nil { utilruntime . Handle new Resource Version := meta . Get Resource if err != nil { utilruntime . Handle if err != nil { utilruntime . Handle if err != nil { utilruntime . Handle case watch . Bookmark : // A `Bookmark` means watch has synced here, just update the resource Version default : utilruntime . Handle * resource Version = new Resource r . set Last Sync Resource Version ( new Resource event watch if watch Duration < 1 * time . Second && event klog . V ( 4 ) . Infof ( " " , r . name , r . expected Type , event } 
func ( r * Reflector ) Last Sync Resource Version ( ) string { r . last Sync Resource Version Mutex . R defer r . last Sync Resource Version Mutex . R return r . last Sync Resource } 
func Hook Client Config For Sink ( a * v1alpha1 . Audit Sink ) webhook . Client Config { c := a . Spec . Webhook . Client ret := webhook . Client Config { Name : a . Name , CA Bundle : c . CA if c . Service != nil { ret . Service = & webhook . Client Config } 
func New Round Tripper ( tls Config * tls . Config , follow Redirects , require Same Host Redirects bool ) httpstream . Upgrade Round Tripper { return New Spdy Round Tripper ( tls Config , follow Redirects , require Same Host } 
func New Spdy Round Tripper ( tls Config * tls . Config , follow Redirects , require Same Host Redirects bool ) * Spdy Round Tripper { return & Spdy Round Tripper { tls Config : tls Config , follow Redirects : follow Redirects , require Same Host Redirects : require Same Host } 
func ( s * Spdy Round } 
func ( s * Spdy Round if proxier == nil { proxier = utilnet . New Proxier With No Proxy CIDR ( http . Proxy From proxy if proxy URL == nil { return s . dial Without // ensure we use a canonical host with proxy Req target Host := netutil . Canonical // proxying logic adapted from http://blog.h6t.eu/post/74098062923/golang-websocket-with-http-proxy-support proxy Req := http . Request { Method : " " , URL : & url . URL { } , Host : target if pa := s . proxy Auth ( proxy URL ) ; pa != " " { proxy proxy proxy Dial Conn , err := s . dial Without Proxy ( req . Context ( ) , proxy proxy Client Conn := httputil . New Proxy Client Conn ( proxy Dial _ , err = proxy Client Conn . Do ( & proxy if err != nil && err != httputil . Err Persist rwc , _ := proxy Client host , _ , err := net . Split Host Port ( target tls Config := s . tls switch { case tls Config == nil : tls Config = & tls . Config { Server case len ( tls Config . Server Name ) == 0 : tls Config = tls tls Config . Server tls Conn := tls . Client ( rwc , tls // need to manually call Handshake() so we can call Verify Hostname() below if err := tls // Return if we were configured to skip validation if tls Config . Insecure Skip Verify { return tls if err := tls Conn . Verify Hostname ( tls Config . Server return tls } 
func ( s * Spdy Round Tripper ) dial Without Proxy ( ctx context . Context , url * url . URL ) ( net . Conn , error ) { dial Addr := netutil . Canonical return d . Dial Context ( ctx , " " , dial } else { return s . Dialer . Dial Context ( ctx , " " , dial // TODO validate the TLS Client if s . Dialer == nil { conn , err = tls . Dial ( " " , dial Addr , s . tls } else { conn , err = tls . Dial With Dialer ( s . Dialer , " " , dial Addr , s . tls // Return if we were configured to skip validation if s . tls Config != nil && s . tls Config . Insecure Skip host , _ , err := net . Split Host Port ( dial if s . tls Config != nil && len ( s . tls Config . Server Name ) > 0 { host = s . tls Config . Server err = conn . Verify } 
func ( s * Spdy Round Tripper ) proxy Auth ( proxy URL * url . URL ) string { if proxy URL == nil || proxy credentials := proxy encoded Auth := base64 . Std Encoding . Encode To return fmt . Sprintf ( " " , encoded } 
func ( s * Spdy Round Tripper ) Round Trip ( req * http . Request ) ( * http . Response , error ) { header := utilnet . Clone header . Add ( httpstream . Header Connection , httpstream . Header header . Add ( httpstream . Header Upgrade , Header raw if s . follow Redirects { conn , raw Response , err = utilnet . Connect With Redirects ( req . Method , req . URL , header , req . Body , s , s . require Same Host } else { clone := utilnet . Clone response Reader := bufio . New Reader ( io . Multi Reader ( bytes . New Buffer ( raw resp , err := http . Read Response ( response } 
func ( s * Spdy Round Tripper ) New Connection ( resp * http . Response ) ( httpstream . Connection , error ) { connection Header := strings . To Lower ( resp . Header . Get ( httpstream . Header upgrade Header := strings . To Lower ( resp . Header . Get ( httpstream . Header if ( resp . Status Code != http . Status Switching Protocols ) || ! strings . Contains ( connection Header , strings . To Lower ( httpstream . Header Upgrade ) ) || ! strings . Contains ( upgrade Header , strings . To Lower ( Header response response Error Bytes , err := ioutil . Read if err != nil { response } else { // TODO: I don't belong here, I should be abstracted from this class if obj , _ , err := status Codecs . Universal Decoder ( ) . Decode ( response Error Bytes , nil , & metav1 . Status { } ) ; err == nil { if status , ok := obj . ( * metav1 . Status ) ; ok { return nil , & apierrors . Status Error { Err response Error = string ( response Error response Error = strings . Trim Space ( response return nil , fmt . Errorf ( " " , response return New Client } 
func Get Validated for _ , source := range sources { switch source { case All Source : return [ ] string { File Source , HTTP Source , Apiserver case File Source , HTTP Source , Apiserver } 
func Get Pod Source ( pod * v1 . Pod ) ( string , error ) { if pod . Annotations != nil { if source , ok := pod . Annotations [ Config Source Annotation } 
func Is Critical Pod ( pod * v1 . Pod ) bool { if utilfeature . Default Feature Gate . Enabled ( features . Pod Priority ) { if pod . Spec . Priority != nil && Is Critical Pod Based On if utilfeature . Default Feature Gate . Enabled ( features . Experimental Critical Pod Annotation ) { if Is } 
func Preemptable ( preemptor , preemptee * v1 . Pod ) bool { if Is Critical Pod ( preemptor ) && ! Is Critical if utilfeature . Default Feature Gate . Enabled ( features . Pod } 
func Is Critical ( ns string , annotations map [ string ] string ) bool { // Critical pods are restricted to "kube-system" namespace as of now. if ns != kubeapi . Namespace val , ok := annotations [ Critical Pod Annotation } 
func New Get Options ( parent string , streams genericclioptions . IO Streams ) * Get Options { return & Get Options { Print Flags : New Get Print Flags ( ) , Cmd Parent : parent , IO Streams : streams , Chunk Size : 500 , Server } 
func New Cmd Get ( parent string , f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := New Get cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : get Long + " \n \n " + cmdutil . Suggest API Resources ( parent ) , Example : get Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check } , Suggest o . Print Flags . Add cmd . Flags ( ) . String cmd . Flags ( ) . Bool Var cmd . Flags ( ) . Bool Var ( & o . Watch Only , " " , o . Watch cmd . Flags ( ) . Int64Var ( & o . Chunk Size , " " , o . Chunk cmd . Flags ( ) . Bool Var ( & o . Ignore Not Found , " " , o . Ignore Not cmd . Flags ( ) . String Var P ( & o . Label Selector , " " , " " , o . Label cmd . Flags ( ) . String Var ( & o . Field Selector , " " , o . Field cmd . Flags ( ) . Bool Var P ( & o . All Namespaces , " " , " " , o . All cmdutil . Add Include Uninitialized add Open API Print Column add Server Print Column cmd . Flags ( ) . Bool cmd . Flags ( ) . Mark cmdutil . Add Filename Option Flags ( cmd , & o . Filename } 
func ( o * Get o . Namespace , o . Explicit Namespace , err = f . To Raw Kube Config if o . All Namespaces { o . Explicit sort By , err := cmd . Flags ( ) . Get o . Sort = len ( sort o . No Headers = cmdutil . Get Flag // TODO (soltysh): currently we don't support custom columns // with server side print. So in these cases force the old behavior. output if output Option == " " { o . Server template if o . Print Flags . Template Flags != nil && o . Print Flags . Template Flags . Template Argument != nil { template Arg = * o . Print Flags . Template Flags . Template // human readable printers have special conversion rules, so we determine if we're using one. if ( len ( * o . Print Flags . Output Format ) == 0 && len ( template Arg ) == 0 ) || * o . Print Flags . Output Format == " " { o . Is Human Readable o . To Printer = func ( mapping * meta . REST Mapping , with Namespace bool , with Kind bool ) ( printers . Resource Printer Func , error ) { // make a new copy of current flags / opts before mutating print Flags := o . Print if mapping != nil { if ! cmd Specifies Output Fmt ( cmd ) && o . Print With Open API Cols { if api Schema , err := f . Open API Schema ( ) ; err == nil { print Flags . Use Open API Columns ( api print Flags . Set Kind ( mapping . Group Version Kind . Group if with Namespace { print Flags . Ensure With if with Kind { print Flags . Ensure With printer , err := print Flags . To if o . Sort { printer = & Sorting Printer { Delegate : printer , Sort Field : sort if o . Server Print { printer = & Table return printer . Print switch { case o . Watch || o . Watch Only : if o . Sort { fmt . Fprintf ( o . IO Streams . Err default : if len ( args ) == 0 && cmdutil . Is Filename Slice Empty ( o . Filenames , o . Kustomize ) { fmt . Fprintf ( o . Err Out , " \n \n " , cmdutil . Suggest API Resources ( o . Cmd full Cmd Name := cmd . Parent ( ) . Command usage if len ( full Cmd Name ) > 0 && cmdutil . Is Sibling Command Exists ( cmd , " " ) { usage String = fmt . Sprintf ( " \n \" \" " , usage String , full Cmd return cmdutil . Usage Errorf ( cmd , usage // openapi printing is mutually exclusive with server side printing if o . Print With Open API Cols && o . Server Print { fmt . Fprintf ( o . IO Streams . Err Out , " \n " , use Open API Print Column Flag Label , use Server Print } 
func ( o * Get Options ) Validate ( cmd * cobra . Command ) error { if len ( o . Raw ) > 0 { if o . Watch || o . Watch Only || len ( o . Label if len ( cmdutil . Get Flag String ( cmd , " " ) ) > 0 { return cmdutil . Usage if _ , err := url . Parse Request URI ( o . Raw ) ; err != nil { return cmdutil . Usage if cmdutil . Get Flag Bool ( cmd , " " ) { output if output Option != " " && output Option != " " { return fmt . Errorf ( " " , output } 
func ( r * Runtime if len ( r . objects ) == 1 { _ , is if ! is includes includes Runtime for _ , obj := range r . objects { switch t := obj . ( type ) { case * metav1beta1 . Table : includes if sorter , err := New Table default : includes Runtime // we use a Nop Positioner when dealing with Table objects // because the objects themselves are not swapped, but rather // the rows in each object are swapped / sorted. r . positioner = & Nop if includes Runtime Objs && includes if includes if r . positioner , err = Sort } 
func ( r * Runtime Sorter ) Original return r . positioner . Original } 
func ( r * Runtime Sorter ) With Decoder ( decoder runtime . Decoder ) * Runtime } 
func New Runtime Sorter ( objects [ ] runtime . Object , sort By string ) * Runtime Sorter { parsed Field , err := Relaxed JSON Path Expression ( sort if err != nil { parsed Field = sort return & Runtime Sorter { field : parsed Field , decoder : legacyscheme . Codecs . Universal } 
func ( o * Get if o . Watch || o . Watch chunk Size := o . Chunk if o . Sort { // TODO(juanvallejo): in the future, we could have the client use chunking // to gather all results, then sort them all at the end to reduce server load. chunk r := f . New Builder ( ) . Unstructured ( ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . All Namespaces ( o . All Namespaces ) . Filename Param ( o . Explicit Namespace , & o . Filename Options ) . Label Selector Param ( o . Label Selector ) . Field Selector Param ( o . Field Selector ) . Export Param ( o . Export ) . Request Chunks Of ( chunk Size ) . Resource Type Or Name Args ( true , args ... ) . Continue On Error ( ) . Latest ( ) . Flatten ( ) . Transform Requests ( o . transform if o . Ignore Not Found { r . Ignore Errors ( kapierrors . Is Not if ! o . Is Human Readable Printer { return o . print all errs := sets . New if err != nil { all Errs = append ( all print With Kind := multiple GV Ks sorting , err := cmd . Flags ( ) . Get var positioner Original if o . Sort { sorter := New Runtime var printer printers . Resource var last Mapping * meta . REST // track if we write any output tracking Writer := & tracking Writer w := utilprinters . Get New Tab Writer ( tracking for ix := range objs { var mapping * meta . REST if positioner != nil { info = infos [ positioner . Original print With Namespace := o . All if mapping != nil && mapping . Scope . Name ( ) == meta . REST Scope Name Root { print With if should Get New Printer For Mapping ( printer , last w . Set Remembered // TODO: this doesn't belong here // add linebreak between resource groups (if there is more than one) // skip linebreak above first resource group if last Mapping != nil && ! o . No Headers { fmt . Fprintln ( o . Err printer , err = o . To Printer ( mapping , print With Namespace , print With all Errs = append ( all last // ensure a versioned object is passed to the custom-columns printer // if we are using Open API columns to print if o . Print With Open API Cols { printer . Print internal Obj , err := legacyscheme . Scheme . Convert To Version ( info . Object , info . Mapping . Group Version Kind . Group Kind ( ) . With Version ( runtime . API Version Internal ) . Group printer . Print } else { printer . Print Obj ( internal if tracking Writer . Written == 0 && ! o . Ignore Not Found && len ( all Errs ) == 0 { // if we wrote no output, and had no errors, and are not ignoring Not Found, be sure we output something fmt . Fprintln ( o . Err return utilerrors . New Aggregate ( all } 
func ( o * Get Options ) raw ( f cmdutil . Factory ) error { rest Client , err := f . REST stream , err := rest Client . Get ( ) . Request } 
func ( o * Get Options ) watch ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { r := f . New Builder ( ) . Unstructured ( ) . Namespace Param ( o . Namespace ) . Default Namespace ( ) . All Namespaces ( o . All Namespaces ) . Filename Param ( o . Explicit Namespace , & o . Filename Options ) . Label Selector Param ( o . Label Selector ) . Field Selector Param ( o . Field Selector ) . Export Param ( o . Export ) . Request Chunks Of ( o . Chunk Size ) . Resource Type Or Name Args ( true , args ... ) . Single Resource Type ( ) . Latest ( ) . Transform Requests ( o . transform if multiple GV Ks mapping := info . Resource printer , err := o . To Printer ( mapping , o . All // watching from resource is List := meta . Is List if is List { // the resource Version of list objects is ~now but won't return // an initial watch event rv , err = meta . New Accessor ( ) . Resource writer := utilprinters . Get New Tab table GK := metainternal . Scheme Group Version . With Kind ( " " ) . Group // print the current object if ! o . Watch Only { var objs To if is List { objs To Print , _ = meta . Extract } else { objs To Print = append ( objs To for _ , obj To Print := range objs To Print { if o . Is Human Readable Printer && obj To Print . Get Object Kind ( ) . Group Version Kind ( ) . Group Kind ( ) != table GK { // printing anything other than tables always takes the internal version, but the watch event uses externals internal GV := mapping . Group Version Kind . Group Kind ( ) . With Version ( runtime . API Version Internal ) . Group obj To Print = attempt To Convert To Internal ( obj To Print , legacyscheme . Scheme , internal if err := printer . Print Obj ( obj To ctx , cancel := context . With intr . Run ( func ( ) error { _ , err := watchtools . Until Without Retry ( ctx , w , func ( e watch . Event ) ( bool , error ) { if ! is // printing always takes the internal version, but the watch event uses externals // TODO fix printing to use server-side or be version agnostic obj To if o . Is Human Readable Printer && obj To Print . Get Object Kind ( ) . Group Version Kind ( ) . Group Kind ( ) != table GK { internal GV := mapping . Group Version Kind . Group Kind ( ) . With Version ( runtime . API Version Internal ) . Group obj To Print = attempt To Convert To Internal ( e . Object , legacyscheme . Scheme , internal if err := printer . Print Obj ( obj To } 
func attempt To Convert To Internal ( obj runtime . Object , converter runtime . Object Convertor , target Version schema . Group Version ) runtime . Object { internal Object , err := converter . Convert To Version ( obj , target if err != nil { klog . V ( 1 ) . Infof ( " " , obj , target return internal } 
func merge Specs ( static Spec * spec . Swagger , crd Specs ... * spec . Swagger ) * spec . Swagger { // create shallow copy of static Spec, but replace paths and definitions because we modify them. spec To Return := * static if static Spec . Definitions != nil { spec To for k , s := range static Spec . Definitions { spec To if static Spec . Paths != nil { spec To Return . Paths = & spec . Paths { Paths : map [ string ] spec . Path for k , p := range static Spec . Paths . Paths { spec To for _ , s := range crd Specs { merge Spec ( & spec To return & spec To } 
func merge for k , v := range source . Paths . Paths { if dest . Paths . Paths == nil { dest . Paths . Paths = map [ string ] spec . Path } 
func ( t * aws Elastic Block Store CSI Translator ) Translate In Tree Storage Class Parameters To CSI ( sc Parameters map [ string ] string ) ( map [ string ] string , error ) { return sc } 
func ( t * aws Elastic Block Store CSI Translator ) Translate In Tree PV To CSI ( pv * v1 . Persistent Volume ) ( * v1 . Persistent Volume , error ) { if pv == nil || pv . Spec . AWS Elastic Block ebs Source := pv . Spec . AWS Elastic Block volume Handle , err := Kubernetes Volume ID To EBS Volume ID ( ebs Source . Volume csi Source := & v1 . CSI Persistent Volume Source { Driver : AWSEBS Driver Name , Volume Handle : volume Handle , Read Only : ebs Source . Read Only , FS Type : ebs Source . FS Type , Volume Attributes : map [ string ] string { " " : strconv . Format Int ( int64 ( ebs pv . Spec . AWS Elastic Block pv . Spec . CSI = csi } 
func ( t * aws Elastic Block Store CSI Translator ) Translate CSIPV To In Tree ( pv * v1 . Persistent Volume ) ( * v1 . Persistent csi ebs Source := & v1 . AWS Elastic Block Store Volume Source { Volume ID : csi Source . Volume Handle , FS Type : csi Source . FS Type , Read Only : csi Source . Read if partition , ok := csi Source . Volume Attributes [ " " ] ; ok { part ebs Source . Partition = int32 ( part pv . Spec . AWS Elastic Block Store = ebs } 
func ( t * aws Elastic Block Store CSI Translator ) Can Support ( pv * v1 . Persistent Volume ) bool { return pv != nil && pv . Spec . AWS Elastic Block } 
func Kubernetes Volume ID To EBS Volume ID ( kubernetes ID string ) ( string , error ) { // name looks like aws://availability-zone/aws Volume Id // The original idea of the URL-style name was to put the AZ into the // host, so we could find the AZ immediately from the name without // querying the API. But it turns out we don't actually need it for // multi-AZ clusters, as we put the AZ into the labels on the PV instead. // However, if in future we want to support multi-AZ cluster // volume-awareness without using Persistent Volumes, we likely will // want the AZ in the host. if ! strings . Has Prefix ( kubernetes ID , " " ) { // Assume a bare aws volume id (vol-1234...) return kubernetes url , err := url . Parse ( kubernetes if err != nil { // TODO: Maybe we should pass a URL into the Volume functions return " " , fmt . Errorf ( " " , kubernetes if url . Scheme != " " { return " " , fmt . Errorf ( " " , kubernetes aws aws ID = strings . Trim ( aws // We sanity check the resulting volume; the two known formats are // vol-12345678 and vol-12345678abcdef01 if ! aws Volume Reg Match . Match String ( aws ID ) { return " " , fmt . Errorf ( " " , kubernetes return aws } 
func ( priority Class Strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { pc := obj . ( * scheduling . Priority } 
func ( priority Class Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { _ = obj . ( * scheduling . Priority _ = old . ( * scheduling . Priority } 
func ( priority Class Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { pc := obj . ( * scheduling . Priority return validation . Validate Priority } 
func ( priority Class Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return validation . Validate Priority Class Update ( obj . ( * scheduling . Priority Class ) , old . ( * scheduling . Priority } 
func ( c * volume Attachments ) Create ( volume Attachment * v1alpha1 . Volume Attachment ) ( result * v1alpha1 . Volume Attachment , err error ) { result = & v1alpha1 . Volume err = c . client . Post ( ) . Resource ( " " ) . Body ( volume } 
func ( c * volume Attachments ) Update ( volume Attachment * v1alpha1 . Volume Attachment ) ( result * v1alpha1 . Volume Attachment , err error ) { result = & v1alpha1 . Volume err = c . client . Put ( ) . Resource ( " " ) . Name ( volume Attachment . Name ) . Body ( volume } 
func ( r * Resource Allocation Priority ) Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { node := node if node == nil { return schedulerapi . Host allocatable := node Info . Allocatable if priority Meta , ok := meta . ( * priority Metadata ) ; ok { requested = * priority Meta . non Zero } else { // We couldn't parse metadata - fallback to computing it. requested = * get Non Zero requested . Milli CPU += node Info . Non Zero Request ( ) . Milli requested . Memory += node Info . Non Zero // Check if the pod has volumes and this could be added to scorer function for balanced resource allocation. if len ( pod . Spec . Volumes ) >= 0 && utilfeature . Default Feature Gate . Enabled ( features . Balance Attached Node Volumes ) && node Info . Transient Info != nil { score = r . scorer ( & requested , & allocatable , true , node Info . Transient Info . Trans Node Info . Requested Volumes , node Info . Transient Info . Trans Node Info . Allocatable Volumes if klog . V ( 10 ) { if len ( pod . Spec . Volumes ) >= 0 && utilfeature . Default Feature Gate . Enabled ( features . Balance Attached Node Volumes ) && node Info . Transient Info != nil { klog . Infof ( " " , pod . Name , node . Name , r . Name , allocatable . Milli CPU , allocatable . Memory , node Info . Transient Info . Trans Node Info . Allocatable Volumes Count , requested . Milli CPU , requested . Memory , node Info . Transient Info . Trans Node Info . Requested } else { klog . Infof ( " " , pod . Name , node . Name , r . Name , allocatable . Milli CPU , allocatable . Memory , requested . Milli return schedulerapi . Host } 
func Config ( ) stats . Resource Metrics Config { return stats . Resource Metrics Config { Node Metrics : [ ] stats . Node Resource Metric { { Name : " " , Description : " " , Value Fn : func ( s summary . Node v := float64 ( * s . CPU . Usage Core Nano } , } , { Name : " " , Description : " " , Value Fn : func ( s summary . Node v := float64 ( * s . Memory . Working Set } , } , } , Container Metrics : [ ] stats . Container Resource Metric { { Name : " " , Description : " " , Value Fn : func ( s summary . Container v := float64 ( * s . CPU . Usage Core Nano } , } , { Name : " " , Description : " " , Value Fn : func ( s summary . Container v := float64 ( * s . Memory . Working Set } 
func New Autoscale Options ( io Streams genericclioptions . IO Streams ) * Autoscale Options { return & Autoscale Options { Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , Filename Options : & resource . Filename Options { } , Record Flags : genericclioptions . New Record Flags ( ) , Recorder : genericclioptions . Noop Recorder { } , IO Streams : io } 
func New Cmd Autoscale ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { o := New Autoscale Options ( io valid cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : autoscale Long , Example : autoscale Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check } , Valid Args : valid // bind flag structs o . Record Flags . Add o . Print Flags . Add cmd . Flags ( ) . String Var ( & o . Generator , " " , generateversioned . Horizontal Pod Autoscaler V1Generator cmd . Mark Flag cmd . Flags ( ) . Int32Var ( & o . CPU cmd . Flags ( ) . String cmdutil . Add Dry Run cmdutil . Add Filename Option Flags ( cmd , o . Filename cmdutil . Add Apply Annotation } 
func ( o * Autoscale o . dry Run = cmdutil . Get Flag o . create Annotation = cmdutil . Get Flag Bool ( cmd , cmdutil . Apply Annotations o . builder = f . New discovery Client , err := f . To Discovery o . scale Kind Resolver = scale . New Discovery Scale Kind Resolver ( discovery o . Record o . Recorder , err = o . Record Flags . To kube Client , err := f . Kubernetes Client o . HPA Client = kube Client . Autoscaling // get the generator o . generator Func = func ( name string , mapping * meta . REST Mapping ) ( generate . Structured Generator , error ) { switch o . Generator { case generateversioned . Horizontal Pod Autoscaler V1Generator Name : return & generateversioned . Horizontal Pod Autoscaler Generator V1 { Name : name , Min Replicas : o . Min , Max Replicas : o . Max , CPU Percent : o . CPU Percent , Scale Ref Name : name , Scale Ref Kind : mapping . Group Version Kind . Kind , Scale Ref API Version : mapping . Group Version Kind . Group default : return nil , cmdutil . Usage o . namespace , o . enforce Namespace , err = f . To Raw Kube Config o . To Printer = func ( operation string ) ( printers . Resource Printer , error ) { o . Print Flags . Name Print if o . dry Run { o . Print return o . Print Flags . To } 
func ( o * Autoscale } 
func ( o * Autoscale Options ) Run ( ) error { r := o . builder . Unstructured ( ) . Continue On Error ( ) . Namespace Param ( o . namespace ) . Default Namespace ( ) . Filename Param ( o . enforce Namespace , o . Filename Options ) . Resource Type Or Name mapping := info . Resource gvr := mapping . Group Version Kind . Group Version ( ) . With if _ , err := o . scale Kind Resolver . Scale For Resource ( gvr ) ; err != nil { return fmt . Errorf ( " " , mapping . Group Version generator , err := o . generator // Generate new object object , err := generator . Structured hpa , ok := object . ( * autoscalingv1 . Horizontal Pod if o . dry printer , err := o . To return printer . Print if err := kubectl . Create Or Update Annotation ( o . create Annotation , hpa , scheme . Default JSON actual HPA , err := o . HPA Client . Horizontal Pod printer , err := o . To return printer . Print Obj ( actual } 
func ( c * Autoscaling V2beta1Client ) REST return c . rest } 
func ( ca * Custom Args ) Group Version Packages ( ) map [ types . Group Version ] string { res := map [ types . Group for _ , pkg := range ca . Groups { for _ , v := range pkg . Versions { res [ types . Group } 
func ( o * Garbage Collector Controller Options ) Add Flags ( fs * pflag . Flag fs . Int32Var ( & o . Concurrent GC Syncs , " " , o . Concurrent GC fs . Bool Var ( & o . Enable Garbage Collector , " " , o . Enable Garbage } 
func ( o * Garbage Collector Controller Options ) Apply To ( cfg * garbagecollectorconfig . Garbage Collector Controller cfg . Concurrent GC Syncs = o . Concurrent GC cfg . GC Ignored Resources = o . GC Ignored cfg . Enable Garbage Collector = o . Enable Garbage } 
func ( o * Garbage Collector Controller } 
func Diskformat Valid for diskformat := range Disk Format Valid validopts = strings . Trim } 
func Check Disk Format Supported ( disk Format string ) bool { if Disk Format Valid Type [ disk Format ] == " " { klog . Errorf ( " " , Diskformat Valid } 
func SCSI Controller Type Valid for _ , controller Type := range SCSI Controller Valid Type { validopts += ( controller validopts = strings . Trim } 
func Check Controller Supported ( ctrl Type string ) bool { for _ , c := range SCSI Controller Valid Type { if ctrl klog . Errorf ( " " , SCSI Controller Type Valid } 
func ( volume Options Volume Options ) Verify Volume Options ( ) bool { // Validate only if SCSI Controller Type is set by user. // Default value is set later in virtual Disk Manager.Create and vm Disk Manager.Create if volume Options . SCSI Controller Type != " " { is Valid := Check Controller Supported ( volume Options . SCSI Controller if ! is // Thin Disk Type is the default, so skip the validation. if volume Options . Disk Format != Thin Disk Type { is Valid := Check Disk Format Supported ( volume Options . Disk if ! is } 
func ( s Role Binding Generator V1 ) Structured role Binding := & rbacv1 . Role role switch { case len ( s . Role ) > 0 : role Binding . Role Ref = rbacv1 . Role Ref { API Group : rbacv1 . Group case len ( s . Cluster Role ) > 0 : role Binding . Role Ref = rbacv1 . Role Ref { API Group : rbacv1 . Group Name , Kind : " " , Name : s . Cluster for _ , user := range sets . New String ( s . Users ... ) . List ( ) { role Binding . Subjects = append ( role Binding . Subjects , rbacv1 . Subject { Kind : rbacv1 . User Kind , API Group : rbacv1 . Group for _ , group := range sets . New String ( s . Groups ... ) . List ( ) { role Binding . Subjects = append ( role Binding . Subjects , rbacv1 . Subject { Kind : rbacv1 . Group Kind , API Group : rbacv1 . Group for _ , sa := range sets . New String ( s . Service role Binding . Subjects = append ( role Binding . Subjects , rbacv1 . Subject { Kind : rbacv1 . Service Account Kind , API return role } 
func ( s Role Binding Generator if ( len ( s . Cluster } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Exec Credential ) ( nil ) , ( * clientauthentication . Exec Credential ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Exec Credential_To_clientauthentication_Exec Credential ( a . ( * Exec Credential ) , b . ( * clientauthentication . Exec if err := s . Add Generated Conversion Func ( ( * clientauthentication . Exec Credential ) ( nil ) , ( * Exec Credential ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential_To_v1beta1_Exec Credential ( a . ( * clientauthentication . Exec Credential ) , b . ( * Exec if err := s . Add Generated Conversion Func ( ( * Exec Credential Spec ) ( nil ) , ( * clientauthentication . Exec Credential Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Exec Credential Spec_To_clientauthentication_Exec Credential Spec ( a . ( * Exec Credential Spec ) , b . ( * clientauthentication . Exec Credential if err := s . Add Generated Conversion Func ( ( * clientauthentication . Exec Credential Spec ) ( nil ) , ( * Exec Credential Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential Spec_To_v1beta1_Exec Credential Spec ( a . ( * clientauthentication . Exec Credential Spec ) , b . ( * Exec Credential if err := s . Add Generated Conversion Func ( ( * Exec Credential Status ) ( nil ) , ( * clientauthentication . Exec Credential Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Exec Credential Status_To_clientauthentication_Exec Credential Status ( a . ( * Exec Credential Status ) , b . ( * clientauthentication . Exec Credential if err := s . Add Generated Conversion Func ( ( * clientauthentication . Exec Credential Status ) ( nil ) , ( * Exec Credential Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential Status_To_v1beta1_Exec Credential Status ( a . ( * clientauthentication . Exec Credential Status ) , b . ( * Exec Credential if err := s . Add Conversion Func ( ( * clientauthentication . Exec Credential Spec ) ( nil ) , ( * Exec Credential Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential Spec_To_v1beta1_Exec Credential Spec ( a . ( * clientauthentication . Exec Credential Spec ) , b . ( * Exec Credential } 
func Convert_v1beta1_Exec Credential_To_clientauthentication_Exec Credential ( in * Exec Credential , out * clientauthentication . Exec Credential , s conversion . Scope ) error { return auto Convert_v1beta1_Exec Credential_To_clientauthentication_Exec } 
func Convert_clientauthentication_Exec Credential_To_v1beta1_Exec Credential ( in * clientauthentication . Exec Credential , out * Exec Credential , s conversion . Scope ) error { return auto Convert_clientauthentication_Exec Credential_To_v1beta1_Exec } 
func Convert_v1beta1_Exec Credential Spec_To_clientauthentication_Exec Credential Spec ( in * Exec Credential Spec , out * clientauthentication . Exec Credential Spec , s conversion . Scope ) error { return auto Convert_v1beta1_Exec Credential Spec_To_clientauthentication_Exec Credential } 
func Convert_v1beta1_Exec Credential Status_To_clientauthentication_Exec Credential Status ( in * Exec Credential Status , out * clientauthentication . Exec Credential Status , s conversion . Scope ) error { return auto Convert_v1beta1_Exec Credential Status_To_clientauthentication_Exec Credential } 
func Convert_clientauthentication_Exec Credential Status_To_v1beta1_Exec Credential Status ( in * clientauthentication . Exec Credential Status , out * Exec Credential Status , s conversion . Scope ) error { return auto Convert_clientauthentication_Exec Credential Status_To_v1beta1_Exec Credential } 
func validate PSP Allowed Host Paths ( fld Path * field . Path , allowed Host Paths [ ] policy . Allowed Host Path ) field . Error List { all Errs := field . Error for i , target := range allowed Host Paths { if target . Path Prefix == " " { all Errs = append ( all Errs , field . Required ( fld parts := strings . Split ( filepath . To Slash ( target . Path for _ , item := range parts { if item == " " { all Errs = append ( all Errs , field . Invalid ( fld Path . Index ( i ) , target . Path return all } 
func validate PSPSE Linux ( fld Path * field . Path , se Linux * policy . SE Linux Strategy Options ) field . Error List { all Errs := field . Error // ensure the selinux strategy has a valid rule supported SE Linux Rules := sets . New String ( string ( policy . SE Linux Strategy Must Run As ) , string ( policy . SE Linux Strategy Run As if ! supported SE Linux Rules . Has ( string ( se Linux . Rule ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , se Linux . Rule , supported SE Linux return all } 
func validate PSP Run As User ( fld Path * field . Path , run As User * policy . Run As User Strategy Options ) field . Error List { all Errs := field . Error // ensure the user strategy has a valid rule supported Run As User Rules := sets . New String ( string ( policy . Run As User Strategy Must Run As ) , string ( policy . Run As User Strategy Must Run As Non Root ) , string ( policy . Run As User Strategy Run As if ! supported Run As User Rules . Has ( string ( run As User . Rule ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , run As User . Rule , supported Run As User // validate range settings for idx , rng := range run As User . Ranges { all Errs = append ( all Errs , validate User ID Range ( fld return all } 
func validate PSP Run As Group ( fld Path * field . Path , run As Group * policy . Run As Group Strategy Options ) field . Error List { var all Errs field . Error if run As Group == nil { return all switch run As Group . Rule { case policy . Run As Group Strategy Run As Any : if len ( run As Group . Ranges ) != 0 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , run As case policy . Run As Group Strategy Must Run As , policy . Run As Group Strategy May Run As : if len ( run As Group . Ranges ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , run As // validate range settings for idx , rng := range run As Group . Ranges { all Errs = append ( all Errs , validate Group ID Range ( fld default : supported Run As Group Rules := [ ] string { string ( policy . Run As Group Strategy Must Run As ) , string ( policy . Run As Group Strategy Run As Any ) , string ( policy . Run As Group Strategy May Run all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , run As Group . Rule , supported Run As Group return all } 
func validate PSPFS Group ( fld Path * field . Path , group Options * policy . FS Group Strategy Options ) field . Error List { all Errs := field . Error supported Rules := sets . New String ( string ( policy . FS Group Strategy Must Run As ) , string ( policy . FS Group Strategy May Run As ) , string ( policy . FS Group Strategy Run As if ! supported Rules . Has ( string ( group Options . Rule ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , group Options . Rule , supported for idx , rng := range group Options . Ranges { all Errs = append ( all Errs , validate Group ID Range ( fld return all } 
func validate PSP Supplemental Group ( fld Path * field . Path , group Options * policy . Supplemental Groups Strategy Options ) field . Error List { all Errs := field . Error supported Rules := sets . New String ( string ( policy . Supplemental Groups Strategy Run As Any ) , string ( policy . Supplemental Groups Strategy May Run As ) , string ( policy . Supplemental Groups Strategy Must Run if ! supported Rules . Has ( string ( group Options . Rule ) ) { all Errs = append ( all Errs , field . Not Supported ( fld Path . Child ( " " ) , group Options . Rule , supported for idx , rng := range group Options . Ranges { all Errs = append ( all Errs , validate Group ID Range ( fld return all } 
func validate Pod Security Policy Volumes ( fld Path * field . Path , volumes [ ] policy . FS Type ) field . Error List { all Errs := field . Error allowed := psputil . Get All FS Types As for _ , v := range volumes { if ! allowed . Has ( string ( v ) ) { all Errs = append ( all Errs , field . Not Supported ( fld return all } 
func validate PSP Default Allow Privilege Escalation ( fld Path * field . Path , default Allow Privilege Escalation * bool , allow Privilege Escalation bool ) field . Error List { all Errs := field . Error if default Allow Privilege Escalation != nil && * default Allow Privilege Escalation && ! allow Privilege Escalation { all Errs = append ( all Errs , field . Invalid ( fld Path , default Allow Privilege return all } 
func validate PSP Allowed Proc Mount Types ( fld Path * field . Path , allowed Proc Mount Types [ ] core . Proc Mount Type ) field . Error List { all Errs := field . Error for i , proc Mount Type := range allowed Proc Mount Types { if err := apivalidation . Validate Proc Mount Type ( fld Path . Index ( i ) , proc Mount Type ) ; err != nil { all Errs = append ( all return all } 
func validate Pod Security Policy Sysctls ( fld Path * field . Path , sysctls [ ] string ) field . Error List { all Errs := field . Error if len ( sysctls ) == 0 { return all covers for i , s := range sysctls { if len ( s ) == 0 { all Errs = append ( all Errs , field . Invalid ( fld } else if ! Is Valid Sysctl Pattern ( string ( s ) ) { all Errs = append ( all Errs , field . Invalid ( fld Path . Index ( i ) , sysctls [ i ] , fmt . Sprintf ( " " , apivalidation . Sysctl Max Length , Sysctl Pattern } else if s [ 0 ] == '*' { covers if covers All && len ( sysctls ) > 1 { all Errs = append ( all Errs , field . Forbidden ( fld return all } 
func validate ID Ranges ( fld Path * field . Path , min , max int64 ) field . Error List { all Errs := field . Error // if 0 <= Min <= Max then we do not need to validate max. It is always greater than or // equal to 0 and Min. if min < 0 { all Errs = append ( all Errs , field . Invalid ( fld if max < 0 { all Errs = append ( all Errs , field . Invalid ( fld if min > max { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func validate PSP Caps Against Drops ( required Drops [ ] core . Capability , caps To Check [ ] core . Capability , fld Path * field . Path ) field . Error List { all Errs := field . Error if required Drops == nil { return all for _ , cap := range caps To Check { if has Cap ( cap , required Drops ) { all Errs = append ( all Errs , field . Invalid ( fld Path , cap , fmt . Sprintf ( " " , fld return all } 
func validate Runtime Class Strategy ( fld Path * field . Path , rc * policy . Runtime Class Strategy Options ) field . Error var all Errs field . Error for i , name := range rc . Allowed Runtime Class Names { if name != policy . Allow All Runtime Class Names { all Errs = append ( all Errs , apivalidation . Validate Runtime Class Name ( name , fld if allowed [ name ] { all Errs = append ( all Errs , field . Duplicate ( fld if rc . Default Runtime Class Name != nil { all Errs = append ( all Errs , apivalidation . Validate Runtime Class Name ( * rc . Default Runtime Class Name , fld if ! allowed [ * rc . Default Runtime Class Name ] && ! allowed [ policy . Allow All Runtime Class Names ] { all Errs = append ( all Errs , field . Required ( fld Path . Child ( " " ) , fmt . Sprintf ( " " , * rc . Default Runtime Class if allowed [ policy . Allow All Runtime Class Names ] && len ( rc . Allowed Runtime Class Names ) > 1 { all Errs = append ( all Errs , field . Invalid ( fld Path . Child ( " " ) , rc . Allowed Runtime Class return all } 
func Validate Pod Security Policy Update ( old * policy . Pod Security Policy , new * policy . Pod Security Policy ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Object Meta Update ( & new . Object Meta , & old . Object Meta , field . New all Errs = append ( all Errs , Validate Pod Security Policy Specific Annotations ( new . Annotations , field . New all Errs = append ( all Errs , Validate Pod Security Policy Spec ( & new . Spec , field . New return all } 
func has } 
func Validate Init Configuration ( c * kubeadm . Init Configuration ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Node Registration Options ( & c . Node Registration , field . New all Errs = append ( all Errs , Validate Bootstrap Tokens ( c . Bootstrap Tokens , field . New all Errs = append ( all Errs , Validate Cluster Configuration ( & c . Cluster all Errs = append ( all Errs , Validate API Endpoint ( & c . Local API Endpoint , field . New return all } 
func Validate Cluster Configuration ( c * kubeadm . Cluster Configuration ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Networking ( & c . Networking , field . New all Errs = append ( all Errs , Validate API Server ( & c . API Server , field . New all Errs = append ( all Errs , Validate Absolute Path ( c . Certificates Dir , field . New all Errs = append ( all Errs , Validate Feature Gates ( c . Feature Gates , field . New all Errs = append ( all Errs , Validate Host Port ( c . Control Plane Endpoint , field . New all Errs = append ( all Errs , Validate Etcd ( & c . Etcd , field . New all Errs = append ( all return all } 
func Validate API Server ( a * kubeadm . API Server , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Cert SA Ns ( a . Cert SA Ns , fld return all } 
func Validate Join Configuration ( c * kubeadm . Join Configuration ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Discovery ( & c . Discovery , field . New all Errs = append ( all Errs , Validate Node Registration Options ( & c . Node Registration , field . New all Errs = append ( all Errs , Validate Join Control Plane ( c . Control Plane , field . New if ! filepath . Is Abs ( c . CA Cert Path ) || ! strings . Has Suffix ( c . CA Cert Path , " " ) { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , c . CA Cert return all } 
func Validate Join Control Plane ( c * kubeadm . Join Control Plane , fld Path * field . Path ) field . Error List { all Errs := field . Error if c != nil { all Errs = append ( all Errs , Validate API Endpoint ( & c . Local API Endpoint , fld return all } 
func Validate Node Registration Options ( nro * kubeadm . Node Registration Options , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( nro . Name ) == 0 { all Errs = append ( all Errs , field . Required ( fld } else { all Errs = append ( all Errs , apivalidation . Validate DNS1123Subdomain ( nro . Name , field . New all Errs = append ( all Errs , Validate Socket Path ( nro . CRI Socket , fld // TODO: Maybe validate .Taints as well in the future using something like validate Node Taints() in pkg/apis/core/validation return all } 
func Validate Discovery ( d * kubeadm . Discovery , fld Path * field . Path ) field . Error List { all Errs := field . Error if d . Bootstrap Token == nil && d . File == nil { all Errs = append ( all Errs , field . Invalid ( fld if d . Bootstrap Token != nil && d . File != nil { all Errs = append ( all Errs , field . Invalid ( fld if d . Bootstrap Token != nil { all Errs = append ( all Errs , Validate Discovery Bootstrap Token ( d . Bootstrap Token , fld all Errs = append ( all Errs , Validate Token ( d . TLS Bootstrap Token , fld if d . File != nil { all Errs = append ( all Errs , Validate Discovery File ( d . File , fld if len ( d . TLS Bootstrap Token ) != 0 { all Errs = append ( all Errs , Validate Token ( d . TLS Bootstrap Token , fld return all } 
func Validate Discovery Bootstrap Token ( b * kubeadm . Bootstrap Token Discovery , fld Path * field . Path ) field . Error List { all Errs := field . Error if len ( b . API Server Endpoint ) == 0 { all Errs = append ( all Errs , field . Required ( fld if len ( b . CA Cert Hashes ) == 0 && ! b . Unsafe Skip CA Verification { all Errs = append ( all Errs , field . Invalid ( fld all Errs = append ( all Errs , Validate Token ( b . Token , fld Path . Child ( kubeadmcmdoptions . Token all Errs = append ( all Errs , Validate Discovery Token API Server ( b . API Server Endpoint , fld return all } 
func Validate Discovery File ( f * kubeadm . File Discovery , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate Discovery Kube Config Path ( f . Kube Config Path , fld return all } 
func Validate Discovery Token API Server ( api Server string , fld Path * field . Path ) field . Error List { all Errs := field . Error _ , _ , err := net . Split Host Port ( api if err != nil { all Errs = append ( all Errs , field . Invalid ( fld Path , api return all } 
func Validate Discovery Kube Config Path ( discovery File string , fld Path * field . Path ) field . Error List { all Errs := field . Error u , err := url . Parse ( discovery if err != nil { all Errs = append ( all Errs , field . Invalid ( fld Path , discovery return all if u . Scheme == " " { // UR Is with no scheme should be treated as files if _ , err := os . Stat ( discovery File ) ; os . Is Not Exist ( err ) { all Errs = append ( all Errs , field . Invalid ( fld Path , discovery return all if u . Scheme != " " { all Errs = append ( all Errs , field . Invalid ( fld Path , discovery return all } 
func Validate Bootstrap Tokens ( bts [ ] kubeadm . Bootstrap Token , fld Path * field . Path ) field . Error List { all Errs := field . Error for i , bt := range bts { bt Path := fld all Errs = append ( all Errs , Validate Token ( bt . Token . String ( ) , bt Path . Child ( kubeadmcmdoptions . Token all Errs = append ( all Errs , Validate Token Usages ( bt . Usages , bt Path . Child ( kubeadmcmdoptions . Token all Errs = append ( all Errs , Validate Token Groups ( bt . Usages , bt . Groups , bt Path . Child ( kubeadmcmdoptions . Token if bt . Expires != nil && bt . TTL != nil { all Errs = append ( all Errs , field . Invalid ( bt return all } 
func Validate Token ( token string , fld Path * field . Path ) field . Error List { all Errs := field . Error if ! bootstraputil . Is Valid Bootstrap Token ( token ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Token Groups ( usages [ ] string , groups [ ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error // adding groups only makes sense for authentication usages Set := sets . New usage Authentication := strings . Trim Prefix ( bootstrapapi . Bootstrap Token Usage Authentication , bootstrapapi . Bootstrap Token Usage if len ( groups ) > 0 && ! usages Set . Has ( usage Authentication ) { all Errs = append ( all Errs , field . Invalid ( fld Path , groups , fmt . Sprintf ( " " , usage // validate any extra group names for _ , group := range groups { if err := bootstraputil . Validate Bootstrap Group Name ( group ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Token Usages ( usages [ ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error // validate usages if err := bootstraputil . Validate Usages ( usages ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Etcd ( e * kubeadm . Etcd , fld Path * field . Path ) field . Error List { all Errs := field . Error local Path := fld external Path := fld if e . Local == nil && e . External == nil { all Errs = append ( all Errs , field . Invalid ( fld return all if e . Local != nil && e . External != nil { all Errs = append ( all Errs , field . Invalid ( fld return all if e . Local != nil { all Errs = append ( all Errs , Validate Absolute Path ( e . Local . Data Dir , local all Errs = append ( all Errs , Validate Cert SA Ns ( e . Local . Server Cert SA Ns , local all Errs = append ( all Errs , Validate Cert SA Ns ( e . Local . Peer Cert SA Ns , local if e . External != nil { require // Only allow the http scheme if no certs/keys are passed if e . External . CA File == " " && e . External . Cert File == " " && e . External . Key File == " " { require // Require either none or both of the cert/key pair if ( e . External . Cert File == " " && e . External . Key File != " " ) || ( e . External . Cert File != " " && e . External . Key File == " " ) { all Errs = append ( all Errs , field . Invalid ( external // If the cert and key are specified, require the VA as well if e . External . Cert File != " " && e . External . Key File != " " && e . External . CA File == " " { all Errs = append ( all Errs , field . Invalid ( external all Errs = append ( all Errs , Validate UR Ls ( e . External . Endpoints , require HTTPS , external if e . External . CA File != " " { all Errs = append ( all Errs , Validate Absolute Path ( e . External . CA File , external if e . External . Cert File != " " { all Errs = append ( all Errs , Validate Absolute Path ( e . External . Cert File , external if e . External . Key File != " " { all Errs = append ( all Errs , Validate Absolute Path ( e . External . Key File , external return all } 
func Validate Cert SA Ns ( altnames [ ] string , fld Path * field . Path ) field . Error List { all Errs := field . Error for _ , altname := range altnames { if errs := validation . Is DNS1123Subdomain ( altname ) ; len ( errs ) != 0 { if errs2 := validation . Is Wildcard DNS1123Subdomain ( altname ) ; len ( errs2 ) != 0 { if net . Parse IP ( altname ) == nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate UR Ls ( urls [ ] string , require HTTPS bool , fld Path * field . Path ) field . Error List { all Errs := field . Error if err != nil { all Errs = append ( all Errs , field . Invalid ( fld if require HTTPS && u . Scheme != " " { all Errs = append ( all Errs , field . Invalid ( fld if u . Scheme == " " { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate IP From String ( ipaddr string , fld Path * field . Path ) field . Error List { all Errs := field . Error if net . Parse IP ( ipaddr ) == nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Port ( port int32 , fld Path * field . Path ) field . Error List { all Errs := field . Error if _ , err := kubeadmutil . Parse Port ( strconv . Itoa ( int ( port ) ) ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Host Port ( endpoint string , fld Path * field . Path ) field . Error List { all Errs := field . Error if _ , _ , err := kubeadmutil . Parse Host Port ( endpoint ) ; endpoint != " " && err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate IP Net From String ( subnet string , min Addrs int64 , fld Path * field . Path ) field . Error List { all Errs := field . Error _ , svc Subnet , err := net . Parse if err != nil { all Errs = append ( all Errs , field . Invalid ( fld return all num Addresses := ipallocator . Range Size ( svc if num Addresses < min Addrs { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Networking ( c * kubeadm . Networking , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate DNS1123Subdomain ( c . DNS Domain , field . New all Errs = append ( all Errs , Validate IP Net From String ( c . Service Subnet , constants . Minimum Addresses In Service Subnet , field . New if len ( c . Pod Subnet ) != 0 { all Errs = append ( all Errs , Validate IP Net From String ( c . Pod Subnet , constants . Minimum Addresses In Service Subnet , field . New return all } 
func Validate Absolute Path ( path string , fld Path * field . Path ) field . Error List { all Errs := field . Error if ! filepath . Is Abs ( path ) { all Errs = append ( all Errs , field . Invalid ( fld return all } 
func Validate Mixed Arguments ( flag * pflag . Flag mixed Invalid flag . Visit ( func ( f * pflag . Flag ) { if is Allowed mixed Invalid Flags = append ( mixed Invalid if len ( mixed Invalid Flags ) != 0 { return errors . Errorf ( " " , mixed Invalid } 
func Validate Feature Gates ( feature Gates map [ string ] bool , fld Path * field . Path ) field . Error List { all Errs := field . Error // check valid feature names are provided for k := range feature Gates { if ! features . Supports ( features . Init Feature Gates , k ) { all Errs = append ( all Errs , field . Invalid ( fld Path , feature return all } 
func Validate API Endpoint ( c * kubeadm . API Endpoint , fld Path * field . Path ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , Validate IP From String ( c . Advertise Address , fld all Errs = append ( all Errs , Validate Port ( c . Bind Port , fld return all } 
func Validate Ignore Preflight Errors ( ignore Preflight Errors [ ] string ) ( sets . String , error ) { ignore Errors := sets . New all Errs := field . Error for _ , item := range ignore Preflight Errors { ignore Errors . Insert ( strings . To if ignore Errors . Has ( " " ) && ignore Errors . Len ( ) > 1 { all Errs = append ( all Errs , field . Invalid ( field . New Path ( " " ) , strings . Join ( ignore return ignore Errors , all Errs . To } 
func Validate Socket Path ( socket string , fld Path * field . Path ) field . Error List { all Errs := field . Error if err != nil { return append ( all Errs , field . Invalid ( fld if u . Scheme == " " { if ! filepath . Is Abs ( u . Path ) { return append ( all Errs , field . Invalid ( fld } else if u . Scheme != kubeadmapiv1beta2 . Default Url Scheme { return append ( all Errs , field . Invalid ( fld return all } 
func ( d * default Cache Mutation Detector ) Add Object ( obj interface { } ) { if _ , ok := obj . ( Deleted Final State if obj , ok := obj . ( runtime . Object ) ; ok { copied Obj := obj . Deep Copy d . cached Objs = append ( d . cached Objs , cache Obj { cached : obj , copied : copied } 
func New Cgroup Notifier ( path , attribute string , threshold int64 ) ( Cgroup return & unsupported Threshold } 
func New Validating Webhook Configuration Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Validating Webhook Configuration Informer ( client , resync } 
func proccess Env File Line ( line [ ] byte , file Path string , current Line int ) ( key , value string , err error ) { if ! utf8 . Valid ( line ) { return `` , `` , fmt . Errorf ( " " , file Path , current // We trim UTF8 BOM from the first line of the file but no others if current Line == 0 { line = bytes . Trim // trim the line from all leading whitespace first line = bytes . Trim Left Func ( line , unicode . Is data := strings . Split if errs := validation . Is Env Var } 
func add From Env File ( file Path string , add To func ( key , value string ) error ) error { f , err := os . Open ( file scanner := bufio . New current for scanner . Scan ( ) { // Process the current line, retrieving a key/value pair if // possible. scanned key , value , err := proccess Env File Line ( scanned Bytes , file Path , current current if err = add } 
func ( in * Cron Job Status ) Deep Copy Into ( out * Cron Job * out = make ( [ ] core . Object if in . Last Schedule Time != nil { in , out := & in . Last Schedule Time , & out . Last Schedule * out = ( * in ) . Deep } 
func ( in * Job ) Deep Copy out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy in . Status . Deep Copy } 
func ( in * Job ) Deep in . Deep Copy } 
func ( in * Job ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Job Condition ) Deep Copy Into ( out * Job in . Last Probe Time . Deep Copy Into ( & out . Last Probe in . Last Transition Time . Deep Copy Into ( & out . Last Transition } 
func ( in * Job Condition ) Deep Copy ( ) * Job out := new ( Job in . Deep Copy } 
func ( in * Job List ) Deep Copy Into ( out * Job out . Type Meta = in . Type out . List Meta = in . List for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Job List ) Deep Copy ( ) * Job out := new ( Job in . Deep Copy } 
func ( in * Job List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Job Spec ) Deep Copy Into ( out * Job if in . Active Deadline Seconds != nil { in , out := & in . Active Deadline Seconds , & out . Active Deadline if in . Backoff Limit != nil { in , out := & in . Backoff Limit , & out . Backoff * out = new ( v1 . Label ( * in ) . Deep Copy if in . Manual Selector != nil { in , out := & in . Manual Selector , & out . Manual in . Template . Deep Copy if in . TTL Seconds After Finished != nil { in , out := & in . TTL Seconds After Finished , & out . TTL Seconds After } 
func ( in * Job Spec ) Deep Copy ( ) * Job out := new ( Job in . Deep Copy } 
func ( in * Job Status ) Deep Copy Into ( out * Job * out = make ( [ ] Job for i := range * in { ( * in ) [ i ] . Deep Copy if in . Start Time != nil { in , out := & in . Start Time , & out . Start * out = ( * in ) . Deep if in . Completion Time != nil { in , out := & in . Completion Time , & out . Completion * out = ( * in ) . Deep } 
func ( in * Job Status ) Deep Copy ( ) * Job out := new ( Job in . Deep Copy } 
func ( r Registry ) Register ( name string , factory Plugin } 
} 
func ( c * dynamic Resource Client ) Patch ( name string , pt types . Patch Type , data [ ] byte , opts metav1 . Patch Options , subresources ... string ) ( * unstructured . Unstructured , error ) { var uncast switch { case len ( c . namespace ) == 0 && len ( subresources ) == 0 : uncast Ret , err = c . client . Fake . Invokes ( testing . New Root Patch case len ( c . namespace ) == 0 && len ( subresources ) > 0 : uncast Ret , err = c . client . Fake . Invokes ( testing . New Root Patch Subresource case len ( c . namespace ) > 0 && len ( subresources ) == 0 : uncast Ret , err = c . client . Fake . Invokes ( testing . New Patch case len ( c . namespace ) > 0 && len ( subresources ) > 0 : uncast Ret , err = c . client . Fake . Invokes ( testing . New Patch Subresource if uncast if err := c . client . scheme . Convert ( uncast } 
func Probe Volume Plugins ( volume Config volume . Volume Config ) [ ] volume . Volume Plugin { return [ ] volume . Volume Plugin { & host Path Plugin { host : nil , config : volume } 
func ( plugin * host Path Plugin ) Recycle ( pv Name string , spec * volume . Spec , event Recorder recyclerclient . Recycle Event Recorder ) error { if spec . Persistent Volume == nil || spec . Persistent Volume . Spec . Host pod := plugin . config . Recycler Pod timeout := util . Calculate Timeout For Volume ( plugin . config . Recycler Minimum Timeout , plugin . config . Recycler Timeout Increment , spec . Persistent // overrides pod . Spec . Active Deadline pod . Spec . Volumes [ 0 ] . Volume Source = v1 . Volume Source { Host Path : & v1 . Host Path Volume Source { Path : spec . Persistent Volume . Spec . Host return recyclerclient . Recycle Volume By Watching Pod Until Completion ( pv Name , pod , plugin . host . Get Kube Client ( ) , event } 
func ( b * host Path Mounter ) Set Up ( fs Group * int64 ) error { err := validation . Validate Path No Backsteps ( b . Get if err != nil { return fmt . Errorf ( " " , b . Get if * b . path Type == v1 . Host Path return check Type ( b . Get Path ( ) , b . path } 
func ( b * host Path Mounter ) Set Up At ( dir string , fs } 
func ( r * host Path Provisioner ) Provision ( selected Node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term ) ( * v1 . Persistent Volume , error ) { if util . Check Persistent Volume Claim Mode Block ( r . options . PVC ) { return nil , fmt . Errorf ( " " , r . plugin . Get Plugin fullpath := fmt . Sprintf ( " " , uuid . New capacity := r . options . PVC . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource pv := & v1 . Persistent Volume { Object Meta : metav1 . Object Meta { Name : r . options . PV Name , Annotations : map [ string ] string { util . Volume Dynamically Created By Key : " " , } , } , Spec : v1 . Persistent Volume Spec { Persistent Volume Reclaim Policy : r . options . Persistent Volume Reclaim Policy , Access Modes : r . options . PVC . Spec . Access Modes , Capacity : v1 . Resource List { v1 . Resource Name ( v1 . Resource Storage ) : capacity , } , Persistent Volume Source : v1 . Persistent Volume Source { Host Path : & v1 . Host Path Volume if len ( r . options . PVC . Spec . Access Modes ) == 0 { pv . Spec . Access Modes = r . plugin . Get Access return pv , os . Mkdir All ( pv . Spec . Host } 
func ( r * host Path Deleter ) Delete ( ) error { regexp := regexp . Must if ! regexp . Match String ( r . Get Path ( ) ) { return fmt . Errorf ( " " , r . Get return os . Remove All ( r . Get } 
func check Type ( path string , path Type * v1 . Host Path Type , mounter mount . Interface ) error { return check Type Internal ( new File Type Checker ( path , mounter ) , path } 
func ( e * Runner ) Append } 
func ( e * Runner ) compute Phase Run Flags ( ) ( map [ string ] bool , error ) { // Initialize support data structure phase Run phase e . visit All ( func ( p * phase Runner ) error { // Initialize phase Run Flags assuming that all the phases should be run. phase Run Flags [ p . generated // Initialize phase Hierarchy for the current phase (the list of phases // depending on the current phase phase Hierarchy [ p . generated for parent != nil { phase Hierarchy [ parent . generated Name ] = append ( phase Hierarchy [ parent . generated Name ] , p . generated // If a filter option is specified, set all phase Run Flags to false except for // the phases included in the filter and their hierarchy of nested phases. if len ( e . Options . Filter Phases ) > 0 { for i := range phase Run Flags { phase Run for _ , f := range e . Options . Filter Phases { if _ , ok := phase Run Flags [ f ] ; ! ok { return phase Run phase Run for _ , c := range phase Hierarchy [ f ] { phase Run // If a phase skip option is specified, set the corresponding phase Run Flags // to false and apply the same change to the underlying hierarchy for _ , f := range e . Options . Skip Phases { if _ , ok := phase Run Flags [ f ] ; ! ok { return phase Run phase Run for _ , c := range phase Hierarchy [ f ] { phase Run return phase Run } 
func ( e * Runner ) Set Data Initializer ( builder func ( cmd * cobra . Command , args [ ] string ) ( Run Data , error ) ) { e . run Data } 
func ( e * Runner ) Init Data ( args [ ] string ) ( Run Data , error ) { if e . run Data == nil && e . run Data if e . run Data , err = e . run Data Initializer ( e . run return e . run } 
func ( e * Runner ) Run ( args [ ] string ) error { e . prepare For // determine which phase should be run according to Runner Options phase Run Flags , err := e . compute Phase Run // builds the runner data var data Run if data , err = e . Init err = e . visit All ( func ( p * phase Runner ) error { // if the phase should not be run, skip the phase. if run , ok := phase Run Flags [ p . generated // Errors if phases that are meant to create special subcommands only // are wrongly assigned Run Methods if p . Run All Siblings && ( p . Run If != nil || p . Run != nil ) { return errors . Wrapf ( err , " " , p . generated // If the phase defines a condition to be checked before executing the phase action. if p . Run If != nil { // Check the condition and returns if the condition isn't satisfied (or fails) ok , err := p . Run if err != nil { return errors . Wrapf ( err , " " , p . generated // Runs the phase action (if defined) if p . Run != nil { if err := p . Run ( data ) ; err != nil { return errors . Wrapf ( err , " " , p . generated } 
func ( e * Runner ) Help ( cmd Use string ) string { e . prepare For // computes the max length of for each phase use line max e . visit All ( func ( p * phase Runner ) error { if ! p . Hidden && ! p . Run All if max Length < length { max // prints the list of phases indented by level and formatted using the maxlength // the list is enclosed in a mardown code block for ensuring better readability in the public web site line := fmt . Sprintf ( " \n " , cmd e . visit All ( func ( p * phase Runner ) error { if ! p . Hidden && ! p . Run All Siblings { padding := max } 
func ( e * Runner ) Set Additional Flags ( fn func ( * pflag . Flag Set ) ) { // creates a new New Flag Set e . cmd Additional Flags = pflag . New Flag Set ( " " , pflag . Continue On // invokes the function that sets additional flags fn ( e . cmd Additional } 
func ( e * Runner ) Bind To Command ( cmd * cobra . Command ) { // keep track of the command triggering the runner e . run e . prepare For // adds the phases subcommand phase cmd . Add Command ( phase e . visit All ( func ( p * phase // initialize phase selector phase Selector := p . generated // if requested, set the phase to run all the sibling phases if p . Run All Siblings { phase Selector = p . parent . generated // creates phase subcommand phase Cmd := & cobra . Command { Use : strings . To // overrides the command triggering the Runner using the phase Cmd e . run e . Options . Filter Phases = [ ] string { phase // makes the new command inherits local flags from the parent command // Nb. global flags will be inherited automatically inherits Flags ( cmd . Flags ( ) , phase Cmd . Flags ( ) , p . Inherit // makes the new command inherits additional flags for phases if e . cmd Additional Flags != nil { inherits Flags ( e . cmd Additional Flags , phase Cmd . Flags ( ) , p . Inherit // If defined, added phase local flags if p . Local Flags != nil { p . Local Flags . Visit All ( func ( f * pflag . Flag ) { phase Cmd . Flags ( ) . Add // if this phase has children (not a leaf) it doesn't accept any args if len ( p . Phases ) > 0 { phase Cmd . Args = cobra . No // adds the command to parent if p . level == 0 { phase Command . Add Command ( phase } else { subcommands [ p . parent . generated Name ] . Add Command ( phase subcommands [ p . generated Name ] = phase // adds phase related flags to the main command cmd . Flags ( ) . String Slice Var ( & e . Options . Skip } 
func ( e * Runner ) visit All ( fn func ( * phase Runner ) error ) error { for _ , current Runner := range e . phase Runners { if err := fn ( current } 
func ( e * Runner ) prepare For Execution ( ) { e . phase Runners = [ ] * phase var parent Runner * phase for _ , phase := range e . Phases { // skips phases that are meant to create special subcommands only if phase . Run All // add phases to the execution list add Phase Runner ( e , parent } 
func add Phase Runner ( e * Runner , parent Runner * phase Runner , phase Phase ) { // computes contextual information derived by the workflow managed by the Runner. use := clean generated self Path := [ ] string { generated if parent Runner != nil { generated Name = strings . Join ( [ ] string { parent Runner . generated Name , generated Name } , phase use = fmt . Sprintf ( " " , phase self Path = append ( parent Runner . self Path , self // creates the phase Runner current Runner := & phase Runner { Phase : phase , parent : parent Runner , level : len ( self Path ) - 1 , self Path : self Path , generated Name : generated // adds to the phase Runners list e . phase Runners = append ( e . phase Runners , current // iterate for the nested, ordered list of phases, thus storing // phases in the expected executing order (child phase are stored immediately after their parent phase). for _ , child Phase := range phase . Phases { add Phase Runner ( e , current Runner , child } 
func clean Name ( name string ) string { ret := strings . To } 
func Remove Object Managed accessor . Set Managed } 
func Decode Object Managed Fields ( from runtime . Object ) ( fieldpath . Managed Fields , error ) { if from == nil { return make ( map [ string ] * fieldpath . Versioned managed , err := decode Managed Fields ( accessor . Get Managed } 
func Encode Object Managed Fields ( obj runtime . Object , fields fieldpath . Managed managed , err := encode Managed accessor . Set Managed } 
func decode Managed Fields ( encoded Managed Fields [ ] metav1 . Managed Fields Entry ) ( managed Fields fieldpath . Managed Fields , err error ) { managed Fields = make ( map [ string ] * fieldpath . Versioned Set , len ( encoded Managed for _ , encoded Versioned Set := range encoded Managed Fields { manager , err := Build Manager Identifier ( & encoded Versioned if err != nil { return nil , fmt . Errorf ( " " , encoded Versioned managed Fields [ manager ] , err = decode Versioned Set ( & encoded Versioned if err != nil { return nil , fmt . Errorf ( " " , encoded Versioned return managed } 
func Build Manager Identifier ( encoded Manager * metav1 . Managed Fields Entry ) ( manager string , err error ) { encoded Manager Copy := * encoded // Never include the fields in the manager identifier encoded Manager // For appliers, don't include the API Version or Time in the manager identifier, // so it will always have the same manager identifier each time it applied. if encoded Manager . Operation == metav1 . Managed Fields Operation Apply { encoded Manager Copy . API encoded Manager // Use the remaining fields to build the manager identifier b , err := json . Marshal ( & encoded Manager } 
func encode Managed Fields ( managed Fields fieldpath . Managed Fields ) ( encoded Managed Fields [ ] metav1 . Managed Fields for manager := range managed encoded Managed Fields = [ ] metav1 . Managed Fields for _ , manager := range managers { versioned Set := managed v , err := encode Manager Versioned Set ( manager , versioned encoded Managed Fields = append ( encoded Managed return sort Encoded Managed Fields ( encoded Managed } 
func New Insufficient Resource Error ( resource Name v1 . Resource Name , requested , used , capacity int64 ) * Insufficient Resource Error { return & Insufficient Resource Error { Resource Name : resource } 
func ( e * Insufficient Resource Error ) Get Insufficient } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Exec Credential ) ( nil ) , ( * clientauthentication . Exec Credential ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Exec Credential_To_clientauthentication_Exec Credential ( a . ( * Exec Credential ) , b . ( * clientauthentication . Exec if err := s . Add Generated Conversion Func ( ( * clientauthentication . Exec Credential ) ( nil ) , ( * Exec Credential ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential_To_v1alpha1_Exec Credential ( a . ( * clientauthentication . Exec Credential ) , b . ( * Exec if err := s . Add Generated Conversion Func ( ( * Exec Credential Spec ) ( nil ) , ( * clientauthentication . Exec Credential Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Exec Credential Spec_To_clientauthentication_Exec Credential Spec ( a . ( * Exec Credential Spec ) , b . ( * clientauthentication . Exec Credential if err := s . Add Generated Conversion Func ( ( * clientauthentication . Exec Credential Spec ) ( nil ) , ( * Exec Credential Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential Spec_To_v1alpha1_Exec Credential Spec ( a . ( * clientauthentication . Exec Credential Spec ) , b . ( * Exec Credential if err := s . Add Generated Conversion Func ( ( * Exec Credential Status ) ( nil ) , ( * clientauthentication . Exec Credential Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Exec Credential Status_To_clientauthentication_Exec Credential Status ( a . ( * Exec Credential Status ) , b . ( * clientauthentication . Exec Credential if err := s . Add Generated Conversion Func ( ( * clientauthentication . Exec Credential Status ) ( nil ) , ( * Exec Credential Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_clientauthentication_Exec Credential Status_To_v1alpha1_Exec Credential Status ( a . ( * clientauthentication . Exec Credential Status ) , b . ( * Exec Credential if err := s . Add Generated Conversion if err := s . Add Generated Conversion } 
func Convert_v1alpha1_Exec Credential_To_clientauthentication_Exec Credential ( in * Exec Credential , out * clientauthentication . Exec Credential , s conversion . Scope ) error { return auto Convert_v1alpha1_Exec Credential_To_clientauthentication_Exec } 
func Convert_clientauthentication_Exec Credential_To_v1alpha1_Exec Credential ( in * clientauthentication . Exec Credential , out * Exec Credential , s conversion . Scope ) error { return auto Convert_clientauthentication_Exec Credential_To_v1alpha1_Exec } 
func Convert_v1alpha1_Exec Credential Spec_To_clientauthentication_Exec Credential Spec ( in * Exec Credential Spec , out * clientauthentication . Exec Credential Spec , s conversion . Scope ) error { return auto Convert_v1alpha1_Exec Credential Spec_To_clientauthentication_Exec Credential } 
func Convert_clientauthentication_Exec Credential Spec_To_v1alpha1_Exec Credential Spec ( in * clientauthentication . Exec Credential Spec , out * Exec Credential Spec , s conversion . Scope ) error { return auto Convert_clientauthentication_Exec Credential Spec_To_v1alpha1_Exec Credential } 
func Convert_v1alpha1_Exec Credential Status_To_clientauthentication_Exec Credential Status ( in * Exec Credential Status , out * clientauthentication . Exec Credential Status , s conversion . Scope ) error { return auto Convert_v1alpha1_Exec Credential Status_To_clientauthentication_Exec Credential } 
func Convert_clientauthentication_Exec Credential Status_To_v1alpha1_Exec Credential Status ( in * clientauthentication . Exec Credential Status , out * Exec Credential Status , s conversion . Scope ) error { return auto Convert_clientauthentication_Exec Credential Status_To_v1alpha1_Exec Credential } 
func Convert_v1alpha1_Response_To_clientauthentication_Response ( in * Response , out * clientauthentication . Response , s conversion . Scope ) error { return auto } 
func Convert_clientauthentication_Response_To_v1alpha1_Response ( in * clientauthentication . Response , out * Response , s conversion . Scope ) error { return auto } 
func ( o * Label o . Record o . Recorder , err = o . Record Flags . To o . output Format = cmdutil . Get Flag o . dryrun = cmdutil . Get Dry Run o . To Printer = func ( operation string ) ( printers . Resource Printer , error ) { o . Print Flags . Name Print if o . dryrun { o . Print return o . Print Flags . To resources , label Args , err := cmdutil . Get Resources And o . new Labels , o . remove Labels , err = parse Labels ( label if o . list && len ( o . output o . namespace , o . enforce Namespace , err = f . To Raw Kube Config o . builder = f . New o . unstructured Client For Mapping = f . Unstructured Client For } 
func ( o * Label if o . all && len ( o . field if len ( o . resources ) < 1 && cmdutil . Is Filename Slice Empty ( o . Filename Options . Filenames , o . Filename if len ( o . new Labels ) < 1 && len ( o . remove } 
func ( o * Label Options ) Run Label ( ) error { b := o . builder . Unstructured ( ) . Local Param ( o . local ) . Continue On Error ( ) . Namespace Param ( o . namespace ) . Default Namespace ( ) . Filename Param ( o . enforce Namespace , & o . Filename if ! o . local { b = b . Label Selector Param ( o . selector ) . Field Selector Param ( o . field Selector ) . Resource Type Or Name r := b . Do ( ) . Into Single Item // only apply resource version locking on a single resource if ! one && len ( o . resource var output var data Change old if o . dryrun || o . local || o . list { err = label Func ( obj , o . overwrite , o . resource Version , o . new Labels , o . remove new data Change Msg = update Data Change Msg ( old Data , new output for _ , label := range o . remove Labels { if _ , ok := accessor . Get if err := label Func ( obj , o . overwrite , o . resource Version , o . new Labels , o . remove new data Change Msg = update Data Change Msg ( old Data , new patch Bytes , err := jsonpatch . Create Merge Patch ( old Data , new created mapping := info . Resource client , err := o . unstructured Client For helper := resource . New if created Patch { output Obj , err = helper . Patch ( namespace , name , types . Merge Patch Type , patch } else { output if o . list { accessor , err := meta . Accessor ( output gvks , _ , err := unstructuredscheme . New Unstructured Object Typer ( ) . Object fmt . Fprintf ( o . Err for k , v := range accessor . Get printer , err := o . To Printer ( data Change return printer . Print } 
func ( c * Fake Limit Ranges ) Get ( name string , options v1 . Get Options ) ( result * corev1 . Limit Range , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( limitranges Resource , c . ns , name ) , & corev1 . Limit return obj . ( * corev1 . Limit } 
func ( c * Fake Limit Ranges ) List ( opts v1 . List Options ) ( result * corev1 . Limit Range List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( limitranges Resource , limitranges Kind , c . ns , opts ) , & corev1 . Limit Range label , _ , _ := testing . Extract From List list := & corev1 . Limit Range List { List Meta : obj . ( * corev1 . Limit Range List ) . List for _ , item := range obj . ( * corev1 . Limit Range } 
func ( c * Fake Limit Ranges ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( limitranges } 
func ( c * Fake Limit Ranges ) Create ( limit Range * corev1 . Limit Range ) ( result * corev1 . Limit Range , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( limitranges Resource , c . ns , limit Range ) , & corev1 . Limit return obj . ( * corev1 . Limit } 
func ( c * Fake Limit Ranges ) Update ( limit Range * corev1 . Limit Range ) ( result * corev1 . Limit Range , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( limitranges Resource , c . ns , limit Range ) , & corev1 . Limit return obj . ( * corev1 . Limit } 
func ( c * Fake Limit Ranges ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( limitranges Resource , c . ns , name ) , & corev1 . Limit } 
func ( c * Fake Limit Ranges ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( limitranges Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & corev1 . Limit Range } 
func ( c * Fake Limit Ranges ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * corev1 . Limit Range , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( limitranges Resource , c . ns , name , pt , data , subresources ... ) , & corev1 . Limit return obj . ( * corev1 . Limit } 
func status Viewer ( mapping * meta . REST Mapping ) ( kubectl . Status Viewer , error ) { return kubectl . Status Viewer For ( mapping . Group Version Kind . Group } 
func Mark FS Resize Finished ( pvc * v1 . Persistent Volume Claim , capacity v1 . Resource List , kube Client clientset . Interface ) error { new PVC := pvc . Deep new new PVC = Merge Resize Condition On PVC ( new PVC , [ ] v1 . Persistent Volume Claim _ , err := Patch PVC Status ( pvc /*old PVC*/ , new PVC , kube } 
func Patch PVC Status ( old PVC * v1 . Persistent Volume Claim , new PVC * v1 . Persistent Volume Claim , kube Client clientset . Interface ) ( * v1 . Persistent Volume Claim , error ) { pvc Name := old old Data , err := json . Marshal ( old if err != nil { return nil , fmt . Errorf ( " " , pvc new Data , err := json . Marshal ( new if err != nil { return nil , fmt . Errorf ( " " , pvc patch Bytes , err := strategicpatch . Create Two Way Merge Patch ( old Data , new Data , old if err != nil { return nil , fmt . Errorf ( " " , pvc updated Claim , update Err := kube Client . Core V1 ( ) . Persistent Volume Claims ( old PVC . Namespace ) . Patch ( pvc Name , types . Strategic Merge Patch Type , patch if update Err != nil { return nil , fmt . Errorf ( " " , pvc Name , update return updated } 
func Merge Resize Condition On PVC ( pvc * v1 . Persistent Volume Claim , resize Conditions [ ] v1 . Persistent Volume Claim Condition ) * v1 . Persistent Volume Claim { resize Condition Map := map [ v1 . Persistent Volume Claim Condition Type ] * resize Process for _ , condition := range resize Conditions { resize Condition Map [ condition . Type ] = & resize Process old new Conditions := [ ] v1 . Persistent Volume Claim for _ , condition := range old Conditions { // If Condition is of not resize type, we keep it. if _ , ok := known Resize Conditions [ condition . Type ] ; ! ok { new Conditions = append ( new if new Condition , ok := resize Condition Map [ condition . Type ] ; ok { if new Condition . condition . Status != condition . Status { new Conditions = append ( new Conditions , new } else { new Conditions = append ( new new // append all unprocessed conditions for _ , new Condition := range resize Condition Map { if ! new Condition . processed { new Conditions = append ( new Conditions , new pvc . Status . Conditions = new } 
func Generic Resize FS ( host volume . Volume Host , plugin Name , device Path , device Mount Path string ) ( bool , error ) { mounter := host . Get Mounter ( plugin disk Formatter := & mount . Safe Format And Mount { Interface : mounter , Exec : host . Get Exec ( plugin resizer := resizefs . New Resize Fs ( disk return resizer . Resize ( device Path , device Mount } 
func split } 
func translate Service Port To Target for _ , port := range ports { local Port , remote Port := split portnum , err := strconv . Atoi ( remote if err != nil { svc Port , err := util . Lookup Service Port Number By Name ( svc , remote portnum = int ( svc if local Port == remote Port { local container Port , err := util . Lookup Container Port Number By Service if int32 ( portnum ) != container Port { translated = append ( translated , fmt . Sprintf ( " " , local Port , container } 
func convert Pod Named Port To for _ , port := range ports { local Port , remote Port := split container Port Str := remote _ , err := strconv . Atoi ( remote if err != nil { container Port , err := util . Lookup Container Port Number By Name ( pod , remote container Port Str = strconv . Itoa ( int ( container if local Port != remote Port { converted = append ( converted , fmt . Sprintf ( " " , local Port , container Port } else { converted = append ( converted , container Port } 
func ( o * Port Forward if len ( args ) < 2 { return cmdutil . Usage o . Namespace , _ , err = f . To Raw Kube Config builder := f . New Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Continue On Error ( ) . Namespace Param ( o . Namespace ) . Default get Pod Timeout , err := cmdutil . Get Pod Running Timeout if err != nil { return cmdutil . Usage resource builder . Resource Names ( " " , resource forwardable Pod , err := polymorphichelpers . Attachable Pod For Object Fn ( f , obj , get Pod o . Pod Name = forwardable // handle service port mapping to target port if needed switch t := obj . ( type ) { case * corev1 . Service : o . Ports , err = translate Service Port To Target Port ( args [ 1 : ] , * t , * forwardable default : o . Ports , err = convert Pod Named Port To Number ( args [ 1 : ] , * forwardable clientset , err := f . Kubernetes Client o . Pod Client = clientset . Core o . Config , err = f . To REST o . REST Client , err = f . REST o . Stop o . Ready } 
func ( o Port Forward Options ) Validate ( ) error { if len ( o . Pod if o . Port Forwarder == nil || o . Pod Client == nil || o . REST } 
func ( o Port Forward Options ) Run Port Forward ( ) error { pod , err := o . Pod Client . Pods ( o . Namespace ) . Get ( o . Pod Name , metav1 . Get if pod . Status . Phase != corev1 . Pod if o . Stop Channel != nil { close ( o . Stop req := o . REST Client . Post ( ) . Resource ( " " ) . Namespace ( o . Namespace ) . Name ( pod . Name ) . Sub return o . Port Forwarder . Forward } 
func new Cron Jobs ( c * Batch V2alpha1Client , namespace string ) * cron Jobs { return & cron Jobs { client : c . REST } 
func ( c * Auditregistration V1alpha1Client ) REST return c . rest } 
func ( util * GCE Disk Util ) Delete Volume ( d * gce Persistent Disk Deleter ) error { cloud , err := get Cloud Provider ( d . gce Persistent Disk . plugin . host . Get Cloud if err = cloud . Delete Disk ( d . pd Name ) ; err != nil { klog . V ( 2 ) . Infof ( " " , d . pd // GCE cloud provider returns volume.deleted Volume In Use klog . V ( 2 ) . Infof ( " " , d . pd } 
func ( util * GCE Disk Util ) Create Volume ( c * gce Persistent Disk Provisioner , node * v1 . Node , allowed Topologies [ ] v1 . Topology Selector Term ) ( string , int , map [ string ] string , string , error ) { cloud , err := get Cloud Provider ( c . gce Persistent Disk . plugin . host . Get Cloud name := volumeutil . Generate Volume Name ( c . options . Cluster Name , c . options . PV capacity := c . options . PVC . Spec . Resources . Requests [ v1 . Resource Name ( v1 . Resource // GCE P Ds are allocated in chunks of Gi Bs request GB := volumehelpers . Round Up To Gi // Apply Parameters. // Values for parameter "replication-type" are canonicalized to lower case. // Values for other parameters are case-insensitive, and we leave validation of these values // to the cloud provider. disk configured var configured zone zones replication Type := replication Type for k , v := range c . options . Parameters { switch strings . To Lower ( k ) { case " " : disk case " " : zone configured case " " : zones configured Zones , err = volumehelpers . Zones To case " " : if ! utilfeature . Default Feature Gate . Enabled ( cloudfeatures . GCE Regional Persistent Disk ) { return " " , 0 , nil , " " , fmt . Errorf ( " " , k , c . plugin . Get Plugin Name ( ) , cloudfeatures . GCE Regional Persistent replication Type = strings . To case volume . Volume Parameter FS default : return " " , 0 , nil , " " , fmt . Errorf ( " " , k , c . plugin . Get Plugin activezones , err = cloud . Get All Current switch replication Type { case replication Type Regional PD : selected Zones , err := volumehelpers . Select Zones For Volume ( zone Present , zones Present , configured Zone , configured Zones , activezones , node , allowed Topologies , c . options . PVC . Name , max Regional PD if err = cloud . Create Regional Disk ( name , disk Type , selected Zones , int64 ( request GB ) , * c . options . Cloud case replication Type None : selected Zone , err := volumehelpers . Select Zone For Volume ( zone Present , zones Present , configured Zone , configured Zones , activezones , node , allowed if err := cloud . Create Disk ( name , disk Type , selected Zone , int64 ( request GB ) , * c . options . Cloud default : return " " , 0 , nil , " " , fmt . Errorf ( " " , replication labels , err := cloud . Get Auto Labels For return name , int ( request } 
func verify Device Path ( device Paths [ ] string , sd Before Set sets . String , disk Name string ) ( string , error ) { if err := udevadm Change To New Drives ( sd Before Set ) ; err != nil { // It's possible udevadm was called on other disks so it should not block this // call. If it did fail on this disk, then the device for _ , path := range device Paths { if path Exists , err := mount . Path } else if path Exists { // validate that the path actually resolves to the correct disk serial , err := get Scsi Serial ( path , disk if serial != disk Name { // The device link is not pointing to the correct device // Trigger udev on this device to try to fix the link if udev Err := udevadm Change To Drive ( path ) ; udev // Return error to retry Wait For Attach and verify Device Path return " " , fmt . Errorf ( " " , serial , path , disk } 
func get Scsi Serial ( device Path , disk Name string ) ( string , error ) { exists , err := utilpath . Exists ( utilpath . Check Follow if ! exists { klog . V ( 6 ) . Infof ( " " , device return disk out , err := exec . New ( ) . Command ( " " , " " , " " , fmt . Sprintf ( " " , device Path ) ) . Combined if err != nil { return " " , fmt . Errorf ( " " , device return parse Scsi } 
func parse Scsi Serial ( output string ) ( string , error ) { substrings := scsi Regex . Find String } 
func get Disk By ID Paths ( pd Name string , partition string ) [ ] string { device Paths := [ ] string { filepath . Join ( disk By ID Path , disk Google Prefix + pd Name ) , filepath . Join ( disk By ID Path , disk Scsi Google Prefix + pd if partition != " " { for i , path := range device Paths { device Paths [ i ] = path + disk Partition return device } 
func get Cloud Provider ( cloud for num Retries := 0 ; num Retries < max Retries ; num Retries ++ { gce Cloud Provider , ok := cloud if ! ok || gce Cloud Provider == nil { // Retry on error. See issue #11321 klog . Errorf ( " " , cloud time . Sleep ( error Sleep return gce Cloud } 
func udevadm Change To New Drives ( sd Before Set sets . String ) error { sd After , err := filepath . Glob ( disk SD if err != nil { return fmt . Errorf ( " \" \" \r " , disk SD for _ , sd := range sd After { if ! sd Before Set . Has ( sd ) { return udevadm Change To } 
func udevadm Change To Drive ( drive Path string ) error { klog . V ( 5 ) . Infof ( " " , drive // Evaluate symlink, if any drive , err := filepath . Eval Symlinks ( drive if err != nil { return fmt . Errorf ( " " , drive // Check to make sure input is "/dev/sd*" if ! strings . Contains ( drive , disk SD Path ) { return fmt . Errorf ( " \" \" " , disk SD // Call "udevadm trigger --action=change --property-match=DEVNAME=/dev/sd..." _ , err = exec . New ( ) . Command ( " " , " " , " " , fmt . Sprintf ( " " , drive ) ) . Combined } 
func is Regional PD ( spec * volume . Spec ) bool { if spec . Persistent Volume != nil { zones Label := spec . Persistent Volume . Labels [ v1 . Label Zone Failure zones := strings . Split ( zones Label , cloudvolume . Label Multi Zone } 
func Parse Resource Arg ( arg string ) ( * Group Version Resource , Group Resource ) { var gvr * Group Version if strings . Count ( arg , " " ) >= 2 { s := strings . Split gvr = & Group Version return gvr , Parse Group } 
func Parse Kind Arg ( arg string ) ( * Group Version Kind , Group Kind ) { var gvk * Group Version if strings . Count ( arg , " " ) >= 2 { s := strings . Split gvk = & Group Version return gvk , Parse Group } 
func Parse Group Resource ( gr string ) Group Resource { if i := strings . Index ( gr , " " ) ; i >= 0 { return Group return Group } 
func ( gvk Group Version } 
func ( gv Group Version ) Kind For Group Version Kinds ( kinds [ ] Group Version Kind ) ( target Group Version for _ , gvk := range kinds { if gvk . Group == gv . Group { return gv . With return Group Version } 
func Parse Group Version ( gv string ) ( Group Version , error ) { // this can be the internal version for the legacy kube types // TODO once we've cleared the last uses as strings, this special case should be removed. if ( len ( gv ) == 0 ) || ( gv == " " ) { return Group switch strings . Count ( gv , " " ) { case 0 : return Group return Group default : return Group } 
func ( gv Group Version ) With Kind ( kind string ) Group Version Kind { return Group Version } 
func ( gv Group Version ) With Resource ( resource string ) Group Version Resource { return Group Version } 
func ( gvs Group Versions ) Kind For Group Version Kinds ( kinds [ ] Group Version Kind ) ( Group Version Kind , bool ) { var targets [ ] Group Version for _ , gv := range gvs { target , ok := gv . Kind For Group Version if len ( targets ) > 1 { return best return Group Version } 
func best Match ( kinds [ ] Group Version Kind , targets [ ] Group Version Kind ) Group Version } 
func ( gvk Group Version Kind ) To API Version And return gvk . Group } 
func New Init Dry Run Getter ( control Plane Name string , service Subnet string ) * Init Dry Run Getter { return & Init Dry Run Getter { control Plane Name : control Plane Name , service Subnet : service } 
func ( idr * Init Dry Run Getter ) Handle Get Action ( action core . Get Action ) ( bool , runtime . Object , error ) { funcs := [ ] func ( core . Get Action ) ( bool , runtime . Object , error ) { idr . handle Kubernetes Service , idr . handle Get Node , idr . handle System Nodes Cluster Role Binding , idr . handle Get Bootstrap } 
func ( idr * Init Dry Run Getter ) Handle List Action ( action core . List } 
func ( idr * Init Dry Run Getter ) handle Kubernetes Service ( action core . Get Action ) ( bool , runtime . Object , error ) { if action . Get Name ( ) != " " || action . Get Namespace ( ) != metav1 . Namespace Default || action . Get _ , svc Subnet , err := net . Parse CIDR ( idr . service if err != nil { return true , nil , errors . Wrapf ( err , " " , idr . service internal API Server Virtual IP , err := ipallocator . Get Indexed IP ( svc if err != nil { return true , nil , errors . Wrapf ( err , " " , svc // The only used field of this Service object is the Cluster IP, which kube-dns uses to calculate its own IP return true , & v1 . Service { Object Meta : metav1 . Object Meta { Name : " " , Namespace : metav1 . Namespace Default , Labels : map [ string ] string { " " : " " , " " : " " , } , } , Spec : v1 . Service Spec { Cluster IP : internal API Server Virtual IP . String ( ) , Ports : [ ] v1 . Service Port { { Name : " " , Port : 443 , Target Port : intstr . From } 
func ( idr * Init Dry Run Getter ) handle Get Node ( action core . Get Action ) ( bool , runtime . Object , error ) { if action . Get Name ( ) != idr . control Plane Name || action . Get return true , & v1 . Node { Object Meta : metav1 . Object Meta { Name : idr . control Plane Name , Labels : map [ string ] string { " " : idr . control Plane } 
func ( idr * Init Dry Run Getter ) handle System Nodes Cluster Role Binding ( action core . Get Action ) ( bool , runtime . Object , error ) { if action . Get Name ( ) != constants . Nodes Cluster Role Binding || action . Get // We can safely return a Not Found error here as the code will just proceed normally and don't care about modifying this clusterrolebinding // This can only happen on an upgrade; and in that case the Client Backed Dry Run Getter impl will be used return true , nil , apierrors . New Not Found ( action . Get Resource ( ) . Group } 
func ( idr * Init Dry Run Getter ) handle Get Bootstrap Token ( action core . Get Action ) ( bool , runtime . Object , error ) { if ! strings . Has Prefix ( action . Get Name ( ) , " " ) || action . Get Namespace ( ) != metav1 . Namespace System || action . Get // We can safely return a Not Found error here as the code will just proceed normally and create the Bootstrap Token return true , nil , apierrors . New Not Found ( action . Get Resource ( ) . Group } 
func ( s * Request Header Authentication Options ) To Authentication Request Header Config ( ) * authenticatorfactory . Request Header Config { if len ( s . Client CA return & authenticatorfactory . Request Header Config { Username Headers : s . Username Headers , Group Headers : s . Group Headers , Extra Header Prefixes : s . Extra Header Prefixes , Client CA : s . Client CA File , Allowed Client Names : s . Allowed } 
func ( s * Delegating Authentication Options ) get Client ( ) ( kubernetes . Interface , error ) { var client if len ( s . Remote Kube Config File ) > 0 { loading Rules := & clientcmd . Client Config Loading Rules { Explicit Path : s . Remote Kube Config loader := clientcmd . New Non Interactive Deferred Loading Client Config ( loading Rules , & clientcmd . Config client Config , err = loader . Client } else { // without the remote kubeconfig file, try to use the in-cluster config. Most addon API servers will // use this path. If it is optional, ignore errors. client Config , err = rest . In Cluster if err != nil && s . Remote Kube Config File Optional { if err != rest . Err Not In // set high qps/burst limits since this will effectively limit API server responsiveness client client return kubernetes . New For Config ( client } 
func ( name Kubernetes Instance ID ) Map To AWS Instance ID ( ) ( Instance if ! strings . Has aws if len ( tokens ) == 1 { // instance Id aws } else if len ( tokens ) == 2 { // az/instance Id aws // We sanity check the resulting volume; the two known formats are // i-12345678 and i-12345678abcdef01 if aws ID == " " || ! aws Instance Reg Match . Match String ( aws return Instance ID ( aws } 
func map To AWS Instance I Ds ( nodes [ ] * v1 . Node ) ( [ ] Instance ID , error ) { var instance I Ds [ ] Instance for _ , node := range nodes { if node . Spec . Provider instance ID , err := Kubernetes Instance ID ( node . Spec . Provider ID ) . Map To AWS Instance if err != nil { return nil , fmt . Errorf ( " " , node . Spec . Provider instance I Ds = append ( instance I Ds , instance return instance I } 
func map To AWS Instance I Ds Tolerant ( nodes [ ] * v1 . Node ) [ ] Instance ID { var instance I Ds [ ] Instance for _ , node := range nodes { if node . Spec . Provider instance ID , err := Kubernetes Instance ID ( node . Spec . Provider ID ) . Map To AWS Instance if err != nil { klog . Warningf ( " " , node . Spec . Provider instance I Ds = append ( instance I Ds , instance return instance I } 
func describe Instance ( ec2Client EC2 , instance ID Instance ID ) ( * ec2 . Instance , error ) { request := & ec2 . Describe Instances Input { Instance Ids : [ ] * string { instance ID . aws instances , err := ec2Client . Describe if len ( instances ) == 0 { return nil , fmt . Errorf ( " " , instance if len ( instances ) > 1 { return nil , fmt . Errorf ( " " , instance } 
func ( c * instance Cache ) describe All Instances Uncached ( ) ( * all Instances instances , err := c . cloud . describe m := make ( map [ Instance for _ , i := range instances { id := Instance ID ( aws . String Value ( i . Instance snapshot := & all Instances if c . snapshot != nil && snapshot . older } 
func ( c * instance Cache ) describe All Instances Cached ( criteria cache Criteria ) ( * all Instances snapshot := c . get if snapshot != nil && ! snapshot . Meets if snapshot == nil { snapshot , err = c . describe All Instances } 
func ( c * instance Cache ) get Snapshot ( ) * all Instances } 
func ( s * all Instances Snapshot ) older Than ( other * all Instances } 
func ( s * all Instances Snapshot ) Meets Criteria ( criteria cache Criteria ) bool { if criteria . Max if now . Sub ( s . timestamp ) > criteria . Max Age { klog . V ( 6 ) . Infof ( " " , criteria . Max if len ( criteria . Has Instances ) != 0 { for _ , id := range criteria . Has } 
func ( s * all Instances Snapshot ) Find Instances ( ids [ ] Instance ID ) map [ Instance ID ] * ec2 . Instance { m := make ( map [ Instance } 
func ( c * Fake Audit Sinks ) Get ( name string , options v1 . Get Options ) ( result * v1alpha1 . Audit Sink , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( auditsinks Resource , name ) , & v1alpha1 . Audit return obj . ( * v1alpha1 . Audit } 
func ( c * Fake Audit Sinks ) List ( opts v1 . List Options ) ( result * v1alpha1 . Audit Sink List , err error ) { obj , err := c . Fake . Invokes ( testing . New Root List Action ( auditsinks Resource , auditsinks Kind , opts ) , & v1alpha1 . Audit Sink label , _ , _ := testing . Extract From List list := & v1alpha1 . Audit Sink List { List Meta : obj . ( * v1alpha1 . Audit Sink List ) . List for _ , item := range obj . ( * v1alpha1 . Audit Sink } 
func ( c * Fake Audit Sinks ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Root Watch Action ( auditsinks } 
func ( c * Fake Audit Sinks ) Create ( audit Sink * v1alpha1 . Audit Sink ) ( result * v1alpha1 . Audit Sink , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Create Action ( auditsinks Resource , audit Sink ) , & v1alpha1 . Audit return obj . ( * v1alpha1 . Audit } 
func ( c * Fake Audit Sinks ) Update ( audit Sink * v1alpha1 . Audit Sink ) ( result * v1alpha1 . Audit Sink , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Update Action ( auditsinks Resource , audit Sink ) , & v1alpha1 . Audit return obj . ( * v1alpha1 . Audit } 
func ( c * Fake Audit Sinks ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( auditsinks Resource , name ) , & v1alpha1 . Audit } 
func ( c * Fake Audit Sinks ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( auditsinks Resource , list _ , err := c . Fake . Invokes ( action , & v1alpha1 . Audit Sink } 
func ( c * Fake Audit Sinks ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1alpha1 . Audit Sink , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( auditsinks Resource , name , pt , data , subresources ... ) , & v1alpha1 . Audit return obj . ( * v1alpha1 . Audit } 
func New REST ( opts Getter generic . REST Options Getter ) * REST { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & scheduling . Priority Class { } } , New List Func : func ( ) runtime . Object { return & scheduling . Priority Class List { } } , Default Qualified Resource : scheduling . Resource ( " " ) , Create Strategy : priorityclass . Strategy , Update Strategy : priorityclass . Strategy , Delete Strategy : priorityclass . Strategy , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts if err := store . Complete With } 
func ( r * REST ) Delete ( ctx context . Context , name string , options * metav1 . Delete Options ) ( runtime . Object , bool , error ) { for _ , spc := range scheduling . System Priority Classes ( ) { if name == spc . Name { return nil , false , apierrors . New } 
func ( r * Proxy REST ) New Connect Options ( ) ( runtime . Object , bool , string ) { return & api . Pod Proxy } 
func ( r * Proxy REST ) Connect ( ctx context . Context , id string , opts runtime . Object , responder rest . Responder ) ( http . Handler , error ) { proxy Opts , ok := opts . ( * api . Pod Proxy location , transport , err := pod . Resource Location ( r . Store , r . Proxy location . Path = net . Join Preserving Trailing Slash ( location . Path , proxy // Return a proxy handler that uses the desired transport, wrapped with additional proxy handling (to get URL rewriting, X-Forwarded-* headers, etc) return new Throttled Upgrade Aware Proxy } 
func ( r * Attach REST ) Connect ( ctx context . Context , name string , opts runtime . Object , responder rest . Responder ) ( http . Handler , error ) { attach Opts , ok := opts . ( * api . Pod Attach location , transport , err := pod . Attach Location ( r . Store , r . Kubelet Conn , ctx , name , attach return new Throttled Upgrade Aware Proxy } 
func ( r * Attach REST ) New Connect Options ( ) ( runtime . Object , bool , string ) { return & api . Pod Attach } 
func ( r * Exec REST ) Connect ( ctx context . Context , name string , opts runtime . Object , responder rest . Responder ) ( http . Handler , error ) { exec Opts , ok := opts . ( * api . Pod Exec location , transport , err := pod . Exec Location ( r . Store , r . Kubelet Conn , ctx , name , exec return new Throttled Upgrade Aware Proxy } 
func ( r * Exec REST ) New Connect Options ( ) ( runtime . Object , bool , string ) { return & api . Pod Exec } 
func ( r * Port Forward REST ) New Connect Options ( ) ( runtime . Object , bool , string ) { return & api . Pod Port Forward } 
func ( r * Port Forward REST ) Connect ( ctx context . Context , name string , opts runtime . Object , responder rest . Responder ) ( http . Handler , error ) { port Forward Opts , ok := opts . ( * api . Pod Port Forward location , transport , err := pod . Port Forward Location ( r . Store , r . Kubelet Conn , ctx , name , port Forward return new Throttled Upgrade Aware Proxy } 
func New Plugin Initializer ( cloud Config [ ] byte , rest Mapper meta . REST Mapper , quota Configuration quota . Configuration , ) * Plugin Initializer { return & Plugin Initializer { cloud Config : cloud Config , rest Mapper : rest Mapper , quota Configuration : quota } 
func ( i * Plugin Initializer ) Initialize ( plugin admission . Interface ) { if wants , ok := plugin . ( Wants Cloud Config ) ; ok { wants . Set Cloud Config ( i . cloud if wants , ok := plugin . ( Wants REST Mapper ) ; ok { wants . Set REST Mapper ( i . rest if wants , ok := plugin . ( Wants Quota Configuration ) ; ok { wants . Set Quota Configuration ( i . quota } 
func add Known Types ( scheme * runtime . Scheme ) error { scheme . Add Known Types ( Scheme Group Version , & Custom Resource Definition { } , & Custom Resource Definition List { } , & Conversion metav1 . Add To Group Version ( scheme , Scheme Group } 
func New Cmd Config Get Clusters ( out io . Writer , config Access clientcmd . Config Access ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : i18n . T ( " " ) , Long : " " , Example : get Clusters Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check Err ( run Get Clusters ( out , config } 
} 
func Is Corrupted var underlying case * os . Path Error : underlying case * os . Link Error : underlying case * os . Syscall Error : underlying return underlying Error == syscall . ENOTCONN || underlying Error == syscall . ESTALE || underlying Error == syscall . EIO || underlying } 
func Validate Priority Class ( pc * scheduling . Priority Class ) field . Error List { all Errs := field . Error all Errs = append ( all Errs , apivalidation . Validate Object Meta ( & pc . Object Meta , false , apimachineryvalidation . Name Is DNS Subdomain , field . New // If the priority Class starts with a system prefix, it must be one of the // predefined system priority classes. if strings . Has Prefix ( pc . Name , scheduling . System Priority Class Prefix ) { if is , err := scheduling . Is Known System Priority Class ( pc ) ; ! is { all Errs = append ( all Errs , field . Forbidden ( field . New Path ( " " , " " ) , " " + scheduling . System Priority Class } else if pc . Value > scheduling . Highest User Definable Priority { // Non-system critical priority classes are not allowed to have a value larger than Highest User Definable Priority. all Errs = append ( all Errs , field . Forbidden ( field . New Path ( " " ) , fmt . Sprintf ( " " , scheduling . Highest User Definable return all } 
func Validate Priority Class Update ( pc , old Pc * scheduling . Priority Class ) field . Error List { all Errs := apivalidation . Validate Object Meta Update ( & pc . Object Meta , & old Pc . Object Meta , field . New // Name is immutable and is checked by the Object Meta validator. if pc . Value != old Pc . Value { all Errs = append ( all Errs , field . Forbidden ( field . New return all } 
func ( o * Persistent Volume Binder Controller Options ) Add Flags ( fs * pflag . Flag fs . Duration Var ( & o . PV Claim Binder Sync Period . Duration , " " , o . PV Claim Binder Sync fs . String Var ( & o . Volume Configuration . Persistent Volume Recycler Configuration . Pod Template File Path NFS , " " , o . Volume Configuration . Persistent Volume Recycler Configuration . Pod Template File Path fs . Int32Var ( & o . Volume Configuration . Persistent Volume Recycler Configuration . Minimum Timeout NFS , " " , o . Volume Configuration . Persistent Volume Recycler Configuration . Minimum Timeout fs . Int32Var ( & o . Volume Configuration . Persistent Volume Recycler Configuration . Increment Timeout NFS , " " , o . Volume Configuration . Persistent Volume Recycler Configuration . Increment Timeout fs . String Var ( & o . Volume Configuration . Persistent Volume Recycler Configuration . Pod Template File Path Host Path , " " , o . Volume Configuration . Persistent Volume Recycler Configuration . Pod Template File Path Host fs . Int32Var ( & o . Volume Configuration . Persistent Volume Recycler Configuration . Minimum Timeout Host Path , " " , o . Volume Configuration . Persistent Volume Recycler Configuration . Minimum Timeout Host fs . Int32Var ( & o . Volume Configuration . Persistent Volume Recycler Configuration . Increment Timeout Host Path , " " , o . Volume Configuration . Persistent Volume Recycler Configuration . Increment Timeout Host fs . Bool Var ( & o . Volume Configuration . Enable Host Path Provisioning , " " , o . Volume Configuration . Enable Host Path fs . Bool Var ( & o . Volume Configuration . Enable Dynamic Provisioning , " " , o . Volume Configuration . Enable Dynamic fs . String Var ( & o . Volume Configuration . Flex Volume Plugin Dir , " " , o . Volume Configuration . Flex Volume Plugin } 
func ( o * Persistent Volume Binder Controller Options ) Apply To ( cfg * persistentvolumeconfig . Persistent Volume Binder Controller cfg . PV Claim Binder Sync Period = o . PV Claim Binder Sync cfg . Volume Configuration = o . Volume } 
func ( o * Persistent Volume Binder Controller } 
func parse Ports ( ports [ ] string ) ( [ ] Forwarded Port , error ) { var forwards [ ] Forwarded for _ , port String := range ports { parts := strings . Split ( port var local String , remote if len ( parts ) == 1 { local remote } else if len ( parts ) == 2 { local if local String == " " { // support :5000 local remote } else { return nil , fmt . Errorf ( " " , port local Port , err := strconv . Parse Uint ( local if err != nil { return nil , fmt . Errorf ( " " , local remote Port , err := strconv . Parse Uint ( remote if err != nil { return nil , fmt . Errorf ( " " , remote if remote forwards = append ( forwards , Forwarded Port { uint16 ( local Port ) , uint16 ( remote } 
func New ( dialer httpstream . Dialer , ports [ ] string , stop Chan <- chan struct { } , ready Chan chan struct { } , out , err Out io . Writer ) ( * Port Forwarder , error ) { return New On Addresses ( dialer , [ ] string { " " } , ports , stop Chan , ready Chan , out , err } 
func New On Addresses ( dialer httpstream . Dialer , addresses [ ] string , ports [ ] string , stop Chan <- chan struct { } , ready Chan chan struct { } , out , err Out io . Writer ) ( * Port parsed Addresses , err := parse parsed Ports , err := parse return & Port Forwarder { dialer : dialer , addresses : parsed Addresses , ports : parsed Ports , stop Chan : stop Chan , Ready : ready Chan , out : out , err Out : err } 
func ( pf * Port Forwarder ) Forward pf . stream Conn , _ , err = pf . dialer . Dial ( Port Forward Protocol defer pf . stream } 
func ( pf * Port listen err = pf . listen On switch { case err == nil : listen default : if pf . err Out != nil { fmt . Fprintf ( pf . err if ! listen // wait for interrupt or conn closure select { case <- pf . stop Chan : case <- pf . stream Conn . Close Chan ( ) : runtime . Handle } 
func ( pf * Port Forwarder ) listen On Port ( port * Forwarded fail success for _ , addr := range pf . addresses { err := pf . listen On Port And fail Counters [ addr . failure } else { success Counters [ addr . failure if success Counters [ " " ] == 0 && fail if fail } 
func ( pf * Port Forwarder ) listen On Port And Address ( port * Forwarded Port , protocol string , address string ) error { listener , err := pf . get go pf . wait For } 
func ( pf * Port Forwarder ) get Listener ( protocol string , hostname string , port * Forwarded Port ) ( net . Listener , error ) { listener , err := net . Listen ( protocol , net . Join Host listener host , local Port , _ := net . Split Host Port ( listener local Port U Int , err := strconv . Parse Uint ( local if err != nil { fmt . Fprintf ( pf . out , " \n " , hostname , local Port U return nil , fmt . Errorf ( " " , err , listener port . Local = uint16 ( local Port U if pf . out != nil { fmt . Fprintf ( pf . out , " \n " , net . Join Host Port ( hostname , strconv . Itoa ( int ( local Port U } 
func ( pf * Port Forwarder ) wait For Connection ( listener net . Listener , port Forwarded if err != nil { // TODO consider using something like https://github.com/hydrogen18/stoppable Listener? if ! strings . Contains ( strings . To Lower ( err . Error ( ) ) , " " ) { runtime . Handle go pf . handle } 
func ( pf * Port Forwarder ) Get Ports ( ) ( [ ] Forwarded } 
func ( e Type Element ) Merge ( v Strategy ) ( Result , error ) { return v . Merge } 
func ( e Type Element ) Has Conflict ( ) error { for _ , item := range e . Get Values ( ) { if item , ok := item . ( Conflict Detector ) ; ok { if err := item . Has } 
func ( g * Cloud ) update Internal Load Balancer ( cluster Name , cluster ID string , svc * v1 . Service , nodes [ ] * v1 . Node ) error { g . shared Resource defer g . shared Resource ig Name := make Instance Group Name ( cluster ig Links , err := g . ensure Internal Instance Groups ( ig // Generate the backend service name _ , protocol := get Ports And scheme := cloud . Scheme load Balancer Name := g . Get Load Balancer Name ( context . TODO ( ) , cluster backend Service Name := make Backend Service Name ( load Balancer Name , cluster ID , share Backend Service ( svc ) , scheme , protocol , svc . Spec . Session // Ensure the backend service has the proper backend/instance-group links return g . ensure Internal Backend Service Groups ( backend Service Name , ig } 
func ( g * Cloud ) ensure Internal Instance Groups ( name string , nodes [ ] * v1 . Node ) ( [ ] string , error ) { zoned Nodes := split Nodes By klog . V ( 2 ) . Infof ( " " , name , len ( nodes ) , len ( zoned var ig for zone , nodes := range zoned Nodes { ig Link , err := g . ensure Internal Instance ig Links = append ( ig Links , ig return ig } 
func ( g * Cloud ) ensure Internal Backend Service Groups ( name string , ig bs , err := g . Get Region Backend backends := backends From Group Links ( ig if backends List if err := g . Update Region Backend } 
func merge Health Checks ( hc , new HC * compute . Health Check ) * compute . Health Check { if hc . Check Interval Sec > new HC . Check Interval Sec { new HC . Check Interval Sec = hc . Check Interval if hc . Timeout Sec > new HC . Timeout Sec { new HC . Timeout Sec = hc . Timeout if hc . Unhealthy Threshold > new HC . Unhealthy Threshold { new HC . Unhealthy Threshold = hc . Unhealthy if hc . Healthy Threshold > new HC . Healthy Threshold { new HC . Healthy Threshold = hc . Healthy return new } 
func need To Update Health Checks ( hc , new HC * compute . Health Check ) bool { if hc . Http Health Check == nil || new HC . Http Health changed := hc . Http Health Check . Port != new HC . Http Health Check . Port || hc . Http Health Check . Request Path != new HC . Http Health Check . Request Path || hc . Description != new changed = changed || hc . Check Interval Sec < new HC . Check Interval Sec || hc . Timeout Sec < new HC . Timeout changed = changed || hc . Unhealthy Threshold < new HC . Unhealthy Threshold || hc . Healthy Threshold < new HC . Healthy } 
func backends List a Set := sets . New for _ , v := range a { a b Set := sets . New for _ , v := range b { b return a Set . Equal ( b } 
func New Custom Columns Printer From Template ( template Reader io . Reader , decoder runtime . Decoder ) ( * Custom Columns Printer , error ) { scanner := bufio . New Scanner ( template headers := split On specs := split On for ix := range headers { spec , err := Relaxed JSON Path columns [ ix ] = Column { Header : headers [ ix ] , Field return & Custom Columns Printer { Columns : columns , Decoder : decoder , No } 
func ( v Element Building Visitor ) Create Primitive Element ( item * primitive Item ) ( apply . Element , error ) { return v . primitive } 
func ( v Element Building Visitor ) Create List Element ( item * list Item ) ( apply . Element , error ) { meta , err := get Field Meta ( item . Get if meta . Get Field Merge Type ( ) == apply . Merge Strategy { return v . merge List return v . replace List } 
func ( v Element Building Visitor ) Create Map Element ( item * map Item ) ( apply . Element , error ) { meta , err := get Field Meta ( item . Get return v . map } 
func ( v Element Building Visitor ) Create Type Element ( item * type Item ) ( apply . Element , error ) { meta , err := get Field Meta ( item . Get return v . type } 
func enforce Requirements ( flags * apply Plan Flags , dry Run bool , new K8s Version string ) ( clientset . Interface , upgrade . Version Getter , * kubeadmapi . Init Configuration , error ) { ignore Preflight Errors Set , err := validation . Validate Ignore Preflight Errors ( flags . ignore Preflight if err := run Preflight Checks ( ignore Preflight Errors client , err := get Client ( flags . kube Config Path , dry if err != nil { return nil , nil , nil , errors . Wrapf ( err , " " , flags . kube Config // Check if the cluster is self-hosted if upgrade . Is Control Plane Self // Run healthchecks against the cluster if err := upgrade . Check Cluster Health ( client , ignore Preflight Errors // Fetch the configuration from a file or Config var cfg * kubeadmapi . Init if flags . cfg Path != " " { cfg , err = configutil . Load Init Configuration From File ( flags . cfg } else { cfg , err = configutil . Fetch Init Configuration From if err != nil { if apierrors . Is Not Found ( err ) { fmt . Printf ( " \n " , constants . Kubeadm Config Config Map , metav1 . Namespace err = errors . Errorf ( " " , constants . Kubeadm Config Config Map , metav1 . Namespace // If a new k8s version should be set, apply the change before printing the config if len ( new K8s Version ) != 0 { cfg . Kubernetes Version = new K8s // If features gates are passed to the command line, use it (otherwise use feature Gates from configuration) if flags . feature Gates String != " " { cfg . Feature Gates , err = features . New Feature Gate ( & features . Init Feature Gates , flags . feature Gates // Check if feature gate flags used in the cluster are consistent with the set of features currently supported by kubeadm if msg := features . Check Deprecated Flags ( & features . Init Feature Gates , cfg . Feature // If the user told us to print this information out; do it! if flags . print Config { print Configuration ( & cfg . Cluster // Use a real version getter interface that queries the API server, the kubeadm client and the Kubernetes CI system for latest versions return client , upgrade . New Offline Version Getter ( upgrade . New Kube Version Getter ( client , os . Stdout ) , new K8s } 
func print Configuration ( clustercfg * kubeadmapi . Cluster cfg Yaml , err := configutil . Marshal Kubeadm Config scanner := bufio . New Scanner ( bytes . New Reader ( cfg } 
func run Preflight Checks ( ignore Preflight return preflight . Run Root Check Only ( ignore Preflight } 
func get Client ( file string , dry Run bool ) ( clientset . Interface , error ) { if dry Run { dry Run Getter , err := apiclient . New Client Backed Dry Run Getter From // In order for fakeclient.Discovery().Server Version() to return the backing API Server's // real version; we have to do some clever API machinery tricks. First, we get the real // API Server's version real Server Version , err := dry Run Getter . Client ( ) . Discovery ( ) . Server // Get the fake clientset dry Run Opts := apiclient . Get Default Dry Run Client Options ( dry Run // Print GET and LIST requests dry Run Opts . Print GET And fakeclient := apiclient . New Dry Run Client With Opts ( dry Run // As we know the return of Discovery() of the fake clientset is of type *fakediscovery.Fake Discovery // we can convert it to that struct. fakeclient Discovery , ok := fakeclient . Discovery ( ) . ( * fakediscovery . Fake // Lastly, set the right server version to be used fakeclient Discovery . Faked Server Version = real Server return kubeconfigutil . Client Set From } 
func get Waiter ( dry Run bool , client clientset . Interface ) apiclient . Waiter { if dry Run { return dryrunutil . New return apiclient . New Kube Waiter ( client , upgrade . Upgrade Manifest } 
func Interactively Confirm scanner := bufio . New if strings . To Lower ( answer ) == " " || strings . To } 
func ( attacher * rbd Attacher ) Volumes Are Attached ( specs [ ] * volume . Spec , node Name types . Node Name ) ( map [ * volume . Spec ] bool , error ) { volumes Attached for _ , spec := range specs { volumes Attached return volumes Attached } 
func ( attacher * rbd Attacher ) Wait For Attach ( spec * volume . Spec , device mounter , err := attacher . plugin . create Mounter From Volume Spec And real Device Path , err := attacher . manager . Attach klog . V ( 3 ) . Infof ( " " , spec . Name ( ) , mounter . Pool , mounter . Image , real Device return real Device } 
func ( attacher * rbd Attacher ) Get Device Mount Path ( spec * volume . Spec ) ( string , error ) { img , err := get Volume Source pool , err := get Volume Source return make PD Name } 
func ( attacher * rbd Attacher ) Mount Device ( spec * volume . Spec , device Path string , device Mount Path string ) error { klog . V ( 4 ) . Infof ( " " , device Path , device Mount not Mnt , err := attacher . mounter . Is Likely Not Mount Point ( device Mount if err != nil { if os . Is Not Exist ( err ) { if err := os . Mkdir All ( device Mount not if ! not fstype , err := get Volume Source FS ro , err := get Volume Source Read mount Options := volutil . Mount Option From err = attacher . mounter . Format And Mount ( device Path , device Mount Path , fstype , mount if err != nil { os . Remove ( device Mount return fmt . Errorf ( " " , device Path , device Mount klog . V ( 3 ) . Infof ( " " , device Path , device Mount } 
func ( detacher * rbd Detacher ) Unmount Device ( device Mount Path string ) error { if path Exists , path Err := mount . Path Exists ( device Mount Path ) ; path Err != nil { return fmt . Errorf ( " " , path } else if ! path Exists { klog . Warningf ( " " , device Mount device Path , _ , err := mount . Get Device Name From Mount ( detacher . mounter , device Mount // Unmount the device from the device mount point. klog . V ( 4 ) . Infof ( " " , device Mount if err = detacher . mounter . Unmount ( device Mount klog . V ( 3 ) . Infof ( " " , device Mount klog . V ( 4 ) . Infof ( " " , device err = detacher . manager . Detach Disk ( detacher . plugin , device Mount Path , device klog . V ( 3 ) . Infof ( " " , device err = os . Remove ( device Mount klog . V ( 3 ) . Infof ( " " , device Mount } 
func ( s * cron Job Lister ) Cron Jobs ( namespace string ) Cron Job Namespace Lister { return cron Job Namespace } 
func ( s cron Job Namespace Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Cron Job , err error ) { err = cache . List All By Namespace ( s . indexer , s . namespace , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Cron } 
func New Cmd Plugin List ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := & Plugin List Options { IO cmd := & cobra . Command { Use : " " , Short : " " , Long : plugin List Long , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmd . Flags ( ) . Bool Var ( & o . Name Only , " " , o . Name } 
func ( v * Command Override bin cmd Path := strings . Split ( bin if len ( cmd Path ) > 1 { // the first argument is always "kubectl" for a plugin binary cmd Path = cmd if is Exec , err := is Executable ( path ) ; err == nil && ! is if existing Path , ok := v . seen Plugins [ bin Name ] ; ok { errors = append ( errors , fmt . Errorf ( " " , path , existing } else { v . seen Plugins [ bin if cmd , _ , err := v . root . Find ( cmd Path ) ; err == nil { errors = append ( errors , fmt . Errorf ( " " , bin Name , cmd . Command } 
func unique Paths new new Paths = append ( new return new } 
func parse var error for { // parse key equal Index := next if equal Index == - 1 { return nil , error key := strings . Trim Space ( statements [ cursor : equal // parse value cursor = next None Space ( statements , equal if cursor == - 1 { return nil , error // like I said, not handling escapes, but this will skip any comma that's // within the quotes which is somewhat more likely close Quote Index := next if close Quote Index == - 1 { return nil , error value := statements [ cursor : close Quote comma Index := next None Space ( statements , close Quote if comma } else if statements [ comma Index ] != ',' { // expect comma immediately after close quote return nil , error } else { cursor = comma } else { comma Index := next end Statements := comma if end Statements { untrimmed = statements [ cursor : comma value := strings . Trim if len ( value ) == 0 { // disallow empty value without quote return nil , error if end cursor = comma } 
func New Selector Spread Priority ( service Lister algorithm . Service Lister , controller Lister algorithm . Controller Lister , replica Set Lister algorithm . Replica Set Lister , stateful Set Lister algorithm . Stateful Set Lister ) ( Priority Map Function , Priority Reduce Function ) { selector Spread := & Selector Spread { service Lister : service Lister , controller Lister : controller Lister , replica Set Lister : replica Set Lister , stateful Set Lister : stateful Set return selector Spread . Calculate Spread Priority Map , selector Spread . Calculate Spread Priority } 
func ( s * Selector Spread ) Calculate Spread Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host node := node if node == nil { return schedulerapi . Host priority Meta , ok := meta . ( * priority if ok { selectors = priority Meta . pod } else { selectors = get Selectors ( pod , s . service Lister , s . controller Lister , s . replica Set Lister , s . stateful Set if len ( selectors ) == 0 { return schedulerapi . Host count := count Matching Pods ( pod . Namespace , selectors , node return schedulerapi . Host } 
func ( s * Selector Spread ) Calculate Spread Priority Reduce ( pod * v1 . Pod , meta interface { } , node Name To Info map [ string ] * schedulernodeinfo . Node Info , result schedulerapi . Host Priority List ) error { counts By max Count By max Count By Node for i := range result { if result [ i ] . Score > max Count By Node Name { max Count By Node zone ID := utilnode . Get Zone Key ( node Name To if zone counts By Zone [ zone for zone ID := range counts By Zone { if counts By Zone [ zone ID ] > max Count By Zone { max Count By Zone = counts By Zone [ zone have Zones := len ( counts By max Count By Node Name Float64 := float64 ( max Count By Node max Count By Zone Float64 := float64 ( max Count By Max Priority Float64 := float64 ( schedulerapi . Max for i := range result { // initializing to the default/max node score of max Priority f Score := Max Priority if max Count By Node Name > 0 { f Score = Max Priority Float64 * ( float64 ( max Count By Node Name - result [ i ] . Score ) / max Count By Node Name // If there is zone information present, incorporate it if have Zones { zone ID := utilnode . Get Zone Key ( node Name To if zone ID != " " { zone Score := Max Priority if max Count By Zone > 0 { zone Score = Max Priority Float64 * ( float64 ( max Count By Zone - counts By Zone [ zone ID ] ) / max Count By Zone f Score = ( f Score * ( 1.0 - zone Weighting ) ) + ( zone Weighting * zone result [ i ] . Score = int ( f if klog . V ( 10 ) { klog . Infof ( " " , pod . Name , result [ i ] . Host , int ( f } 
func New Service Anti Affinity Priority ( pod Lister algorithm . Pod Lister , service Lister algorithm . Service Lister , label string ) ( Priority Map Function , Priority Reduce Function ) { anti Affinity := & Service Anti Affinity { pod Lister : pod Lister , service Lister : service return anti Affinity . Calculate Anti Affinity Priority Map , anti Affinity . Calculate Anti Affinity Priority } 
func ( s * Service Anti Affinity ) get Node Classification By Labels ( nodes [ ] * v1 . Node ) ( map [ string ] string , [ ] string ) { labeled non Labeled labeled } else { non Labeled Nodes = append ( non Labeled return labeled Nodes , non Labeled } 
func count Matching Pods ( namespace string , selectors [ ] labels . Selector , node Info * schedulernodeinfo . Node Info ) int { if node Info . Pods ( ) == nil || len ( node for _ , pod := range node Info . Pods ( ) { // Ignore pods being deleted for spreading purposes // Similar to how it is done for Selector Spread Priority if namespace == pod . Namespace && pod . Deletion } 
func ( s * Service Anti Affinity ) Calculate Anti Affinity Priority Map ( pod * v1 . Pod , meta interface { } , node Info * schedulernodeinfo . Node Info ) ( schedulerapi . Host Priority , error ) { var first Service node := node if node == nil { return schedulerapi . Host priority Meta , ok := meta . ( * priority if ok { first Service Selector = priority Meta . pod First Service } else { first Service Selector = get First Service Selector ( pod , s . service if first Service Selector != nil { selectors = append ( selectors , first Service score := count Matching Pods ( pod . Namespace , selectors , node return schedulerapi . Host } 
func ( s * Service Anti Affinity ) Calculate Anti Affinity Priority Reduce ( pod * v1 . Pod , meta interface { } , node Name To Info map [ string ] * schedulernodeinfo . Node Info , result schedulerapi . Host Priority List ) error { var num Service pod label Nodes max Priority Float64 := float64 ( schedulerapi . Max for _ , host Priority := range result { num Service Pods += host if ! labels . Set ( node Name To Info [ host label = labels . Set ( node Name To Info [ host label Nodes Status [ host pod Counts [ label ] += host //score int - scale of 0-max Priority // 0 being the lowest priority and max Priority being the highest for i , host Priority := range result { label , ok := label Nodes Status [ host if ! ok { result [ i ] . Host = host // initializing to the default/max node score of max Priority f Score := max Priority if num Service Pods > 0 { f Score = max Priority Float64 * ( float64 ( num Service Pods - pod Counts [ label ] ) / float64 ( num Service result [ i ] . Host = host result [ i ] . Score = int ( f } 
func New Node Config Status ( ) Node Config Status { // channels must have capacity at least 1, since we signal with non-blocking writes sync // prime new status managers to sync with the API server on the first call to Sync sync return & node Config Status { sync Ch : sync } 
func ( s * node Config } 
func ( s * node Config Status ) Sync ( client clientset . Interface , node Name string ) { select { case <- s . sync // get the Node so we can check the current status old Node , err := client . Core V1 ( ) . Nodes ( ) . Get ( node Name , metav1 . Get if err != nil { err = fmt . Errorf ( " " , node // override error, if necessary if len ( s . error Override ) > 0 { // copy the status, so we don't overwrite the prior error // with the override status = status . Deep status . Error = s . error // update metrics based on the status we will sync metrics . Set Config err = metrics . Set Assigned err = metrics . Set Active err = metrics . Set Last Known Good Config ( status . Last Known // apply the status to a copy of the node so we don't modify the object in the informer's store new Node := old Node . Deep new // patch the node with the new status if _ , _ , err := nodeutil . Patch Node Status ( client . Core V1 ( ) , types . Node Name ( node Name ) , old Node , new } 
func ( c * Fake Flunders ) Get ( name string , options v1 . Get Options ) ( result * v1beta1 . Flunder , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( flunders } 
func ( c * Fake Flunders ) Create ( flunder * v1beta1 . Flunder ) ( result * v1beta1 . Flunder , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( flunders } 
func ( c * Fake Flunders ) Update ( flunder * v1beta1 . Flunder ) ( result * v1beta1 . Flunder , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( flunders } 
func ( c * Fake Flunders ) Update Status ( flunder * v1beta1 . Flunder ) ( * v1beta1 . Flunder , error ) { obj , err := c . Fake . Invokes ( testing . New Update Subresource Action ( flunders } 
func ( c * Fake Flunders ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( flunders Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & v1beta1 . Flunder } 
func ( c * Fake Flunders ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * v1beta1 . Flunder , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( flunders } 
func ( s * pod Disruption Budget Lister ) Get Pod Pod Disruption Budgets ( pod * v1 . Pod ) ( [ ] * policy . Pod Disruption list , err := s . Pod Disruption var pdb List [ ] * policy . Pod Disruption selector , err = metav1 . Label Selector As pdb List = append ( pdb if len ( pdb return pdb } 
func Deep Hash Object ( hasher hash . Hash , object To printer := spew . Config State { Indent : " " , Sort Keys : true , Disable Methods : true , Spew printer . Fprintf ( hasher , " " , object To } 
func Get Default Mutators ( ) map [ string ] [ ] Pod Spec Mutator Func { return map [ string ] [ ] Pod Spec Mutator Func { kubeadmconstants . Kube API Server : { add Node Selector To Pod Spec , set Control Plane Toleration On Pod Spec , set Right DNS Policy On Pod Spec , set Host IP On Pod Spec , } , kubeadmconstants . Kube Controller Manager : { add Node Selector To Pod Spec , set Control Plane Toleration On Pod Spec , set Right DNS Policy On Pod Spec , } , kubeadmconstants . Kube Scheduler : { add Node Selector To Pod Spec , set Control Plane Toleration On Pod Spec , set Right DNS Policy On Pod } 
func Get Mutators From Feature Gates ( certs In Secrets bool ) map [ string ] [ ] Pod Spec Mutator Func { // Here the map of different mutators to use for the control plane's podspec is stored mutators := Get Default if certs In Secrets { // Some extra work to be done if we should store the control plane certificates in Secrets // Add the store-certs-in-secrets-specific mutators here so that the self-hosted component starts using them mutators [ kubeadmconstants . Kube API Server ] = append ( mutators [ kubeadmconstants . Kube API Server ] , set Self Hosted Volumes For API mutators [ kubeadmconstants . Kube Controller Manager ] = append ( mutators [ kubeadmconstants . Kube Controller Manager ] , set Self Hosted Volumes For Controller mutators [ kubeadmconstants . Kube Scheduler ] = append ( mutators [ kubeadmconstants . Kube Scheduler ] , set Self Hosted Volumes For } 
func mutate Pod Spec ( mutators map [ string ] [ ] Pod Spec Mutator Func , name string , pod Spec * v1 . Pod Spec ) { // Get the mutator functions for the component in question, then loop through and execute them mutators For for _ , mutate Func := range mutators For Component { mutate Func ( pod } 
func add Node Selector To Pod Spec ( pod Spec * v1 . Pod Spec ) { if pod Spec . Node Selector == nil { pod Spec . Node Selector = map [ string ] string { kubeadmconstants . Label Node Role pod Spec . Node Selector [ kubeadmconstants . Label Node Role } 
func set Control Plane Toleration On Pod Spec ( pod Spec * v1 . Pod Spec ) { if pod Spec . Tolerations == nil { pod Spec . Tolerations = [ ] v1 . Toleration { kubeadmconstants . Control Plane pod Spec . Tolerations = append ( pod Spec . Tolerations , kubeadmconstants . Control Plane } 
func set Host IP On Pod Spec ( pod Spec * v1 . Pod Spec ) { env Var := v1 . Env Var { Name : " " , Value From : & v1 . Env Var Source { Field Ref : & v1 . Object Field Selector { Field pod Spec . Containers [ 0 ] . Env = append ( pod Spec . Containers [ 0 ] . Env , env for i := range pod Spec . Containers [ 0 ] . Command { if strings . Contains ( pod Spec . Containers [ 0 ] . Command [ i ] , " " ) { pod } 
func set Self Hosted Volumes For API Server ( pod Spec * v1 . Pod Spec ) { for i , v := range pod Spec . Volumes { // If the volume name matches the expected one; switch the volume source from host Path to cluster-hosted if v . Name == kubeadmconstants . Kube Certificates Volume Name { pod Spec . Volumes [ i ] . Volume Source = api Server Certificates Volume } 
func set Self Hosted Volumes For Controller Manager ( pod Spec * v1 . Pod Spec ) { for i , v := range pod Spec . Volumes { // If the volume name matches the expected one; switch the volume source from host Path to cluster-hosted if v . Name == kubeadmconstants . Kube Certificates Volume Name { pod Spec . Volumes [ i ] . Volume Source = controller Manager Certificates Volume } else if v . Name == kubeadmconstants . Kube Config Volume Name { pod Spec . Volumes [ i ] . Volume Source = kube Config Volume Source ( kubeadmconstants . Controller Manager Kube Config File // Change directory for the kubeconfig directory to self Hosted Kube Config Dir for i , vm := range pod Spec . Containers [ 0 ] . Volume Mounts { if vm . Name == kubeadmconstants . Kube Config Volume Name { pod Spec . Containers [ 0 ] . Volume Mounts [ i ] . Mount Path = self Hosted Kube Config // Rewrite the --kubeconfig path as the volume mount path may not overlap with certs dir, which it does by default (/etc/kubernetes and /etc/kubernetes/pki) // This is not a problem with host Path mounts as host Path supports mounting one file only, instead of always a full directory. Secrets and Projected Volumes // don't support that. pod Spec . Containers [ 0 ] . Command = kubeadmutil . Replace Argument ( pod Spec . Containers [ 0 ] . Command , func ( arg Map map [ string ] string ) map [ string ] string { controller Manager Kube Config Path := filepath . Join ( self Hosted Kube Config Dir , kubeadmconstants . Controller Manager Kube Config File arg Map [ " " ] = controller Manager Kube Config if _ , ok := arg Map [ " " ] ; ok { arg Map [ " " ] = controller Manager Kube Config if _ , ok := arg Map [ " " ] ; ok { arg Map [ " " ] = controller Manager Kube Config return arg } 
func set Self Hosted Volumes For Scheduler ( pod Spec * v1 . Pod Spec ) { for i , v := range pod Spec . Volumes { // If the volume name matches the expected one; switch the volume source from host Path to cluster-hosted if v . Name == kubeadmconstants . Kube Config Volume Name { pod Spec . Volumes [ i ] . Volume Source = kube Config Volume Source ( kubeadmconstants . Scheduler Kube Config File // Change directory for the kubeconfig directory to self Hosted Kube Config Dir for i , vm := range pod Spec . Containers [ 0 ] . Volume Mounts { if vm . Name == kubeadmconstants . Kube Config Volume Name { pod Spec . Containers [ 0 ] . Volume Mounts [ i ] . Mount Path = self Hosted Kube Config // Rewrite the --kubeconfig path as the volume mount path may not overlap with certs dir, which it does by default (/etc/kubernetes and /etc/kubernetes/pki) // This is not a problem with host Path mounts as host Path supports mounting one file only, instead of always a full directory. Secrets and Projected Volumes // don't support that. pod Spec . Containers [ 0 ] . Command = kubeadmutil . Replace Argument ( pod Spec . Containers [ 0 ] . Command , func ( arg Map map [ string ] string ) map [ string ] string { arg Map [ " " ] = filepath . Join ( self Hosted Kube Config Dir , kubeadmconstants . Scheduler Kube Config File return arg } 
func New Sync Result ( action Sync Action , target interface { } ) * Sync Result { return & Sync } 
func ( r * Sync } 
func ( p * Pod Sync Result ) Add Sync Result ( result ... * Sync Result ) { p . Sync Results = append ( p . Sync } 
func ( p * Pod Sync Result ) Add Pod Sync Result ( result Pod Sync Result ) { p . Add Sync Result ( result . Sync p . Sync Error = result . Sync } 
func ( p * Pod Sync if p . Sync Error != nil { errlist = append ( errlist , fmt . Errorf ( " \n " , p . Sync for _ , result := range p . Sync return utilerrors . New } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Metric List Options ) ( nil ) , ( * custommetrics . Metric List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Metric List Options_To_custom_metrics_Metric List Options ( a . ( * Metric List Options ) , b . ( * custommetrics . Metric List if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric List Options ) ( nil ) , ( * Metric List Options ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric List Options_To_v1beta1_Metric List Options ( a . ( * custommetrics . Metric List Options ) , b . ( * Metric List if err := s . Add Generated Conversion Func ( ( * Metric Value ) ( nil ) , ( * custommetrics . Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Metric Value_To_custom_metrics_Metric Value ( a . ( * Metric Value ) , b . ( * custommetrics . Metric if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric Value ) ( nil ) , ( * Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric Value_To_v1beta1_Metric Value ( a . ( * custommetrics . Metric Value ) , b . ( * Metric if err := s . Add Generated Conversion Func ( ( * Metric Value List ) ( nil ) , ( * custommetrics . Metric Value List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Metric Value List_To_custom_metrics_Metric Value List ( a . ( * Metric Value List ) , b . ( * custommetrics . Metric Value if err := s . Add Generated Conversion Func ( ( * custommetrics . Metric Value List ) ( nil ) , ( * Metric Value List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric Value List_To_v1beta1_Metric Value List ( a . ( * custommetrics . Metric Value List ) , b . ( * Metric Value if err := s . Add Conversion Func ( ( * custommetrics . Metric Value ) ( nil ) , ( * Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_custom_metrics_Metric Value_To_v1beta1_Metric Value ( a . ( * custommetrics . Metric Value ) , b . ( * Metric if err := s . Add Conversion Func ( ( * Metric Value ) ( nil ) , ( * custommetrics . Metric Value ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1beta1_Metric Value_To_custom_metrics_Metric Value ( a . ( * Metric Value ) , b . ( * custommetrics . Metric } 
func Convert_v1beta1_Metric List Options_To_custom_metrics_Metric List Options ( in * Metric List Options , out * custommetrics . Metric List Options , s conversion . Scope ) error { return auto Convert_v1beta1_Metric List Options_To_custom_metrics_Metric List } 
func Convert_custom_metrics_Metric List Options_To_v1beta1_Metric List Options ( in * custommetrics . Metric List Options , out * Metric List Options , s conversion . Scope ) error { return auto Convert_custom_metrics_Metric List Options_To_v1beta1_Metric List } 
func Convert_v1beta1_Metric Value List_To_custom_metrics_Metric Value List ( in * Metric Value List , out * custommetrics . Metric Value List , s conversion . Scope ) error { return auto Convert_v1beta1_Metric Value List_To_custom_metrics_Metric Value } 
func Convert_custom_metrics_Metric Value List_To_v1beta1_Metric Value List ( in * custommetrics . Metric Value List , out * Metric Value List , s conversion . Scope ) error { return auto Convert_custom_metrics_Metric Value List_To_v1beta1_Metric Value } 
func New Control Plane Phase ( ) workflow . Phase { phase := workflow . Phase { Name : " " , Short : " " , Long : cmdutil . Macro Command Long Description , Phases : [ ] workflow . Phase { { Name : " " , Short : " " , Inherit Flags : get Control Plane Phase Flags ( " " ) , Example : control Plane Example , Run All Siblings : true , } , new Control Plane Subphase ( kubeadmconstants . Kube API Server ) , new Control Plane Subphase ( kubeadmconstants . Kube Controller Manager ) , new Control Plane Subphase ( kubeadmconstants . Kube Scheduler ) , } , Run : run Control Plane } 
func ( s * fischer Lister ) List ( selector labels . Selector ) ( ret [ ] * v1alpha1 . Fischer , err error ) { err = cache . List } 
func ( s * fischer Lister ) Get ( name string ) ( * v1alpha1 . Fischer , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not } 
func ( f User Conversion } 
func New ( opts x509 . Verify Options , user User } 
func ( a * Authenticator ) Authenticate Request ( req * http . Request ) ( * authenticator . Response , bool , error ) { if req . TLS == nil || len ( req . TLS . Peer // Use intermediates, if provided opts if opts Copy . Intermediates == nil && len ( req . TLS . Peer Certificates ) > 1 { opts Copy . Intermediates = x509 . New Cert for _ , intermediate := range req . TLS . Peer Certificates [ 1 : ] { opts Copy . Intermediates . Add remaining := req . TLS . Peer Certificates [ 0 ] . Not client Certificate Expiration chains , err := req . TLS . Peer Certificates [ 0 ] . Verify ( opts return nil , false , utilerrors . New } 
func New Verifier ( opts x509 . Verify Options , auth authenticator . Request , allowed Common Names sets . String ) authenticator . Request { return & Verifier { opts , auth , allowed Common } 
func ( a * Verifier ) Authenticate Request ( req * http . Request ) ( * authenticator . Response , bool , error ) { if req . TLS == nil || len ( req . TLS . Peer // Use intermediates, if provided opts if opts Copy . Intermediates == nil && len ( req . TLS . Peer Certificates ) > 1 { opts Copy . Intermediates = x509 . New Cert for _ , intermediate := range req . TLS . Peer Certificates [ 1 : ] { opts Copy . Intermediates . Add if _ , err := req . TLS . Peer Certificates [ 0 ] . Verify ( opts if err := a . verify Subject ( req . TLS . Peer return a . auth . Authenticate } 
func Default Verify Options ( ) x509 . Verify Options { return x509 . Verify Options { Key Usages : [ ] x509 . Ext Key Usage { x509 . Ext Key Usage Client } 
func ( cfg * Config ) Complete ( ) Completed Config { c := completed Config { cfg . Generic Config . Complete ( ) , & cfg . Extra // the kube aggregator wires its own discovery mechanism // TODO eventually collapse this by extracting all of the discovery out c . Generic Config . Enable c . Generic return Completed } 
func ( c completed Config ) New With Delegate ( delegation Target genericapiserver . Delegation Target ) ( * API Aggregator , error ) { // Prevent generic API server to install Open API handler. Aggregator server // has its own customized Open API handler. open API Config := c . Generic Config . Open API c . Generic Config . Open API generic Server , err := c . Generic Config . New ( " " , delegation apiregistration Client , err := internalclientset . New For Config ( c . Generic Config . Loopback Client informer Factory := informers . New Shared Informer Factory ( apiregistration s := & API Aggregator { Generic API Server : generic Server , delegate Handler : delegation Target . Unprotected Handler ( ) , proxy Client Cert : c . Extra Config . Proxy Client Cert , proxy Client Key : c . Extra Config . Proxy Client Key , proxy Transport : c . Extra Config . Proxy Transport , proxy Handlers : map [ string ] * proxy Handler { } , handled Groups : sets . String { } , lister : informer Factory . Apiregistration ( ) . Internal Version ( ) . API Services ( ) . Lister ( ) , API Registration Informers : informer Factory , service Resolver : c . Extra Config . Service api Group Info := apiservicerest . New REST Storage ( c . Generic Config . Merged Resource Config , c . Generic Config . REST Options if err := s . Generic API Server . Install API Group ( & api Group apis Handler := & apis s . Generic API Server . Handler . Non Go Restful Mux . Handle ( " " , apis s . Generic API Server . Handler . Non Go Restful Mux . Unlisted Handle ( " " , apis apiservice Registration Controller := New API Service Registration Controller ( informer Factory . Apiregistration ( ) . Internal Version ( ) . API available Controller , err := statuscontrollers . New Available Condition Controller ( informer Factory . Apiregistration ( ) . Internal Version ( ) . API Services ( ) , c . Generic Config . Shared Informer Factory . Core ( ) . V1 ( ) . Services ( ) , c . Generic Config . Shared Informer Factory . Core ( ) . V1 ( ) . Endpoints ( ) , apiregistration Client . Apiregistration ( ) , c . Extra Config . Proxy Transport , c . Extra Config . Proxy Client Cert , c . Extra Config . Proxy Client Key , s . service s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { informer Factory . Start ( context . Stop c . Generic Config . Shared Informer Factory . Start ( context . Stop s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { go apiservice Registration Controller . Run ( context . Stop s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { // if we end up blocking for long periods of time, we may need to increase threadiness. go available Controller . Run ( 5 , context . Stop if open API Config != nil { spec Downloader := openapiaggregator . New open API Aggregator , err := openapiaggregator . Build And Register Aggregator ( & spec Downloader , delegation Target , s . Generic API Server . Handler . Go Restful Container . Registered Web Services ( ) , open API Config , s . Generic API Server . Handler . Non Go Restful s . open API Aggregation Controller = openapicontroller . New Aggregation Controller ( & spec Downloader , open API s . Generic API Server . Add Post Start Hook Or Die ( " " , func ( context genericapiserver . Post Start Hook Context ) error { go s . open API Aggregation Controller . Run ( context . Stop } 
func ( s * API Aggregator ) Add API Service ( api Service * apiregistration . API Service ) error { // if the proxy Handler already exists, it needs to be updated. The aggregation bits do not // since they are wired against listers because they require multiple resources to respond if proxy Handler , exists := s . proxy Handlers [ api Service . Name ] ; exists { proxy Handler . update API Service ( api if s . open API Aggregation Controller != nil { s . open API Aggregation Controller . Update API Service ( proxy Handler , api proxy Path := " " + api Service . Spec . Group + " " + api // v1. is a special case for the legacy API. It proxies to a wider set of endpoints. if api Service . Name == legacy API Service Name { proxy // register the proxy handler proxy Handler := & proxy Handler { local Delegate : s . delegate Handler , proxy Client Cert : s . proxy Client Cert , proxy Client Key : s . proxy Client Key , proxy Transport : s . proxy Transport , service Resolver : s . service proxy Handler . update API Service ( api if s . open API Aggregation Controller != nil { s . open API Aggregation Controller . Add API Service ( proxy Handler , api s . proxy Handlers [ api Service . Name ] = proxy s . Generic API Server . Handler . Non Go Restful Mux . Handle ( proxy Path , proxy s . Generic API Server . Handler . Non Go Restful Mux . Unlisted Handle Prefix ( proxy Path + " " , proxy // if we're dealing with the legacy group, we're done here if api Service . Name == legacy API Service // if we've already registered the path with the handler, we don't want to do it again. if s . handled Groups . Has ( api // it's time to register the group aggregation endpoint group Path := " " + api group Discovery Handler := & api Group Handler { codecs : aggregatorscheme . Codecs , group Name : api Service . Spec . Group , lister : s . lister , delegate : s . delegate // aggregation is protected s . Generic API Server . Handler . Non Go Restful Mux . Handle ( group Path , group Discovery s . Generic API Server . Handler . Non Go Restful Mux . Unlisted Handle ( group Path + " " , group Discovery s . handled Groups . Insert ( api } 
func ( s * API Aggregator ) Remove API Service ( api Service Name string ) { version := apiregistration . API Service Name To Group Version ( api Service proxy // v1. is a special case for the legacy API. It proxies to a wider set of endpoints. if api Service Name == legacy API Service Name { proxy s . Generic API Server . Handler . Non Go Restful Mux . Unregister ( proxy s . Generic API Server . Handler . Non Go Restful Mux . Unregister ( proxy if s . open API Aggregation Controller != nil { s . open API Aggregation Controller . Remove API Service ( api Service delete ( s . proxy Handlers , api Service // TODO unregister group level discovery when there are no more versions for the group // We don't need this right away because the handler properly delegates when no versions are present } 
func Default API Resource Config Source ( ) * serverstorage . Resource Config { ret := serverstorage . New Resource // NOTE: Group Versions listed here will be enabled by default. Don't put alpha versions in the list. ret . Enable Versions ( v1 . Scheme Group Version , v1beta1 . Scheme Group } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Client Connection Configuration ) ( nil ) , ( * config . Client Connection Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Client Connection Configuration_To_config_Client Connection Configuration ( a . ( * Client Connection Configuration ) , b . ( * config . Client Connection if err := s . Add Generated Conversion Func ( ( * config . Client Connection Configuration ) ( nil ) , ( * Client Connection Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Client Connection Configuration_To_v1alpha1_Client Connection Configuration ( a . ( * config . Client Connection Configuration ) , b . ( * Client Connection if err := s . Add Generated Conversion Func ( ( * Debugging Configuration ) ( nil ) , ( * config . Debugging Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Debugging Configuration_To_config_Debugging Configuration ( a . ( * Debugging Configuration ) , b . ( * config . Debugging if err := s . Add Generated Conversion Func ( ( * config . Debugging Configuration ) ( nil ) , ( * Debugging Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Debugging Configuration_To_v1alpha1_Debugging Configuration ( a . ( * config . Debugging Configuration ) , b . ( * Debugging if err := s . Add Generated Conversion Func ( ( * Leader Election Configuration ) ( nil ) , ( * config . Leader Election Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Leader Election Configuration_To_config_Leader Election Configuration ( a . ( * Leader Election Configuration ) , b . ( * config . Leader Election if err := s . Add Generated Conversion Func ( ( * config . Leader Election Configuration ) ( nil ) , ( * Leader Election Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Leader Election Configuration_To_v1alpha1_Leader Election Configuration ( a . ( * config . Leader Election Configuration ) , b . ( * Leader Election if err := s . Add Conversion Func ( ( * config . Client Connection Configuration ) ( nil ) , ( * Client Connection Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Client Connection Configuration_To_v1alpha1_Client Connection Configuration ( a . ( * config . Client Connection Configuration ) , b . ( * Client Connection if err := s . Add Conversion Func ( ( * config . Debugging Configuration ) ( nil ) , ( * Debugging Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Debugging Configuration_To_v1alpha1_Debugging Configuration ( a . ( * config . Debugging Configuration ) , b . ( * Debugging if err := s . Add Conversion Func ( ( * config . Leader Election Configuration ) ( nil ) , ( * Leader Election Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Leader Election Configuration_To_v1alpha1_Leader Election Configuration ( a . ( * config . Leader Election Configuration ) , b . ( * Leader Election if err := s . Add Conversion Func ( ( * Client Connection Configuration ) ( nil ) , ( * config . Client Connection Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Client Connection Configuration_To_config_Client Connection Configuration ( a . ( * Client Connection Configuration ) , b . ( * config . Client Connection if err := s . Add Conversion Func ( ( * Debugging Configuration ) ( nil ) , ( * config . Debugging Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Debugging Configuration_To_config_Debugging Configuration ( a . ( * Debugging Configuration ) , b . ( * config . Debugging if err := s . Add Conversion Func ( ( * Leader Election Configuration ) ( nil ) , ( * config . Leader Election Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Leader Election Configuration_To_config_Leader Election Configuration ( a . ( * Leader Election Configuration ) , b . ( * config . Leader Election } 
func New Proxier ( load Balancer Load Balancer , listen IP net . IP , iptables iptables . Interface , exec utilexec . Interface , pr utilnet . Port Range , sync Period , min Sync Period , udp Idle Timeout time . Duration , node Port Addresses [ ] string ) ( * Proxier , error ) { return New Custom Proxier ( load Balancer , listen IP , iptables , exec , pr , sync Period , min Sync Period , udp Idle Timeout , node Port Addresses , new Proxy } 
func New Custom Proxier ( load Balancer Load Balancer , listen IP net . IP , iptables iptables . Interface , exec utilexec . Interface , pr utilnet . Port Range , sync Period , min Sync Period , udp Idle Timeout time . Duration , node Port Addresses [ ] string , make Proxy Socket Proxy Socket Func ) ( * Proxier , error ) { if listen IP . Equal ( localhost I Pv4 ) || listen IP . Equal ( localhost I Pv6 ) { return nil , Err Proxy On // If listen host IP := listen if host IP . Equal ( net . I Pv4zero ) || host IP . Equal ( net . I Pv6zero ) { host IP , err = utilnet . Choose Host err = set R proxy Ports := new Port klog . V ( 2 ) . Infof ( " " , host return create Proxier ( load Balancer , listen IP , iptables , exec , host IP , proxy Ports , sync Period , min Sync Period , udp Idle Timeout , make Proxy } 
func Cleanup Leftovers ( ipt iptables . Interface ) ( encountered Error bool ) { // NOTE: Warning, this needs to be kept in sync with the userspace Proxier, // we want to ensure we remove all of the iptables rules it creates. // Currently they are all in iptables if err := ipt . Delete Rule ( iptables . Table NAT , iptables . Chain Output , append ( args , " " , string ( iptables Host Portal Chain ) ) ... ) ; err != nil { if ! iptables . Is Not Found encountered if err := ipt . Delete Rule ( iptables . Table NAT , iptables . Chain Prerouting , append ( args , " " , string ( iptables Container Portal Chain ) ) ... ) ; err != nil { if ! iptables . Is Not Found encountered if err := ipt . Delete Rule ( iptables . Table NAT , iptables . Chain Output , append ( args , " " , string ( iptables Host Node Port Chain ) ) ... ) ; err != nil { if ! iptables . Is Not Found encountered if err := ipt . Delete Rule ( iptables . Table NAT , iptables . Chain Prerouting , append ( args , " " , string ( iptables Container Node Port Chain ) ) ... ) ; err != nil { if ! iptables . Is Not Found encountered if err := ipt . Delete Rule ( iptables . Table Filter , iptables . Chain Input , append ( args , " " , string ( iptables Non Local Node Port Chain ) ) ... ) ; err != nil { if ! iptables . Is Not Found encountered // flush and delete chains. table Chains := map [ iptables . Table ] [ ] iptables . Chain { iptables . Table NAT : { iptables Container Portal Chain , iptables Host Portal Chain , iptables Host Node Port Chain , iptables Container Node Port Chain } , iptables . Table Filter : { iptables Non Local Node Port for table , chains := range table Chains { for _ , c := range chains { // flush chain, then if successful delete, delete will fail if flush fails. if err := ipt . Flush Chain ( table , c ) ; err != nil { if ! iptables . Is Not Found encountered } else { if err = ipt . Delete Chain ( table , c ) ; err != nil { if ! iptables . Is Not Found encountered return encountered } 
for service Name , info := range proxier . service Map { proxier . stop Proxy ( service proxier . cleanup Stale Sticky close ( proxier . stop } 
func ( proxier * Proxier ) ensure Portals ( ) { // NB: This does not remove rules that should not be present. for name , info := range proxier . service Map { err := proxier . open } 
func ( proxier * Proxier ) cleanup Stale Sticky Sessions ( ) { for name := range proxier . service Map { proxier . load Balancer . Cleanup Stale Sticky } 
func ( proxier * Proxier ) add Service On Port ( service proxy . Service Port Name , protocol v1 . Protocol , proxy Port int , timeout time . Duration ) ( * Service return proxier . add Service On Port Internal ( service , protocol , proxy } 
func ( proxier * Proxier ) add Service On Port Internal ( service proxy . Service Port Name , protocol v1 . Protocol , proxy Port int , timeout time . Duration ) ( * Service Info , error ) { sock , err := proxier . make Proxy Socket ( protocol , proxier . listen IP , proxy _ , port Str , err := net . Split Host port Num , err := strconv . Atoi ( port si := & Service Info { Timeout : timeout , Active Clients : new Client Cache ( ) , is Alive Atomic : 1 , proxy Port : port Num , protocol : protocol , socket : sock , session Affinity Type : v1 . Service Affinity proxier . service klog . V ( 2 ) . Infof ( " " , service , protocol , port go func ( service proxy . Service Port Name , proxier * Proxier ) { defer runtime . Handle atomic . Add Int32 ( & proxier . num Proxy sock . Proxy Loop ( service , si , proxier . load atomic . Add Int32 ( & proxier . num Proxy } 
func ( proxier * Proxier ) claim Node Port ( ip net . IP , port int , protocol v1 . Protocol , owner proxy . Service Port Name ) error { proxier . port Map defer proxier . port Map // TODO: We could pre-populate some reserved ports into port Map and/or blacklist some well-known ports key := port Map existing , found := proxier . port if ! found { // Hold the actual port open, even though we use iptables to redirect // it. This ensures that a) it's safe to take and b) that stays true. // NOTE: We should not need to have a real listen()ing socket - bind() // should be enough, but I can't figure out a way to e2e test without // it. Tools like 'ss' and 'netstat' do not show sockets that are // bind()ed but not listen()ed, and at least the default debian netcat // has no way to avoid about 10 seconds of retries. socket , err := proxier . make Proxy proxier . port Map [ key ] = & port Map } 
func ( proxier * Proxier ) release Node Port ( ip net . IP , port int , protocol v1 . Protocol , owner proxy . Service Port Name ) error { proxier . port Map defer proxier . port Map key := port Map existing , found := proxier . port delete ( proxier . port } 
func iptables Init ( ipt iptables . Interface ) error { // TODO: There is almost certainly room for optimization here. E.g. If // we knew the service-cluster-ip-range CIDR we could fast-track outbound packets not // destined for a service. There's probably more, help wanted. // Danger - order of these rules matters here: // // We match portal rules first, then Node Port rules. For Node Port rules, we filter primarily on --dst-type LOCAL, // because we want to listen on all local addresses, but don't match internet traffic with the same dst port number. // // There is one complication (per thockin): // -m addrtype --dst-type LOCAL is what we want except that it is broken (by intent without foresight to our usecase) // on at least GCE. Specifically, GCE machines have a daemon which learns what external I Ps are forwarded to that // machine, and configure a local route for that IP, making a match for --dst-type LOCAL when we don't want it to. // Removing the route gives correct behavior until the daemon recreates it. // Killing the daemon is an option, but means that any non-kubernetes use of the machine with external IP will be broken. // // This applies to I Ps on GCE that are actually from a load-balancer; they will be categorized as LOCAL. // _If_ the chains were in the wrong order, and the LB traffic had dst-port == a Node Port on some other service, // the Node if _ , err := ipt . Ensure Chain ( iptables . Table NAT , iptables Container Portal if _ , err := ipt . Ensure Rule ( iptables . Prepend , iptables . Table NAT , iptables . Chain Prerouting , append ( args , " " , string ( iptables Container Portal if _ , err := ipt . Ensure Chain ( iptables . Table NAT , iptables Host Portal if _ , err := ipt . Ensure Rule ( iptables . Prepend , iptables . Table NAT , iptables . Chain Output , append ( args , " " , string ( iptables Host Portal if _ , err := ipt . Ensure Chain ( iptables . Table NAT , iptables Container Node Port if _ , err := ipt . Ensure Rule ( iptables . Append , iptables . Table NAT , iptables . Chain Prerouting , append ( args , " " , string ( iptables Container Node Port if _ , err := ipt . Ensure Chain ( iptables . Table NAT , iptables Host Node Port if _ , err := ipt . Ensure Rule ( iptables . Append , iptables . Table NAT , iptables . Chain Output , append ( args , " " , string ( iptables Host Node Port // Create a chain intended to explicitly allow non-local Node if _ , err := ipt . Ensure Chain ( iptables . Table Filter , iptables Non Local Node Port if _ , err := ipt . Ensure Rule ( iptables . Prepend , iptables . Table Filter , iptables . Chain Input , append ( args , " " , string ( iptables Non Local Node Port } 
func iptables if err := ipt . Flush Chain ( iptables . Table NAT , iptables Container Portal if err := ipt . Flush Chain ( iptables . Table NAT , iptables Host Portal if err := ipt . Flush Chain ( iptables . Table NAT , iptables Container Node Port if err := ipt . Flush Chain ( iptables . Table NAT , iptables Host Node Port if err := ipt . Flush Chain ( iptables . Table Filter , iptables Non Local Node Port return utilerrors . New } 
func iptables Common Portal Args ( dest IP net . IP , add Physical Interface Match bool , add Dst Local Match bool , dest Port int , protocol v1 . Protocol , service proxy . Service Port Name ) [ ] string { // This list needs to include all fields as they are eventually spit out // by iptables-save. This is because some systems do not support the // 'iptables -C' arg, and so fall back on parsing iptables-save output. // If this does not match, it will not pass the check. For example: // adding the /32 on the destination IP arg is not strictly required, // but causes this list to not match the final iptables-save output. // This is fragile and I hope one day we can stop supporting such old // iptables versions. args := [ ] string { " " , " " , " " , service . String ( ) , " " , strings . To Lower ( string ( protocol ) ) , " " , strings . To Lower ( string ( protocol ) ) , " " , fmt . Sprintf ( " " , dest if dest IP != nil { args = append ( args , " " , utilproxy . To CIDR ( dest if add Physical Interface if add Dst Local } 
func ( proxier * Proxier ) iptables Container Portal Args ( dest IP net . IP , add Physical Interface Match bool , add Dst Local Match bool , dest Port int , protocol v1 . Protocol , proxy IP net . IP , proxy Port int , service proxy . Service Port Name ) [ ] string { args := iptables Common Portal Args ( dest IP , add Physical Interface Match , add Dst Local Match , dest // This is tricky. // // If the proxy is bound (see Proxier.listen IP) to 0.0.0.0 ("any // interface") we want to use REDIRECT, which sends traffic to the // "primary address of the incoming interface" which means the container // bridge, if there is one. When the response comes, it comes from that // same interface, so the NAT matches and the response packet is // correct. This matters for UDP, since there is no per-connection port // number. // // The alternative would be to use DNAT, except that it doesn't work // (empirically): // * DNAT to 127.0.0.1 = Packets just disappear - this seems to be a // well-known limitation of iptables. // * DNAT to eth0's IP = Response packets come from the bridge, which // breaks the NAT, and makes things like DNS not accept them. If // this could be resolved, it would simplify all of this code. // // If the proxy is bound to a specific IP, then we have to use DNAT to // that IP. Unlike the previous case, this works because the proxy is // ONLY listening on that IP, not the bridge. // // Why would anyone bind to an address that is not inclusive of // localhost? Apparently some cloud environments have their public IP // exposed as a real network interface AND do not have firewalling. We // don't want to expose everything out to the world. // // Unfortunately, I don't know of any way to listen on some (N > 1) // interfaces but not ALL interfaces, short of doing it manually, and // this is simpler than that. // // If the proxy is bound to localhost only, all of this is broken. Not // allowed. if proxy IP . Equal ( zero I Pv4 ) || proxy IP . Equal ( zero I Pv6 ) { // TODO: Can we REDIRECT with I Pv6? args = append ( args , " " , " " , " " , fmt . Sprintf ( " " , proxy } else { // TODO: Can we DNAT with I Pv6? args = append ( args , " " , " " , " " , net . Join Host Port ( proxy IP . String ( ) , strconv . Itoa ( proxy } 
func ( proxier * Proxier ) iptables Host Node Port Args ( node Port int , protocol v1 . Protocol , proxy IP net . IP , proxy Port int , service proxy . Service Port Name ) [ ] string { args := iptables Common Portal Args ( nil , false , false , node if proxy IP . Equal ( zero I Pv4 ) || proxy IP . Equal ( zero I Pv6 ) { proxy IP = proxier . host // TODO: Can we DNAT with I Pv6? args = append ( args , " " , " " , " " , net . Join Host Port ( proxy IP . String ( ) , strconv . Itoa ( proxy } 
func ( proxier * Proxier ) iptables Non Local Node Port Args ( node Port int , protocol v1 . Protocol , proxy IP net . IP , proxy Port int , service proxy . Service Port Name ) [ ] string { args := iptables Common Portal Args ( nil , false , false , proxy } 
func New Cmd Upgrade ( out io . Writer ) * cobra . Command { flags := & apply Plan Flags { kube Config Path : kubeadmconstants . Get Admin Kube Config Path ( ) , cfg Path : " " , feature Gates String : " " , allow Experimental Upgrades : false , allow RC Upgrades : false , print cmd := & cobra . Command { Use : " " , Short : " " , Run E : cmdutil . Sub Cmd Run cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd cmd . Add Command ( New Cmd } 
func New Caching Secret Manager ( kube Client clientset . Interface , get TTL manager . Get Object TTL Func ) Manager { get Secret := func ( namespace , name string , opts metav1 . Get Options ) ( runtime . Object , error ) { return kube Client . Core secret Store := manager . New Object Store ( get Secret , clock . Real Clock { } , get TTL , default return & secret Manager { manager : manager . New Cache Based Manager ( secret Store , get Secret } 
func New Watching Secret Manager ( kube Client clientset . Interface ) Manager { list Secret := func ( namespace string , opts metav1 . List Options ) ( runtime . Object , error ) { return kube Client . Core watch Secret := func ( namespace string , opts metav1 . List Options ) ( watch . Interface , error ) { return kube Client . Core new return & secret Manager { manager : manager . New Watch Based Manager ( list Secret , watch Secret , new Secret , gr , get Secret } 
func ( g * Group ) Start With Channel ( stop Ch <- chan struct { } , f func ( stop Ch <- chan struct { } ) ) { g . Start ( func ( ) { f ( stop } 
func ( g * Group ) Start With } 
} 
func Until ( f func ( ) , period time . Duration , stop Ch <- chan struct { } ) { Jitter Until ( f , period , 0.0 , true , stop } 
func Until With Context ( ctx context . Context , f func ( context . Context ) , period time . Duration ) { Jitter Until With } 
func Non Sliding Until ( f func ( ) , period time . Duration , stop Ch <- chan struct { } ) { Jitter Until ( f , period , 0.0 , false , stop } 
func Non Sliding Until With Context ( ctx context . Context , f func ( context . Context ) , period time . Duration ) { Jitter Until With } 
func Jitter Until ( f func ( ) , period time . Duration , jitter Factor float64 , sliding bool , stop var saw for { select { case <- stop jittered if jitter Factor > 0.0 { jittered Period = Jitter ( period , jitter if ! sliding { t = reset Or Reuse Timer ( t , jittered Period , saw func ( ) { defer runtime . Handle if sliding { t = reset Or Reuse Timer ( t , jittered Period , saw // NOTE: b/c there is no priority selection in golang // it is possible for this to race, meaning we could // trigger t.C and stop Ch, and t.C select falls through. // In order to mitigate we re-check stop Ch at the beginning // of every loop to prevent extra executions of f(). select { case <- stop case <- t . C : saw } 
func Jitter Until With Context ( ctx context . Context , f func ( context . Context ) , period time . Duration , jitter Factor float64 , sliding bool ) { Jitter Until ( func ( ) { f ( ctx ) } , period , jitter } 
func Jitter ( duration time . Duration , max Factor float64 ) time . Duration { if max Factor <= 0.0 { max wait := duration + time . Duration ( rand . Float64 ( ) * max } 
} 
func context For Channel ( parent Ch <- chan struct { } ) ( context . Context , context . Cancel Func ) { ctx , cancel := context . With go func ( ) { select { case <- parent } 
func Exponential Backoff ( backoff Backoff , condition Condition return Err Wait } 
func Poll ( interval , timeout time . Duration , condition Condition Func ) error { return poll } 
func Poll Immediate ( interval , timeout time . Duration , condition Condition Func ) error { return poll Immediate } 
func Poll Infinite ( interval time . Duration , condition Condition return Poll } 
func Poll Immediate Infinite ( interval time . Duration , condition Condition return Poll } 
func Poll Until ( interval time . Duration , condition Condition Func , stop Ch <- chan struct { } ) error { ctx , cancel := context For Channel ( stop return Wait } 
func Poll Immediate Until ( interval time . Duration , condition Condition Func , stop select { case <- stop Ch : return Err Wait default : return Poll Until ( interval , condition , stop } 
func poller ( interval , timeout time . Duration ) Wait Func { return Wait tick := time . New if timeout != 0 { // time.After is more convenient, but it // potentially leaves timers around much longer // than necessary if we exit early. timer := time . New } 
func reset Or Reuse Timer ( t * time . Timer , d time . Duration , saw Timeout bool ) * time . Timer { if t == nil { return time . New if ! t . Stop ( ) && ! saw } 
func User Info ( namespace , name , uid string ) user . Info { return ( & Service Account Info { Name : name , Namespace : namespace , UID : uid , } ) . User } 
func Is Service Account Token ( secret * v1 . Secret , sa * v1 . Service Account ) bool { if secret . Type != v1 . Secret Type Service Account name := secret . Annotations [ v1 . Service Account Name uid := secret . Annotations [ v1 . Service Account UID } 
func ( m * mutating Webhook Configuration Manager ) Webhooks ( ) [ ] v1beta1 . Webhook { return m . configuration . Load ( ) . ( * v1beta1 . Mutating Webhook } 
func milli CPU To Shares ( milli CPU int64 ) int64 { if milli CPU == 0 { // Return 2 here to really match kernel default for zero milli CPU. return min // Conceptually (milli CPU / milli CPU To CPU) * shares Per CPU, but factored to improve rounding. shares := ( milli CPU * shares Per CPU ) / milli CPU To if shares < min Shares { return min } 
func milli CPU To Quota ( milli CPU int64 , period int64 ) ( quota int64 ) { // CFS quota is measured in two values: // - cfs_period_us=100ms (the amount of time to measure usage across) // - cfs_quota=20ms (the amount of cpu time allowed to be used across a period) // so in the above example, you are limited to 20% of a single CPU // for multi-cpu environments, you just scale equivalent amounts // see https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt for details if milli // we then convert your milli CPU to a value normalized over a period quota = ( milli CPU * period ) / milli CPU To // quota needs to be a minimum of 1ms. if quota < min Quota Period { quota = min Quota } 
func ( c * Authentication V1beta1Client ) REST return c . rest } 
func ( t * type Name ) Visit Array ( a * proto . Array ) { s := & type a . Sub } 
func ( t * type Name ) Visit Map ( m * proto . Map ) { s := & type m . Sub } 
func ( t * type Name ) Visit } 
func ( t * type Name ) Visit Reference ( r proto . Reference ) { r . Sub } 
func Get Type Name ( schema proto . Schema ) string { t := & type } 
func Validate ( config * kubeproxyconfig . Kube Proxy Configuration ) field . Error List { all Errs := field . Error new Path := field . New all Errs = append ( all Errs , validate Kube Proxy IP Tables Configuration ( config . IP Tables , new if config . Mode == kubeproxyconfig . Proxy Mode IPVS { all Errs = append ( all Errs , validate Kube Proxy IPVS Configuration ( config . IPVS , new all Errs = append ( all Errs , validate Kube Proxy Conntrack Configuration ( config . Conntrack , new all Errs = append ( all Errs , validate Proxy Mode ( config . Mode , new all Errs = append ( all Errs , validate Client Connection Configuration ( config . Client Connection , new if config . OOM Score Adj != nil && ( * config . OOM Score Adj < - 1000 || * config . OOM Score Adj > 1000 ) { all Errs = append ( all Errs , field . Invalid ( new Path . Child ( " " ) , * config . OOM Score if config . UDP Idle Timeout . Duration <= 0 { all Errs = append ( all Errs , field . Invalid ( new Path . Child ( " " ) , config . UDP Idle if config . Config Sync Period . Duration <= 0 { all Errs = append ( all Errs , field . Invalid ( new Path . Child ( " " ) , config . Config Sync if net . Parse IP ( config . Bind Address ) == nil { all Errs = append ( all Errs , field . Invalid ( new Path . Child ( " " ) , config . Bind if config . Healthz Bind Address != " " { all Errs = append ( all Errs , validate Host Port ( config . Healthz Bind Address , new all Errs = append ( all Errs , validate Host Port ( config . Metrics Bind Address , new if config . Cluster CIDR != " " { if _ , _ , err := net . Parse CIDR ( config . Cluster CIDR ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( new Path . Child ( " " ) , config . Cluster if _ , err := utilnet . Parse Port Range ( config . Port Range ) ; err != nil { all Errs = append ( all Errs , field . Invalid ( new Path . Child ( " " ) , config . Port all Errs = append ( all Errs , validate Kube Proxy Node Port Address ( config . Node Port Addresses , new return all } 
func new Events ( c * Events V1beta1Client , namespace string ) * events { return & events { client : c . REST } 
func resolve Port ( port Reference intstr . Int Or String , container * v1 . Container ) ( int , error ) { if port Reference . Type == intstr . Int { return port Reference . Int port Name := port Reference . Str port , err := strconv . Atoi ( port for _ , port Spec := range container . Ports { if port Spec . Name == port Name { return int ( port Spec . Container return - 1 , fmt . Errorf ( " " , port } 
func ( w * fake Watcher ) Trigger Event ( op fsnotify . Op , filename string ) { w . event } 
func ( in * Audit Sink ) Deep Copy Into ( out * Audit out . Type Meta = in . Type in . Object Meta . Deep Copy Into ( & out . Object in . Spec . Deep Copy } 
func ( in * Audit Sink ) Deep Copy ( ) * Audit out := new ( Audit in . Deep Copy } 
func ( in * Audit Sink ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Audit Sink List ) Deep Copy Into ( out * Audit Sink out . Type Meta = in . Type out . List Meta = in . List * out = make ( [ ] Audit for i := range * in { ( * in ) [ i ] . Deep Copy } 
func ( in * Audit Sink List ) Deep Copy ( ) * Audit Sink out := new ( Audit Sink in . Deep Copy } 
func ( in * Audit Sink List ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Audit Sink Spec ) Deep Copy Into ( out * Audit Sink in . Policy . Deep Copy in . Webhook . Deep Copy } 
func ( in * Audit Sink Spec ) Deep Copy ( ) * Audit Sink out := new ( Audit Sink in . Deep Copy } 
func ( in * Policy ) Deep Copy } 
func ( in * Service Reference ) Deep Copy Into ( out * Service } 
func ( in * Webhook ) Deep Copy * out = new ( Webhook Throttle ( * in ) . Deep Copy in . Client Config . Deep Copy Into ( & out . Client } 
func ( in * Webhook Throttle Config ) Deep Copy Into ( out * Webhook Throttle } 
func ( in * Webhook Throttle Config ) Deep Copy ( ) * Webhook Throttle out := new ( Webhook Throttle in . Deep Copy } 
func ( crc Container Runtime Check ) Check ( ) ( warnings , error if err := crc . runtime . Is Running ( ) ; err != nil { error List = append ( error return warnings , error } 
func ( sc Service } 
func ( sc Service Check ) Check ( ) ( warnings , error init System , err := initsystem . Get Init if ! init System . Service if ! init System . Service Is if sc . Check If Active && ! init System . Service Is Active ( sc . Service ) { error List = append ( error return warnings , error } 
func ( fc Firewalld Check ) Check ( ) ( warnings , error init System , err := initsystem . Get Init if ! init System . Service if init System . Service Is return warnings , error } 
func ( poc Port Open } 
func ( poc Port Open Check ) Check ( ) ( warnings , error error if err != nil { error List = append ( error return nil , error } 
func ( dac Dir Available } 
func ( dac Dir Available Check ) Check ( ) ( warnings , error error // If it doesn't exist we are good: if _ , err := os . Stat ( dac . Path ) ; os . Is Not if err != nil { error List = append ( error return nil , error if err != io . EOF { error List = append ( error return nil , error } 
func ( fac File Available } 
func ( fac File Available Check ) Check ( ) ( warnings , error error if _ , err := os . Stat ( fac . Path ) ; err == nil { error List = append ( error return nil , error } 
func ( fcc File Content } 
func ( fcc File Content Check ) Check ( ) ( warnings , error lr := io . Limit } 
func ( ipc In Path } 
func ( ipc In Path _ , err := ipc . exec . Look // Return as a warning: warning if ipc . suggestion != " " { warning return [ ] error { errors . New ( warning } 
func ( hc Hostname Check ) Check ( ) ( warnings , error error addr , err := net . Lookup Host ( hc . node if addr == nil { warnings = append ( warnings , errors . Errorf ( " \" \" " , hc . node if err != nil { warnings = append ( warnings , errors . Wrapf ( err , " \" \" " , hc . node return warnings , error } 
func ( hst HTTP Proxy Check ) Check ( ) ( warnings , error req , err := http . New proxy , err := netutil . Set Old Transport } 
func ( subnet HTTP Proxy CIDR Check ) Check ( ) ( warnings , error _ , cidr , err := net . Parse test IP , err := ipallocator . Get Indexed test I Pstring := test if len ( test IP ) == net . I Pv6len { test I Pstring = fmt . Sprintf ( " " , test url := fmt . Sprintf ( " " , subnet . Proto , test I req , err := http . New // Utilize same transport defaults as it will be used by API server proxy , err := netutil . Set Old Transport } 
func ( sysver System Verification Check ) Check ( ) ( warnings , error // Create a buffered writer and choose a quite large value (1M) and suppose the output from the system verification test won't exceed the limit // Run the system verification check, but write to out buffered writer instead of stdout bufw := bufio . New Writer reporter := & system . Stream Reporter { Write // All the common validators we'd like to run: var validators = [ ] system . Validator { & system . Kernel // run the docker validator only with docker runtime if sysver . Is Docker { validators = append ( validators , & system . Docker if runtime . GOOS == " " { //add linux validators validators = append ( validators , & system . OS Validator { Reporter : reporter } , & system . Cgroups // Run all validators for _ , v := range validators { warn , err := v . Validate ( system . Default Sys } 
func ( kubever Kubernetes Version Check ) Check ( ) ( warnings , error // Skip this check for "super-custom builds", where apimachinery/the overall codebase version is not set. if strings . Has Prefix ( kubever . Kubeadm kadm Version , err := versionutil . Parse Semantic ( kubever . Kubeadm if err != nil { return nil , [ ] error { errors . Wrapf ( err , " " , kubever . Kubeadm k8s Version , err := versionutil . Parse Semantic ( kubever . Kubernetes if err != nil { return nil , [ ] error { errors . Wrapf ( err , " " , kubever . Kubernetes // Checks if k8s Version greater or equal than the first unsupported versions by current version of kubeadm, // that is major.minor+1 (all patch and pre-releases versions included) // NB. in semver patches number is a numeric, while prerelease is a string where numeric identifiers always have lower precedence than non-numeric identifiers. // thus setting the value to x.y.0-0 we are defining the very first patch - prereleases within x.y minor release. first Unsupported Version := versionutil . Must Parse Semantic ( fmt . Sprintf ( " " , kadm Version . Major ( ) , kadm if k8s Version . At Least ( first Unsupported Version ) { return [ ] error { errors . Errorf ( " " , k8s Version , kadm Version . Components ( ) [ 0 ] , kadm } 
func ( kubever Kubelet Version Check ) Check ( ) ( warnings , error kubelet Version , err := Get Kubelet if kubelet Version . Less Than ( kubeadmconstants . Minimum Kubelet Version ) { return nil , [ ] error { errors . Errorf ( " " , kubelet if kubever . Kubernetes Version != " " { k8s Version , err := versionutil . Parse Semantic ( kubever . Kubernetes if err != nil { return nil , [ ] error { errors . Wrapf ( err , " " , kubever . Kubernetes if kubelet Version . Major ( ) > k8s Version . Major ( ) || kubelet Version . Minor ( ) > k8s Version . Minor ( ) { return nil , [ ] error { errors . Errorf ( " " , kubelet Version , k8s } 
func ( swc Swap Check ) Check ( ) ( warnings , error scanner := bufio . New } 
func ( evc External Etcd Version Check ) Check ( ) ( warnings , error if config , err = evc . config Root C As ( config ) ; err != nil { error List = append ( error return nil , error if config , err = evc . config Cert And Key ( config ) ; err != nil { error List = append ( error return nil , error client := evc . get HTTP for _ , endpoint := range evc . Etcd . External . Endpoints { if _ , err := url . Parse ( endpoint ) ; err != nil { error List = append ( error resp := etcd Version version if tmp Version URL , err := purell . Normalize URL String ( version URL , purell . Flag Remove Duplicate Slashes ) ; err != nil { error List = append ( error List , errors . Wrapf ( err , " " , version } else { version URL = tmp Version if err = get Etcd Version Response ( client , version URL , & resp ) ; err != nil { error List = append ( error etcd if err != nil { error List = append ( error if etcd Version . LT ( min External Etcd Version ) { error List = append ( error List , errors . Errorf ( " " , kubeadmconstants . Min External Etcd return nil , error } 
func ( evc External Etcd Version Check ) config Root C As ( config * tls . Config ) ( * tls . Config , error ) { var CA Cert Pool * x509 . Cert if evc . Etcd . External . CA File != " " { CA Cert , err := ioutil . Read File ( evc . Etcd . External . CA if err != nil { return nil , errors . Wrapf ( err , " " , evc . Etcd . External . CA CA Cert Pool = x509 . New Cert CA Cert Pool . Append Certs From PEM ( CA if CA Cert config . Root C As = CA Cert } 
func ( evc External Etcd Version Check ) config Cert And if evc . Etcd . External . Cert File != " " && evc . Etcd . External . Key cert , err = tls . Load X509Key Pair ( evc . Etcd . External . Cert File , evc . Etcd . External . Key if err != nil { return nil , errors . Wrapf ( err , " " , evc . Etcd . External . Cert File , evc . Etcd . External . Key } 
func ( ipc Image Pull Check ) Check ( ) ( warnings , error List [ ] error ) { for _ , image := range ipc . image List { ret , err := ipc . runtime . Image if err != nil { error List = append ( error if err := ipc . runtime . Pull Image ( image ) ; err != nil { error List = append ( error return warnings , error } 
func ( ncc Num CPU Check ) Check ( ) ( warnings , error List [ ] error ) { num CPU := runtime . Num if num CPU < ncc . Num CPU { error List = append ( error List , errors . Errorf ( " " , num CPU , ncc . Num return warnings , error } 
func Run Init Node Checks ( execer utilsexec . Interface , cfg * kubeadmapi . Init Configuration , ignore Preflight Errors sets . String , is Secondary Control Plane bool , download Certs bool ) error { if ! is Secondary Control Plane { // First, check if we're root separately from the other preflight checks and fail fast if err := Run Root Check Only ( ignore Preflight manifests Dir := filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Manifests Sub Dir checks := [ ] Checker { Num CPU Check { Num CPU : kubeadmconstants . Control Plane Num CPU } , Kubernetes Version Check { Kubernetes Version : cfg . Kubernetes Version , Kubeadm Version : kubeadmversion . Get ( ) . Git Version } , Firewalld Check { ports : [ ] int { int ( cfg . Local API Endpoint . Bind Port ) , ports . Kubelet Port } } , Port Open Check { port : int ( cfg . Local API Endpoint . Bind Port ) } , Port Open Check { port : ports . Insecure Scheduler Port } , Port Open Check { port : ports . Insecure Kube Controller Manager Port } , File Available Check { Path : kubeadmconstants . Get Static Pod Filepath ( kubeadmconstants . Kube API Server , manifests Dir ) } , File Available Check { Path : kubeadmconstants . Get Static Pod Filepath ( kubeadmconstants . Kube Controller Manager , manifests Dir ) } , File Available Check { Path : kubeadmconstants . Get Static Pod Filepath ( kubeadmconstants . Kube Scheduler , manifests Dir ) } , File Available Check { Path : kubeadmconstants . Get Static Pod Filepath ( kubeadmconstants . Etcd , manifests Dir ) } , HTTP Proxy Check { Proto : " " , Host : cfg . Local API Endpoint . Advertise Address } , HTTP Proxy CIDR Check { Proto : " " , CIDR : cfg . Networking . Service Subnet } , HTTP Proxy CIDR Check { Proto : " " , CIDR : cfg . Networking . Pod if ! is Secondary Control Plane { checks = add Common Checks ( execer , cfg . Kubernetes Version , & cfg . Node // Check if IVPS kube-proxy mode is supported if cfg . Component Configs . Kube Proxy != nil && cfg . Component Configs . Kube Proxy . Mode == ipvsutil . IPVS Proxy Mode { checks = append ( checks , IPVS Proxier // Check if Bridge-netfilter and I Pv6 relevant flags are set if ip := net . Parse IP ( cfg . Local API Endpoint . Advertise Address ) ; ip != nil { if ip . To4 ( ) == nil && ip . To16 ( ) != nil { checks = append ( checks , File Content Check { Path : bridgenf6 , Content : [ ] byte { '1' } } , File Content Check { Path : ipv6Default // if using an external etcd if cfg . Etcd . External != nil { // Check external etcd version before creating the cluster checks = append ( checks , External Etcd Version if cfg . Etcd . Local != nil { // Only do etcd related checks when required to install a local etcd checks = append ( checks , Port Open Check { port : kubeadmconstants . Etcd Listen Client Port } , Port Open Check { port : kubeadmconstants . Etcd Listen Peer Port } , Dir Available Check { Path : cfg . Etcd . Local . Data if cfg . Etcd . External != nil && ! ( is Secondary Control Plane && download Certs ) { // Only check etcd certificates when using an external etcd and not joining with automatic download of certs if cfg . Etcd . External . CA File != " " { checks = append ( checks , File Existing Check { Path : cfg . Etcd . External . CA if cfg . Etcd . External . Cert File != " " { checks = append ( checks , File Existing Check { Path : cfg . Etcd . External . Cert if cfg . Etcd . External . Key File != " " { checks = append ( checks , File Existing Check { Path : cfg . Etcd . External . Key return Run Checks ( checks , os . Stderr , ignore Preflight } 
func Run Join Node Checks ( execer utilsexec . Interface , cfg * kubeadmapi . Join Configuration , ignore Preflight Errors sets . String ) error { // First, check if we're root separately from the other preflight checks and fail fast if err := Run Root Check Only ( ignore Preflight checks := [ ] Checker { Dir Available Check { Path : filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Manifests Sub Dir Name ) } , File Available Check { Path : filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Kubelet Kube Config File Name ) } , File Available Check { Path : filepath . Join ( kubeadmconstants . Kubernetes Dir , kubeadmconstants . Kubelet Bootstrap Kube Config File checks = add Common Checks ( execer , " " , & cfg . Node if cfg . Control Plane == nil { checks = append ( checks , File Available Check { Path : cfg . CA Cert add I if cfg . Discovery . Bootstrap Token != nil { ipstr , _ , err := net . Split Host Port ( cfg . Discovery . Bootstrap Token . API Server if err == nil { checks = append ( checks , HTTP Proxy if ! add I Pv6Checks { if ip := net . Parse IP ( ipstr ) ; ip != nil { if ip . To4 ( ) == nil && ip . To16 ( ) != nil { add I if add I Pv6Checks { checks = append ( checks , File Content Check { Path : bridgenf6 , Content : [ ] byte { '1' } } , File Content Check { Path : ipv6Default return Run Checks ( checks , os . Stderr , ignore Preflight } 
func Run Optional Join Node Checks ( execer utilsexec . Interface , cfg * kubeadmapi . Cluster Configuration , ignore Preflight // Check if IVPS kube-proxy mode is supported if cfg . Component Configs . Kube Proxy != nil && cfg . Component Configs . Kube Proxy . Mode == ipvsutil . IPVS Proxy Mode { checks = append ( checks , IPVS Proxier return Run Checks ( checks , os . Stderr , ignore Preflight } 
func add Common Checks ( execer utilsexec . Interface , k8s Version string , node Reg * kubeadmapi . Node Registration Options , checks [ ] Checker ) [ ] Checker { container Runtime , err := utilruntime . New Container Runtime ( execer , node Reg . CRI is } else { checks = append ( checks , Container Runtime Check { runtime : container if container Runtime . Is Docker ( ) { is checks = append ( checks , Service Check { Service : " " , Check If // Linux only // TODO: support other CR Is for this check eventually // https://github.com/kubernetes/kubeadm/issues/874 checks = append ( checks , Is Docker Systemd // non-windows checks if runtime . GOOS == " " { if ! is Docker { checks = append ( checks , In Path checks = append ( checks , File Content Check { Path : bridgenf , Content : [ ] byte { '1' } } , File Content Check { Path : ipv4Forward , Content : [ ] byte { '1' } } , Swap Check { } , In Path Check { executable : " " , mandatory : true , exec : execer } , In Path Check { executable : " " , mandatory : true , exec : execer } , In Path Check { executable : " " , mandatory : true , exec : execer } , In Path Check { executable : " " , mandatory : true , exec : execer } , In Path Check { executable : " " , mandatory : false , exec : execer } , In Path Check { executable : " " , mandatory : false , exec : execer } , In Path Check { executable : " " , mandatory : false , exec : execer } , In Path Check { executable : " " , mandatory : false , exec : execer } , In Path checks = append ( checks , System Verification Check { Is Docker : is Docker } , Hostname Check { node Name : node Reg . Name } , Kubelet Version Check { Kubernetes Version : k8s Version , exec : execer } , Service Check { Service : " " , Check If Active : false } , Port Open Check { port : ports . Kubelet } 
func Run Root Check Only ( ignore Preflight Errors sets . String ) error { checks := [ ] Checker { Is Privileged User return Run Checks ( checks , os . Stderr , ignore Preflight } 
func Run Pull Images Check ( execer utilsexec . Interface , cfg * kubeadmapi . Init Configuration , ignore Preflight Errors sets . String ) error { container Runtime , err := utilruntime . New Container Runtime ( utilsexec . New ( ) , cfg . Node Registration . CRI checks := [ ] Checker { Image Pull Check { runtime : container Runtime , image List : images . Get Control Plane Images ( & cfg . Cluster return Run Checks ( checks , os . Stderr , ignore Preflight } 
func Run Checks ( checks [ ] Checker , ww io . Writer , ignore Preflight Errors sets . String ) error { var errs if set Has Item Or All ( ignore Preflight for _ , w := range warnings { io . Write for _ , i := range errs { errs Buffer . Write if errs Buffer . Len ( ) > 0 { return & Error { Msg : errs } 
func set Has Item Or All ( s sets . String , item string ) bool { if s . Has ( " " ) || s . Has ( strings . To } 
func apply Experimental Create Config ( create Config * dockertypes . Container Create Config , annotations map [ string ] string ) { if kubeletapis . Should Isolated By Hyper V ( annotations ) { create Config . Host Config . Isolation = kubeletapis . Hyperv Isolation if network Mode := os . Getenv ( " " ) ; network Mode == " " { create Config . Host Config . Network Mode = dockercontainer . Network } 
func apply Windows Container Security Context ( wsc * runtimeapi . Windows Container Security Context , config * dockercontainer . Config , hc * dockercontainer . Host if wsc . Get Run As Username ( ) != " " { config . User = wsc . Get Run As } 
func Create Or Update Config Map ( client clientset . Interface , cm * v1 . Config Map ) error { if _ , err := client . Core V1 ( ) . Config Maps ( cm . Object Meta . Namespace ) . Create ( cm ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Core V1 ( ) . Config Maps ( cm . Object } 
func Create Or Mutate Config Map ( client clientset . Interface , cm * v1 . Config Map , mutator Config Map Mutator ) error { if _ , err := client . Core V1 ( ) . Config Maps ( cm . Object Meta . Namespace ) . Create ( cm ) ; err != nil { if ! apierrors . Is Already return Mutate Config Map ( client , metav1 . Object Meta { Namespace : cm . Object Meta . Namespace , Name : cm . Object } 
func Mutate Config Map ( client clientset . Interface , meta metav1 . Object Meta , mutator Config Map Mutator ) error { return clientsetretry . Retry On Conflict ( wait . Backoff { Steps : 20 , Duration : 500 * time . Millisecond , Factor : 1.0 , Jitter : 0.1 , } , func ( ) error { config Map , err := client . Core V1 ( ) . Config Maps ( meta . Namespace ) . Get ( meta . Name , metav1 . Get if err = mutator ( config _ , err = client . Core V1 ( ) . Config Maps ( config Map . Object Meta . Namespace ) . Update ( config } 
func Create Or Retain Config Map ( client clientset . Interface , cm * v1 . Config Map , config Map Name string ) error { if _ , err := client . Core V1 ( ) . Config Maps ( cm . Object Meta . Namespace ) . Get ( config Map Name , metav1 . Get Options { } ) ; err != nil { if ! apierrors . Is Not if _ , err := client . Core V1 ( ) . Config Maps ( cm . Object Meta . Namespace ) . Create ( cm ) ; err != nil { if ! apierrors . Is Already } 
func Create Or Update Secret ( client clientset . Interface , secret * v1 . Secret ) error { if _ , err := client . Core V1 ( ) . Secrets ( secret . Object Meta . Namespace ) . Create ( secret ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Core V1 ( ) . Secrets ( secret . Object } 
func Create Or Update Service Account ( client clientset . Interface , sa * v1 . Service Account ) error { if _ , err := client . Core V1 ( ) . Service Accounts ( sa . Object Meta . Namespace ) . Create ( sa ) ; err != nil { // Note: We don't run .Update here afterwards as that's probably not required // Only thing that could be updated is annotations/labels in .metadata, but we don't use that currently if ! apierrors . Is Already } 
func Create Or Update Deployment ( client clientset . Interface , deploy * apps . Deployment ) error { if _ , err := client . Apps V1 ( ) . Deployments ( deploy . Object Meta . Namespace ) . Create ( deploy ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Apps V1 ( ) . Deployments ( deploy . Object } 
func Create Or Update Daemon Set ( client clientset . Interface , ds * apps . Daemon Set ) error { if _ , err := client . Apps V1 ( ) . Daemon Sets ( ds . Object Meta . Namespace ) . Create ( ds ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Apps V1 ( ) . Daemon Sets ( ds . Object } 
func Delete Daemon Set Foreground ( client clientset . Interface , namespace , name string ) error { foreground Delete := metav1 . Delete Propagation delete Options := & metav1 . Delete Options { Propagation Policy : & foreground return client . Apps V1 ( ) . Daemon Sets ( namespace ) . Delete ( name , delete } 
func Delete Deployment Foreground ( client clientset . Interface , namespace , name string ) error { foreground Delete := metav1 . Delete Propagation delete Options := & metav1 . Delete Options { Propagation Policy : & foreground return client . Apps V1 ( ) . Deployments ( namespace ) . Delete ( name , delete } 
func Create Or Update Role ( client clientset . Interface , role * rbac . Role ) error { if _ , err := client . Rbac V1 ( ) . Roles ( role . Object Meta . Namespace ) . Create ( role ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Rbac V1 ( ) . Roles ( role . Object } 
func Create Or Update Role Binding ( client clientset . Interface , role Binding * rbac . Role Binding ) error { if _ , err := client . Rbac V1 ( ) . Role Bindings ( role Binding . Object Meta . Namespace ) . Create ( role Binding ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Rbac V1 ( ) . Role Bindings ( role Binding . Object Meta . Namespace ) . Update ( role } 
func Create Or Update Cluster Role ( client clientset . Interface , cluster Role * rbac . Cluster Role ) error { if _ , err := client . Rbac V1 ( ) . Cluster Roles ( ) . Create ( cluster Role ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Rbac V1 ( ) . Cluster Roles ( ) . Update ( cluster } 
func Create Or Update Cluster Role Binding ( client clientset . Interface , cluster Role Binding * rbac . Cluster Role Binding ) error { if _ , err := client . Rbac V1 ( ) . Cluster Role Bindings ( ) . Create ( cluster Role Binding ) ; err != nil { if ! apierrors . Is Already if _ , err := client . Rbac V1 ( ) . Cluster Role Bindings ( ) . Update ( cluster Role } 
func Patch Node Once ( client clientset . Interface , node Name string , patch Fn func ( * v1 . Node ) ) func ( ) ( bool , error ) { return func ( ) ( bool , error ) { // First get the node object n , err := client . Core V1 ( ) . Nodes ( ) . Get ( node Name , metav1 . Get // The node may appear to have no labels at first, // so we wait for it to get hostname label. if _ , found := n . Object Meta . Labels [ v1 . Label old // Execute the mutating function patch new patch Bytes , err := strategicpatch . Create Two Way Merge Patch ( old Data , new if _ , err := client . Core V1 ( ) . Nodes ( ) . Patch ( n . Name , types . Strategic Merge Patch Type , patch Bytes ) ; err != nil { // TODO also check for timeouts if apierrors . Is } 
func Patch Node ( client clientset . Interface , node Name string , patch Fn func ( * v1 . Node ) ) error { // wait.Poll will rerun the condition function every interval function if // the function returns false. If the condition function returns an error // then the retries end and the error is returned. return wait . Poll ( constants . API Call Retry Interval , constants . Patch Node Timeout , Patch Node Once ( client , node Name , patch } 
func ( c Auth Provider } 
func ( c Exec return fmt . Sprintf ( " " , c . Command , args , env , c . API } 
func New Config ( ) * Config { return & Config { Preferences : * New Preferences ( ) , Clusters : make ( map [ string ] * Cluster ) , Auth Infos : make ( map [ string ] * Auth } 
func New Auth Info ( ) * Auth Info { return & Auth Info { Extensions : make ( map [ string ] runtime . Object ) , Impersonate User } 
func ( s * certificate Signing Request Lister ) List ( selector labels . Selector ) ( ret [ ] * v1beta1 . Certificate Signing Request , err error ) { err = cache . List All ( s . indexer , selector , func ( m interface { } ) { ret = append ( ret , m . ( * v1beta1 . Certificate Signing } 
func ( s * certificate Signing Request Lister ) Get ( name string ) ( * v1beta1 . Certificate Signing Request , error ) { obj , exists , err := s . indexer . Get By if ! exists { return nil , errors . New Not return obj . ( * v1beta1 . Certificate Signing } 
func ( c * Events V1beta1Client ) REST return c . rest } 
func ( az * Cloud ) create File Share ( account Name , account Key , name string , size Gi B int ) error { return az . File Client . create File Share ( account Name , account Key , name , size Gi } 
func ( f * azure File Client ) delete File Share ( account Name , account Key , name string ) error { file Client , err := f . get File Svc Client ( account Name , account return file Client . Get Share } 
func New Volume Binder ( client clientset . Interface , node Informer coreinformers . Node Informer , pvc Informer coreinformers . Persistent Volume Claim Informer , pv Informer coreinformers . Persistent Volume Informer , storage Class Informer storageinformers . Storage Class Informer , bind Timeout time . Duration ) * Volume Binder { return & Volume Binder { Binder : persistentvolume . New Volume Binder ( client , node Informer , pvc Informer , pv Informer , storage Class Informer , bind } 
func New Fake Volume Binder ( config * persistentvolume . Fake Volume Binder Config ) * Volume Binder { return & Volume Binder { Binder : persistentvolume . New Fake Volume } 
func ( b * Volume Binder ) Delete Pod Bindings ( pod * v1 . Pod ) { cache := b . Binder . Get Bindings if cache != nil && pod != nil { cache . Delete } 
func Convert_v1alpha1_SA Controller Configuration_To_config_SA Controller Configuration ( in * v1alpha1 . SA Controller Configuration , out * config . SA Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_SA Controller Configuration_To_config_SA Controller } 
func Convert_config_SA Controller Configuration_To_v1alpha1_SA Controller Configuration ( in * config . SA Controller Configuration , out * v1alpha1 . SA Controller Configuration , s conversion . Scope ) error { return auto Convert_config_SA Controller Configuration_To_v1alpha1_SA Controller } 
func New ( kube Client clientset . Interface , cloud cloudprovider . Interface , node Informer informers . Node Informer , allocator Type CIDR Allocator Type , cluster CIDR , service CIDR * net . IP Net , node CIDR Mask Size int ) ( CIDR Allocator , error ) { node List , err := list Nodes ( kube switch allocator Type { case Range Allocator Type : return New CIDR Range Allocator ( kube Client , node Informer , cluster CIDR , service CIDR , node CIDR Mask Size , node case Cloud Allocator Type : return New Cloud CIDR Allocator ( kube Client , cloud , node default : return nil , fmt . Errorf ( " " , allocator } 
func ( lb * Load Balancer RR ) Service Has Endpoints ( svc Port proxy . Service Port Name ) bool { lb . lock . R defer lb . lock . R state , exists := lb . services [ svc } 
func New Proxy Server ( o * Options ) ( * Proxy Server , error ) { return new Proxy Server ( o . config , o . Cleanup And } 
func Install ( scheme * runtime . Scheme ) { utilruntime . Must ( authentication . Add To utilruntime . Must ( v1beta1 . Add To utilruntime . Must ( v1 . Add To utilruntime . Must ( scheme . Set Version Priority ( v1 . Scheme Group Version , v1beta1 . Scheme Group } 
func ( cs Checksum ) Verify ( data interface { } ) error { if cs != New ( data ) { return errors . Err Corrupt } 
func get hashutil . Deep Hash } 
func New Node Lifecycle Controller ( lease Informer coordinformers . Lease Informer , pod Informer coreinformers . Pod Informer , node Informer coreinformers . Node Informer , daemon Set Informer appsv1informers . Daemon Set Informer , kube Client clientset . Interface , node Monitor Period time . Duration , node Startup Grace Period time . Duration , node Monitor Grace Period time . Duration , pod Eviction Timeout time . Duration , eviction Limiter QPS float32 , secondary Eviction Limiter QPS float32 , large Cluster Threshold int32 , unhealthy Zone Threshold float32 , run Taint Manager bool , use Taint Based Evictions bool , taint Node By Condition bool ) ( * Controller , error ) { if kube event Broadcaster := record . New recorder := event Broadcaster . New Recorder ( scheme . Scheme , v1 . Event event Broadcaster . Start event Broadcaster . Start Recording To Sink ( & v1core . Event Sink Impl { Interface : v1core . New ( kube Client . Core V1 ( ) . REST if kube Client . Core V1 ( ) . REST Client ( ) . Get Rate Limiter ( ) != nil { metrics . Register Metric And Track Rate Limiter Usage ( " " , kube Client . Core V1 ( ) . REST Client ( ) . Get Rate nc := & Controller { kube Client : kube Client , now : metav1 . Now , known Node Set : make ( map [ string ] * v1 . Node ) , node Health Map : make ( map [ string ] * node Health Data ) , recorder : recorder , node Monitor Period : node Monitor Period , node Startup Grace Period : node Startup Grace Period , node Monitor Grace Period : node Monitor Grace Period , zone Pod Evictor : make ( map [ string ] * scheduler . Rate Limited Timed Queue ) , zone No Execute Tainter : make ( map [ string ] * scheduler . Rate Limited Timed Queue ) , zone States : make ( map [ string ] Zone State ) , pod Eviction Timeout : pod Eviction Timeout , eviction Limiter QPS : eviction Limiter QPS , secondary Eviction Limiter QPS : secondary Eviction Limiter QPS , large Cluster Threshold : large Cluster Threshold , unhealthy Zone Threshold : unhealthy Zone Threshold , run Taint Manager : run Taint Manager , use Taint Based Evictions : use Taint Based Evictions && run Taint Manager , taint Node By Condition : taint Node By Condition , node Update Queue : workqueue . New if use Taint Based nc . enter Partial Disruption Func = nc . Reduced QPS nc . enter Full Disruption Func = nc . Healthy QPS nc . compute Zone State Func = nc . Compute Zone pod Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add if nc . taint Manager != nil { nc . taint Manager . Pod } , Update Func : func ( prev , obj interface { } ) { prev new if nc . taint Manager != nil { nc . taint Manager . Pod Updated ( prev Pod , new } , Delete Func : func ( obj interface { } ) { pod , is // We can get Deleted Final State Unknown instead of *v1.Pod here and we need to handle that correctly. if ! is Pod { deleted State , ok := obj . ( cache . Deleted Final State pod , ok = deleted if ! ok { klog . Errorf ( " " , deleted if nc . taint Manager != nil { nc . taint Manager . Pod nc . pod Informer Synced = pod Informer . Informer ( ) . Has if nc . run Taint Manager { pod Lister := pod pod Getter := func ( name , namespace string ) ( * v1 . Pod , error ) { return pod node Lister := node node Getter := func ( name string ) ( * v1 . Node , error ) { return node nc . taint Manager = scheduler . New No Execute Taint Manager ( kube Client , pod Getter , node node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : nodeutil . Create Add Node Handler ( func ( node * v1 . Node ) error { nc . taint Manager . Node } ) , Update Func : nodeutil . Create Update Node Handler ( func ( old Node , new Node * v1 . Node ) error { nc . taint Manager . Node Updated ( old Node , new } ) , Delete Func : nodeutil . Create Delete Node Handler ( func ( node * v1 . Node ) error { nc . taint Manager . Node node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : nodeutil . Create Add Node Handler ( func ( node * v1 . Node ) error { nc . node Update } ) , Update Func : nodeutil . Create Update Node Handler ( func ( _ , new Node * v1 . Node ) error { nc . node Update Queue . Add ( new if nc . taint Node By nc . lease Lister = lease if utilfeature . Default Feature Gate . Enabled ( features . Node Lease ) { nc . lease Informer Synced = lease Informer . Informer ( ) . Has } else { // Always indicate that lease is synced to prevent syncing lease. nc . lease Informer nc . node Lister = node nc . node Informer Synced = node Informer . Informer ( ) . Has nc . daemon Set Store = daemon Set nc . daemon Set Informer Synced = daemon Set Informer . Informer ( ) . Has } 
func ( nc * Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle if ! controller . Wait For Cache Sync ( " " , stop Ch , nc . lease Informer Synced , nc . node Informer Synced , nc . pod Informer Synced , nc . daemon Set Informer if nc . run Taint Manager { go nc . taint Manager . Run ( stop // Close node update queue to cleanup go routine. defer nc . node Update Queue . Shut // Start workers to reconcile labels and/or update No Schedule taint for nodes. for i := 0 ; i < scheduler . Update Worker Size ; i ++ { // Thanks to "workqueue", each worker just need to get item from queue, because // the item is flagged when got from queue: if new event come, the new item will // be re-queued until "Done", so no more than one worker handle the same item and // no event missed. go wait . Until ( nc . do Node Processing Pass Worker , time . Second , stop if nc . use Taint Based Evictions { // Handling taint based evictions. Because we don't want a dedicated logic in Taint Manager for NC-originated // taints and we normally don't rate limit evictions caused by taints, we need to rate limit adding taints. go wait . Until ( nc . do No Execute Tainting Pass , scheduler . Node Eviction Period , stop } else { // Managing eviction of nodes: // When we delete pods off a node, if the node was not empty at the time we then // queue an eviction watcher. If we hit an error, retry deletion. go wait . Until ( nc . do Eviction Pass , scheduler . Node Eviction Period , stop // Incorporate the results of node health signal pushed from kubelet to master. go wait . Until ( func ( ) { if err := nc . monitor Node } , nc . node Monitor Period , stop <- stop } 
func ( nc * Controller ) monitor Node Health ( ) error { // We are listing nodes from local cache as we can tolerate some small delays // comparing to state from etcd and there is eventual consistency anyway. nodes , err := nc . node added , deleted , new Zone Representatives := nc . classify for i := range new Zone Representatives { nc . add Pod Evictor For New Zone ( new Zone nodeutil . Record Node Event ( nc . recorder , added [ i ] . Name , string ( added [ i ] . UID ) , v1 . Event Type nc . known Node nc . add Pod Evictor For New if nc . use Taint Based Evictions { nc . mark Node As } else { nc . cancel Pod nodeutil . Record Node Event ( nc . recorder , deleted [ i ] . Name , string ( deleted [ i ] . UID ) , v1 . Event Type delete ( nc . known Node zone To Node Conditions := map [ string ] [ ] * v1 . Node for i := range nodes { var grace var observed Ready Condition v1 . Node var current Ready Condition * v1 . Node node := nodes [ i ] . Deep if err := wait . Poll Immediate ( retry Sleep Time , retry Sleep Time * scheduler . Node Health Update Retry , func ( ) ( bool , error ) { grace Period , observed Ready Condition , current Ready Condition , err = nc . try Update Node node , err = nc . kube Client . Core V1 ( ) . Nodes ( ) . Get ( name , metav1 . Get // We do not treat a master node as a part of the cluster for network disruption checking. if ! system . Is Master Node ( node . Name ) { zone To Node Conditions [ utilnode . Get Zone Key ( node ) ] = append ( zone To Node Conditions [ utilnode . Get Zone Key ( node ) ] , current Ready decision if current Ready Condition != nil { // Check eviction timeout against decision Timestamp if observed Ready Condition . Status == v1 . Condition False { if nc . use Taint Based Evictions { // We want to update the taint straight away if Node is already tainted with the Unreachable Taint if taintutils . Taint Exists ( node . Spec . Taints , Unreachable Taint Template ) { taint To Add := * Not Ready Taint if ! nodeutil . Swap Node Controller Taint ( nc . kube Client , [ ] * v1 . Taint { & taint To Add } , [ ] * v1 . Taint { Unreachable Taint } else if nc . mark Node For Tainting ( node ) { klog . V ( 2 ) . Infof ( " " , node . Name , decision } else { if decision Timestamp . After ( nc . node Health Map [ node . Name ] . ready Transition Timestamp . Add ( nc . pod Eviction Timeout ) ) { if nc . evict Pods ( node ) { klog . V ( 2 ) . Infof ( " " , node . Name , decision Timestamp , nc . node Health Map [ node . Name ] . ready Transition Timestamp , nc . pod Eviction if observed Ready Condition . Status == v1 . Condition Unknown { if nc . use Taint Based Evictions { // We want to update the taint straight away if Node is already tainted with the Unreachable Taint if taintutils . Taint Exists ( node . Spec . Taints , Not Ready Taint Template ) { taint To Add := * Unreachable Taint if ! nodeutil . Swap Node Controller Taint ( nc . kube Client , [ ] * v1 . Taint { & taint To Add } , [ ] * v1 . Taint { Not Ready Taint } else if nc . mark Node For Tainting ( node ) { klog . V ( 2 ) . Infof ( " " , node . Name , decision } else { if decision Timestamp . After ( nc . node Health Map [ node . Name ] . probe Timestamp . Add ( nc . pod Eviction Timeout ) ) { if nc . evict Pods ( node ) { klog . V ( 2 ) . Infof ( " " , node . Name , decision Timestamp , nc . node Health Map [ node . Name ] . ready Transition Timestamp , nc . pod Eviction Timeout - grace if observed Ready Condition . Status == v1 . Condition True { if nc . use Taint Based Evictions { removed , err := nc . mark Node As } else { if nc . cancel Pod // Report node event. if current Ready Condition . Status != v1 . Condition True && observed Ready Condition . Status == v1 . Condition True { nodeutil . Record Node Status if err = nodeutil . Mark All Pods Not Ready ( nc . kube Client , node ) ; err != nil { utilruntime . Handle nc . handle Disruption ( zone To Node } 
func ( nc * Controller ) try Update Node Health ( node * v1 . Node ) ( time . Duration , v1 . Node Condition , * v1 . Node var grace var observed Ready Condition v1 . Node _ , current Ready Condition := nodeutil . Get Node Condition ( & node . Status , v1 . Node if current Ready Condition == nil { // If ready condition is nil, then kubelet (or nodecontroller) never posted node status. // A fake ready condition is created, where Last Heartbeat Time and Last Transition Time is set // to node.Creation Timestamp to avoid handle the corner case. observed Ready Condition = v1 . Node Condition { Type : v1 . Node Ready , Status : v1 . Condition Unknown , Last Heartbeat Time : node . Creation Timestamp , Last Transition Time : node . Creation grace Period = nc . node Startup Grace if _ , found := nc . node Health Map [ node . Name ] ; found { nc . node Health } else { nc . node Health Map [ node . Name ] = & node Health Data { status : & node . Status , probe Timestamp : node . Creation Timestamp , ready Transition Timestamp : node . Creation } else { // If ready condition is not nil, make a copy of it, since we may modify it in place later. observed Ready Condition = * current Ready grace Period = nc . node Monitor Grace saved Node Health , found := nc . node Health // There are following cases to check: // - both saved and new status have no Ready Condition set - we leave everything as it is, // - saved status have no Ready Condition, but current one does - Controller was restarted with Node data already present in etcd, // - saved status have some Ready Condition, but current one does not - it's an error, but we fill it up because that's probably a good thing to do, // - both saved and current statuses have Ready Conditions and they have the same Last Probe Time - nothing happened on that Node, it may be // unresponsive, so we leave it as it is, // - both saved and current statuses have Ready Conditions, they have different Last Probe Times, but the same Ready Condition State - // everything's in order, no transition occurred, we update only probe Timestamp, // - both saved and current statuses have Ready Conditions, different Last Probe Times and different Ready Condition State - // Ready Condition changed it state since we last seen it, so we update both probe Timestamp and ready Transition Timestamp. // TODO: things to consider: // - if 'Last Probe Time' have gone back in time its probably an error, currently we ignore it, // - currently only correct Ready State transition outside of Node Controller is marking it ready by Kubelet, we don't check // if that's the case, but it does not seem necessary. var saved Condition * v1 . Node var saved if found { _ , saved Condition = nodeutil . Get Node Condition ( saved Node Health . status , v1 . Node saved Lease = saved Node _ , observed Condition := nodeutil . Get Node Condition ( & node . Status , v1 . Node saved Node Health = & node Health Data { status : & node . Status , probe Timestamp : nc . now ( ) , ready Transition } else if saved Condition == nil && observed saved Node Health = & node Health Data { status : & node . Status , probe Timestamp : nc . now ( ) , ready Transition } else if saved Condition != nil && observed // TODO: figure out what to do in this case. For now we do the same thing as above. saved Node Health = & node Health Data { status : & node . Status , probe Timestamp : nc . now ( ) , ready Transition } else if saved Condition != nil && observed Condition != nil && saved Condition . Last Heartbeat Time != observed Condition . Last Heartbeat Time { var transition // If Ready Condition changed since the last time we checked, we update the transition timestamp to "now", // otherwise we leave it as it is. if saved Condition . Last Transition Time != observed Condition . Last Transition Time { klog . V ( 3 ) . Infof ( " " , node . Name , saved Condition , observed transition } else { transition Time = saved Node Health . ready Transition if klog . V ( 5 ) { klog . V ( 5 ) . Infof ( " " , node . Name , saved Node saved Node Health = & node Health Data { status : & node . Status , probe Timestamp : nc . now ( ) , ready Transition Timestamp : transition var observed if utilfeature . Default Feature Gate . Enabled ( features . Node Lease ) { // Always update the probe time if node lease is renewed. // Note: If kubelet never posted the node status, but continues renewing the // heartbeat leases, the node controller will assume the node is healthy and // take no action. observed Lease , _ = nc . lease Lister . Leases ( v1 . Namespace Node if observed Lease != nil && ( saved Lease == nil || saved Lease . Spec . Renew Time . Before ( observed Lease . Spec . Renew Time ) ) { saved Node Health . lease = observed saved Node Health . probe nc . node Health Map [ node . Name ] = saved Node if nc . now ( ) . After ( saved Node Health . probe Timestamp . Add ( grace Period ) ) { // Node Ready condition or lease was last set longer ago than grace Period, so // update it to Unknown (regardless of its current value) in the master. if current Ready node . Status . Conditions = append ( node . Status . Conditions , v1 . Node Condition { Type : v1 . Node Ready , Status : v1 . Condition Unknown , Reason : " " , Message : fmt . Sprintf ( " " ) , Last Heartbeat Time : node . Creation Timestamp , Last Transition } else { klog . V ( 4 ) . Infof ( " " , node . Name , nc . now ( ) . Time . Sub ( saved Node Health . probe Timestamp . Time ) , observed Ready if observed Ready Condition . Status != v1 . Condition Unknown { current Ready Condition . Status = v1 . Condition current Ready current Ready // Last Probe Time is the last time we heard from kubelet. current Ready Condition . Last Heartbeat Time = observed Ready Condition . Last Heartbeat current Ready Condition . Last Transition // remaining node conditions should also be set to Unknown remaining Node Condition Types := [ ] v1 . Node Condition Type { v1 . Node Memory Pressure , v1 . Node Disk Pressure , v1 . Node PID Pressure , // We don't change 'Node Network Unavailable' condition, as it's managed on a control plane level. // v1.Node Network now for _ , node Condition Type := range remaining Node Condition Types { _ , current Condition := nodeutil . Get Node Condition ( & node . Status , node Condition if current Condition == nil { klog . V ( 2 ) . Infof ( " " , node Condition node . Status . Conditions = append ( node . Status . Conditions , v1 . Node Condition { Type : node Condition Type , Status : v1 . Condition Unknown , Reason : " " , Message : " " , Last Heartbeat Time : node . Creation Timestamp , Last Transition Time : now } else { klog . V ( 4 ) . Infof ( " " , node . Name , nc . now ( ) . Time . Sub ( saved Node Health . probe Timestamp . Time ) , node Condition Type , current if current Condition . Status != v1 . Condition Unknown { current Condition . Status = v1 . Condition current current current Condition . Last Transition Time = now _ , current Condition := nodeutil . Get Node Condition ( & node . Status , v1 . Node if ! apiequality . Semantic . Deep Equal ( current Condition , & observed Ready Condition ) { if _ , err = nc . kube Client . Core V1 ( ) . Nodes ( ) . Update return grace Period , observed Ready Condition , current Ready nc . node Health Map [ node . Name ] = & node Health Data { status : & node . Status , probe Timestamp : nc . node Health Map [ node . Name ] . probe Timestamp , ready Transition Timestamp : nc . now ( ) , lease : observed return grace Period , observed Ready Condition , current Ready return grace Period , observed Ready Condition , current Ready } 
func ( nc * Controller ) classify Nodes ( all Nodes [ ] * v1 . Node ) ( added , deleted , new Zone Representatives [ ] * v1 . Node ) { for i := range all Nodes { if _ , has := nc . known Node Set [ all Nodes [ i ] . Name ] ; ! has { added = append ( added , all } else { // Currently, we only consider new zone as updated. zone := utilnode . Get Zone Key ( all if _ , found := nc . zone States [ zone ] ; ! found { new Zone Representatives = append ( new Zone Representatives , all // If there's a difference between lengths of known Nodes and observed nodes // we must have removed some Node. if len ( nc . known Node Set ) + len ( added ) != len ( all Nodes ) { know Set for k , v := range nc . known Node Set { know Set for i := range all Nodes { delete ( know Set Copy , all for i := range know Set Copy { deleted = append ( deleted , know Set } 
func ( nc * Controller ) Reduced QPS Func ( node Num int ) float32 { if int32 ( node Num ) > nc . large Cluster Threshold { return nc . secondary Eviction Limiter } 
func ( nc * Controller ) add Pod Evictor For New Zone ( node * v1 . Node ) { nc . evictor defer nc . evictor zone := utilnode . Get Zone if _ , found := nc . zone States [ zone ] ; ! found { nc . zone States [ zone ] = state if ! nc . use Taint Based Evictions { nc . zone Pod Evictor [ zone ] = scheduler . New Rate Limited Timed Queue ( flowcontrol . New Token Bucket Rate Limiter ( nc . eviction Limiter QPS , scheduler . Eviction Rate Limiter } else { nc . zone No Execute Tainter [ zone ] = scheduler . New Rate Limited Timed Queue ( flowcontrol . New Token Bucket Rate Limiter ( nc . eviction Limiter QPS , scheduler . Eviction Rate Limiter evictions Number . With Label } 
func ( nc * Controller ) cancel Pod Eviction ( node * v1 . Node ) bool { zone := utilnode . Get Zone nc . evictor defer nc . evictor was Deleting := nc . zone Pod if was } 
func ( nc * Controller ) evict Pods ( node * v1 . Node ) bool { nc . evictor defer nc . evictor return nc . zone Pod Evictor [ utilnode . Get Zone } 
func ( nc * Controller ) Compute Zone State ( node Ready Conditions [ ] * v1 . Node Condition ) ( int , Zone State ) { ready not Ready for i := range node Ready Conditions { if node Ready Conditions [ i ] != nil && node Ready Conditions [ i ] . Status == v1 . Condition True { ready } else { not Ready switch { case ready Nodes == 0 && not Ready Nodes > 0 : return not Ready Nodes , state Full case not Ready Nodes > 2 && float32 ( not Ready Nodes ) / float32 ( not Ready Nodes + ready Nodes ) >= nc . unhealthy Zone Threshold : return not Ready Nodes , state Partial default : return not Ready Nodes , state } 
func ( nc * Controller ) reconcile Node Labels ( node Name string ) error { node , err := nc . node Lister . Get ( node if err != nil { // If node not found, just ignore it. if apierrors . Is Not labels To for _ , r := range label Reconcile Info { primary Value , primary Exists := node . Labels [ r . primary secondary Value , secondary Exists := node . Labels [ r . secondary if ! primary if secondary Exists && primary Value != secondary Value { // Secondary label exists, but not consistent with the primary // label. Need to reconcile. labels To Update [ r . secondary Key ] = primary } else if ! secondary Exists && r . ensure Secondary Exists { // Apply secondary label based on primary label. labels To Update [ r . secondary Key ] = primary if len ( labels To if ! nodeutil . Add Or Update Labels On Node ( nc . kube Client , labels To } 
func add Known Types ( scheme * runtime . Scheme ) error { // TODO this gets cleaned up when the types are fixed scheme . Add Known Types ( Scheme Group Version , & apps . Deployment { } , & apps . Deployment List { } , & apps . Deployment Rollback { } , & Replication Controller Dummy { } , & apps . Daemon Set List { } , & apps . Daemon Set { } , & networking . Ingress { } , & networking . Ingress List { } , & apps . Replica Set { } , & apps . Replica Set List { } , & policy . Pod Security Policy { } , & policy . Pod Security Policy List { } , & autoscaling . Scale { } , & networking . Network Policy { } , & networking . Network Policy } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v1 . Config Map { } , func ( obj interface { } ) { Set Object Defaults_Config Map ( obj . ( * v1 . Config scheme . Add Type Defaulting Func ( & v1 . Config Map List { } , func ( obj interface { } ) { Set Object Defaults_Config Map List ( obj . ( * v1 . Config Map scheme . Add Type Defaulting Func ( & v1 . Endpoints { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Endpoints List { } , func ( obj interface { } ) { Set Object Defaults_Endpoints List ( obj . ( * v1 . Endpoints scheme . Add Type Defaulting Func ( & v1 . Limit Range { } , func ( obj interface { } ) { Set Object Defaults_Limit Range ( obj . ( * v1 . Limit scheme . Add Type Defaulting Func ( & v1 . Limit Range List { } , func ( obj interface { } ) { Set Object Defaults_Limit Range List ( obj . ( * v1 . Limit Range scheme . Add Type Defaulting Func ( & v1 . Namespace { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Namespace List { } , func ( obj interface { } ) { Set Object Defaults_Namespace List ( obj . ( * v1 . Namespace scheme . Add Type Defaulting Func ( & v1 . Node { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Node List { } , func ( obj interface { } ) { Set Object Defaults_Node List ( obj . ( * v1 . Node scheme . Add Type Defaulting Func ( & v1 . Persistent Volume { } , func ( obj interface { } ) { Set Object Defaults_Persistent Volume ( obj . ( * v1 . Persistent scheme . Add Type Defaulting Func ( & v1 . Persistent Volume Claim { } , func ( obj interface { } ) { Set Object Defaults_Persistent Volume Claim ( obj . ( * v1 . Persistent Volume scheme . Add Type Defaulting Func ( & v1 . Persistent Volume Claim List { } , func ( obj interface { } ) { Set Object Defaults_Persistent Volume Claim List ( obj . ( * v1 . Persistent Volume Claim scheme . Add Type Defaulting Func ( & v1 . Persistent Volume List { } , func ( obj interface { } ) { Set Object Defaults_Persistent Volume List ( obj . ( * v1 . Persistent Volume scheme . Add Type Defaulting Func ( & v1 . Pod { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Pod List { } , func ( obj interface { } ) { Set Object Defaults_Pod List ( obj . ( * v1 . Pod scheme . Add Type Defaulting Func ( & v1 . Pod Template { } , func ( obj interface { } ) { Set Object Defaults_Pod Template ( obj . ( * v1 . Pod scheme . Add Type Defaulting Func ( & v1 . Pod Template List { } , func ( obj interface { } ) { Set Object Defaults_Pod Template List ( obj . ( * v1 . Pod Template scheme . Add Type Defaulting Func ( & v1 . Replication Controller { } , func ( obj interface { } ) { Set Object Defaults_Replication Controller ( obj . ( * v1 . Replication scheme . Add Type Defaulting Func ( & v1 . Replication Controller List { } , func ( obj interface { } ) { Set Object Defaults_Replication Controller List ( obj . ( * v1 . Replication Controller scheme . Add Type Defaulting Func ( & v1 . Resource Quota { } , func ( obj interface { } ) { Set Object Defaults_Resource Quota ( obj . ( * v1 . Resource scheme . Add Type Defaulting Func ( & v1 . Resource Quota List { } , func ( obj interface { } ) { Set Object Defaults_Resource Quota List ( obj . ( * v1 . Resource Quota scheme . Add Type Defaulting Func ( & v1 . Secret { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Secret List { } , func ( obj interface { } ) { Set Object Defaults_Secret List ( obj . ( * v1 . Secret scheme . Add Type Defaulting Func ( & v1 . Service { } , func ( obj interface { } ) { Set Object scheme . Add Type Defaulting Func ( & v1 . Service List { } , func ( obj interface { } ) { Set Object Defaults_Service List ( obj . ( * v1 . Service } 
func Request Certificate ( client certificatesclient . Certificate Signing Request Interface , csr Data [ ] byte , name string , usages [ ] certificates . Key Usage , private Key interface { } ) ( req * certificates . Certificate Signing Request , err error ) { csr := & certificates . Certificate Signing Request { // Username, UID, Groups will be injected by API server. Type Meta : metav1 . Type Meta { Kind : " " } , Object Meta : metav1 . Object Meta { Name : name , } , Spec : certificates . Certificate Signing Request Spec { Request : csr if len ( csr . Name ) == 0 { csr . Generate switch { case err == nil : case errors . Is Already req , err = client . Get ( name , metav1 . Get if err != nil { return nil , format if err := ensure Compatible ( req , csr , private default : return nil , format } 
func Wait For Certificate ( client certificatesclient . Certificate Signing Request Interface , req * certificates . Certificate Signing Request , timeout time . Duration ) ( cert Data [ ] byte , err error ) { field Selector := fields . One Term Equal lw := & cache . List Watch { List Func : func ( options metav1 . List Options ) ( runtime . Object , error ) { options . Field Selector = field } , Watch Func : func ( options metav1 . List Options ) ( watch . Interface , error ) { options . Field Selector = field ctx , cancel := watchtools . Context With Optional event , err := watchtools . Until With Sync ( ctx , lw , & certificates . Certificate Signing csr := event . Object . ( * certificates . Certificate Signing for _ , c := range csr . Status . Conditions { if c . Type == certificates . Certificate if c . Type == certificates . Certificate if err == wait . Err Wait Timeout { return nil , wait . Err Wait if err != nil { return nil , format return event . Object . ( * certificates . Certificate Signing } 
func ensure Compatible ( new , orig * certificates . Certificate Signing Request , private Key interface { } ) error { new CSR , err := parse orig CSR , err := parse if ! reflect . Deep Equal ( new CSR . Subject , orig CSR . Subject ) { return fmt . Errorf ( " " , new CSR . Subject , orig signer , ok := private new CSR . Public if err := new CSR . Check if len ( new . Status . Certificate ) > 0 { certs , err := certutil . Parse Certs for _ , cert := range certs { if now . After ( cert . Not After ) { return fmt . Errorf ( " " , cert . Not } 
func format Error ( format string , err error ) error { if s , ok := err . ( errors . API Status ) ; ok { se := & errors . Status Error { Err se . Err Status . Message = fmt . Sprintf ( format , se . Err } 
func parse CSR ( obj * certificates . Certificate Signing Request ) ( * x509 . Certificate return x509 . Parse Certificate } 
func Create ( dirpath string , metadata [ ] byte ) ( * WAL , error ) { if Exist ( dirpath ) { return nil , os . Err if err := os . Mkdir All ( dirpath , private Dir p := path . Join ( dirpath , wal f , err := os . Open l , err := fileutil . New w := & WAL { dir : dirpath , metadata : metadata , seq : 0 , f : f , encoder : new if err := w . save if err := w . encoder . encode ( & walpb . Record { Type : metadata if err := w . Save } 
func ( w * WAL ) Release Lock for i , l := range w . locks { _ , lock Index , err := parse Wal if lock } 
func Tunnel Sync Health lag := tunneler . Seconds Since ssh Key Lag := tunneler . Seconds Since SSH Key // Since we are syncing ssh-keys every 5 minutes, the allowed // lag since last sync should be more than 2x higher than that // to allow for single failure, which can always happen. // For now set it to 3x, which is 15 minutes. // For more details see: http://pr.k8s.io/59347 if ssh Key Lag > 900 { return fmt . Errorf ( " " , ssh Key } 
func ( c * SSH Tunneler ) Run ( get Addresses Address Func ) { if c . stop c . stop // Save the address getter if get Addresses != nil { c . get Addresses = get // Usernames are capped @ 32 if len ( c . SSH c . SSH User = c . SSH klog . Infof ( " " , c . SSH User , c . SSH // public keyfile is written last, so check for that. public Key File := c . SSH exists , err := utilpath . Exists ( utilpath . Check Follow Symlink , public Key if err := generate SSH Key ( c . SSH Keyfile , public Key c . tunnels = ssh . New SSH Tunnel List ( c . SSH User , c . SSH Keyfile , c . Health Check URL , c . stop // Sync loop to ensure that the SSH key has been installed. c . last SSH Key c . install SSH Key Sync Loop ( c . SSH User , public Key // Sync tunnel List w/ nodes. c . last c . nodes Sync } 
func ( c * SSH Tunneler ) Stop ( ) { if c . stop Chan != nil { close ( c . stop c . stop } 
func ( c * SSH Tunneler ) nodes Sync Loop ( ) { // TODO (cjcullen) make this watch. go wait . Until ( func ( ) { addrs , err := c . get atomic . Store Int64 ( & c . last } , 15 * time . Second , c . stop } 
func responsible For Pod ( pod * v1 . Pod , scheduler Name string ) bool { return scheduler Name == pod . Spec . Scheduler } 
func ( sched * Scheduler ) skip Pod Update ( pod * v1 . Pod ) bool { // Non-assumed pods should never be skipped. is Assumed , err := sched . config . Scheduler Cache . Is Assumed if err != nil { utilruntime . Handle if ! is // Gets the assumed pod from the cache. assumed Pod , err := sched . config . Scheduler Cache . Get if err != nil { utilruntime . Handle // Compares the assumed pod in the cache with the pod update. If they are // equal (with certain fields excluded), this pod update will be skipped. f := func ( pod * v1 . Pod ) * v1 . Pod { p := pod . Deep // Resource Version must be excluded because each object update will // have a new resource version. p . Resource // Spec.Node Name must be excluded because the pod assumed in the cache // is expected to have a node assigned while the pod update may nor may // not have this field set. p . Spec . Node assumed Pod Copy , pod Copy := f ( assumed if ! reflect . Deep Equal ( assumed Pod Copy , pod } 
func Add All Event Handlers ( sched * Scheduler , scheduler Name string , node Informer coreinformers . Node Informer , pod Informer coreinformers . Pod Informer , pv Informer coreinformers . Persistent Volume Informer , pvc Informer coreinformers . Persistent Volume Claim Informer , service Informer coreinformers . Service Informer , storage Class Informer storageinformers . Storage Class Informer , ) { // scheduled pod cache pod Informer . Informer ( ) . Add Event Handler ( cache . Filtering Resource Event Handler { Filter Func : func ( obj interface { } ) bool { switch t := obj . ( type ) { case * v1 . Pod : return assigned case cache . Deleted Final State Unknown : if pod , ok := t . Obj . ( * v1 . Pod ) ; ok { return assigned utilruntime . Handle default : utilruntime . Handle } , Handler : cache . Resource Event Handler Funcs { Add Func : sched . add Pod To Cache , Update Func : sched . update Pod In Cache , Delete Func : sched . delete Pod From // unscheduled pod queue pod Informer . Informer ( ) . Add Event Handler ( cache . Filtering Resource Event Handler { Filter Func : func ( obj interface { } ) bool { switch t := obj . ( type ) { case * v1 . Pod : return ! assigned Pod ( t ) && responsible For Pod ( t , scheduler case cache . Deleted Final State Unknown : if pod , ok := t . Obj . ( * v1 . Pod ) ; ok { return ! assigned Pod ( pod ) && responsible For Pod ( pod , scheduler utilruntime . Handle default : utilruntime . Handle } , Handler : cache . Resource Event Handler Funcs { Add Func : sched . add Pod To Scheduling Queue , Update Func : sched . update Pod In Scheduling Queue , Delete Func : sched . delete Pod From Scheduling node Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : sched . add Node To Cache , Update Func : sched . update Node In Cache , Delete Func : sched . delete Node From // On add and delete of P Vs, it will affect equivalence cache items // related to persistent volume pv Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { // Max PD Volume Count Predicate: since it relies on the counts of PV. Add Func : sched . on Pv Add , Update Func : sched . on Pv // This is for Max PD Volume Count Predicate: add/delete PVC will affect counts of PV when it is bound. pvc Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : sched . on Pvc Add , Update Func : sched . on Pvc // This is for Service Affinity: affected by the selector of the service is updated. // Also, if new service is added, equivalence cache will also become invalid since // existing pods may be "captured" by this service and change this predicate result. service Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : sched . on Service Add , Update Func : sched . on Service Update , Delete Func : sched . on Service storage Class Informer . Informer ( ) . Add Event Handler ( cache . Resource Event Handler Funcs { Add Func : sched . on Storage Class } 
func Empty Predicate Metadata Producer ( pod * v1 . Pod , node Name To Info map [ string ] * schedulernodeinfo . Node Info ) Predicate } 
func Register Predicate Metadata Producer With Extended Resource Options ( ignored Extended Resources sets . String ) { Register Predicate Metadata Producer ( " " , func ( pm * predicate Metadata ) { pm . ignored Extended Resources = ignored Extended } 
func New Predicate Metadata Factory ( pod Lister algorithm . Pod Lister ) Predicate Metadata Producer { factory := & Predicate Metadata Factory { pod return factory . Get } 
func ( pfactory * Predicate Metadata Factory ) Get Metadata ( pod * v1 . Pod , node Name To Info Map map [ string ] * schedulernodeinfo . Node Info ) Predicate // existing Pod Anti Affinity Map will be used later for efficient check on existing pods' anti-affinity existing Pod Anti Affinity Map , err := get TP Map Matching Existing Anti Affinity ( pod , node Name To Info // incoming Pod Affinity Map will be used later for efficient check on incoming pod's affinity // incoming Pod Anti Affinity Map will be used later for efficient check on incoming pod's anti-affinity incoming Pod Affinity Map , incoming Pod Anti Affinity Map , err := get TP Map Matching Incoming Affinity Anti Affinity ( pod , node Name To Info predicate Metadata := & predicate Metadata { pod : pod , pod Best Effort : is Pod Best Effort ( pod ) , pod Request : Get Resource Request ( pod ) , pod Ports : schedutil . Get Container Ports ( pod ) , topology Pairs Potential Affinity Pods : incoming Pod Affinity Map , topology Pairs Potential Anti Affinity Pods : incoming Pod Anti Affinity Map , topology Pairs Anti Affinity Pods Map : existing Pod Anti Affinity for predicate Name , precompute Func := range predicate Metadata Producers { klog . V ( 10 ) . Infof ( " " , predicate precompute Func ( predicate return predicate } 
func new Topology Pairs Maps ( ) * topology Pairs Maps { return & topology Pairs Maps { topology Pair To Pods : make ( map [ topology Pair ] pod Set ) , pod To Topology Pairs : make ( map [ string ] topology Pair } 
func ( meta * predicate Metadata ) Remove Pod ( deleted Pod * v1 . Pod ) error { deleted Pod Full Name := schedutil . Get Pod Full Name ( deleted if deleted Pod Full Name == schedutil . Get Pod Full meta . topology Pairs Anti Affinity Pods Map . remove Pod ( deleted // Delete pod from the matching affinity or anti-affinity topology pairs maps. meta . topology Pairs Potential Affinity Pods . remove Pod ( deleted meta . topology Pairs Potential Anti Affinity Pods . remove Pod ( deleted // All pods in the service Affinity Matching Pod List are in the same namespace. // So, if the namespace of the first one is not the same as the namespace of the // deleted Pod, we don't need to check the list, as deleted Pod isn't in the list. if meta . service Affinity In Use && len ( meta . service Affinity Matching Pod List ) > 0 && deleted Pod . Namespace == meta . service Affinity Matching Pod List [ 0 ] . Namespace { for i , pod := range meta . service Affinity Matching Pod List { if schedutil . Get Pod Full Name ( pod ) == deleted Pod Full Name { meta . service Affinity Matching Pod List = append ( meta . service Affinity Matching Pod List [ : i ] , meta . service Affinity Matching Pod } 
func ( meta * predicate Metadata ) Add Pod ( added Pod * v1 . Pod , node Info * schedulernodeinfo . Node Info ) error { added Pod Full Name := schedutil . Get Pod Full Name ( added if added Pod Full Name == schedutil . Get Pod Full if node // Add matching anti-affinity terms of the added Pod to the map. topology Pairs Maps , err := get Matching Anti Affinity Topology Pairs Of Pod ( meta . pod , added Pod , node meta . topology Pairs Anti Affinity Pods Map . append Maps ( topology Pairs // Add the pod to node Name To Matching Affinity Pods and node Name To Matching Anti Affinity pod Node Name := added Pod . Spec . Node if affinity != nil && len ( pod Node Name ) > 0 { pod Node := node // It is assumed that when the added pod matches affinity of the meta.pod, all the terms must match, // this should be changed when the implementation of target Pod Matches Affinity Of Pod/pod Matches Affinity Term Properties // is changed if target Pod Matches Affinity Of Pod ( meta . pod , added Pod ) { affinity Terms := Get Pod Affinity Terms ( affinity . Pod for _ , term := range affinity Terms { if topology Value , ok := pod Node . Labels [ term . Topology Key ] ; ok { pair := topology Pair { key : term . Topology Key , value : topology meta . topology Pairs Potential Affinity Pods . add Topology Pair ( pair , added if target Pod Matches Anti Affinity Of Pod ( meta . pod , added Pod ) { anti Affinity Terms := Get Pod Anti Affinity Terms ( affinity . Pod Anti for _ , term := range anti Affinity Terms { if topology Value , ok := pod Node . Labels [ term . Topology Key ] ; ok { pair := topology Pair { key : term . Topology Key , value : topology meta . topology Pairs Potential Anti Affinity Pods . add Topology Pair ( pair , added // If added Pod is in the same namespace as the meta.pod, update the list // of matching pods if applicable. if meta . service Affinity In Use && added Pod . Namespace == meta . pod . Namespace { selector := Create Selector From if selector . Matches ( labels . Set ( added Pod . Labels ) ) { meta . service Affinity Matching Pod List = append ( meta . service Affinity Matching Pod List , added } 
func ( meta * predicate Metadata ) Shallow Copy ( ) Predicate Metadata { new Pred Meta := & predicate Metadata { pod : meta . pod , pod Best Effort : meta . pod Best Effort , pod Request : meta . pod Request , service Affinity In Use : meta . service Affinity In Use , ignored Extended Resources : meta . ignored Extended new Pred Meta . pod Ports = append ( [ ] * v1 . Container Port ( nil ) , meta . pod new Pred Meta . topology Pairs Potential Affinity Pods = new Topology Pairs new Pred Meta . topology Pairs Potential Affinity Pods . append Maps ( meta . topology Pairs Potential Affinity new Pred Meta . topology Pairs Potential Anti Affinity Pods = new Topology Pairs new Pred Meta . topology Pairs Potential Anti Affinity Pods . append Maps ( meta . topology Pairs Potential Anti Affinity new Pred Meta . topology Pairs Anti Affinity Pods Map = new Topology Pairs new Pred Meta . topology Pairs Anti Affinity Pods Map . append Maps ( meta . topology Pairs Anti Affinity Pods new Pred Meta . service Affinity Matching Pod Services = append ( [ ] * v1 . Service ( nil ) , meta . service Affinity Matching Pod new Pred Meta . service Affinity Matching Pod List = append ( [ ] * v1 . Pod ( nil ) , meta . service Affinity Matching Pod return ( Predicate Metadata ) ( new Pred } 
func get Affinity Term Properties ( pod * v1 . Pod , terms [ ] v1 . Pod Affinity Term ) ( properties [ ] * affinity Term for _ , term := range terms { namespaces := priorityutil . Get Namespaces From Pod Affinity selector , err := metav1 . Label Selector As Selector ( term . Label properties = append ( properties , & affinity Term } 
func pod Matches All Affinity Term Properties ( pod * v1 . Pod , properties [ ] * affinity Term for _ , property := range properties { if ! priorityutil . Pod Matches Terms Namespace And } 
func get TP Map Matching Existing Anti Affinity ( pod * v1 . Pod , node Info Map map [ string ] * schedulernodeinfo . Node Info ) ( * topology Pairs Maps , error ) { all Node Names := make ( [ ] string , 0 , len ( node Info for name := range node Info Map { all Node Names = append ( all Node var first topology Maps := new Topology Pairs append Topology Pairs Maps := func ( to Append * topology Pairs topology Maps . append Maps ( to catch if first Error == nil { first ctx , cancel := context . With process Node := func ( i int ) { node Info := node Info Map [ all Node node := node if node == nil { catch for _ , existing Pod := range node Info . Pods With Affinity ( ) { existing Pod Topology Maps , err := get Matching Anti Affinity Topology Pairs Of Pod ( pod , existing if err != nil { catch append Topology Pairs Maps ( existing Pod Topology workqueue . Parallelize Until ( ctx , 16 , len ( all Node Names ) , process return topology Maps , first } 
func get TP Map Matching Incoming Affinity Anti Affinity ( pod * v1 . Pod , node Info Map map [ string ] * schedulernodeinfo . Node Info ) ( topology Pairs Affinity Pods Maps * topology Pairs Maps , topology Pairs Anti Affinity Pods Maps * topology Pairs if affinity == nil || ( affinity . Pod Affinity == nil && affinity . Pod Anti Affinity == nil ) { return new Topology Pairs Maps ( ) , new Topology Pairs all Node Names := make ( [ ] string , 0 , len ( node Info for name := range node Info Map { all Node Names = append ( all Node var first topology Pairs Affinity Pods Maps = new Topology Pairs topology Pairs Anti Affinity Pods Maps = new Topology Pairs append Result := func ( node Name string , node Topology Pairs Affinity Pods Maps , node Topology Pairs Anti Affinity Pods Maps * topology Pairs if len ( node Topology Pairs Affinity Pods Maps . topology Pair To Pods ) > 0 { topology Pairs Affinity Pods Maps . append Maps ( node Topology Pairs Affinity Pods if len ( node Topology Pairs Anti Affinity Pods Maps . topology Pair To Pods ) > 0 { topology Pairs Anti Affinity Pods Maps . append Maps ( node Topology Pairs Anti Affinity Pods catch if first Error == nil { first affinity Terms := Get Pod Affinity Terms ( affinity . Pod affinity Properties , err := get Affinity Term Properties ( pod , affinity anti Affinity Terms := Get Pod Anti Affinity Terms ( affinity . Pod Anti ctx , cancel := context . With process Node := func ( i int ) { node Info := node Info Map [ all Node node := node if node == nil { catch node Topology Pairs Affinity Pods Maps := new Topology Pairs node Topology Pairs Anti Affinity Pods Maps := new Topology Pairs for _ , existing Pod := range node Info . Pods ( ) { // Check affinity properties. if pod Matches All Affinity Term Properties ( existing Pod , affinity Properties ) { for _ , term := range affinity Terms { if topology Value , ok := node . Labels [ term . Topology Key ] ; ok { pair := topology Pair { key : term . Topology Key , value : topology node Topology Pairs Affinity Pods Maps . add Topology Pair ( pair , existing // Check anti-affinity properties. for _ , term := range anti Affinity Terms { namespaces := priorityutil . Get Namespaces From Pod Affinity selector , err := metav1 . Label Selector As Selector ( term . Label if err != nil { catch if priorityutil . Pod Matches Terms Namespace And Selector ( existing Pod , namespaces , selector ) { if topology Value , ok := node . Labels [ term . Topology Key ] ; ok { pair := topology Pair { key : term . Topology Key , value : topology node Topology Pairs Anti Affinity Pods Maps . add Topology Pair ( pair , existing if len ( node Topology Pairs Affinity Pods Maps . topology Pair To Pods ) > 0 || len ( node Topology Pairs Anti Affinity Pods Maps . topology Pair To Pods ) > 0 { append Result ( node . Name , node Topology Pairs Affinity Pods Maps , node Topology Pairs Anti Affinity Pods workqueue . Parallelize Until ( ctx , 16 , len ( all Node Names ) , process return topology Pairs Affinity Pods Maps , topology Pairs Anti Affinity Pods Maps , first } 
func target Pod Matches Affinity Of Pod ( pod , target if affinity == nil || affinity . Pod affinity Properties , err := get Affinity Term Properties ( pod , Get Pod Affinity Terms ( affinity . Pod return pod Matches All Affinity Term Properties ( target Pod , affinity } 
func target Pod Matches Anti Affinity Of Pod ( pod , target if affinity == nil || affinity . Pod Anti properties , err := get Affinity Term Properties ( pod , Get Pod Anti Affinity Terms ( affinity . Pod Anti return pod Matches Any Affinity Term Properties ( target } 
func ( g * gen } 
func New Shared Informer ( lw Lister Watcher , obj Type runtime . Object , resync Period time . Duration ) Shared Informer { return New Shared Index Informer ( lw , obj Type , resync } 
func New Shared Index Informer ( lw Lister Watcher , obj Type runtime . Object , default Event Handler Resync Period time . Duration , indexers Indexers ) Shared Index Informer { real Clock := & clock . Real shared Index Informer := & shared Index Informer { processor : & shared Processor { clock : real Clock } , indexer : New Indexer ( Deletion Handling Meta Namespace Key Func , indexers ) , lister Watcher : lw , object Type : obj Type , resync Check Period : default Event Handler Resync Period , default Event Handler Resync Period : default Event Handler Resync Period , cache Mutation Detector : New Cache Mutation Detector ( fmt . Sprintf ( " " , obj Type ) ) , clock : real return shared Index } 
func Wait For Cache Sync ( stop Ch <- chan struct { } , cache Syncs ... Informer Synced ) bool { err := wait . Poll Until ( synced Poll Period , func ( ) ( bool , error ) { for _ , sync Func := range cache Syncs { if ! sync } , stop } 
func ( p * shared Processor ) should Resync ( ) bool { p . listeners defer p . listeners p . syncing Listeners = [ ] * processor resync for _ , listener := range p . listeners { // need to loop through all the listeners to see if they need to resync so we can prepare any // listeners that are going to be resyncing. if listener . should Resync ( now ) { resync p . syncing Listeners = append ( p . syncing listener . determine Next return resync } 
func ( p * processor Listener ) should Resync ( now time . Time ) bool { p . resync defer p . resync if p . resync return now . After ( p . next Resync ) || now . Equal ( p . next } 
func Add Or Update Taint On Node ( c clientset . Interface , node first return clientretry . Retry On Conflict ( update Taint var old // First we try getting node from the API server cache, as it's cheaper. If it fails // we get it from etcd to be sure to have fresh data. if first Try { old Node , err = c . Core V1 ( ) . Nodes ( ) . Get ( node Name , metav1 . Get Options { Resource first } else { old Node , err = c . Core V1 ( ) . Nodes ( ) . Get ( node Name , metav1 . Get var new old Node Copy := old for _ , taint := range taints { cur New Node , ok , err := add Or Update Taint ( old Node new Node = cur New old Node Copy = cur New return Patch Node Taints ( c , node Name , old Node , new } 
func Patch Node Taints ( c clientset . Interface , node Name string , old Node * v1 . Node , new Node * v1 . Node ) error { old Data , err := json . Marshal ( old if err != nil { return fmt . Errorf ( " " , old Node , node new Taints := new new Node Clone := old Node . Deep new Node Clone . Spec . Taints = new new Data , err := json . Marshal ( new Node if err != nil { return fmt . Errorf ( " " , new Node Clone , node patch Bytes , err := strategicpatch . Create Two Way Merge Patch ( old Data , new if err != nil { return fmt . Errorf ( " " , node _ , err = c . Core V1 ( ) . Nodes ( ) . Patch ( node Name , types . Strategic Merge Patch Type , patch } 
func add Or Update Taint ( node * v1 . Node , taint * v1 . Taint ) ( * v1 . Node , bool , error ) { new Node := node . Deep node Taints := new var new for i := range node Taints { if taint . Match Taint ( & node Taints [ i ] ) { if equality . Semantic . Deep Equal ( * taint , node Taints [ i ] ) { return new new Taints = append ( new new Taints = append ( new Taints , node if ! updated { new Taints = append ( new new Node . Spec . Taints = new return new } 
func Load From if _ , err := os . Stat ( path ) ; os . Is Not data , err := ioutil . Read } 
func ( info Info ) Merge With config . CA File = info . CA config . Cert File = info . Cert config . Key File = info . Key config . Bearer Token = info . Bearer } 
func New REST ( scheme * runtime . Scheme , opts Getter generic . REST Options Getter ) * REST { strategy := apiservice . New store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & apiregistration . API Service { } } , New List Func : func ( ) runtime . Object { return & apiregistration . API Service List { } } , Predicate Func : apiservice . Match API Service , Default Qualified Resource : apiregistration . Resource ( " " ) , Create Strategy : strategy , Update Strategy : strategy , Delete options := & generic . Store Options { REST Options : opts Getter , Attr Func : apiservice . Get if err := store . Complete With } 
func ( c * REST ) Convert To Table ( ctx context . Context , obj runtime . Object , table Options runtime . Object ) ( * metav1beta1 . Table , error ) { table := & metav1beta1 . Table { Column Definitions : [ ] metav1beta1 . Table Column Definition { { Name : " " , Type : " " , Format : " " , Description : swagger Metadata Descriptions [ " " ] } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : " " } , { Name : " " , Type : " " , Description : swagger Metadata if m , err := meta . List Accessor ( obj ) ; err == nil { table . Resource Version = m . Get Resource table . Self Link = m . Get Self table . Continue = m . Get } else { if m , err := meta . Common Accessor ( obj ) ; err == nil { table . Resource Version = m . Get Resource table . Self Link = m . Get Self table . Rows , err = metatable . Meta To Table Row ( obj , func ( obj runtime . Object , m metav1 . Object , name , age string ) ( [ ] interface { } , error ) { svc := obj . ( * apiregistration . API status := string ( apiregistration . Condition if condition := get Condition ( svc . Status . Conditions , " " ) ; condition != nil { switch { case condition . Status == apiregistration . Condition } 
func New Etcd Migrate Server ( cfg * Etcd Migrate Cfg , client Etcd Migrate Client ) * Etcd Migrate Server { return & Etcd Migrate } 
func ( r * Etcd Migrate Server ) Start ( version * Etcd Version ) error { etcd Cmd := exec . Command ( fmt . Sprintf ( " " , r . cfg . bin Path , version ) , " " , r . cfg . name , " " , r . cfg . initial Cluster , " " , " " , r . cfg . data Directory , " " , fmt . Sprintf ( " " , r . cfg . port ) , " " , fmt . Sprintf ( " " , r . cfg . port ) , " " , r . cfg . peer Listen Urls , " " , r . cfg . peer Advertise if r . cfg . etcd Server Args != " " { extra Args := strings . Fields ( r . cfg . etcd Server etcd Cmd . Args = append ( etcd Cmd . Args , extra fmt . Printf ( " \n " , r . cfg . name , etcd etcd etcd err := etcd interval := time . New for { select { case <- interval . C : err := r . client . Set Etcd Version Key r . cmd = etcd case <- done : err = etcd } 
func ( r * Etcd Migrate graceful go func ( ) { time . Sleep ( graceful case <- timedout : klog . Infof ( " " , graceful if exiterr , ok := err . ( * exec . Exit } 
func ( client Config Fn Client Config Func ) client For Group Version ( gv schema . Group Version , negotiated Serializer runtime . Negotiated Serializer ) ( REST Client , error ) { cfg , err := client Config if negotiated Serializer != nil { cfg . Content Config . Negotiated Serializer = negotiated cfg . Group if len ( gv . Group ) == 0 { cfg . API } else { cfg . API return rest . REST Client } 
func ( c * Clientset ) Admissionregistration V1beta1 ( ) admissionregistrationv1beta1 . Admissionregistration V1beta1Interface { return & fakeadmissionregistrationv1beta1 . Fake Admissionregistration } 
func ( c * Clientset ) Apps V1 ( ) appsv1 . Apps V1Interface { return & fakeappsv1 . Fake Apps } 
func ( c * Clientset ) Apps V1beta1 ( ) appsv1beta1 . Apps V1beta1Interface { return & fakeappsv1beta1 . Fake Apps } 
func ( c * Clientset ) Apps V1beta2 ( ) appsv1beta2 . Apps V1beta2Interface { return & fakeappsv1beta2 . Fake Apps } 
func ( c * Clientset ) Auditregistration V1alpha1 ( ) auditregistrationv1alpha1 . Auditregistration V1alpha1Interface { return & fakeauditregistrationv1alpha1 . Fake Auditregistration } 
func ( c * Clientset ) Authentication V1 ( ) authenticationv1 . Authentication V1Interface { return & fakeauthenticationv1 . Fake Authentication } 
func ( c * Clientset ) Authentication V1beta1 ( ) authenticationv1beta1 . Authentication V1beta1Interface { return & fakeauthenticationv1beta1 . Fake Authentication } 
func ( c * Clientset ) Authorization V1 ( ) authorizationv1 . Authorization V1Interface { return & fakeauthorizationv1 . Fake Authorization } 
func ( c * Clientset ) Authorization V1beta1 ( ) authorizationv1beta1 . Authorization V1beta1Interface { return & fakeauthorizationv1beta1 . Fake Authorization } 
func ( c * Clientset ) Autoscaling V1 ( ) autoscalingv1 . Autoscaling V1Interface { return & fakeautoscalingv1 . Fake Autoscaling } 
func ( c * Clientset ) Autoscaling V2beta1 ( ) autoscalingv2beta1 . Autoscaling V2beta1Interface { return & fakeautoscalingv2beta1 . Fake Autoscaling } 
func ( c * Clientset ) Autoscaling V2beta2 ( ) autoscalingv2beta2 . Autoscaling V2beta2Interface { return & fakeautoscalingv2beta2 . Fake Autoscaling } 
func ( c * Clientset ) Batch V1 ( ) batchv1 . Batch V1Interface { return & fakebatchv1 . Fake Batch } 
func ( c * Clientset ) Batch V1beta1 ( ) batchv1beta1 . Batch V1beta1Interface { return & fakebatchv1beta1 . Fake Batch } 
func ( c * Clientset ) Batch V2alpha1 ( ) batchv2alpha1 . Batch V2alpha1Interface { return & fakebatchv2alpha1 . Fake Batch } 
func ( c * Clientset ) Certificates V1beta1 ( ) certificatesv1beta1 . Certificates V1beta1Interface { return & fakecertificatesv1beta1 . Fake Certificates } 
func ( c * Clientset ) Coordination V1beta1 ( ) coordinationv1beta1 . Coordination V1beta1Interface { return & fakecoordinationv1beta1 . Fake Coordination } 
func ( c * Clientset ) Coordination V1 ( ) coordinationv1 . Coordination V1Interface { return & fakecoordinationv1 . Fake Coordination } 
func ( c * Clientset ) Core V1 ( ) corev1 . Core V1Interface { return & fakecorev1 . Fake Core } 
func ( c * Clientset ) Events V1beta1 ( ) eventsv1beta1 . Events V1beta1Interface { return & fakeeventsv1beta1 . Fake Events } 
func ( c * Clientset ) Extensions V1beta1 ( ) extensionsv1beta1 . Extensions V1beta1Interface { return & fakeextensionsv1beta1 . Fake Extensions } 
func ( c * Clientset ) Networking V1 ( ) networkingv1 . Networking V1Interface { return & fakenetworkingv1 . Fake Networking } 
func ( c * Clientset ) Networking V1beta1 ( ) networkingv1beta1 . Networking V1beta1Interface { return & fakenetworkingv1beta1 . Fake Networking } 
func ( c * Clientset ) Node V1beta1 ( ) nodev1beta1 . Node V1beta1Interface { return & fakenodev1beta1 . Fake Node } 
func ( c * Clientset ) Policy V1beta1 ( ) policyv1beta1 . Policy V1beta1Interface { return & fakepolicyv1beta1 . Fake Policy } 
func ( c * Clientset ) Rbac V1 ( ) rbacv1 . Rbac V1Interface { return & fakerbacv1 . Fake Rbac } 
func ( c * Clientset ) Rbac V1beta1 ( ) rbacv1beta1 . Rbac V1beta1Interface { return & fakerbacv1beta1 . Fake Rbac } 
func ( c * Clientset ) Rbac V1alpha1 ( ) rbacv1alpha1 . Rbac V1alpha1Interface { return & fakerbacv1alpha1 . Fake Rbac } 
func ( c * Clientset ) Scheduling V1alpha1 ( ) schedulingv1alpha1 . Scheduling V1alpha1Interface { return & fakeschedulingv1alpha1 . Fake Scheduling } 
func ( c * Clientset ) Scheduling V1beta1 ( ) schedulingv1beta1 . Scheduling V1beta1Interface { return & fakeschedulingv1beta1 . Fake Scheduling } 
func ( c * Clientset ) Scheduling V1 ( ) schedulingv1 . Scheduling V1Interface { return & fakeschedulingv1 . Fake Scheduling } 
func ( c * Clientset ) Settings V1alpha1 ( ) settingsv1alpha1 . Settings V1alpha1Interface { return & fakesettingsv1alpha1 . Fake Settings } 
func ( c * Clientset ) Storage V1beta1 ( ) storagev1beta1 . Storage V1beta1Interface { return & fakestoragev1beta1 . Fake Storage } 
func ( c * Clientset ) Storage V1 ( ) storagev1 . Storage V1Interface { return & fakestoragev1 . Fake Storage } 
func ( c * Clientset ) Storage V1alpha1 ( ) storagev1alpha1 . Storage V1alpha1Interface { return & fakestoragev1alpha1 . Fake Storage } 
func New REST ( opts Getter generic . REST Options Getter ) ( * REST , * Status REST ) { store := & genericregistry . Store { New Func : func ( ) runtime . Object { return & api . Persistent Volume { } } , New List Func : func ( ) runtime . Object { return & api . Persistent Volume List { } } , Predicate Func : persistentvolume . Match Persistent Volumes , Default Qualified Resource : api . Resource ( " " ) , Create Strategy : persistentvolume . Strategy , Update Strategy : persistentvolume . Strategy , Delete Strategy : persistentvolume . Strategy , Return Deleted Object : true , Table Convertor : printerstorage . Table Convertor { Table Generator : printers . New Table Generator ( ) . With ( printersinternal . Add options := & generic . Store Options { REST Options : opts Getter , Attr Func : persistentvolume . Get if err := store . Complete With status status Store . Update Strategy = persistentvolume . Status return & REST { store } , & Status REST { store : & status } 
func ( ss * scale Set ) Attach Disk ( is Managed Disk bool , disk Name , disk URI string , node Name types . Node Name , lun int32 , caching Mode compute . Caching Types ) error { vm Name := map Node Name To VM Name ( node ss Name , instance ID , vm , err := ss . get Vmss VM ( vm node Resource Group , err := ss . Get Node Resource Group ( vm disks := [ ] compute . Data if vm . Storage Profile != nil && vm . Storage Profile . Data Disks != nil { disks = make ( [ ] compute . Data Disk , len ( * vm . Storage Profile . Data copy ( disks , * vm . Storage Profile . Data if is Managed Disk { disks = append ( disks , compute . Data Disk { Name : & disk Name , Lun : & lun , Caching : compute . Caching Types ( caching Mode ) , Create Option : " " , Managed Disk : & compute . Managed Disk Parameters { ID : & disk } else { disks = append ( disks , compute . Data Disk { Name : & disk Name , Vhd : & compute . Virtual Hard Disk { URI : & disk URI , } , Lun : & lun , Caching : compute . Caching Types ( caching Mode ) , Create new VM := compute . Virtual Machine Scale Set VM { Sku : vm . Sku , Location : vm . Location , Virtual Machine Scale Set VM Properties : & compute . Virtual Machine Scale Set VM Properties { Hardware Profile : vm . Hardware Profile , Storage Profile : & compute . Storage Profile { Os Disk : vm . Storage Profile . Os Disk , Data ctx , cancel := get Context With // Invalidate the cache right after updating key := build Vmss Cache Key ( node Resource Group , ss . make Vmss VM Name ( ss Name , instance defer ss . vmss VM klog . V ( 2 ) . Infof ( " " , node Resource Group , node Name , disk Name , disk _ , err = ss . Virtual Machine Scale Set V Ms Client . Update ( ctx , node Resource Group , ss Name , instance ID , new if strings . Contains ( detail , err Lease Failed ) || strings . Contains ( detail , err Disk Blob Not Found ) { // if lease cannot be acquired or disk not found, immediately detach the disk and return the original error klog . Infof ( " " , detail , disk Name , disk ss . Detach Disk ( disk Name , disk URI , node } else { klog . V ( 2 ) . Infof ( " " , disk Name , disk } 
func ( ss * scale Set ) Detach Disk ( disk Name , disk URI string , node Name types . Node Name ) ( * http . Response , error ) { vm Name := map Node Name To VM Name ( node ss Name , instance ID , vm , err := ss . get Vmss VM ( vm node Resource Group , err := ss . Get Node Resource Group ( vm disks := [ ] compute . Data if vm . Storage Profile != nil && vm . Storage Profile . Data Disks != nil { disks = make ( [ ] compute . Data Disk , len ( * vm . Storage Profile . Data copy ( disks , * vm . Storage Profile . Data b Found for i , disk := range disks { if disk . Lun != nil && ( disk . Name != nil && disk Name != " " && * disk . Name == disk Name ) || ( disk . Vhd != nil && disk . Vhd . URI != nil && disk URI != " " && * disk . Vhd . URI == disk URI ) || ( disk . Managed Disk != nil && disk URI != " " && * disk . Managed Disk . ID == disk URI ) { // found the disk klog . V ( 2 ) . Infof ( " " , disk Name , disk b Found if ! b Found Disk { return nil , fmt . Errorf ( " " , disk Name , disk new VM := compute . Virtual Machine Scale Set VM { Sku : vm . Sku , Location : vm . Location , Virtual Machine Scale Set VM Properties : & compute . Virtual Machine Scale Set VM Properties { Hardware Profile : vm . Hardware Profile , Storage Profile : & compute . Storage Profile { Os Disk : vm . Storage Profile . Os Disk , Data ctx , cancel := get Context With // Invalidate the cache right after updating key := build Vmss Cache Key ( node Resource Group , ss . make Vmss VM Name ( ss Name , instance defer ss . vmss VM klog . V ( 2 ) . Infof ( " " , node Resource Group , node Name , disk Name , disk return ss . Virtual Machine Scale Set V Ms Client . Update ( ctx , node Resource Group , ss Name , instance ID , new } 
func ( ss * scale Set ) Get Data Disks ( node Name types . Node Name ) ( [ ] compute . Data Disk , error ) { _ , _ , vm , err := ss . get Vmss VM ( string ( node if vm . Storage Profile == nil || vm . Storage Profile . Data return * vm . Storage Profile . Data } 
func Register Defaults ( scheme * runtime . Scheme ) error { scheme . Add Type Defaulting Func ( & v2alpha1 . Cron Job { } , func ( obj interface { } ) { Set Object Defaults_Cron Job ( obj . ( * v2alpha1 . Cron scheme . Add Type Defaulting Func ( & v2alpha1 . Cron Job List { } , func ( obj interface { } ) { Set Object Defaults_Cron Job List ( obj . ( * v2alpha1 . Cron Job scheme . Add Type Defaulting Func ( & v2alpha1 . Job Template { } , func ( obj interface { } ) { Set Object Defaults_Job Template ( obj . ( * v2alpha1 . Job } 
func prime Aggregated Cluster Roles ( cluster Roles To Aggregate map [ string ] string , cluster Role Client rbacv1client . Cluster Roles Getter ) error { for old Name , new Name := range cluster Roles To Aggregate { _ , err := cluster Role Client . Cluster Roles ( ) . Get ( new Name , metav1 . Get if ! apierrors . Is Not existing Role , err := cluster Role Client . Cluster Roles ( ) . Get ( old Name , metav1 . Get if apierrors . Is Not if existing Role . Aggregation klog . V ( 1 ) . Infof ( " " , existing Role . Name , new existing Role . Name = new existing Role . Resource if _ , err := cluster Role Client . Cluster Roles ( ) . Create ( existing Role ) ; err != nil && ! apierrors . Is Already } 
func prime Split Cluster Role Bindings ( cluster Role Binding To Split map [ string ] rbacapiv1 . Cluster Role Binding , cluster Role Binding Client rbacv1client . Cluster Role Bindings Getter ) error { for existing Binding Name , cluster Role Binding To Create := range cluster Role Binding To Split { // If source Cluster Role Binding does not exist, do nothing. existing Role Binding , err := cluster Role Binding Client . Cluster Role Bindings ( ) . Get ( existing Binding Name , metav1 . Get if apierrors . Is Not // If the target Cluster Role Binding already exists, do nothing. _ , err = cluster Role Binding Client . Cluster Role Bindings ( ) . Get ( cluster Role Binding To Create . Name , metav1 . Get if ! apierrors . Is Not // If the source exists, but the target does not, // copy the subjects, labels, and annotations from the former to create the latter. klog . V ( 1 ) . Infof ( " " , existing Binding Name , cluster Role Binding To new CRB := cluster Role Binding To Create . Deep new CRB . Subjects = existing Role new CRB . Labels = existing Role new CRB . Annotations = existing Role if _ , err := cluster Role Binding Client . Cluster Role Bindings ( ) . Create ( new CRB ) ; err != nil && ! apierrors . Is Already } 
func ( ts * azure Token Source ) Token ( ) ( * azure token := ts . cache . get Token ( azure Token if token == nil { token , err = ts . retrieve Token From if ! token . token . Is Expired ( ) { ts . cache . set Token ( azure Token err = ts . store Token In if token . token . Is Expired ( ) { token , err = ts . refresh ts . cache . set Token ( azure Token err = ts . store Token In } 
func V1Resource By Storage Class ( storage Class string , resource Name corev1 . Resource Name ) corev1 . Resource Name { return corev1 . Resource Name ( string ( storage Class + storage Class Suffix + string ( resource } 
func New Persistent Volume Claim Evaluator ( f quota . Lister For Resource Func ) quota . Evaluator { list Func By Namespace := generic . List Resource Using Lister Func ( f , corev1 . Scheme Group Version . With pvc Evaluator := & pvc Evaluator { list Func By Namespace : list Func By return pvc } 
func ( p * pvc Evaluator ) Constraints ( required [ ] corev1 . Resource } 
func ( p * pvc Evaluator ) Handles ( a admission . Attributes ) bool { op := a . Get if op == admission . Update && utilfeature . Default Feature Gate . Enabled ( k8sfeatures . Expand Persistent } 
func ( p * pvc Evaluator ) Matches ( resource Quota * corev1 . Resource Quota , item runtime . Object ) ( bool , error ) { return generic . Matches ( resource Quota , item , p . Matching Resources , generic . Matches No Scope } 
func ( p * pvc Evaluator ) Matching Resources ( items [ ] corev1 . Resource Name ) [ ] corev1 . Resource Name { result := [ ] corev1 . Resource for _ , item := range items { // match object count quota fields if quota . Contains ( [ ] corev1 . Resource Name { pvc Object Count // match pvc resources if quota . Contains ( pvc // match pvc resources scoped by storage class (<storage-class-name>.storage-class.kubernetes.io/<resource>) for _ , resource := range pvc Resources { by Storage Class := storage Class if strings . Has Suffix ( string ( item ) , by Storage } 
func ( p * pvc Evaluator ) Usage ( item runtime . Object ) ( corev1 . Resource List , error ) { result := corev1 . Resource pvc , err := to External Persistent Volume Claim Or // charge for claim result [ corev1 . Resource Persistent Volume Claims ] = * ( resource . New Quantity ( 1 , resource . Decimal result [ pvc Object Count Name ] = * ( resource . New Quantity ( 1 , resource . Decimal storage Class Ref := helper . Get Persistent Volume Claim if len ( storage Class Ref ) > 0 { storage Class Claim := corev1 . Resource Name ( storage Class Ref + storage Class Suffix + string ( corev1 . Resource Persistent Volume result [ storage Class Claim ] = * ( resource . New Quantity ( 1 , resource . Decimal // charge for storage if request , found := pvc . Spec . Resources . Requests [ corev1 . Resource Storage ] ; found { result [ corev1 . Resource Requests // charge usage to the storage class (if present) if len ( storage Class Ref ) > 0 { storage Class Storage := corev1 . Resource Name ( storage Class Ref + storage Class Suffix + string ( corev1 . Resource Requests result [ storage Class } 
func ( strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new Cluster Role := obj . ( * rbac . Cluster old Cluster Role := old . ( * rbac . Cluster _ , _ = new Cluster Role , old Cluster } 
func ( strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { cluster Role := obj . ( * rbac . Cluster return validation . Validate Cluster Role ( cluster } 
func ( strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { new Obj := obj . ( * rbac . Cluster error List := validation . Validate Cluster Role ( new return append ( error List , validation . Validate Cluster Role Update ( new Obj , old . ( * rbac . Cluster } 
func ( wg * Safe Wait Group ) Add ( delta int ) error { wg . mu . R defer wg . mu . R } 
func ( wg * Safe Wait } 
func New Sources Ready ( sources Ready Fn Sources Ready Fn ) Sources Ready { return & sources Impl { sources Seen : sets . New String ( ) , sources Ready Fn : sources Ready } 
func ( s * sources Impl ) Add s . sources } 
func ( s * sources Impl ) All Ready ( ) bool { s . lock . R defer s . lock . R return s . sources Ready Fn ( s . sources } 
func Default Generators ( cmd switch cmd Name { case " " : generator = map [ string ] generate . Generator { Service V1Generator Name : Service Generator V1 { } , Service V2Generator Name : Service Generator case " " : generator = map [ string ] generate . Generator { Service Cluster IP Generator V1Name : Service Cluster IP Generator case " " : generator = map [ string ] generate . Generator { Service Node Port Generator V1Name : Service Node Port Generator case " " : generator = map [ string ] generate . Generator { Service Load Balancer Generator V1Name : Service Load Balancer Generator case " " : // Create Deployment has only Structured Generators and no // param-based Generators. // The Structured Generators are as follows (as of 2018-03-16): // Deployment Basic V1Beta1Generator Name -> Deployment Basic Generator V1 // Deployment Basic Apps V1Beta1Generator Name -> Deployment Basic Apps Generator V1Beta1 // Deployment Basic Apps V1Generator Name -> Deployment Basic Apps Generator case " " : generator = map [ string ] generate . Generator { Run V1Generator Name : Basic Replication Controller { } , Run Pod V1Generator Name : Basic Pod { } , Deployment V1Beta1Generator Name : Deployment V1Beta1 { } , Deployment Apps V1Beta1Generator Name : Deployment Apps V1Beta1 { } , Deployment Apps V1Generator Name : Deployment Apps V1 { } , Job V1Generator Name : Job V1 { } , Cron Job V2Alpha1Generator Name : Cron Job V2Alpha1 { } , Cron Job V1Beta1Generator Name : Cron Job case " " : generator = map [ string ] generate . Generator { Namespace V1Generator Name : Namespace Generator case " " : generator = map [ string ] generate . Generator { Resource Quota V1Generator Name : Resource Quota Generator case " " : generator = map [ string ] generate . Generator { Secret V1Generator Name : Secret Generator case " " : generator = map [ string ] generate . Generator { Secret For Docker Registry V1Generator Name : Secret For Docker Registry Generator case " " : generator = map [ string ] generate . Generator { Secret For TLSV1Generator Name : Secret For TLS Generator } 
func Fallback Generator Name If Necessary ( generator Name string , discovery Client discovery . Discovery Interface , cmd Err io . Writer , ) ( string , error ) { switch generator Name { case Deployment Apps V1Generator Name : has Resource , err := Has Resource ( discovery Client , appsv1 . Scheme Group Version . With if ! has Resource { return Fallback Generator Name If Necessary ( Deployment Apps V1Beta1Generator Name , discovery Client , cmd case Deployment Apps V1Beta1Generator Name : has Resource , err := Has Resource ( discovery Client , appsv1beta1 . Scheme Group Version . With if ! has Resource { return Fallback Generator Name If Necessary ( Deployment V1Beta1Generator Name , discovery Client , cmd case Deployment V1Beta1Generator Name : has Resource , err := Has Resource ( discovery Client , extensionsv1beta1 . Scheme Group Version . With if ! has Resource { return Run V1Generator case Deployment Basic Apps V1Generator Name : has Resource , err := Has Resource ( discovery Client , appsv1 . Scheme Group Version . With if ! has Resource { return Fallback Generator Name If Necessary ( Deployment Basic Apps V1Beta1Generator Name , discovery Client , cmd case Deployment Basic Apps V1Beta1Generator Name : has Resource , err := Has Resource ( discovery Client , appsv1beta1 . Scheme Group Version . With if ! has Resource { return Deployment Basic V1Beta1Generator case Job V1Generator Name : has Resource , err := Has Resource ( discovery Client , batchv1 . Scheme Group Version . With if ! has Resource { return Run Pod V1Generator case Cron Job V1Beta1Generator Name : has Resource , err := Has Resource ( discovery Client , batchv1beta1 . Scheme Group Version . With if ! has Resource { return Fallback Generator Name If Necessary ( Cron Job V2Alpha1Generator Name , discovery Client , cmd case Cron Job V2Alpha1Generator Name : has Resource , err := Has Resource ( discovery Client , batchv2alpha1 . Scheme Group Version . With if ! has Resource { return Job V1Generator return generator } 
func New Selector Options ( streams genericclioptions . IO Streams ) * Set Selector Options { return & Set Selector Options { Resource Builder Flags : genericclioptions . New Resource Builder Flags ( ) . With Scheme ( scheme . Scheme ) . With All ( false ) . With Local ( false ) . With Latest ( ) , Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , Record Flags : genericclioptions . New Record Flags ( ) , Recorder : genericclioptions . Noop Recorder { } , IO } 
func New Cmd Selector ( f cmdutil . Factory , streams genericclioptions . IO Streams ) * cobra . Command { o := New Selector cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : fmt . Sprintf ( selector Long , validation . Label Value Max Length ) , Example : selector Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check cmdutil . Check Err ( o . Run o . Resource Builder Flags . Add o . Print Flags . Add o . Record Flags . Add cmdutil . Add Dry Run } 
func ( o * Set Selector o . Record o . Recorder , err = o . Record Flags . To o . dryrun = cmdutil . Get Dry Run o . resources , o . selector , err = get Resources And o . Resource Finder = o . Resource Builder Flags . To o . Write To Server = ! ( * o . Resource Builder if o . dryrun { o . Print printer , err := o . Print Flags . To o . Print Obj = printer . Print } 
func ( o * Set Selector } 
func ( o * Set Selector Options ) Run Selector ( ) error { r := o . Resource Calculate Patch ( patch , scheme . Default JSON Encoder ( ) , func ( obj runtime . Object ) ( [ ] byte , error ) { select Err := update Selector For if select Err != nil { return nil , select return runtime . Encode ( scheme . Default JSON if ! o . Write To Server { return o . Print actual , err := resource . New Helper ( info . Client , info . Mapping ) . Patch ( info . Namespace , info . Name , types . Strategic Merge Patch return o . Print } 
func get Resources And Selector ( args [ ] string ) ( resources [ ] string , selector * metav1 . Label selector , err = metav1 . Parse To Label } 
func New Object Cache ( f func ( ) ( interface { } , error ) , ttl time . Duration ) * Object Cache { return & Object Cache { updater : f , cache : expirationcache . New TTL Store ( string Key } 
func string Key Func ( obj interface { } ) ( string , error ) { key := obj . ( object } 
func ( c * Object Cache ) Get ( key string ) ( interface { } , error ) { value , ok , err := c . cache . Get ( object err = c . cache . Add ( object return value . ( object } 
func ( c * Object Cache ) Add ( key string , obj interface { } ) error { err := c . cache . Add ( object } 
func ( kl * Kubelet ) provider Requires Networking Configuration ( ) bool { // TODO: We should have a mechanism to say whether native cloud provider // is used or whether we are using overlay networking. We should return // true for cloud providers if they implement Routes() interface and // we are not using overlay networking. if kl . cloud == nil || kl . cloud . Provider } 
func ( kl * Kubelet ) update Pod CIDR ( cidr string ) ( bool , error ) { kl . update Pod CIDR defer kl . update Pod CIDR pod CIDR := kl . runtime State . pod if pod // kubelet -> generic runtime -> runtime shim -> network plugin // docker/non-cri implementations have a passthrough Update Pod CIDR if err := kl . get Runtime ( ) . Update Pod CIDR ( cidr ) ; err != nil { // If update Pod klog . Infof ( " " , pod kl . runtime State . set Pod } 
func ( kl * Kubelet ) Get Pod DNS ( pod * v1 . Pod ) ( * runtimeapi . DNS Config , error ) { return kl . dns Configurer . Get Pod } 
func New Cmd Create Secret ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { cmd := & cobra . Command { Use : " " , Short : i18n . T ( " " ) , Long : " " , Run : cmdutil . Default Sub Command Run ( io Streams . Err cmd . Add Command ( New Cmd Create Secret Docker Registry ( f , io cmd . Add Command ( New Cmd Create Secret TLS ( f , io cmd . Add Command ( New Cmd Create Secret Generic ( f , io } 
func New Cmd Create Secret Generic ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Secret Generic Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : secret Long , Example : secret Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Secret V1Generator cmd . Flags ( ) . String cmd . Flags ( ) . String } 
func New Cmd Create Secret Docker Registry ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Secret Docker Registry Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : secret For Docker Registry Long , Example : secret For Docker Registry Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Secret For Docker Registry V1Generator cmd . Mark Flag cmd . Mark Flag cmd . Flags ( ) . String } 
func ( o * Secret Docker Registry Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command from File Flag := cmdutil . Get Flag String if len ( from File Flag ) == 0 { required for _ , required Flag := range required Flags { if value := cmdutil . Get Flag String ( cmd , required Flag ) ; len ( value ) == 0 { return cmdutil . Usage Errorf ( cmd , " " , required var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Secret For Docker Registry V1Generator Name : generator = & generateversioned . Secret For Docker Registry Generator V1 { Name : name , Username : cmdutil . Get Flag String ( cmd , " " ) , Email : cmdutil . Get Flag String ( cmd , " " ) , Password : cmdutil . Get Flag String ( cmd , " " ) , Server : cmdutil . Get Flag String ( cmd , " " ) , Append Hash : cmdutil . Get Flag Bool ( cmd , " " ) , File Sources : cmdutil . Get Flag String default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func New Cmd Create Secret TLS ( f cmdutil . Factory , io Streams genericclioptions . IO Streams ) * cobra . Command { options := & Secret TLS Opts { Create Subcommand Options : New Create Subcommand Options ( io cmd := & cobra . Command { Use : " " , Disable Flags In Use Line : true , Short : i18n . T ( " " ) , Long : secret For TLS Long , Example : secret For TLS Example , Run : func ( cmd * cobra . Command , args [ ] string ) { cmdutil . Check cmdutil . Check options . Create Subcommand Options . Print Flags . Add cmdutil . Add Apply Annotation cmdutil . Add Validate cmdutil . Add Generator Flags ( cmd , generateversioned . Secret For TLSV1Generator } 
func ( o * Secret TLS Opts ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args [ ] string ) error { name , err := Name From Command required for _ , required Flag := range required Flags { if value := cmdutil . Get Flag String ( cmd , required Flag ) ; len ( value ) == 0 { return cmdutil . Usage Errorf ( cmd , " " , required var generator generate . Structured switch generator Name := cmdutil . Get Flag String ( cmd , " " ) ; generator Name { case generateversioned . Secret For TLSV1Generator Name : generator = & generateversioned . Secret For TLS Generator V1 { Name : name , Key : cmdutil . Get Flag String ( cmd , " " ) , Cert : cmdutil . Get Flag String ( cmd , " " ) , Append Hash : cmdutil . Get Flag default : return err Unsupported Generator ( cmd , generator return o . Create Subcommand } 
func Delete Resource ( r rest . Graceful Deleter , allows Options bool , scope * Request Scope , admit admission . Interface ) http . Handler Func { return func ( w http . Response defer trace . Log If if is Dry Run ( req . URL ) && ! utilfeature . Default Feature Gate . Enabled ( features . Dry Run ) { scope . err ( errors . New Bad // TODO: we either want to remove timeout or document it (if we document, move timeout out of this function and declare it in api_installer) timeout := parse ctx = request . With ae := request . Audit Event admit = admission . With output Media Type , _ , err := negotiation . Negotiate Output Media options := & metav1 . Delete if allows Options { body , err := limited Read Body ( req , scope . Max Request Body if len ( body ) > 0 { s , err := negotiation . Negotiate Input // For backwards compatibility, we need to allow existing clients to submit per group Delete Options // It is also allowed to pass a body with meta.k8s.io/v1.Delete Options default GVK := scope . Meta Group Version . With obj , _ , err := metainternalversion . Codecs . Decoder To Version ( s . Serializer , default GVK . Group Version ( ) ) . Decode ( body , & default ae := request . Audit Event audit . Log Request } else { if err := metainternalversion . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , scope . Meta Group Version , options ) ; err != nil { err = errors . New Bad if errs := validation . Validate Delete Options ( options ) ; len ( errs ) > 0 { err := errors . New Invalid ( schema . Group Kind { Group : metav1 . Group if admit != nil && admit . Handles ( admission . Delete ) { user Info , _ := request . User attrs := admission . New Attributes Record ( nil , nil , scope . Kind , namespace , name , scope . Resource , scope . Subresource , admission . Delete , dryrun . Is Dry Run ( options . Dry Run ) , user if mutating Admission , ok := admit . ( admission . Mutation Interface ) ; ok { if err := mutating if validating Admission , ok := admit . ( admission . Validation Interface ) ; ok { if err := validating was result , err := finish was status := http . Status // Return http.Status Accepted if the resource was not deleted immediately and // user requested cascading deletion by setting Orphan Dependents=false. // Note: We want to do this always if resource was not deleted immediately, but // that will break existing clients. // Other cases where resource is not instantly deleted are: namespace deletion // and pod graceful deletion. if ! was Deleted && options . Orphan Dependents != nil && * options . Orphan Dependents == false { status = http . Status // if the rest.Deleter returns a nil object, fill out a status. Callers may return a valid // object with the response. if result == nil { result = & metav1 . Status { Status : metav1 . Status Success , Code : int32 ( status ) , Details : & metav1 . Status transform Response Object ( ctx , scope , trace , req , w , status , output Media } 
func Delete Collection ( r rest . Collection Deleter , check Body bool , scope * Request Scope , admit admission . Interface ) http . Handler Func { return func ( w http . Response defer trace . Log If if is Dry Run ( req . URL ) && ! utilfeature . Default Feature Gate . Enabled ( features . Dry Run ) { scope . err ( errors . New Bad // TODO: we either want to remove timeout or document it (if we document, move timeout out of this function and declare it in api_installer) timeout := parse ctx = request . With ae := request . Audit Event output Media Type , _ , err := negotiation . Negotiate Output Media list Options := metainternalversion . List if err := metainternalversion . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , scope . Meta Group Version , & list Options ) ; err != nil { err = errors . New Bad // transform fields // TODO: Decode Parameters Into should do this. if list Options . Field Selector != nil { fn := func ( label , value string ) ( new Label , new Value string , err error ) { return scope . Convertor . Convert Field if list Options . Field Selector , err = list Options . Field Selector . Transform ( fn ) ; err != nil { // TODO: allow bad request to set field causes based on query parameters err = errors . New Bad options := & metav1 . Delete if check Body { body , err := limited Read Body ( req , scope . Max Request Body if len ( body ) > 0 { s , err := negotiation . Negotiate Input default GVK := scope . Kind . Group Version ( ) . With obj , _ , err := scope . Serializer . Decoder To Version ( s . Serializer , default GVK . Group Version ( ) ) . Decode ( body , & default ae := request . Audit Event audit . Log Request } else { if err := metainternalversion . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , scope . Meta Group Version , options ) ; err != nil { err = errors . New Bad if errs := validation . Validate Delete Options ( options ) ; len ( errs ) > 0 { err := errors . New Invalid ( schema . Group Kind { Group : metav1 . Group admit = admission . With if admit != nil && admit . Handles ( admission . Delete ) { user Info , _ := request . User attrs := admission . New Attributes Record ( nil , nil , scope . Kind , namespace , " " , scope . Resource , scope . Subresource , admission . Delete , dryrun . Is Dry Run ( options . Dry Run ) , user if mutating Admission , ok := admit . ( admission . Mutation Interface ) ; ok { err = mutating if validating Admission , ok := admit . ( admission . Validation Interface ) ; ok { err = validating result , err := finish Request ( timeout , func ( ) ( runtime . Object , error ) { return r . Delete Collection ( ctx , options , & list // if the rest.Deleter returns a nil object, fill out a status. Callers may return a valid // object with the response. if result == nil { result = & metav1 . Status { Status : metav1 . Status Success , Code : http . Status OK , Details : & metav1 . Status transform Response Object ( ctx , scope , trace , req , w , http . Status OK , output Media } 
func new Services ( c * Core V1Client , namespace string ) * services { return & services { client : c . REST } 
func Try Start Kubelet ( ) { // If we notice that the kubelet service is inactive, try to start it init System , err := initsystem . Get Init if ! init System . Service // This runs "systemctl daemon-reload && systemctl restart kubelet" if err := init System . Service } 
func Try Stop Kubelet ( ) { // If we notice that the kubelet service is inactive, try to start it init System , err := initsystem . Get Init if ! init System . Service // This runs "systemctl daemon-reload && systemctl stop kubelet" if err := init System . Service } 
func ( dc * Disruption Controller ) finders ( ) [ ] pod Controller Finder { return [ ] pod Controller Finder { dc . get Pod Replication Controller , dc . get Pod Deployment , dc . get Pod Replica Set , dc . get Pod Stateful } 
func ( dc * Disruption Controller ) get Pod Replica Set ( pod * v1 . Pod ) ( * controller And Scale , error ) { controller Ref := metav1 . Get Controller if controller if controller Ref . Kind != controller Kind rs , err := dc . rs Lister . Replica Sets ( pod . Namespace ) . Get ( controller if err != nil { // The only possible error is Not if rs . UID != controller controller Ref = metav1 . Get Controller if controller Ref != nil && controller Ref . Kind == controller Kind return & controller And } 
func ( dc * Disruption Controller ) get Pod Stateful Set ( pod * v1 . Pod ) ( * controller And Scale , error ) { controller Ref := metav1 . Get Controller if controller if controller Ref . Kind != controller Kind ss , err := dc . ss Lister . Stateful Sets ( pod . Namespace ) . Get ( controller if err != nil { // The only possible error is Not if ss . UID != controller return & controller And } 
func ( dc * Disruption Controller ) get Pod Deployment ( pod * v1 . Pod ) ( * controller And Scale , error ) { controller Ref := metav1 . Get Controller if controller if controller Ref . Kind != controller Kind rs , err := dc . rs Lister . Replica Sets ( pod . Namespace ) . Get ( controller if err != nil { // The only possible error is Not if rs . UID != controller controller Ref = metav1 . Get Controller if controller if controller Ref . Kind != controller Kind deployment , err := dc . d Lister . Deployments ( rs . Namespace ) . Get ( controller if err != nil { // The only possible error is Not if deployment . UID != controller return & controller And } 
func ( dc * Disruption Controller ) get Pods For Pdb ( pdb * policy . Pod Disruption Budget ) ( [ ] * v1 . Pod , error ) { sel , err := metav1 . Label Selector As pods , err := dc . pod } 
func ( dc * Disruption Controller ) build Disrupted Pod Map ( pods [ ] * v1 . Pod , pdb * policy . Pod Disruption Budget , current Time time . Time ) ( map [ string ] metav1 . Time , * time . Time ) { disrupted Pods := pdb . Status . Disrupted var recheck if disrupted Pods == nil || len ( disrupted Pods ) == 0 { return result , recheck for _ , pod := range pods { if pod . Deletion disruption Time , found := disrupted expected Deletion := disruption Time . Time . Add ( Deletion if expected Deletion . Before ( current Time ) { klog . V ( 1 ) . Infof ( " " , pod . Namespace , pod . Name , disruption dc . recorder . Eventf ( pod , v1 . Event Type } else { if recheck Time == nil || expected Deletion . Before ( * recheck Time ) { recheck Time = & expected result [ pod . Name ] = disruption return result , recheck } 
func ( dc * Disruption Controller ) fail Safe ( pdb * policy . Pod Disruption Budget ) error { new Pdb := pdb . Deep new Pdb . Status . Pod Disruptions return dc . get Updater ( ) ( new } 
func refresh ( pdb Client policyclientset . Pod Disruption Budget Interface , pdb * policy . Pod Disruption Budget ) * policy . Pod Disruption Budget { new Pdb , err := pdb Client . Get ( pdb . Name , metav1 . Get if err == nil { return new } 
func Get Kubelet Version ( execer utilsexec . Interface ) ( * version . Version , error ) { kubelet Version Regex := regexp . Must Compile ( `^\s*Kubernetes v((0|[1-9][0-9]*)\.(0|[1-9][0-9]*)\.(0|[1-9][0-9]*)([-0-9a-z out , err := command . Combined clean Output := strings . Trim subs := kubelet Version Regex . Find All String Submatch ( clean if len ( subs ) != 1 || len ( subs [ 0 ] ) < 2 { return nil , errors . Errorf ( " " , clean return version . Parse } 
func Make Docker Keyring ( passed Secrets [ ] v1 . Secret , default Keyring credentialprovider . Docker Keyring ) ( credentialprovider . Docker Keyring , error ) { passed Credentials := [ ] credentialprovider . Docker for _ , passed Secret := range passed Secrets { if docker Config JSON Bytes , docker Config JSON Exists := passed Secret . Data [ v1 . Docker Config Json Key ] ; ( passed Secret . Type == v1 . Secret Type Docker Config Json ) && docker Config JSON Exists && ( len ( docker Config JSON Bytes ) > 0 ) { docker Config JSON := credentialprovider . Docker Config if err := json . Unmarshal ( docker Config JSON Bytes , & docker Config passed Credentials = append ( passed Credentials , docker Config } else if dockercfg Bytes , dockercfg Exists := passed Secret . Data [ v1 . Docker Config Key ] ; ( passed Secret . Type == v1 . Secret Type Dockercfg ) && dockercfg Exists && ( len ( dockercfg Bytes ) > 0 ) { dockercfg := credentialprovider . Docker if err := json . Unmarshal ( dockercfg passed Credentials = append ( passed if len ( passed Credentials ) > 0 { basic Keyring := & credentialprovider . Basic Docker for _ , curr Credentials := range passed Credentials { basic Keyring . Add ( curr return credentialprovider . Union Docker Keyring { basic Keyring , default return default } 
func ( in * API Endpoint ) Deep Copy ( ) * API out := new ( API in . Deep Copy } 
func ( in * API Server ) Deep Copy Into ( out * API in . Control Plane Component . Deep Copy Into ( & out . Control Plane if in . Cert SA Ns != nil { in , out := & in . Cert SA Ns , & out . Cert SA if in . Timeout For Control Plane != nil { in , out := & in . Timeout For Control Plane , & out . Timeout For Control } 
func ( in * API Server ) Deep Copy ( ) * API out := new ( API in . Deep Copy } 
func ( in * Bootstrap Token ) Deep Copy Into ( out * Bootstrap * out = new ( Bootstrap Token * out = ( * in ) . Deep } 
func ( in * Bootstrap Token ) Deep Copy ( ) * Bootstrap out := new ( Bootstrap in . Deep Copy } 
func ( in * Bootstrap Token Discovery ) Deep Copy Into ( out * Bootstrap Token if in . CA Cert Hashes != nil { in , out := & in . CA Cert Hashes , & out . CA Cert } 
func ( in * Bootstrap Token Discovery ) Deep Copy ( ) * Bootstrap Token out := new ( Bootstrap Token in . Deep Copy } 
func ( in * Bootstrap Token String ) Deep Copy ( ) * Bootstrap Token out := new ( Bootstrap Token in . Deep Copy } 
func ( in * Cluster Configuration ) Deep Copy Into ( out * Cluster out . Type Meta = in . Type in . Component Configs . Deep Copy Into ( & out . Component in . Etcd . Deep Copy in . API Server . Deep Copy Into ( & out . API in . Controller Manager . Deep Copy Into ( & out . Controller in . Scheduler . Deep Copy if in . Feature Gates != nil { in , out := & in . Feature Gates , & out . Feature } 
func ( in * Cluster Configuration ) Deep Copy ( ) * Cluster out := new ( Cluster in . Deep Copy } 
func ( in * Cluster Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Cluster Status ) Deep Copy Into ( out * Cluster out . Type Meta = in . Type if in . API Endpoints != nil { in , out := & in . API Endpoints , & out . API * out = make ( map [ string ] API } 
func ( in * Cluster Status ) Deep Copy ( ) * Cluster out := new ( Cluster in . Deep Copy } 
func ( in * Cluster Status ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Component Configs ) Deep Copy Into ( out * Component * out = new ( config . Kubelet ( * in ) . Deep Copy if in . Kube Proxy != nil { in , out := & in . Kube Proxy , & out . Kube * out = new ( apisconfig . Kube Proxy ( * in ) . Deep Copy } 
func ( in * Component Configs ) Deep Copy ( ) * Component out := new ( Component in . Deep Copy } 
func ( in * Control Plane Component ) Deep Copy Into ( out * Control Plane if in . Extra Args != nil { in , out := & in . Extra Args , & out . Extra if in . Extra Volumes != nil { in , out := & in . Extra Volumes , & out . Extra * out = make ( [ ] Host Path } 
func ( in * Control Plane Component ) Deep Copy ( ) * Control Plane out := new ( Control Plane in . Deep Copy } 
func ( in * DNS ) Deep Copy out . Image Meta = in . Image } 
func ( in * DNS ) Deep in . Deep Copy } 
func ( in * Discovery ) Deep Copy if in . Bootstrap Token != nil { in , out := & in . Bootstrap Token , & out . Bootstrap * out = new ( Bootstrap Token ( * in ) . Deep Copy * out = new ( File } 
func ( in * Discovery ) Deep in . Deep Copy } 
func ( in * Etcd ) Deep Copy * out = new ( Local ( * in ) . Deep Copy * out = new ( External ( * in ) . Deep Copy } 
func ( in * Etcd ) Deep in . Deep Copy } 
func ( in * External Etcd ) Deep Copy Into ( out * External } 
func ( in * External Etcd ) Deep Copy ( ) * External out := new ( External in . Deep Copy } 
func ( in * File Discovery ) Deep Copy ( ) * File out := new ( File in . Deep Copy } 
func ( in * Host Path Mount ) Deep Copy ( ) * Host Path out := new ( Host Path in . Deep Copy } 
func ( in * Image Meta ) Deep Copy ( ) * Image out := new ( Image in . Deep Copy } 
func ( in * Init Configuration ) Deep Copy Into ( out * Init out . Type Meta = in . Type in . Cluster Configuration . Deep Copy Into ( & out . Cluster if in . Bootstrap Tokens != nil { in , out := & in . Bootstrap Tokens , & out . Bootstrap * out = make ( [ ] Bootstrap for i := range * in { ( * in ) [ i ] . Deep Copy in . Node Registration . Deep Copy Into ( & out . Node out . Local API Endpoint = in . Local API } 
func ( in * Init Configuration ) Deep Copy ( ) * Init out := new ( Init in . Deep Copy } 
func ( in * Init Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Join Configuration ) Deep Copy Into ( out * Join out . Type Meta = in . Type in . Node Registration . Deep Copy Into ( & out . Node in . Discovery . Deep Copy if in . Control Plane != nil { in , out := & in . Control Plane , & out . Control * out = new ( Join Control } 
func ( in * Join Configuration ) Deep Copy ( ) * Join out := new ( Join in . Deep Copy } 
func ( in * Join Configuration ) Deep Copy Object ( ) runtime . Object { if c := in . Deep } 
func ( in * Join Control Plane ) Deep Copy Into ( out * Join Control out . Local API Endpoint = in . Local API } 
func ( in * Join Control Plane ) Deep Copy ( ) * Join Control out := new ( Join Control in . Deep Copy } 
func ( in * Local Etcd ) Deep Copy Into ( out * Local out . Image Meta = in . Image if in . Extra Args != nil { in , out := & in . Extra Args , & out . Extra if in . Server Cert SA Ns != nil { in , out := & in . Server Cert SA Ns , & out . Server Cert SA if in . Peer Cert SA Ns != nil { in , out := & in . Peer Cert SA Ns , & out . Peer Cert SA } 
func ( in * Local Etcd ) Deep Copy ( ) * Local out := new ( Local in . Deep Copy } 
func ( in * Networking ) Deep in . Deep Copy } 
func ( in * Node Registration Options ) Deep Copy Into ( out * Node Registration for i := range * in { ( * in ) [ i ] . Deep Copy if in . Kubelet Extra Args != nil { in , out := & in . Kubelet Extra Args , & out . Kubelet Extra } 
func ( in * Node Registration Options ) Deep Copy ( ) * Node Registration out := new ( Node Registration in . Deep Copy } 
func New Namespace Informer ( client kubernetes . Interface , resync Period time . Duration , indexers cache . Indexers ) cache . Shared Index Informer { return New Filtered Namespace Informer ( client , resync } 
func ( c * Fake Storage Classes ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( storageclasses Resource , name ) , & storagev1 . Storage } 
func ( c * Fake Secrets ) Get ( name string , options v1 . Get Options ) ( result * corev1 . Secret , err error ) { obj , err := c . Fake . Invokes ( testing . New Get Action ( secrets } 
func ( c * Fake Secrets ) List ( opts v1 . List Options ) ( result * corev1 . Secret List , err error ) { obj , err := c . Fake . Invokes ( testing . New List Action ( secrets Resource , secrets Kind , c . ns , opts ) , & corev1 . Secret label , _ , _ := testing . Extract From List list := & corev1 . Secret List { List Meta : obj . ( * corev1 . Secret List ) . List for _ , item := range obj . ( * corev1 . Secret } 
func ( c * Fake Secrets ) Watch ( opts v1 . List Options ) ( watch . Interface , error ) { return c . Fake . Invokes Watch ( testing . New Watch Action ( secrets } 
func ( c * Fake Secrets ) Create ( secret * corev1 . Secret ) ( result * corev1 . Secret , err error ) { obj , err := c . Fake . Invokes ( testing . New Create Action ( secrets } 
func ( c * Fake Secrets ) Update ( secret * corev1 . Secret ) ( result * corev1 . Secret , err error ) { obj , err := c . Fake . Invokes ( testing . New Update Action ( secrets } 
func ( c * Fake Secrets ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Delete Action ( secrets } 
func ( c * Fake Secrets ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Delete Collection Action ( secrets Resource , c . ns , list _ , err := c . Fake . Invokes ( action , & corev1 . Secret } 
func ( c * Fake Secrets ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * corev1 . Secret , err error ) { obj , err := c . Fake . Invokes ( testing . New Patch Subresource Action ( secrets } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Job Condition ) ( nil ) , ( * batch . Job Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Job Condition_To_batch_Job Condition ( a . ( * v1 . Job Condition ) , b . ( * batch . Job if err := s . Add Generated Conversion Func ( ( * batch . Job Condition ) ( nil ) , ( * v1 . Job Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job Condition_To_v1_Job Condition ( a . ( * batch . Job Condition ) , b . ( * v1 . Job if err := s . Add Generated Conversion Func ( ( * v1 . Job List ) ( nil ) , ( * batch . Job List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Job List_To_batch_Job List ( a . ( * v1 . Job List ) , b . ( * batch . Job if err := s . Add Generated Conversion Func ( ( * batch . Job List ) ( nil ) , ( * v1 . Job List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job List_To_v1_Job List ( a . ( * batch . Job List ) , b . ( * v1 . Job if err := s . Add Generated Conversion Func ( ( * v1 . Job Spec ) ( nil ) , ( * batch . Job Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Job Spec_To_batch_Job Spec ( a . ( * v1 . Job Spec ) , b . ( * batch . Job if err := s . Add Generated Conversion Func ( ( * batch . Job Spec ) ( nil ) , ( * v1 . Job Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job Spec_To_v1_Job Spec ( a . ( * batch . Job Spec ) , b . ( * v1 . Job if err := s . Add Generated Conversion Func ( ( * v1 . Job Status ) ( nil ) , ( * batch . Job Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Job Status_To_batch_Job Status ( a . ( * v1 . Job Status ) , b . ( * batch . Job if err := s . Add Generated Conversion Func ( ( * batch . Job Status ) ( nil ) , ( * v1 . Job Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job Status_To_v1_Job Status ( a . ( * batch . Job Status ) , b . ( * v1 . Job if err := s . Add Conversion Func ( ( * batch . Job Spec ) ( nil ) , ( * v1 . Job Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_batch_Job Spec_To_v1_Job Spec ( a . ( * batch . Job Spec ) , b . ( * v1 . Job if err := s . Add Conversion Func ( ( * v1 . Job Spec ) ( nil ) , ( * batch . Job Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Job Spec_To_batch_Job Spec ( a . ( * v1 . Job Spec ) , b . ( * batch . Job } 
func Convert_v1_Job_To_batch_Job ( in * v1 . Job , out * batch . Job , s conversion . Scope ) error { return auto } 
func Convert_batch_Job_To_v1_Job ( in * batch . Job , out * v1 . Job , s conversion . Scope ) error { return auto } 
func Convert_v1_Job Condition_To_batch_Job Condition ( in * v1 . Job Condition , out * batch . Job Condition , s conversion . Scope ) error { return auto Convert_v1_Job Condition_To_batch_Job } 
func Convert_batch_Job Condition_To_v1_Job Condition ( in * batch . Job Condition , out * v1 . Job Condition , s conversion . Scope ) error { return auto Convert_batch_Job Condition_To_v1_Job } 
func Convert_v1_Job List_To_batch_Job List ( in * v1 . Job List , out * batch . Job List , s conversion . Scope ) error { return auto Convert_v1_Job List_To_batch_Job } 
func Convert_batch_Job List_To_v1_Job List ( in * batch . Job List , out * v1 . Job List , s conversion . Scope ) error { return auto Convert_batch_Job List_To_v1_Job } 
func Convert_v1_Job Status_To_batch_Job Status ( in * v1 . Job Status , out * batch . Job Status , s conversion . Scope ) error { return auto Convert_v1_Job Status_To_batch_Job } 
func Convert_batch_Job Status_To_v1_Job Status ( in * batch . Job Status , out * v1 . Job Status , s conversion . Scope ) error { return auto Convert_batch_Job Status_To_v1_Job } 
func New Manager ( config * Config ) ( Manager , error ) { cert , force Rotation , err := get Current Certificate Or Bootstrap ( config . Certificate Store , config . Bootstrap Certificate PEM , config . Bootstrap Key get Template := config . Get if get Template == nil { get Template = func ( ) * x509 . Certificate m := manager { stop Ch : make ( chan struct { } ) , client Fn : config . Client Fn , get Template : get Template , dynamic Template : config . Get Template != nil , usages : config . Usages , cert Store : config . Certificate Store , cert : cert , force Rotation : force Rotation , certificate Expiration : config . Certificate } 
func ( m * manager ) Current ( ) * tls . Certificate { m . cert Access Lock . R defer m . cert Access Lock . R if m . cert != nil && m . cert . Leaf != nil && time . Now ( ) . After ( m . cert . Leaf . Not } 
func ( m * manager ) Server Healthy ( ) bool { m . cert Access Lock . R defer m . cert Access Lock . R return m . server } 
func ( m * manager ) Stop ( ) { m . client Access defer m . client Access close ( m . stop } 
func ( m * manager ) Start ( ) { // Certificate rotation depends on access to the API server certificate // signing API, so don't start the certificate manager if we don't have a // client. if m . client template go wait . Until ( func ( ) { deadline := m . next Rotation if sleep Interval := deadline . Sub ( time . Now ( ) ) ; sleep Interval > 0 { klog . V ( 2 ) . Infof ( " " , sleep timer := time . New Timer ( sleep select { case <- timer . C : // unblock when deadline expires case <- template Changed : if reflect . Deep Equal ( m . get Last Request ( ) , m . get // Don't enter rotate Certs and trigger backoff if we don't even have a template to request yet if m . get if err := wait . Exponential Backoff ( backoff , m . rotate Certs ) ; err != nil { utilruntime . Handle wait . Poll Infinite ( 32 * time . Second , m . rotate } , time . Second , m . stop if m . dynamic Template { go wait . Until ( func ( ) { // check if the current template matches what we last requested if ! m . cert Satisfies Template ( ) && ! reflect . Deep Equal ( m . get Last Request ( ) , m . get Template ( ) ) { // if the template is different, queue up an interrupt of the rotation deadline loop. // if we've requested a CSR that matches the new template by the time the interrupt is handled, the interrupt is disregarded. template } , time . Second , m . stop } 
func ( m * manager ) rotate template , csr PEM , key PEM , private Key , err := m . generate if err != nil { utilruntime . Handle // request the client each time client , err := m . get if err != nil { utilruntime . Handle // Call the Certificate Signing Request API to get a certificate for the // new private key. req , err := csr . Request Certificate ( client , csr PEM , " " , m . usages , private if err != nil { utilruntime . Handle return false , m . update Server // Once we've successfully submitted a CSR for this template, record that we did so m . set Last // Wait for the certificate to be signed. This interface and internal timout // is a remainder after the old design using raw watch wrapped with backoff. crt PEM , err := csr . Wait For Certificate ( client , req , certificate Wait if err != nil { utilruntime . Handle cert , err := m . cert Store . Update ( crt PEM , key if err != nil { utilruntime . Handle m . update } 
func ( m * manager ) cert Satisfies Template if template := m . get Template ( ) ; template != nil { if template . Subject . Common Name != m . cert . Leaf . Subject . Common Name { klog . V ( 2 ) . Infof ( " " , m . cert . Leaf . Subject . Common Name , template . Subject . Common current DNS Names := sets . New String ( m . cert . Leaf . DNS desired DNS Names := sets . New String ( template . DNS missing DNS Names := desired DNS Names . Difference ( current DNS if len ( missing DNS Names ) > 0 { klog . V ( 2 ) . Infof ( " " , missing DNS current I Ps := sets . New for _ , ip := range m . cert . Leaf . IP Addresses { current I desired I Ps := sets . New for _ , ip := range template . IP Addresses { desired I missing I Ps := desired I Ps . Difference ( current I if len ( missing I Ps ) > 0 { klog . V ( 2 ) . Infof ( " " , missing I current Orgs := sets . New desired Orgs := sets . New missing Orgs := desired Orgs . Difference ( current if len ( missing Orgs ) > 0 { klog . V ( 2 ) . Infof ( " " , missing } 
func ( m * manager ) next Rotation Deadline ( ) time . Time { // force Rotation is not protected by locks if m . force Rotation { m . force m . cert Access Lock . R defer m . cert Access Lock . R if ! m . cert Satisfies Template not After := m . cert . Leaf . Not total Duration := float64 ( not After . Sub ( m . cert . Leaf . Not deadline := m . cert . Leaf . Not Before . Add ( jittery Duration ( total klog . V ( 2 ) . Infof ( " " , not if m . certificate Expiration != nil { m . certificate Expiration . Set ( float64 ( not } 
func ( m * manager ) update Cached ( cert * tls . Certificate ) { m . cert Access defer m . cert Access m . server } 
func ( m * manager ) update Server Error ( err error ) error { m . cert Access defer m . cert Access switch { case errors . Is Unauthorized ( err ) : // SSL terminating proxies may report this error instead of the master m . server case errors . Is Unexpected Server Error ( err ) : // generally indicates a proxy or other load balancer problem, rather than a problem coming // from the master m . server default : // Identify known errors that could be expected for a cert request that // indicate everything is working normally m . server Health = errors . Is Not Found ( err ) || errors . Is } 
func ( node Strategy ) Prepare For // Nodes allow *all* fields, including status, to be set on create. if ! utilfeature . Default Feature Gate . Enabled ( features . Dynamic Kubelet Config ) { node . Spec . Config } 
func ( node Strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new old new Node . Status = old if ! utilfeature . Default Feature Gate . Enabled ( features . Dynamic Kubelet Config ) && ! node Config Source In Use ( old Node ) { new Node . Spec . Config } 
func node Config Source In if node . Spec . Config } 
func ( node Strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error return validation . Validate } 
func ( node Strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { error List := validation . Validate return append ( error List , validation . Validate Node } 
func node Status Config In } 
func Node To Selectable Fields ( node * api . Node ) fields . Set { object Meta Fields Set := generic . Object Meta Fields Set ( & node . Object specific Fields return generic . Merge Fields Sets ( object Meta Fields Set , specific Fields } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { node return labels . Set ( node Obj . Object Meta . Labels ) , Node To Selectable Fields ( node } 
func Resource Location ( getter Resource Getter , connection client . Connection Info Getter , proxy Transport http . Round Tripper , ctx context . Context , id string ) ( * url . URL , http . Round Tripper , error ) { scheme Req , name , port Req , valid := utilnet . Split Scheme Name if ! valid { return nil , nil , errors . New Bad info , err := connection . Get Connection Info ( ctx , types . Node // We check if we want to get a default Kubelet's transport. It happens if either: // - no port is specified in request (Kubelet's port is default) // - the requested port matches the kubelet port for this node if port Req == " " || port Req == info . Port { return & url . URL { Scheme : info . Scheme , Host : net . Join Host if err := proxyutil . Is Proxyable Hostname ( ctx , & net . Resolver { } , info . Hostname ) ; err != nil { return nil , nil , errors . New Bad // Otherwise, return the requested scheme and port, and the proxy transport return & url . URL { Scheme : scheme Req , Host : net . Join Host Port ( info . Hostname , port Req ) } , proxy } 
func Convert_v1alpha1_Garbage Collector Controller Configuration_To_config_Garbage Collector Controller Configuration ( in * v1alpha1 . Garbage Collector Controller Configuration , out * config . Garbage Collector Controller Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Garbage Collector Controller Configuration_To_config_Garbage Collector Controller } 
func Convert_config_Garbage Collector Controller Configuration_To_v1alpha1_Garbage Collector Controller Configuration ( in * config . Garbage Collector Controller Configuration , out * v1alpha1 . Garbage Collector Controller Configuration , s conversion . Scope ) error { return auto Convert_config_Garbage Collector Controller Configuration_To_v1alpha1_Garbage Collector Controller } 
func ( s * Location Streamer ) Input Stream ( ctx context . Context , api Version , accept Header string ) ( stream io . Read Closer , flush bool , content if transport == nil { transport = http . Default client := & http . Client { Transport : transport , Check Redirect : s . Redirect req , err := http . New // Pass the parent context down to the request to ensure that the resources // will be release properly. req = req . With if s . Response Checker != nil { if err = s . Response content Type = s . Content if len ( content Type ) == 0 { content if len ( content Type ) > 0 { content Type = strings . Trim Space ( strings . Split N ( content } 
func Prevent } 
func New Bootstrap Token Options ( ) * Bootstrap Token Options { bto := & Bootstrap Token Options { & kubeadmapiv1beta2 . Bootstrap kubeadmapiv1beta2 . Set Defaults_Bootstrap Token ( bto . Bootstrap } 
func ( bto * Bootstrap Token Options ) Add Token Flag ( fs * pflag . Flag Set ) { fs . String Var ( & bto . Token Str , Token } 
func ( bto * Bootstrap Token Options ) Add TTL Flag ( fs * pflag . Flag Set ) { bto . Add TTL Flag With Name ( fs , Token } 
func ( bto * Bootstrap Token Options ) Add TTL Flag With Name ( fs * pflag . Flag Set , flag Name string ) { fs . Duration Var ( & bto . TTL . Duration , flag } 
func ( bto * Bootstrap Token Options ) Add Usages Flag ( fs * pflag . Flag Set ) { fs . String Slice Var ( & bto . Usages , Token Usages , bto . Usages , fmt . Sprintf ( " " , strings . Join ( kubeadmconstants . Default Token } 
func ( bto * Bootstrap Token Options ) Add Groups Flag ( fs * pflag . Flag Set ) { fs . String Slice Var ( & bto . Groups , Token Groups , bto . Groups , fmt . Sprintf ( " " , bootstrapapi . Bootstrap Group } 
func ( bto * Bootstrap Token Options ) Add Description Flag ( fs * pflag . Flag Set ) { fs . String Var ( & bto . Description , Token } 
func ( bto * Bootstrap Token Options ) Apply To ( cfg * kubeadmapiv1beta2 . Init Configuration ) error { if len ( bto . Token bto . Token , err = kubeadmapiv1beta2 . New Bootstrap Token String ( bto . Token // Set the token specified by the flags as the first and only token to create in case --config is not specified cfg . Bootstrap Tokens = [ ] kubeadmapiv1beta2 . Bootstrap Token { * bto . Bootstrap } 
func ( r * rest Mapping Error ) Message ( ) string { version err Msg := fmt . Sprintf ( " " , version err Msg += fmt . Sprintf ( " " , version Kind , version return err } 
func parse Taints ( spec [ ] string ) ( [ ] corev1 . Taint , [ ] corev1 . Taint , error ) { var taints , taints To unique Taints := map [ corev1 . Taint for _ , taint Spec := range spec { if strings . Has Suffix ( taint Spec , " " ) { taint To Remove , err := parse Taint ( strings . Trim Suffix ( taint taints To Remove = append ( taints To Remove , corev1 . Taint { Key : taint To Remove . Key , Effect : taint To } else { new Taint , err := parse Taint ( taint // validate that the taint has an effect, which is required to add the taint if len ( new Taint . Effect ) == 0 { return nil , nil , fmt . Errorf ( " " , taint // validate if taint is unique by <key, effect> if len ( unique Taints [ new Taint . Effect ] ) > 0 && unique Taints [ new Taint . Effect ] . Has ( new Taint . Key ) { return nil , nil , fmt . Errorf ( " " , new // add taint to existing Taints for uniqueness check if len ( unique Taints [ new Taint . Effect ] ) == 0 { unique Taints [ new unique Taints [ new Taint . Effect ] . Insert ( new taints = append ( taints , new return taints , taints To } 
func delete Taints ( taints To Remove [ ] corev1 . Taint , new Taints * [ ] corev1 . Taint ) ( [ ] error , bool ) { all for _ , taint To Remove := range taints To if len ( taint To Remove . Effect ) > 0 { * new Taints , removed = delete Taint ( * new Taints , & taint To } else { * new Taints , removed = delete Taints By Key ( * new Taints , taint To if ! removed { all Errs = append ( all Errs , fmt . Errorf ( " " , taint To Remove . To return all } 
func add Taints ( old Taints [ ] corev1 . Taint , new Taints * [ ] corev1 . Taint ) bool { for _ , old Taint := range old Taints { exists In for _ , taint := range * new Taints { if taint . Match Taint ( & old Taint ) { exists In if ! exists In New { * new Taints = append ( * new Taints , old return len ( old Taints ) != len ( * new } 
func check If Taints Already Exists ( old Taints [ ] corev1 . Taint , taints [ ] corev1 . Taint ) string { var existing Taint for _ , taint := range taints { for _ , old Taint := range old Taints { if taint . Key == old Taint . Key && taint . Effect == old Taint . Effect { existing Taint List = append ( existing Taint return strings . Join ( existing Taint } 
func delete Taints By Key ( taints [ ] corev1 . Taint , taint Key string ) ( [ ] corev1 . Taint , bool ) { new for i := range taints { if taint new Taints = append ( new return new } 
func delete Taint ( taints [ ] corev1 . Taint , taint To Delete * corev1 . Taint ) ( [ ] corev1 . Taint , bool ) { new for i := range taints { if taint To Delete . Match new Taints = append ( new return new } 
func ( c * Clientset ) Metrics V1alpha1 ( ) metricsv1alpha1 . Metrics V1alpha1Interface { return & fakemetricsv1alpha1 . Fake Metrics } 
func ( c * Clientset ) Metrics V1beta1 ( ) metricsv1beta1 . Metrics V1beta1Interface { return & fakemetricsv1beta1 . Fake Metrics } 
func new Volume Stat Calculator ( stats Provider Provider , jitter Period time . Duration , pod * v1 . Pod ) * volume Stat Calculator { return & volume Stat Calculator { stats Provider : stats Provider , jitter Period : jitter Period , pod : pod , stop } 
func ( s * volume Stat Calculator ) Start Once ( ) * volume Stat Calculator { s . start O . Do ( func ( ) { go wait . Jitter Until ( func ( ) { s . calc And Store } , s . jitter Period , 1.0 , true , s . stop } 
func ( s * volume Stat Calculator ) Stop Once ( ) * volume Stat Calculator { s . stop O . Do ( func ( ) { close ( s . stop } 
func ( s * volume Stat Calculator ) calc And Store Stats ( ) { // Find all Volumes for the Pod volumes , found := s . stats Provider . List Volumes For // Get volume specs for the pod - key'd by volume name volumes for _ , v := range s . pod . Spec . Volumes { volumes // Call Get Metrics on each Volume and copy the result to a new Volume Stats.Fs Stats var ephemeral Stats [ ] stats . Volume var persistent Stats [ ] stats . Volume for name , v := range volumes { metric , err := v . Get if err != nil { // Expected for Volumes that don't support Metrics if ! volume . Is Not // Lookup the volume spec and add a 'PVC Reference' for volumes that reference a PVC vol Spec := volumes var pvc Ref * stats . PVC if pvc Source := vol Spec . Persistent Volume Claim ; pvc Source != nil { pvc Ref = & stats . PVC Reference { Name : pvc Source . Claim Name , Namespace : s . pod . Get volume Stats := s . parse Pod Volume Stats ( name , pvc Ref , metric , vol if is Volume Ephemeral ( vol Spec ) { ephemeral Stats = append ( ephemeral Stats , volume } else { persistent Stats = append ( persistent Stats , volume // Store the new stats s . latest . Store ( Pod Volume Stats { Ephemeral Volumes : ephemeral Stats , Persistent Volumes : persistent } 
func ( s * volume Stat Calculator ) parse Pod Volume Stats ( pod Name string , pvc Ref * stats . PVC Reference , metric * volume . Metrics , vol Spec v1 . Volume ) stats . Volume inodes Free := uint64 ( metric . Inodes inodes Used := uint64 ( metric . Inodes return stats . Volume Stats { Name : pod Name , PVC Ref : pvc Ref , Fs Stats : stats . Fs Stats { Time : metric . Time , Available Bytes : & available , Capacity Bytes : & capacity , Used Bytes : & used , Inodes : & inodes , Inodes Free : & inodes Free , Inodes Used : & inodes } 
func ( c * Cloud Node Lifecycle Controller ) Run ( stop Ch <- chan struct { } ) { defer utilruntime . Handle // The following loops run communicate with the API Server with a worst case complexity // of O(num_nodes) per cycle. These functions are justified here because these events fire // very infrequently. DO NOT MODIFY this to perform frequent operations. // Start a loop to periodically check if any nodes have been // deleted or shutdown from the cloudprovider wait . Until ( c . Monitor Nodes , c . node Monitor Period , stop } 
func ( c * Cloud Node Lifecycle Controller ) Monitor if ! ok { utilruntime . Handle nodes , err := c . node for _ , node := range nodes { // Default Node Ready status to v1.Condition Unknown status := v1 . Condition if _ , c := nodeutil . Get Node Condition ( & node . Status , v1 . Node if status == v1 . Condition True { // if taint exist remove taint err = controller . Remove Taint Off Node ( c . kube Client , node . Name , node , Shutdown // we need to check this first to get taint working in similar in all cloudproviders // current problem is that shutdown nodes are not working in similar way ie. all cloudproviders // does not delete node from kubernetes cluster when instance it is shutdown see issue #46442 shutdown , err := shutdown In Cloud if shutdown && err == nil { // if node is shutdown add shutdown taint err = controller . Add Or Update Taint On Node ( c . kube Client , node . Name , Shutdown // At this point the node has Not Ready status, we need to check if the node has been removed // from the cloud provider. If node cannot be found in cloudprovider, then delete the node exists , err := ensure Node Exists By Provider ref := & v1 . Object c . recorder . Eventf ( ref , v1 . Event Type Normal , fmt . Sprintf ( " " , node . Name ) , " " , node . Name , delete Node if err := c . kube Client . Core } 
func shutdown In Cloud shutdown , err := instances . Instance Shutdown By Provider ID ( ctx , node . Spec . Provider if err == cloudprovider . Not } 
func max Resource List ( list , new v1 . Resource } 
func Get Resource Request ( pod * v1 . Pod , resource v1 . Resource Name ) int64 { if resource == v1 . Resource total for _ , container := range pod . Spec . Containers { if r Quantity , ok := container . Resources . Requests [ resource ] ; ok { if resource == v1 . Resource CPU { total Resources += r Quantity . Milli } else { total Resources += r // take max_resource(sum_pod, any_init_container) for _ , container := range pod . Spec . Init Containers { if r Quantity , ok := container . Resources . Requests [ resource ] ; ok { if resource == v1 . Resource CPU && r Quantity . Milli Value ( ) > total Resources { total Resources = r Quantity . Milli } else if r Quantity . Value ( ) > total Resources { total Resources = r return total } 
func Extract Resource Value By Container Name ( fs * v1 . Resource Field Selector , pod * v1 . Pod , container Name string ) ( string , error ) { container , err := find Container In Pod ( pod , container return Extract Container Resource } 
func Extract Resource Value By Container Name And Node Allocatable ( fs * v1 . Resource Field Selector , pod * v1 . Pod , container Name string , node Allocatable v1 . Resource List ) ( string , error ) { real Container , err := find Container In Pod ( pod , container container := real Container . Deep Merge Container Resource Limits ( container , node return Extract Container Resource } 
func find Container In Pod ( pod * v1 . Pod , container Name string ) ( * v1 . Container , error ) { for _ , container := range pod . Spec . Containers { if container . Name == container return nil , fmt . Errorf ( " " , container } 
func Merge Container Resource Limits ( container * v1 . Container , allocatable v1 . Resource List ) { if container . Resources . Limits == nil { container . Resources . Limits = make ( v1 . Resource for _ , resource := range [ ] v1 . Resource Name { v1 . Resource CPU , v1 . Resource Memory , v1 . Resource Ephemeral Storage } { if quantity , exists := container . Resources . Limits [ resource ] ; ! exists || quantity . Is } 
func ( v * version ) Controller Revisions ( ) Controller Revision Informer { return & controller Revision Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( v * version ) Daemon Sets ( ) Daemon Set Informer { return & daemon Set Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( v * version ) Deployments ( ) Deployment Informer { return & deployment Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( v * version ) Replica Sets ( ) Replica Set Informer { return & replica Set Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( v * version ) Stateful Sets ( ) Stateful Set Informer { return & stateful Set Informer { factory : v . factory , namespace : v . namespace , tweak List Options : v . tweak List } 
func ( in * Leader Election Configuration ) Deep Copy Into ( out * Leader Election out . Lease Duration = in . Lease out . Renew Deadline = in . Renew out . Retry Period = in . Retry } 
func Named Check ( name string , check func ( r * http . Request ) error ) Healthz Checker { return & healthz } 
func Install Path Handler ( mux mux , path string , checks ... Healthz checks = [ ] Healthz Checker { Ping klog . V ( 5 ) . Info ( " " , format Quoted ( checker mux . Handle ( path , handle Root for _ , check := range checks { mux . Handle ( fmt . Sprintf ( " " , path , check . Name ( ) ) , adapt Check To } 
func get Excluded if found { return sets . New return sets . New } 
func handle Root Healthz ( checks ... Healthz Checker ) http . Handler Func { return http . Handler Func ( func ( w http . Response excluded := get Excluded var verbose fmt . Fprintf ( & verbose fmt . Fprintf ( & verbose } else { fmt . Fprintf ( & verbose if excluded . Len ( ) > 0 { fmt . Fprintf ( & verbose Out , " \n " , format klog . Warningf ( " " , format // always be verbose on failure if failed { klog . V ( 2 ) . Infof ( " " , verbose http . Error ( w , fmt . Sprintf ( " " , verbose Out . String ( ) ) , http . Status Internal Server verbose Out . Write } 
func adapt Check To Handler ( c func ( r * http . Request ) error ) http . Handler Func { return http . Handler Func ( func ( w http . Response if err != nil { http . Error ( w , fmt . Sprintf ( " " , err ) , http . Status Internal Server } 
func checker Names ( checks ... Healthz Checker ) [ ] string { // accumulate the names of checks for printing them out. checker for _ , check := range checks { checker Names = append ( checker return checker } 
func format } 
func new REST ( resource schema . Group Resource , kind , list Kind schema . Group Version Kind , strategy custom Resource Strategy , opts Getter generic . REST Options Getter , categories [ ] string , table Convertor rest . Table Convertor ) ( * REST , * Status REST ) { store := & genericregistry . Store { New ret . Set Group Version } , New List Func : func ( ) runtime . Object { // lists are never stored, only manufactured, so stomp in the right kind ret := & unstructured . Unstructured ret . Set Group Version Kind ( list } , Predicate Func : strategy . Match Custom Resource Definition Storage , Default Qualified Resource : resource , Create Strategy : strategy , Update Strategy : strategy , Delete Strategy : strategy , Table Convertor : table options := & generic . Store Options { REST Options : opts Getter , Attr Func : strategy . Get if err := store . Complete With status status Store . Update Strategy = New Status return & REST { store , categories } , & Status REST { store : & status } 
func ( e * REST ) List ( ctx context . Context , options * metainternalversion . List // Shallow copy Object Meta in returned list for each item. Native types have `Items []Item` fields and therefore // implicitly shallow copy Object Meta. The generic store sets the self-link for each item. So this is necessary // to avoid mutation of the objects from the cache. if ul , ok := l . ( * unstructured . Unstructured List ) ; ok { for i := range ul . Items { shallow Copy Object } 
func scale From Custom Resource ( cr * unstructured . Unstructured , spec Replicas Path , status Replicas Path , label Selector Path string ) ( * autoscalingv1 . Scale , bool , error ) { spec Replicas Path = strings . Trim Prefix ( spec Replicas spec Replicas , found Spec Replicas , err := unstructured . Nested Int64 ( cr . Unstructured Content ( ) , strings . Split ( spec Replicas } else if ! found Spec Replicas { spec status Replicas Path = strings . Trim Prefix ( status Replicas status Replicas , found , err := unstructured . Nested Int64 ( cr . Unstructured Content ( ) , strings . Split ( status Replicas } else if ! found { status var label if len ( label Selector Path ) > 0 { label Selector Path = strings . Trim Prefix ( label Selector label Selector , found , err = unstructured . Nested String ( cr . Unstructured Content ( ) , strings . Split ( label Selector scale := & autoscalingv1 . Scale { // Populate api Version and kind so conversion recognizes we are already in the desired GVK and doesn't try to convert Type Meta : metav1 . Type Meta { API Version : " " , Kind : " " , } , Object Meta : metav1 . Object Meta { Name : cr . Get Name ( ) , Namespace : cr . Get Namespace ( ) , UID : cr . Get UID ( ) , Resource Version : cr . Get Resource Version ( ) , Creation Timestamp : cr . Get Creation Timestamp ( ) , } , Spec : autoscalingv1 . Scale Spec { Replicas : int32 ( spec Replicas ) , } , Status : autoscalingv1 . Scale Status { Replicas : int32 ( status Replicas ) , Selector : label return scale , found Spec } 
func New Edit Options ( edit Mode Edit Mode , io Streams genericclioptions . IO Streams ) * Edit Options { return & Edit Options { Record Flags : genericclioptions . New Record Flags ( ) , Edit Mode : edit Mode , Print Flags : genericclioptions . New Print Flags ( " " ) . With Type Setter ( scheme . Scheme ) , edit Printer Options : & edit Printer Options { // create new editor-specific Print Flags, with all // output flags disabled, except json / yaml print Flags : ( & genericclioptions . Print Flags { JSON Yaml Print Flags : genericclioptions . New JSON Yaml Print Flags ( ) , } ) . With Default Output ( " " ) , ext : " " , add Header : true , } , Windows Line Endings : goruntime . GOOS == " " , Recorder : genericclioptions . Noop Recorder { } , IO Streams : io } 
func ( o * Edit o . Record o . Recorder , err = o . Record Flags . To if o . Edit Mode != Normal Edit Mode && o . Edit Mode != Edit Before Create Mode && o . Edit Mode != Apply Edit Mode { return fmt . Errorf ( " " , o . Edit o . edit Printer Options . Complete ( o . Print if o . Output Patch && o . Edit Mode != Normal Edit cmd Namespace , enforce Namespace , err := f . To Raw Kube Config b := f . New if o . Edit Mode == Normal Edit Mode || o . Edit Mode == Apply Edit Mode { // when do normal edit or apply edit we need to always retrieve the latest resource from server b = b . Resource Type Or Name r := b . Namespace Param ( cmd Namespace ) . Default Namespace ( ) . Filename Param ( enforce Namespace , & o . Filename Options ) . Continue On o . Original o . updated Result Getter = func ( data [ ] byte ) * resource . Result { // resource builder to read objects from edited data return f . New Builder ( ) . Unstructured ( ) . Stream ( bytes . New Reader ( data ) , " " ) . Continue On o . To Printer = func ( operation string ) ( printers . Resource Printer , error ) { o . Print Flags . Name Print return o . Print Flags . To o . Cmd Namespace = cmd } 
func ( o * Edit Options ) Run ( ) error { edit := New Default Editor ( editor // edit Fn is invoked for each edit session (once with a list for normal edit, once for each individual resource in a edit-on-create invocation) edit Fn := func ( infos [ ] * resource . Info ) error { var ( results = edit contains // loop until we succeed or cancel editing for { // get the object we're going to serialize as input to the editor var original switch len ( infos ) { case 1 : original default : l := & unstructured . Unstructured original if o . Windows Line Endings { w = crlf . New CRLF if o . edit Printer Options . add Header { results . header . write To ( w , o . Edit if ! contains Error { if err := o . edit Printer Options . Print Obj ( original Obj , w ) ; err != nil { return preserved File ( err , results . file , o . Err } else { // In case of an error, preserve the edited file. // Remove the comments (header) from it since we already // have included the latest header in the buffer above. buf . Write ( cmdutil . Manual // launch the editor edited edited , file , err = edit . Launch Temp File ( fmt . Sprintf ( " " , filepath . Base ( os . Args [ 0 ] ) ) , o . edit Printer if err != nil { return preserved File ( err , results . file , o . Err // If we're retrying the loop because of an error, and no change was made in the file, short-circuit if contains Error && bytes . Equal ( cmdutil . Strip Comments ( edited Diff ) , cmdutil . Strip Comments ( edited ) ) { return preserved File ( fmt . Errorf ( " " , " " ) , file , o . Err // Apply validation schema , err := o . f . Validator ( o . Enable if err != nil { return preserved File ( err , file , o . Err err = schema . Validate Bytes ( cmdutil . Strip if err != nil { results = edit contains fmt . Fprintln ( o . Err Out , results . add Error ( apierrors . New Invalid ( corev1 . Scheme Group Version . With Kind ( " " ) . Group Kind ( ) , " " , field . Error // Compare content without comments if bytes . Equal ( cmdutil . Strip Comments ( original ) , cmdutil . Strip fmt . Fprintln ( o . Err lines , err := has Lines ( bytes . New if err != nil { return preserved File ( err , file , o . Err fmt . Fprintln ( o . Err results = edit // parse the edited file updated Infos , err := o . updated Result if err != nil { // syntax error contains results . header . reasons = append ( results . header . reasons , edit // not a syntax error as it turns out... contains updated Visitor := resource . Info List Visitor ( updated // need to make sure the original namespace wasn't changed while editing if err := updated Visitor . Visit ( resource . Require Namespace ( o . Cmd Namespace ) ) ; err != nil { return preserved File ( err , file , o . Err // iterate through all items to apply annotations if err := o . visit Annotation ( updated Visitor ) ; err != nil { return preserved File ( err , file , o . Err switch o . Edit Mode { case Normal Edit Mode : err = o . visit To Patch ( infos , updated case Apply Edit Mode : err = o . visit To Apply Edit Patch ( infos , updated case Edit Before Create Mode : err = o . visit To Create ( updated default : err = fmt . Errorf ( " " , o . Edit if err != nil { return preserved File ( err , results . file , o . Err // Handle all possible errors // // 1. retryable: propose kubectl replace -f // 2. notfound: indicate the location of the saved configuration of the deleted resource // 3. invalid: retry those on the spot by looping ie. reloading the editor if results . retryable > 0 { fmt . Fprintf ( o . Err return cmdutil . Err if results . notfound > 0 { fmt . Fprintf ( o . Err return cmdutil . Err if len ( results . header . reasons ) > 0 { contains switch o . Edit Mode { // If doing normal edit we cannot use Visit because we need to edit a list for convenience. Ref: #20519 case Normal Edit Mode : infos , err := o . Original return edit case Apply Edit Mode : infos , err := o . Original var annotation for i := range infos { data , err := kubectl . Get Original temp Infos , err := o . updated Result annotation Infos = append ( annotation Infos , temp if len ( annotation return edit Fn ( annotation // If doing an edit before created, we don't want a list and instead want the normal behavior as kubectl create. case Edit Before Create Mode : return o . Original Result . Visit ( func ( info * resource . Info , err error ) error { return edit default : return fmt . Errorf ( " " , o . Edit } 
func Get Apply Patch ( obj runtime . Unstructured ) ( [ ] byte , [ ] byte , types . Patch Type , error ) { before JSON , err := encode To if err != nil { return nil , [ ] byte ( " " ) , types . Merge Patch obj Copy := obj . Deep Copy accessor := meta . New annotations , err := accessor . Annotations ( obj if err != nil { return nil , before JSON , types . Merge Patch annotations [ corev1 . Last Applied Config Annotation ] = string ( before accessor . Set Annotations ( obj after JSON , err := encode To JSON ( obj if err != nil { return nil , before JSON , types . Merge Patch patch , err := jsonpatch . Create Merge Patch ( before JSON , after return patch , before JSON , types . Merge Patch } 
func ( h * edit Header ) write To ( w io . Writer , edit Mode Edit Mode ) error { if edit Mode == Apply Edit for _ , r := range h . reasons { if len ( r . other ) > 0 { fmt . Fprintf ( w , " \n " , hash On Line } else { fmt . Fprintf ( w , " \n " , hash On Line for _ , o := range r . other { fmt . Fprintf ( w , " \n " , hash On Line } 
func preserved File ( err error , path string , out io . Writer ) error { if len ( path ) > 0 { if _ , err := os . Stat ( path ) ; ! os . Is Not } 
func has Lines ( r io . Reader ) ( bool , error ) { // TODO: if any files we read have > 64KB lines, we'll need to switch to bytes.Read Line // TODO: probably going to be secrets s := bufio . New for s . Scan ( ) { if line := strings . Trim } 
func hash On Line } 
func ( f * factory ) Build Delegate ( ) ( * delegate , error ) { backend , err := f . build Webhook backend = f . apply Enforced backend = f . apply Buffered return & delegate { Backend : backend , configuration : f . sink , stop } 
func ( c * Fake API Services ) Get ( name string , options v1 . Get Options ) ( result * apiregistration . API Service , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Get Action ( apiservices Resource , name ) , & apiregistration . API return obj . ( * apiregistration . API } 
func ( c * Fake API Services ) Delete ( name string , options * v1 . Delete Options ) error { _ , err := c . Fake . Invokes ( testing . New Root Delete Action ( apiservices Resource , name ) , & apiregistration . API } 
func ( c * Fake API Services ) Delete Collection ( options * v1 . Delete Options , list Options v1 . List Options ) error { action := testing . New Root Delete Collection Action ( apiservices Resource , list _ , err := c . Fake . Invokes ( action , & apiregistration . API Service } 
func ( c * Fake API Services ) Patch ( name string , pt types . Patch Type , data [ ] byte , subresources ... string ) ( result * apiregistration . API Service , err error ) { obj , err := c . Fake . Invokes ( testing . New Root Patch Subresource Action ( apiservices Resource , name , pt , data , subresources ... ) , & apiregistration . API return obj . ( * apiregistration . API } 
func new Active Deadline Handler ( pod Status Provider status . Pod Status Provider , recorder record . Event Recorder , clock clock . Clock , ) ( * active Deadline Handler , error ) { // check for all required fields if clock == nil || pod Status Provider == nil || recorder == nil { return nil , fmt . Errorf ( " " , clock , pod Status return & active Deadline Handler { clock : clock , pod Status Provider : pod Status } 
func ( m * active Deadline Handler ) Should Sync ( pod * v1 . Pod ) bool { return m . past Active } 
func ( m * active Deadline Handler ) Should Evict ( pod * v1 . Pod ) lifecycle . Should Evict Response { if ! m . past Active Deadline ( pod ) { return lifecycle . Should Evict m . recorder . Eventf ( pod , v1 . Event Type return lifecycle . Should Evict } 
func ( m * active Deadline Handler ) past Active Deadline ( pod * v1 . Pod ) bool { // no active deadline was specified if pod . Spec . Active Deadline // get the latest status to determine if it was started pod Status , ok := m . pod Status Provider . Get Pod if ! ok { pod // we have no start time so just return if pod Status . Start Time . Is // determine if the deadline was exceeded start := pod Status . Start allowed Duration := time . Duration ( * pod . Spec . Active Deadline return duration >= allowed } 
func New Desired State Of World Populator ( loop Sleep Duration time . Duration , list Pods Retry Duration time . Duration , pod Lister corelisters . Pod Lister , desired State Of World cache . Desired State Of World , volume Plugin Mgr * volume . Volume Plugin Mgr , pvc Lister corelisters . Persistent Volume Claim Lister , pv Lister corelisters . Persistent Volume Lister ) Desired State Of World Populator { return & desired State Of World Populator { loop Sleep Duration : loop Sleep Duration , list Pods Retry Duration : list Pods Retry Duration , pod Lister : pod Lister , desired State Of World : desired State Of World , volume Plugin Mgr : volume Plugin Mgr , pvc Lister : pvc Lister , pv Lister : pv } 
func ( dswp * desired State Of World Populator ) find And Remove Deleted Pods ( ) { for dsw Pod UID , dsw Pod To Add := range dswp . desired State Of World . Get Pod To Add ( ) { dsw Pod Key , err := kcache . Meta Namespace Key Func ( dsw Pod To if err != nil { klog . Errorf ( " " , dsw Pod Key , dsw Pod // Retrieve the pod object from pod informer with the namespace key namespace , name , err := kcache . Split Meta Namespace Key ( dsw Pod if err != nil { utilruntime . Handle Error ( fmt . Errorf ( " " , dsw Pod informer Pod , err := dswp . pod switch { case errors . Is Not Found ( err ) : // if we can't find the pod, we need to delete it below case err != nil : klog . Errorf ( " " , dsw Pod Key , dsw Pod default : volume Action Flag := util . Determine Volume Action ( informer Pod , dswp . desired State Of if volume Action Flag { informer Pod UID := volutil . Get Unique Pod Name ( informer // Check whether the unique identifier of the pod from dsw matches the one retrieved from pod informer if informer Pod UID == dsw Pod UID { klog . V ( 10 ) . Infof ( " " , dsw Pod Key , dsw Pod // the pod from dsw does not exist in pod informer, or it does not match the unique identifier retrieved // from the informer, delete it from dsw klog . V ( 1 ) . Infof ( " " , dsw Pod Key , dsw Pod dswp . desired State Of World . Delete Pod ( dsw Pod UID , dsw Pod To Add . Volume Name , dsw Pod To Add . Node } 
func ( d * api Versions From Discovery ) fetch Versions ( ) ( * metav1 . API Group , error ) { // TODO(directxman12): amend the discovery interface to ask for a particular group (/apis/foo) groups , err := d . client . Server // Determine the preferred version on the server by first finding the custom metrics group var api Group * metav1 . API for _ , group := range groups . Groups { if group . Name == cmint . Group Name { api if api Group == nil { return nil , fmt . Errorf ( " " , cmint . Group return api } 
func ( d * api Versions From Discovery ) choose Version ( api Group * metav1 . API Group ) ( schema . Group Version , error ) { var preferred Version * schema . Group if gv , present := metric Versions To GV [ api Group . Preferred Version . Group Version ] ; present && len ( api Group . Preferred Version . Group Version ) != 0 { preferred } else { for _ , version := range api Group . Versions { if gv , present := metric Versions To GV [ version . Group Version ] ; present { preferred if preferred Version == nil { return schema . Group return * preferred } 
func ( d * api Versions From Discovery ) Preferred Version ( ) ( schema . Group Version , error ) { d . mu . R if d . pref Version != nil { // if we've already got one, proceed with that defer d . mu . R return * d . pref d . mu . R // double check, someone might have beaten us to it if d . pref Version != nil { return * d . pref // populate our cache group Info , err := d . fetch if err != nil { return schema . Group pref Version , err := d . choose Version ( group if err != nil { return schema . Group d . pref Version = & pref return * d . pref } 
func ( d * api Versions From d . pref } 
func count for _ , b := range n . Bytes ( ) { count += int ( bit } 
func ( oa Open API ) Install ( c * restful . Container , mux * mux . Path Recorder Mux ) ( * handler . Open API Service , * spec . Swagger ) { spec , err := builder . Build Open API Spec ( c . Registered Web open API Versioned Service , err := handler . Register Open API Versioned return open API Versioned } 
func ( l Logs ) Install ( c * restful . Container ) { // use restful: ws.Route(ws.GET("/logs/{logpath:*}").To(file Handler)) // See github.com/emicklei/go-restful/blob/master/examples/restful-serve-static.go ws := new ( restful . Web ws . Route ( ws . GET ( " " ) . To ( log File Handler ) . Param ( ws . Path Parameter ( " " , " " ) . Data ws . Route ( ws . GET ( " " ) . To ( log File List } 
// Set up a service to return the git code version. version WS := new ( restful . Web version version version WS . Route ( version WS . GET ( " " ) . To ( v . handle c . Add ( version } 
func ( v Version ) handle Version ( req * restful . Request , resp * restful . Response ) { responsewriters . Write Raw JSON ( http . Status OK , * v . Version , resp . Response } 
func load Configuration ( config io . Reader ) ( * internalapi . Configuration , error ) { // if no config is provided, return a default configuration if config == nil { external scheme . Default ( external internal if err := scheme . Convert ( external Config , internal return internal // we have a config so parse it. data , err := ioutil . Read decoder := codecs . Universal decoded external Config , ok := decoded if ! ok { return nil , fmt . Errorf ( " " , decoded if err := validation . Validate Configuration ( external return external } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Group Resource ) ( nil ) , ( * v1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Group Resource_To_v1_Group Resource ( a . ( * v1alpha1 . Group Resource ) , b . ( * v1 . Group if err := s . Add Generated Conversion Func ( ( * v1 . Group Resource ) ( nil ) , ( * v1alpha1 . Group Resource ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Group Resource_To_v1alpha1_Group Resource ( a . ( * v1 . Group Resource ) , b . ( * v1alpha1 . Group if err := s . Add Generated Conversion Func ( ( * v1alpha1 . Node Lifecycle Controller Configuration ) ( nil ) , ( * config . Node Lifecycle Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Node Lifecycle Controller Configuration_To_config_Node Lifecycle Controller Configuration ( a . ( * v1alpha1 . Node Lifecycle Controller Configuration ) , b . ( * config . Node Lifecycle Controller if err := s . Add Generated Conversion Func ( ( * config . Node Lifecycle Controller Configuration ) ( nil ) , ( * v1alpha1 . Node Lifecycle Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Node Lifecycle Controller Configuration_To_v1alpha1_Node Lifecycle Controller Configuration ( a . ( * config . Node Lifecycle Controller Configuration ) , b . ( * v1alpha1 . Node Lifecycle Controller if err := s . Add Conversion Func ( ( * config . Node Lifecycle Controller Configuration ) ( nil ) , ( * v1alpha1 . Node Lifecycle Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Node Lifecycle Controller Configuration_To_v1alpha1_Node Lifecycle Controller Configuration ( a . ( * config . Node Lifecycle Controller Configuration ) , b . ( * v1alpha1 . Node Lifecycle Controller if err := s . Add Conversion Func ( ( * v1alpha1 . Node Lifecycle Controller Configuration ) ( nil ) , ( * config . Node Lifecycle Controller Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Node Lifecycle Controller Configuration_To_config_Node Lifecycle Controller Configuration ( a . ( * v1alpha1 . Node Lifecycle Controller Configuration ) , b . ( * config . Node Lifecycle Controller } 
func ( g * group ) V1 ( ) v1 . Interface { return v1 . New ( g . factory , g . namespace , g . tweak List } 
func ( g * group ) V1beta1 ( ) v1beta1 . Interface { return v1beta1 . New ( g . factory , g . namespace , g . tweak List } 
func ( g * group ) V2alpha1 ( ) v2alpha1 . Interface { return v2alpha1 . New ( g . factory , g . namespace , g . tweak List } 
func ( p * Exec Options ) Complete ( f cmdutil . Factory , cmd * cobra . Command , args In [ ] string , args Len At Dash int ) error { // Let kubectl exec follow rules for `--`, see #13004 issue if len ( p . Pod Name ) == 0 && ( len ( args In ) == 0 || args Len At Dash == 0 ) { return cmdutil . Usage Errorf ( cmd , exec Usage if len ( p . Pod Name ) != 0 { if len ( args In ) < 1 { return cmdutil . Usage Errorf ( cmd , exec Usage p . Command = args } else { p . Resource Name = args p . Command = args p . Namespace , _ , err = f . To Raw Kube Config p . Executable Pod Fn = polymorphichelpers . Attachable Pod For Object p . Get Pod Timeout , err = cmdutil . Get Pod Running Timeout if err != nil { return cmdutil . Usage p . Builder = f . New p . rest Client cmd if cmd Parent != nil { p . Full Cmd Name = cmd Parent . Command if len ( p . Full Cmd Name ) > 0 && cmdutil . Is Sibling Command Exists ( cmd , " " ) { p . Suggested Cmd Usage = fmt . Sprintf ( " " , p . Full Cmd Name , p . Resource p . Config , err = f . To REST clientset , err := f . Kubernetes Client p . Pod Client = clientset . Core } 
func ( p * Exec Options ) Validate ( ) error { if len ( p . Pod Name ) == 0 && len ( p . Resource if p . Out == nil || p . Err } 
func ( p * Exec // we still need legacy pod getter when Pod Name in Exec Options struct is provided, // since there are any other command run this function by providing Podname with Pods Getter // and without resource builder, eg: `kubectl cp`. if len ( p . Pod Name ) != 0 { p . Pod , err = p . Pod Client . Pods ( p . Namespace ) . Get ( p . Pod Name , metav1 . Get } else { builder := p . Builder ( ) . With Scheme ( scheme . Scheme , scheme . Scheme . Prioritized Versions All Groups ( ) ... ) . Namespace Param ( p . Namespace ) . Default Namespace ( ) . Resource Names ( " " , p . Resource p . Pod , err = p . Executable Pod Fn ( p . rest Client Getter , obj , p . Get Pod if pod . Status . Phase == corev1 . Pod Succeeded || pod . Status . Phase == corev1 . Pod container Name := p . Container if len ( container Name ) == 0 { if len ( pod . Spec . Containers ) > 1 { usage if len ( p . Suggested Cmd Usage ) > 0 { usage String = fmt . Sprintf ( " \n " , usage String , p . Suggested Cmd fmt . Fprintf ( p . Err Out , " \n " , usage container // ensure we can recover the terminal while attached t := p . Setup var size Queue remotecommand . Terminal Size if t . Raw { // this call spawns a goroutine to monitor/update the terminal size size Queue = t . Monitor Size ( t . Get // unset p.Err if it was previously set because both stdout and stderr go over p.Out when tty is // true p . Err fn := func ( ) error { rest Client , err := restclient . REST Client // TODO: consider abstracting into a client invocation or client helper req := rest Client . Post ( ) . Resource ( " " ) . Name ( pod . Name ) . Namespace ( pod . Namespace ) . Sub req . Versioned Params ( & corev1 . Pod Exec Options { Container : container Name , Command : p . Command , Stdin : p . Stdin , Stdout : p . Out != nil , Stderr : p . Err Out != nil , TTY : t . Raw , } , scheme . Parameter return p . Executor . Execute ( " " , req . URL ( ) , p . Config , p . In , p . Out , p . Err Out , t . Raw , size } 
func map Address By Port ( addr * v1 . Endpoint Address , port v1 . Endpoint Port , ready bool , all Addrs map [ address Key ] * v1 . Endpoint Address , port To Addr Ready Map map [ v1 . Endpoint Port ] address Set ) * v1 . Endpoint Address { // use address Key to distinguish between two endpoints that are identical addresses // but may have come from different hosts, for attribution. For instance, Mesos // assigns pods the node IP, but the pods are distinct. key := address if addr . Target Ref != nil { key . uid = addr . Target // Accumulate the address. The full Endpoint Address structure is preserved for use when // we rebuild the subsets so that the final Target Ref has all of the necessary data. existing Address := all if existing Address == nil { // Make a copy so we don't write to the // input args of this function. existing Address = & v1 . Endpoint * existing all Addrs [ key ] = existing // Remember that this port maps to this address. if _ , found := port To Addr Ready Map [ port ] ; ! found { port To Addr Ready Map [ port ] = address // if we have not yet recorded this port for this address, or if the previous // state was ready, write the current ready state. not ready always trumps // ready. if was Ready , found := port To Addr Ready Map [ port ] [ existing Address ] ; ! found || was Ready { port To Addr Ready Map [ port ] [ existing return existing } 
func Less Endpoint Address ( a , b * v1 . Endpoint Address ) bool { ip if ip Comparison != 0 { return ip if b . Target if a . Target return a . Target Ref . UID < b . Target } 
func Sort Subsets ( subsets [ ] v1 . Endpoint Subset ) [ ] v1 . Endpoint sort . Sort ( addrs By IP And sort . Sort ( addrs By IP And UID ( ss . Not Ready sort . Sort ( ports By sort . Sort ( subsets By } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * Cloud Controller Manager Configuration ) ( nil ) , ( * config . Cloud Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1alpha1_Cloud Controller Manager Configuration_To_config_Cloud Controller Manager Configuration ( a . ( * Cloud Controller Manager Configuration ) , b . ( * config . Cloud Controller Manager if err := s . Add Generated Conversion Func ( ( * config . Cloud Controller Manager Configuration ) ( nil ) , ( * Cloud Controller Manager Configuration ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_config_Cloud Controller Manager Configuration_To_v1alpha1_Cloud Controller Manager Configuration ( a . ( * config . Cloud Controller Manager Configuration ) , b . ( * Cloud Controller Manager } 
func Convert_v1alpha1_Cloud Controller Manager Configuration_To_config_Cloud Controller Manager Configuration ( in * Cloud Controller Manager Configuration , out * config . Cloud Controller Manager Configuration , s conversion . Scope ) error { return auto Convert_v1alpha1_Cloud Controller Manager Configuration_To_config_Cloud Controller Manager } 
func Convert_config_Cloud Controller Manager Configuration_To_v1alpha1_Cloud Controller Manager Configuration ( in * config . Cloud Controller Manager Configuration , out * Cloud Controller Manager Configuration , s conversion . Scope ) error { return auto Convert_config_Cloud Controller Manager Configuration_To_v1alpha1_Cloud Controller Manager } 
func transform Object ( ctx context . Context , obj runtime . Object , opts interface { } , media Type negotiation . Media Type Options , scope * Request if err := set Object Self switch target := media case target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group Version : return as V1Beta1Partial Object case target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group Version : return as V1Beta1Partial Object Metadata case target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group Version : options , ok := opts . ( * metav1beta1 . Table return as default : accepted , _ := negotiation . Media Types For err := negotiation . New Not Acceptable } 
func options For Transform ( media Type negotiation . Media Type Options , req * http . Request ) ( interface { } , error ) { switch target := media Type . Convert ; { case target == nil : case target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group Version : opts := & metav1beta1 . Table if err := metav1beta1 . Parameter Codec . Decode Parameters ( req . URL . Query ( ) , metav1beta1 . Scheme Group switch errs := validation . Validate Table case 1 : return nil , errors . New Bad default : return nil , errors . New Bad } 
func target Encoding For Transform ( scope * Request Scope , media Type negotiation . Media Type Options , req * http . Request ) ( schema . Group Version Kind , runtime . Negotiated Serializer , bool ) { switch target := media Type . Convert ; { case target == nil : case target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group Version , target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group Version , target . Kind == " " && target . Group Version ( ) == metav1beta1 . Scheme Group } 
func transform Response Object ( ctx context . Context , scope * Request Scope , trace * utiltrace . Trace , req * http . Request , w http . Response Writer , status Code int , media Type negotiation . Media Type Options , result runtime . Object ) { options , err := options For Transform ( media obj , err := transform Object ( ctx , result , options , media kind , serializer , _ := target Encoding For Transform ( scope , media responsewriters . Write Object Negotiated ( serializer , scope , kind . Group Version ( ) , w , req , status } 
func add SE Linux Options ( config [ ] string , selinux Opts * runtimeapi . SE Linux Option , separator rune ) [ ] string { // Note, strictly speaking, we are actually mutating the values of these // keys, rather than formatting name and value into a string. Docker re- // uses the same option name multiple times (it's just 'label') with // different values which are themselves key-value pairs. For example, // the SE Linux type is represented by the security opt: // // label<separator>type:<selinux_type> // // In Docker API versions before 1.23, the separator was the `:` rune; in // API version 1.23 it changed to the `=` rune. config = modify Security Option ( config , selinux Label User ( separator ) , selinux config = modify Security Option ( config , selinux Label Role ( separator ) , selinux config = modify Security Option ( config , selinux Label Type ( separator ) , selinux config = modify Security Option ( config , selinux Label Level ( separator ) , selinux } 
func modify Security } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . IP Block ) ( nil ) , ( * networking . IP Block ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_IP Block_To_networking_IP Block ( a . ( * v1 . IP Block ) , b . ( * networking . IP if err := s . Add Generated Conversion Func ( ( * networking . IP Block ) ( nil ) , ( * v1 . IP Block ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_IP Block_To_v1_IP Block ( a . ( * networking . IP Block ) , b . ( * v1 . IP if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy ) ( nil ) , ( * networking . Network Policy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy_To_networking_Network Policy ( a . ( * v1 . Network Policy ) , b . ( * networking . Network if err := s . Add Generated Conversion Func ( ( * networking . Network Policy ) ( nil ) , ( * v1 . Network Policy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy_To_v1_Network Policy ( a . ( * networking . Network Policy ) , b . ( * v1 . Network if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy Egress Rule ) ( nil ) , ( * networking . Network Policy Egress Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy Egress Rule_To_networking_Network Policy Egress Rule ( a . ( * v1 . Network Policy Egress Rule ) , b . ( * networking . Network Policy Egress if err := s . Add Generated Conversion Func ( ( * networking . Network Policy Egress Rule ) ( nil ) , ( * v1 . Network Policy Egress Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy Egress Rule_To_v1_Network Policy Egress Rule ( a . ( * networking . Network Policy Egress Rule ) , b . ( * v1 . Network Policy Egress if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy Ingress Rule ) ( nil ) , ( * networking . Network Policy Ingress Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy Ingress Rule_To_networking_Network Policy Ingress Rule ( a . ( * v1 . Network Policy Ingress Rule ) , b . ( * networking . Network Policy Ingress if err := s . Add Generated Conversion Func ( ( * networking . Network Policy Ingress Rule ) ( nil ) , ( * v1 . Network Policy Ingress Rule ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy Ingress Rule_To_v1_Network Policy Ingress Rule ( a . ( * networking . Network Policy Ingress Rule ) , b . ( * v1 . Network Policy Ingress if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy List ) ( nil ) , ( * networking . Network Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy List_To_networking_Network Policy List ( a . ( * v1 . Network Policy List ) , b . ( * networking . Network Policy if err := s . Add Generated Conversion Func ( ( * networking . Network Policy List ) ( nil ) , ( * v1 . Network Policy List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy List_To_v1_Network Policy List ( a . ( * networking . Network Policy List ) , b . ( * v1 . Network Policy if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy Peer ) ( nil ) , ( * networking . Network Policy Peer ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy Peer_To_networking_Network Policy Peer ( a . ( * v1 . Network Policy Peer ) , b . ( * networking . Network Policy if err := s . Add Generated Conversion Func ( ( * networking . Network Policy Peer ) ( nil ) , ( * v1 . Network Policy Peer ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy Peer_To_v1_Network Policy Peer ( a . ( * networking . Network Policy Peer ) , b . ( * v1 . Network Policy if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy Port ) ( nil ) , ( * networking . Network Policy Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy Port_To_networking_Network Policy Port ( a . ( * v1 . Network Policy Port ) , b . ( * networking . Network Policy if err := s . Add Generated Conversion Func ( ( * networking . Network Policy Port ) ( nil ) , ( * v1 . Network Policy Port ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy Port_To_v1_Network Policy Port ( a . ( * networking . Network Policy Port ) , b . ( * v1 . Network Policy if err := s . Add Generated Conversion Func ( ( * v1 . Network Policy Spec ) ( nil ) , ( * networking . Network Policy Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Network Policy Spec_To_networking_Network Policy Spec ( a . ( * v1 . Network Policy Spec ) , b . ( * networking . Network Policy if err := s . Add Generated Conversion Func ( ( * networking . Network Policy Spec ) ( nil ) , ( * v1 . Network Policy Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_networking_Network Policy Spec_To_v1_Network Policy Spec ( a . ( * networking . Network Policy Spec ) , b . ( * v1 . Network Policy } 
func Convert_v1_IP Block_To_networking_IP Block ( in * v1 . IP Block , out * networking . IP Block , s conversion . Scope ) error { return auto Convert_v1_IP Block_To_networking_IP } 
func Convert_networking_IP Block_To_v1_IP Block ( in * networking . IP Block , out * v1 . IP Block , s conversion . Scope ) error { return auto Convert_networking_IP Block_To_v1_IP } 
func Convert_v1_Network Policy_To_networking_Network Policy ( in * v1 . Network Policy , out * networking . Network Policy , s conversion . Scope ) error { return auto Convert_v1_Network Policy_To_networking_Network } 
func Convert_networking_Network Policy_To_v1_Network Policy ( in * networking . Network Policy , out * v1 . Network Policy , s conversion . Scope ) error { return auto Convert_networking_Network Policy_To_v1_Network } 
func Convert_v1_Network Policy Egress Rule_To_networking_Network Policy Egress Rule ( in * v1 . Network Policy Egress Rule , out * networking . Network Policy Egress Rule , s conversion . Scope ) error { return auto Convert_v1_Network Policy Egress Rule_To_networking_Network Policy Egress } 
func Convert_networking_Network Policy Egress Rule_To_v1_Network Policy Egress Rule ( in * networking . Network Policy Egress Rule , out * v1 . Network Policy Egress Rule , s conversion . Scope ) error { return auto Convert_networking_Network Policy Egress Rule_To_v1_Network Policy Egress } 
func Convert_v1_Network Policy Ingress Rule_To_networking_Network Policy Ingress Rule ( in * v1 . Network Policy Ingress Rule , out * networking . Network Policy Ingress Rule , s conversion . Scope ) error { return auto Convert_v1_Network Policy Ingress Rule_To_networking_Network Policy Ingress } 
func Convert_networking_Network Policy Ingress Rule_To_v1_Network Policy Ingress Rule ( in * networking . Network Policy Ingress Rule , out * v1 . Network Policy Ingress Rule , s conversion . Scope ) error { return auto Convert_networking_Network Policy Ingress Rule_To_v1_Network Policy Ingress } 
func Convert_v1_Network Policy List_To_networking_Network Policy List ( in * v1 . Network Policy List , out * networking . Network Policy List , s conversion . Scope ) error { return auto Convert_v1_Network Policy List_To_networking_Network Policy } 
func Convert_networking_Network Policy List_To_v1_Network Policy List ( in * networking . Network Policy List , out * v1 . Network Policy List , s conversion . Scope ) error { return auto Convert_networking_Network Policy List_To_v1_Network Policy } 
func Convert_v1_Network Policy Peer_To_networking_Network Policy Peer ( in * v1 . Network Policy Peer , out * networking . Network Policy Peer , s conversion . Scope ) error { return auto Convert_v1_Network Policy Peer_To_networking_Network Policy } 
func Convert_networking_Network Policy Peer_To_v1_Network Policy Peer ( in * networking . Network Policy Peer , out * v1 . Network Policy Peer , s conversion . Scope ) error { return auto Convert_networking_Network Policy Peer_To_v1_Network Policy } 
func Convert_v1_Network Policy Port_To_networking_Network Policy Port ( in * v1 . Network Policy Port , out * networking . Network Policy Port , s conversion . Scope ) error { return auto Convert_v1_Network Policy Port_To_networking_Network Policy } 
func Convert_networking_Network Policy Port_To_v1_Network Policy Port ( in * networking . Network Policy Port , out * v1 . Network Policy Port , s conversion . Scope ) error { return auto Convert_networking_Network Policy Port_To_v1_Network Policy } 
func Convert_v1_Network Policy Spec_To_networking_Network Policy Spec ( in * v1 . Network Policy Spec , out * networking . Network Policy Spec , s conversion . Scope ) error { return auto Convert_v1_Network Policy Spec_To_networking_Network Policy } 
func Convert_networking_Network Policy Spec_To_v1_Network Policy Spec ( in * networking . Network Policy Spec , out * v1 . Network Policy Spec , s conversion . Scope ) error { return auto Convert_networking_Network Policy Spec_To_v1_Network Policy } 
func ( strategy ) Prepare For Create ( ctx context . Context , obj runtime . Object ) { crd := obj . ( * apiextensions . Custom Resource crd . Status = apiextensions . Custom Resource Definition drop Disabled for _ , v := range crd . Spec . Versions { if v . Storage { if ! apiextensions . Is Stored Version ( crd , v . Name ) { crd . Status . Stored Versions = append ( crd . Status . Stored } 
func ( strategy ) Prepare For Update ( ctx context . Context , obj , old runtime . Object ) { new CRD := obj . ( * apiextensions . Custom Resource old CRD := old . ( * apiextensions . Custom Resource new CRD . Status = old // Any changes to the spec increment the generation number, any changes to the // status should reflect the generation number of the corresponding object. We push // the burden of managing the status onto the clients because we can't (in general) // know here what version of spec the writer of the status has seen. It may seem like // we can at first -- since obj contains spec -- but in the future we will probably make // status its own object, and even if we don't, writes may be the result of a // read-update-write loop, so the contents of spec may not actually be the spec that // the controller has *seen*. if ! apiequality . Semantic . Deep Equal ( old CRD . Spec , new CRD . Spec ) { new CRD . Generation = old drop Disabled Fields ( & new CRD . Spec , & old for _ , v := range new CRD . Spec . Versions { if v . Storage { if ! apiextensions . Is Stored Version ( new CRD , v . Name ) { new CRD . Status . Stored Versions = append ( new CRD . Status . Stored } 
func ( strategy ) Validate ( ctx context . Context , obj runtime . Object ) field . Error List { return validation . Validate Custom Resource Definition ( obj . ( * apiextensions . Custom Resource } 
func ( strategy ) Validate Update ( ctx context . Context , obj , old runtime . Object ) field . Error List { return validation . Validate Custom Resource Definition Update ( obj . ( * apiextensions . Custom Resource Definition ) , old . ( * apiextensions . Custom Resource } 
func Get Attrs ( obj runtime . Object ) ( labels . Set , fields . Set , error ) { apiserver , ok := obj . ( * apiextensions . Custom Resource return labels . Set ( apiserver . Object Meta . Labels ) , Custom Resource Definition To Selectable } 
func Custom Resource Definition To Selectable Fields ( obj * apiextensions . Custom Resource Definition ) fields . Set { return generic . Object Meta Fields Set ( & obj . Object } 
func has Per Version Field ( crd Spec * apiextensions . Custom Resource Definition Spec ) bool { if crd for _ , v := range crd Spec . Versions { if v . Schema != nil || v . Subresources != nil || len ( v . Additional Printer } 
func cadvisor Info To Container Stats ( name string , info * cadvisorapiv2 . Container Info , root Fs , image Fs * cadvisorapiv2 . Fs Info ) * statsapi . Container Stats { result := & statsapi . Container Stats { Start Time : metav1 . New Time ( info . Spec . Creation cstat , found := latest Container cpu , memory := cadvisor Info To CP Uand Memory if root Fs != nil { // The container logs live on the node rootfs device result . Logs = build Logs Stats ( cstat , root if image Fs != nil { // The container root Fs lives on the image Fs devices (which may not be the node root fs) result . Rootfs = build Rootfs Stats ( cstat , image if cfs != nil { if cfs . Base Usage Bytes != nil { if result . Rootfs != nil { rootfs Usage := * cfs . Base Usage result . Rootfs . Used Bytes = & rootfs if cfs . Total Usage Bytes != nil && result . Logs != nil { logs Usage := * cfs . Total Usage Bytes - * cfs . Base Usage result . Logs . Used Bytes = & logs if cfs . Inode Usage != nil && result . Rootfs != nil { root Inodes := * cfs . Inode result . Rootfs . Inodes Used = & root for _ , acc := range cstat . Accelerators { result . Accelerators = append ( result . Accelerators , statsapi . Accelerator Stats { Make : acc . Make , Model : acc . Model , ID : acc . ID , Memory Total : acc . Memory Total , Memory Used : acc . Memory Used , Duty Cycle : acc . Duty result . User Defined Metrics = cadvisor Info To User Defined } 
func cadvisor Info To Container CPU And Memory Stats ( name string , info * cadvisorapiv2 . Container Info ) * statsapi . Container Stats { result := & statsapi . Container Stats { Start Time : metav1 . New Time ( info . Spec . Creation cpu , memory := cadvisor Info To CP Uand Memory } 
func cadvisor Info To Network Stats ( name string , info * cadvisorapiv2 . Container Info ) * statsapi . Network Stats { if ! info . Spec . Has cstat , found := latest Container i Stats := statsapi . Network Stats { Time : metav1 . New i Stat := statsapi . Interface Stats { Name : inter . Name , Rx Bytes : & inter . Rx Bytes , Rx Errors : & inter . Rx Errors , Tx Bytes : & inter . Tx Bytes , Tx Errors : & inter . Tx if inter . Name == default Network Interface Name { i Stats . Interface Stats = i i Stats . Interfaces = append ( i Stats . Interfaces , i return & i } 
func cadvisor Info To User Defined Metrics ( info * cadvisorapiv2 . Container Info ) [ ] statsapi . User Defined Metric { type spec Val struct { ref statsapi . User Defined Metric val Type cadvisorapiv1 . Data udm Map := map [ string ] * spec for _ , spec := range info . Spec . Custom Metrics { udm Map [ spec . Name ] = & spec Val { ref : statsapi . User Defined Metric Descriptor { Name : spec . Name , Type : statsapi . User Defined Metric Type ( spec . Type ) , Units : spec . Units , } , val for _ , stat := range info . Stats { for name , values := range stat . Custom Metrics { spec Val , ok := udm if ! ok { klog . Warningf ( " " , name , info . Spec , stat . Custom for _ , value := range values { // Pick the most recent value if value . Timestamp . Before ( spec spec spec Val . value = value . Float if spec Val . val Type == cadvisorapiv1 . Int Type { spec Val . value = float64 ( value . Int var udm [ ] statsapi . User Defined for _ , spec Val := range udm Map { udm = append ( udm , statsapi . User Defined Metric { User Defined Metric Descriptor : spec Val . ref , Time : metav1 . New Time ( spec Val . time ) , Value : spec } 
func get Cgroup Info ( cadvisor cadvisor . Interface , container Name string , update Stats bool ) ( * cadvisorapiv2 . Container Info , error ) { var max if update max info Map , err := cadvisor . Container Info V2 ( container Name , cadvisorapiv2 . Request Options { Id Type : cadvisorapiv2 . Type Name , Count : 2 , // 2 samples are needed to compute "instantaneous" CPU Recursive : false , Max Age : max if err != nil { return nil , fmt . Errorf ( " " , container if len ( info Map ) != 1 { return nil , fmt . Errorf ( " " , len ( info info := info Map [ container } 
func get Cgroup Stats ( cadvisor cadvisor . Interface , container Name string , update Stats bool ) ( * cadvisorapiv2 . Container Stats , error ) { info , err := get Cgroup Info ( cadvisor , container Name , update stats , found := latest Container if ! found { return nil , fmt . Errorf ( " " , container } 
func Kubelet Configuration Path Refs ( kc * Kubelet paths = append ( paths , & kc . Static Pod paths = append ( paths , & kc . Authentication . X509 . Client CA paths = append ( paths , & kc . TLS Cert paths = append ( paths , & kc . TLS Private Key paths = append ( paths , & kc . Resolver } 
func Register Conversions ( s * runtime . Scheme ) error { if err := s . Add Generated Conversion Func ( ( * v1 . Controller Revision ) ( nil ) , ( * apps . Controller Revision ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Controller Revision_To_apps_Controller Revision ( a . ( * v1 . Controller Revision ) , b . ( * apps . Controller if err := s . Add Generated Conversion Func ( ( * apps . Controller Revision ) ( nil ) , ( * v1 . Controller Revision ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Controller Revision_To_v1_Controller Revision ( a . ( * apps . Controller Revision ) , b . ( * v1 . Controller if err := s . Add Generated Conversion Func ( ( * v1 . Controller Revision List ) ( nil ) , ( * apps . Controller Revision List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Controller Revision List_To_apps_Controller Revision List ( a . ( * v1 . Controller Revision List ) , b . ( * apps . Controller Revision if err := s . Add Generated Conversion Func ( ( * apps . Controller Revision List ) ( nil ) , ( * v1 . Controller Revision List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Controller Revision List_To_v1_Controller Revision List ( a . ( * apps . Controller Revision List ) , b . ( * v1 . Controller Revision if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Set ) ( nil ) , ( * apps . Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set_To_apps_Daemon Set ( a . ( * v1 . Daemon Set ) , b . ( * apps . Daemon if err := s . Add Generated Conversion Func ( ( * apps . Daemon Set ) ( nil ) , ( * v1 . Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set_To_v1_Daemon Set ( a . ( * apps . Daemon Set ) , b . ( * v1 . Daemon if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Set Condition ) ( nil ) , ( * apps . Daemon Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set Condition_To_apps_Daemon Set Condition ( a . ( * v1 . Daemon Set Condition ) , b . ( * apps . Daemon Set if err := s . Add Generated Conversion Func ( ( * apps . Daemon Set Condition ) ( nil ) , ( * v1 . Daemon Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set Condition_To_v1_Daemon Set Condition ( a . ( * apps . Daemon Set Condition ) , b . ( * v1 . Daemon Set if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Set List ) ( nil ) , ( * apps . Daemon Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set List_To_apps_Daemon Set List ( a . ( * v1 . Daemon Set List ) , b . ( * apps . Daemon Set if err := s . Add Generated Conversion Func ( ( * apps . Daemon Set List ) ( nil ) , ( * v1 . Daemon Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set List_To_v1_Daemon Set List ( a . ( * apps . Daemon Set List ) , b . ( * v1 . Daemon Set if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Set Spec ) ( nil ) , ( * apps . Daemon Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set Spec_To_apps_Daemon Set Spec ( a . ( * v1 . Daemon Set Spec ) , b . ( * apps . Daemon Set if err := s . Add Generated Conversion Func ( ( * apps . Daemon Set Spec ) ( nil ) , ( * v1 . Daemon Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set Spec_To_v1_Daemon Set Spec ( a . ( * apps . Daemon Set Spec ) , b . ( * v1 . Daemon Set if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Set Status ) ( nil ) , ( * apps . Daemon Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set Status_To_apps_Daemon Set Status ( a . ( * v1 . Daemon Set Status ) , b . ( * apps . Daemon Set if err := s . Add Generated Conversion Func ( ( * apps . Daemon Set Status ) ( nil ) , ( * v1 . Daemon Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set Status_To_v1_Daemon Set Status ( a . ( * apps . Daemon Set Status ) , b . ( * v1 . Daemon Set if err := s . Add Generated Conversion Func ( ( * v1 . Daemon Set Update Strategy ) ( nil ) , ( * apps . Daemon Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set Update Strategy_To_apps_Daemon Set Update Strategy ( a . ( * v1 . Daemon Set Update Strategy ) , b . ( * apps . Daemon Set Update if err := s . Add Generated Conversion Func ( ( * apps . Daemon Set Update Strategy ) ( nil ) , ( * v1 . Daemon Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set Update Strategy_To_v1_Daemon Set Update Strategy ( a . ( * apps . Daemon Set Update Strategy ) , b . ( * v1 . Daemon Set Update if err := s . Add Generated Conversion if err := s . Add Generated Conversion if err := s . Add Generated Conversion Func ( ( * v1 . Deployment Condition ) ( nil ) , ( * apps . Deployment Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment Condition_To_apps_Deployment Condition ( a . ( * v1 . Deployment Condition ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Condition ) ( nil ) , ( * v1 . Deployment Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Condition_To_v1_Deployment Condition ( a . ( * apps . Deployment Condition ) , b . ( * v1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1 . Deployment List ) ( nil ) , ( * apps . Deployment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment List_To_apps_Deployment List ( a . ( * v1 . Deployment List ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment List ) ( nil ) , ( * v1 . Deployment List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment List_To_v1_Deployment List ( a . ( * apps . Deployment List ) , b . ( * v1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1 . Deployment Spec ) ( nil ) , ( * apps . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment Spec_To_apps_Deployment Spec ( a . ( * v1 . Deployment Spec ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Spec ) ( nil ) , ( * v1 . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Spec_To_v1_Deployment Spec ( a . ( * apps . Deployment Spec ) , b . ( * v1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1 . Deployment Status ) ( nil ) , ( * apps . Deployment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment Status_To_apps_Deployment Status ( a . ( * v1 . Deployment Status ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Status ) ( nil ) , ( * v1 . Deployment Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Status_To_v1_Deployment Status ( a . ( * apps . Deployment Status ) , b . ( * v1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1 . Deployment Strategy ) ( nil ) , ( * apps . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment Strategy_To_apps_Deployment Strategy ( a . ( * v1 . Deployment Strategy ) , b . ( * apps . Deployment if err := s . Add Generated Conversion Func ( ( * apps . Deployment Strategy ) ( nil ) , ( * v1 . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Strategy_To_v1_Deployment Strategy ( a . ( * apps . Deployment Strategy ) , b . ( * v1 . Deployment if err := s . Add Generated Conversion Func ( ( * v1 . Replica Set ) ( nil ) , ( * apps . Replica Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replica Set_To_apps_Replica Set ( a . ( * v1 . Replica Set ) , b . ( * apps . Replica if err := s . Add Generated Conversion Func ( ( * apps . Replica Set ) ( nil ) , ( * v1 . Replica Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set_To_v1_Replica Set ( a . ( * apps . Replica Set ) , b . ( * v1 . Replica if err := s . Add Generated Conversion Func ( ( * v1 . Replica Set Condition ) ( nil ) , ( * apps . Replica Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replica Set Condition_To_apps_Replica Set Condition ( a . ( * v1 . Replica Set Condition ) , b . ( * apps . Replica Set if err := s . Add Generated Conversion Func ( ( * apps . Replica Set Condition ) ( nil ) , ( * v1 . Replica Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set Condition_To_v1_Replica Set Condition ( a . ( * apps . Replica Set Condition ) , b . ( * v1 . Replica Set if err := s . Add Generated Conversion Func ( ( * v1 . Replica Set List ) ( nil ) , ( * apps . Replica Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replica Set List_To_apps_Replica Set List ( a . ( * v1 . Replica Set List ) , b . ( * apps . Replica Set if err := s . Add Generated Conversion Func ( ( * apps . Replica Set List ) ( nil ) , ( * v1 . Replica Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set List_To_v1_Replica Set List ( a . ( * apps . Replica Set List ) , b . ( * v1 . Replica Set if err := s . Add Generated Conversion Func ( ( * v1 . Replica Set Spec ) ( nil ) , ( * apps . Replica Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replica Set Spec_To_apps_Replica Set Spec ( a . ( * v1 . Replica Set Spec ) , b . ( * apps . Replica Set if err := s . Add Generated Conversion Func ( ( * apps . Replica Set Spec ) ( nil ) , ( * v1 . Replica Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set Spec_To_v1_Replica Set Spec ( a . ( * apps . Replica Set Spec ) , b . ( * v1 . Replica Set if err := s . Add Generated Conversion Func ( ( * v1 . Replica Set Status ) ( nil ) , ( * apps . Replica Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replica Set Status_To_apps_Replica Set Status ( a . ( * v1 . Replica Set Status ) , b . ( * apps . Replica Set if err := s . Add Generated Conversion Func ( ( * apps . Replica Set Status ) ( nil ) , ( * v1 . Replica Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set Status_To_v1_Replica Set Status ( a . ( * apps . Replica Set Status ) , b . ( * v1 . Replica Set if err := s . Add Generated Conversion Func ( ( * v1 . Rolling Update Daemon Set ) ( nil ) , ( * apps . Rolling Update Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Rolling Update Daemon Set_To_apps_Rolling Update Daemon Set ( a . ( * v1 . Rolling Update Daemon Set ) , b . ( * apps . Rolling Update Daemon if err := s . Add Generated Conversion Func ( ( * apps . Rolling Update Daemon Set ) ( nil ) , ( * v1 . Rolling Update Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Daemon Set_To_v1_Rolling Update Daemon Set ( a . ( * apps . Rolling Update Daemon Set ) , b . ( * v1 . Rolling Update Daemon if err := s . Add Generated Conversion Func ( ( * v1 . Rolling Update Deployment ) ( nil ) , ( * apps . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Rolling Update Deployment_To_apps_Rolling Update Deployment ( a . ( * v1 . Rolling Update Deployment ) , b . ( * apps . Rolling Update if err := s . Add Generated Conversion Func ( ( * apps . Rolling Update Deployment ) ( nil ) , ( * v1 . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Deployment_To_v1_Rolling Update Deployment ( a . ( * apps . Rolling Update Deployment ) , b . ( * v1 . Rolling Update if err := s . Add Generated Conversion Func ( ( * v1 . Rolling Update Stateful Set Strategy ) ( nil ) , ( * apps . Rolling Update Stateful Set Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Rolling Update Stateful Set Strategy_To_apps_Rolling Update Stateful Set Strategy ( a . ( * v1 . Rolling Update Stateful Set Strategy ) , b . ( * apps . Rolling Update Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Rolling Update Stateful Set Strategy ) ( nil ) , ( * v1 . Rolling Update Stateful Set Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Stateful Set Strategy_To_v1_Rolling Update Stateful Set Strategy ( a . ( * apps . Rolling Update Stateful Set Strategy ) , b . ( * v1 . Rolling Update Stateful Set if err := s . Add Generated Conversion Func ( ( * v1 . Stateful Set ) ( nil ) , ( * apps . Stateful Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set_To_apps_Stateful Set ( a . ( * v1 . Stateful Set ) , b . ( * apps . Stateful if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set ) ( nil ) , ( * v1 . Stateful Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set_To_v1_Stateful Set ( a . ( * apps . Stateful Set ) , b . ( * v1 . Stateful if err := s . Add Generated Conversion Func ( ( * v1 . Stateful Set Condition ) ( nil ) , ( * apps . Stateful Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Condition_To_apps_Stateful Set Condition ( a . ( * v1 . Stateful Set Condition ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Condition ) ( nil ) , ( * v1 . Stateful Set Condition ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Condition_To_v1_Stateful Set Condition ( a . ( * apps . Stateful Set Condition ) , b . ( * v1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1 . Stateful Set List ) ( nil ) , ( * apps . Stateful Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set List_To_apps_Stateful Set List ( a . ( * v1 . Stateful Set List ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set List ) ( nil ) , ( * v1 . Stateful Set List ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set List_To_v1_Stateful Set List ( a . ( * apps . Stateful Set List ) , b . ( * v1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1 . Stateful Set Spec ) ( nil ) , ( * apps . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Spec_To_apps_Stateful Set Spec ( a . ( * v1 . Stateful Set Spec ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Spec ) ( nil ) , ( * v1 . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Spec_To_v1_Stateful Set Spec ( a . ( * apps . Stateful Set Spec ) , b . ( * v1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1 . Stateful Set Status ) ( nil ) , ( * apps . Stateful Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Status_To_apps_Stateful Set Status ( a . ( * v1 . Stateful Set Status ) , b . ( * apps . Stateful Set if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Status ) ( nil ) , ( * v1 . Stateful Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Status_To_v1_Stateful Set Status ( a . ( * apps . Stateful Set Status ) , b . ( * v1 . Stateful Set if err := s . Add Generated Conversion Func ( ( * v1 . Stateful Set Update Strategy ) ( nil ) , ( * apps . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Update Strategy_To_apps_Stateful Set Update Strategy ( a . ( * v1 . Stateful Set Update Strategy ) , b . ( * apps . Stateful Set Update if err := s . Add Generated Conversion Func ( ( * apps . Stateful Set Update Strategy ) ( nil ) , ( * v1 . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Update Strategy_To_v1_Stateful Set Update Strategy ( a . ( * apps . Stateful Set Update Strategy ) , b . ( * v1 . Stateful Set Update if err := s . Add Conversion Func ( ( * apps . Daemon Set Spec ) ( nil ) , ( * v1 . Daemon Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set Spec_To_v1_Daemon Set Spec ( a . ( * apps . Daemon Set Spec ) , b . ( * v1 . Daemon Set if err := s . Add Conversion Func ( ( * apps . Daemon Set Update Strategy ) ( nil ) , ( * v1 . Daemon Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set Update Strategy_To_v1_Daemon Set Update Strategy ( a . ( * apps . Daemon Set Update Strategy ) , b . ( * v1 . Daemon Set Update if err := s . Add Conversion Func ( ( * apps . Daemon Set ) ( nil ) , ( * v1 . Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Daemon Set_To_v1_Daemon Set ( a . ( * apps . Daemon Set ) , b . ( * v1 . Daemon if err := s . Add Conversion Func ( ( * apps . Deployment Spec ) ( nil ) , ( * v1 . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Spec_To_v1_Deployment Spec ( a . ( * apps . Deployment Spec ) , b . ( * v1 . Deployment if err := s . Add Conversion Func ( ( * apps . Deployment Strategy ) ( nil ) , ( * v1 . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Deployment Strategy_To_v1_Deployment Strategy ( a . ( * apps . Deployment Strategy ) , b . ( * v1 . Deployment if err := s . Add Conversion if err := s . Add Conversion Func ( ( * apps . Replica Set Spec ) ( nil ) , ( * v1 . Replica Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Replica Set Spec_To_v1_Replica Set Spec ( a . ( * apps . Replica Set Spec ) , b . ( * v1 . Replica Set if err := s . Add Conversion Func ( ( * apps . Rolling Update Daemon Set ) ( nil ) , ( * v1 . Rolling Update Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Daemon Set_To_v1_Rolling Update Daemon Set ( a . ( * apps . Rolling Update Daemon Set ) , b . ( * v1 . Rolling Update Daemon if err := s . Add Conversion Func ( ( * apps . Rolling Update Deployment ) ( nil ) , ( * v1 . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Rolling Update Deployment_To_v1_Rolling Update Deployment ( a . ( * apps . Rolling Update Deployment ) , b . ( * v1 . Rolling Update if err := s . Add Conversion Func ( ( * apps . Stateful Set Spec ) ( nil ) , ( * v1 . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Spec_To_v1_Stateful Set Spec ( a . ( * apps . Stateful Set Spec ) , b . ( * v1 . Stateful Set if err := s . Add Conversion Func ( ( * apps . Stateful Set Status ) ( nil ) , ( * v1 . Stateful Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Status_To_v1_Stateful Set Status ( a . ( * apps . Stateful Set Status ) , b . ( * v1 . Stateful Set if err := s . Add Conversion Func ( ( * apps . Stateful Set Update Strategy ) ( nil ) , ( * v1 . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_apps_Stateful Set Update Strategy_To_v1_Stateful Set Update Strategy ( a . ( * apps . Stateful Set Update Strategy ) , b . ( * v1 . Stateful Set Update if err := s . Add Conversion Func ( ( * v1 . Daemon Set Spec ) ( nil ) , ( * apps . Daemon Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set Spec_To_apps_Daemon Set Spec ( a . ( * v1 . Daemon Set Spec ) , b . ( * apps . Daemon Set if err := s . Add Conversion Func ( ( * v1 . Daemon Set Update Strategy ) ( nil ) , ( * apps . Daemon Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set Update Strategy_To_apps_Daemon Set Update Strategy ( a . ( * v1 . Daemon Set Update Strategy ) , b . ( * apps . Daemon Set Update if err := s . Add Conversion Func ( ( * v1 . Daemon Set ) ( nil ) , ( * apps . Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Daemon Set_To_apps_Daemon Set ( a . ( * v1 . Daemon Set ) , b . ( * apps . Daemon if err := s . Add Conversion Func ( ( * v1 . Deployment Spec ) ( nil ) , ( * apps . Deployment Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment Spec_To_apps_Deployment Spec ( a . ( * v1 . Deployment Spec ) , b . ( * apps . Deployment if err := s . Add Conversion Func ( ( * v1 . Deployment Strategy ) ( nil ) , ( * apps . Deployment Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Deployment Strategy_To_apps_Deployment Strategy ( a . ( * v1 . Deployment Strategy ) , b . ( * apps . Deployment if err := s . Add Conversion if err := s . Add Conversion Func ( ( * v1 . Replica Set Spec ) ( nil ) , ( * apps . Replica Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Replica Set Spec_To_apps_Replica Set Spec ( a . ( * v1 . Replica Set Spec ) , b . ( * apps . Replica Set if err := s . Add Conversion Func ( ( * v1 . Rolling Update Daemon Set ) ( nil ) , ( * apps . Rolling Update Daemon Set ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Rolling Update Daemon Set_To_apps_Rolling Update Daemon Set ( a . ( * v1 . Rolling Update Daemon Set ) , b . ( * apps . Rolling Update Daemon if err := s . Add Conversion Func ( ( * v1 . Rolling Update Deployment ) ( nil ) , ( * apps . Rolling Update Deployment ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Rolling Update Deployment_To_apps_Rolling Update Deployment ( a . ( * v1 . Rolling Update Deployment ) , b . ( * apps . Rolling Update if err := s . Add Conversion Func ( ( * v1 . Stateful Set Spec ) ( nil ) , ( * apps . Stateful Set Spec ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Spec_To_apps_Stateful Set Spec ( a . ( * v1 . Stateful Set Spec ) , b . ( * apps . Stateful Set if err := s . Add Conversion Func ( ( * v1 . Stateful Set Status ) ( nil ) , ( * apps . Stateful Set Status ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Status_To_apps_Stateful Set Status ( a . ( * v1 . Stateful Set Status ) , b . ( * apps . Stateful Set if err := s . Add Conversion Func ( ( * v1 . Stateful Set Update Strategy ) ( nil ) , ( * apps . Stateful Set Update Strategy ) ( nil ) , func ( a , b interface { } , scope conversion . Scope ) error { return Convert_v1_Stateful Set Update Strategy_To_apps_Stateful Set Update Strategy ( a . ( * v1 . Stateful Set Update Strategy ) , b . ( * apps . Stateful Set Update } 
func Convert_v1_Controller Revision_To_apps_Controller Revision ( in * v1 . Controller Revision , out * apps . Controller Revision , s conversion . Scope ) error { return auto Convert_v1_Controller Revision_To_apps_Controller } 
func Convert_apps_Controller Revision_To_v1_Controller Revision ( in * apps . Controller Revision , out * v1 . Controller Revision , s conversion . Scope ) error { return auto Convert_apps_Controller Revision_To_v1_Controller } 
func Convert_v1_Controller Revision List_To_apps_Controller Revision List ( in * v1 . Controller Revision List , out * apps . Controller Revision List , s conversion . Scope ) error { return auto Convert_v1_Controller Revision List_To_apps_Controller Revision } 
func Convert_apps_Controller Revision List_To_v1_Controller Revision List ( in * apps . Controller Revision List , out * v1 . Controller Revision List , s conversion . Scope ) error { return auto Convert_apps_Controller Revision List_To_v1_Controller Revision } 
func Convert_v1_Daemon Set Condition_To_apps_Daemon Set Condition ( in * v1 . Daemon Set Condition , out * apps . Daemon Set Condition , s conversion . Scope ) error { return auto Convert_v1_Daemon Set Condition_To_apps_Daemon Set } 
func Convert_apps_Daemon Set Condition_To_v1_Daemon Set Condition ( in * apps . Daemon Set Condition , out * v1 . Daemon Set Condition , s conversion . Scope ) error { return auto Convert_apps_Daemon Set Condition_To_v1_Daemon Set } 
func Convert_v1_Daemon Set List_To_apps_Daemon Set List ( in * v1 . Daemon Set List , out * apps . Daemon Set List , s conversion . Scope ) error { return auto Convert_v1_Daemon Set List_To_apps_Daemon Set } 
func Convert_apps_Daemon Set List_To_v1_Daemon Set List ( in * apps . Daemon Set List , out * v1 . Daemon Set List , s conversion . Scope ) error { return auto Convert_apps_Daemon Set List_To_v1_Daemon Set } 
func Convert_v1_Daemon Set Status_To_apps_Daemon Set Status ( in * v1 . Daemon Set Status , out * apps . Daemon Set Status , s conversion . Scope ) error { return auto Convert_v1_Daemon Set Status_To_apps_Daemon Set } 
func Convert_apps_Daemon Set Status_To_v1_Daemon Set Status ( in * apps . Daemon Set Status , out * v1 . Daemon Set Status , s conversion . Scope ) error { return auto Convert_apps_Daemon Set Status_To_v1_Daemon Set } 
func Convert_v1_Deployment Condition_To_apps_Deployment Condition ( in * v1 . Deployment Condition , out * apps . Deployment Condition , s conversion . Scope ) error { return auto Convert_v1_Deployment Condition_To_apps_Deployment } 
func Convert_apps_Deployment Condition_To_v1_Deployment Condition ( in * apps . Deployment Condition , out * v1 . Deployment Condition , s conversion . Scope ) error { return auto Convert_apps_Deployment Condition_To_v1_Deployment } 
func Convert_v1_Deployment List_To_apps_Deployment List ( in * v1 . Deployment List , out * apps . Deployment List , s conversion . Scope ) error { return auto Convert_v1_Deployment List_To_apps_Deployment } 
func Convert_apps_Deployment List_To_v1_Deployment List ( in * apps . Deployment List , out * v1 . Deployment List , s conversion . Scope ) error { return auto Convert_apps_Deployment List_To_v1_Deployment } 
func Convert_v1_Deployment Status_To_apps_Deployment Status ( in * v1 . Deployment Status , out * apps . Deployment Status , s conversion . Scope ) error { return auto Convert_v1_Deployment Status_To_apps_Deployment } 
func Convert_apps_Deployment Status_To_v1_Deployment Status ( in * apps . Deployment Status , out * v1 . Deployment Status , s conversion . Scope ) error { return auto Convert_apps_Deployment Status_To_v1_Deployment } 
func Convert_v1_Replica Set_To_apps_Replica Set ( in * v1 . Replica Set , out * apps . Replica Set , s conversion . Scope ) error { return auto Convert_v1_Replica Set_To_apps_Replica } 
func Convert_apps_Replica Set_To_v1_Replica Set ( in * apps . Replica Set , out * v1 . Replica Set , s conversion . Scope ) error { return auto Convert_apps_Replica Set_To_v1_Replica } 
func Convert_v1_Replica Set Condition_To_apps_Replica Set Condition ( in * v1 . Replica Set Condition , out * apps . Replica Set Condition , s conversion . Scope ) error { return auto Convert_v1_Replica Set Condition_To_apps_Replica Set } 
func Convert_apps_Replica Set Condition_To_v1_Replica Set Condition ( in * apps . Replica Set Condition , out * v1 . Replica Set Condition , s conversion . Scope ) error { return auto Convert_apps_Replica Set Condition_To_v1_Replica Set } 
func Convert_v1_Replica Set List_To_apps_Replica Set List ( in * v1 . Replica Set List , out * apps . Replica Set List , s conversion . Scope ) error { return auto Convert_v1_Replica Set List_To_apps_Replica Set } 
func Convert_apps_Replica Set List_To_v1_Replica Set List ( in * apps . Replica Set List , out * v1 . Replica Set List , s conversion . Scope ) error { return auto Convert_apps_Replica Set List_To_v1_Replica Set } 
func Convert_v1_Replica Set Status_To_apps_Replica Set Status ( in * v1 . Replica Set Status , out * apps . Replica Set Status , s conversion . Scope ) error { return auto Convert_v1_Replica Set Status_To_apps_Replica Set } 
func Convert_apps_Replica Set Status_To_v1_Replica Set Status ( in * apps . Replica Set Status , out * v1 . Replica Set Status , s conversion . Scope ) error { return auto Convert_apps_Replica Set Status_To_v1_Replica Set } 
func Convert_v1_Rolling Update Stateful Set Strategy_To_apps_Rolling Update Stateful Set Strategy ( in * v1 . Rolling Update Stateful Set Strategy , out * apps . Rolling Update Stateful Set Strategy , s conversion . Scope ) error { return auto Convert_v1_Rolling Update Stateful Set Strategy_To_apps_Rolling Update Stateful Set } 
func Convert_apps_Rolling Update Stateful Set Strategy_To_v1_Rolling Update Stateful Set Strategy ( in * apps . Rolling Update Stateful Set Strategy , out * v1 . Rolling Update Stateful Set Strategy , s conversion . Scope ) error { return auto Convert_apps_Rolling Update Stateful Set Strategy_To_v1_Rolling Update Stateful Set } 
func Convert_v1_Stateful Set_To_apps_Stateful Set ( in * v1 . Stateful Set , out * apps . Stateful Set , s conversion . Scope ) error { return auto Convert_v1_Stateful Set_To_apps_Stateful } 
func Convert_apps_Stateful Set_To_v1_Stateful Set ( in * apps . Stateful Set , out * v1 . Stateful Set , s conversion . Scope ) error { return auto Convert_apps_Stateful Set_To_v1_Stateful } 
func Convert_v1_Stateful Set Condition_To_apps_Stateful Set Condition ( in * v1 . Stateful Set Condition , out * apps . Stateful Set Condition , s conversion . Scope ) error { return auto Convert_v1_Stateful Set Condition_To_apps_Stateful Set } 
func Convert_apps_Stateful Set Condition_To_v1_Stateful Set Condition ( in * apps . Stateful Set Condition , out * v1 . Stateful Set Condition , s conversion . Scope ) error { return auto Convert_apps_Stateful Set Condition_To_v1_Stateful Set } 
func Convert_v1_Stateful Set List_To_apps_Stateful Set List ( in * v1 . Stateful Set List , out * apps . Stateful Set List , s conversion . Scope ) error { return auto Convert_v1_Stateful Set List_To_apps_Stateful Set } 
func Convert_apps_Stateful Set List_To_v1_Stateful Set List ( in * apps . Stateful Set List , out * v1 . Stateful Set List , s conversion . Scope ) error { return auto Convert_apps_Stateful Set List_To_v1_Stateful Set } 
func Short Human } 
func Human } 
func New ( authenticator authenticator . Token , cache Errs bool , success TTL , failure TTL time . Duration ) authenticator . Token { return new With Clock ( authenticator , cache Errs , success TTL , failure TTL , utilclock . Real } 
func ( a * cached Token Authenticator ) Authenticate Token ( ctx context . Context , token string ) ( * authenticator . Response , bool , error ) { auds , _ := authenticator . Audiences key := key resp , ok , err := a . authenticator . Authenticate if ! a . cache switch { case ok && a . success TTL > 0 : a . cache . set ( key , & cache Record { resp : resp , ok : ok , err : err } , a . success case ! ok && a . failure TTL > 0 : a . cache . set ( key , & cache Record { resp : resp , ok : ok , err : err } , a . failure } 
func ( mp * Multipoint Example ) Reserve ( pc * framework . Plugin Context , pod * v1 . Pod , node Name string ) * framework . Status { mp . num } 
func ( mp * Multipoint Example ) Prebind ( pc * framework . Plugin Context , pod * v1 . Pod , node Name string ) * framework . Status { mp . num if pod == nil { return framework . New } 
func New ( config * runtime . Unknown , _ framework . Framework mp := Multipoint Example { mp } 
